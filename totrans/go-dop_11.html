<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer028">
			<h1 id="_idParaDest-222"><a id="_idTextAnchor461"/>Chapter <a id="_idTextAnchor462"/>9: Observability with OpenTelemetry</h1>
			<p>In the early hours of the morning as you are sleeping in bed, your phone starts to ring. It's not the normal ring that you've set for friends and family but the red-alert ring you set for emergencies. As you are startled awake by the noise, you begin to come to your senses. You think of the recent release of your company's application. A sense of dread fills you as you pick up the call to be greeted by the automated voice on the other end, informing you that you've been requested to join a priority video conference with a team debugging a live site problem with the new release. You get out of bed quickly and join the call.</p>
			<p>Once you are on the call, you are greeted by the on-call triage team. The triage team informs you that the application is experiencing a service outage affecting one of your largest customers, which represents a substantial portion of your company's revenue. This outage has been escalated by the customer to the highest levels of your company. Even your CEO is aware of the outage. The triage team is unable to determine the cause of the downtime and has called you in to help mitigate the issue and determine the root cause of the outage.</p>
			<p>You go to work to determine the root cause. You open your administrative dashboard for the application but find no information about the application. There are no logs, no traces, and no metrics. The application is not emitting telemetry to help you to debug the outage. You are effectively blind to the runtime behavior of the application and what is causing the outage. A feeling of overwhelming terror fills you as you fear this could be the end of your company if you are unable to determine what is causing the outage.</p>
			<p>Right about then is when I wake up. What I've just described is a reoccurring nightmare I have about waking up to an outage and not having the information I need to determine the runtime state of my application. </p>
			<p>Without being able to introspect the runtime state of your application, you are effectively blind to what may be causing abnormal behaviors in the application. You are unable to diagnose and quickly mitigate issues. It is a profoundly helpless and terrifying position to be in during an outage.</p>
			<p>Observability is the ability to measure the internal state of an application by measuring outputs from that application and infrastructure. We will focus on three outputs from an application: logs, traces, and metrics. In this chapter, you will learn how to instrument, generate, collect, and export telemetry data so that you will never find yourself in a situation where you do not have insight into the runtime behavior of your application. We will use OpenTelemetry SDKs to instrument a Go client and server so that the application will emit telemetry to the OpenTelemetry Collector service. The OpenTelemetry Collector service will transform and export that telemetry data to backend systems to enable visualization, analysis, and alerting.</p>
			<p>We will cover the following topics in this chapter:</p>
			<ul>
				<li>An introduction to OpenTelemetry</li>
				<li>Logging with context</li>
				<li>Instrumenting for distributed tracing</li>
				<li>Instrumenting for metrics</li>
				<li>Alerting on metrics abnormalities</li>
			</ul>
			<h1 id="_idParaDest-223"><a id="_idTextAnchor463"/>Technical requirements</h1>
			<p>This chapter will require Docker and Docker Compose.</p>
			<p>Let's get started by learning about OpenTelemetry, its components, and how OpenTelemetry can enable a vendor-agnostic approach to observability. The code used in this chapter is derived from <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/examples/demo">https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/examples/demo</a> with some changes made to provide additional clarity.</p>
			<p>The code files for this chapter can be downloaded from <a href="https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/9">https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/9</a></p>
			<h1 id="_idParaDest-224"><a id="_idTextAnchor464"/>An introduction to OpenTelemetry</h1>
			<p>OpenTelemetry began as a project<a id="_idIndexMarker830"/> to merge the OpenTracing and OpenCensus projects to create a single project to achieve their shared mission of high-quality telemetry for all. OpenTelemetry is a vendor-agnostic set of specifications, APIs, SDKs, and tooling designed for the creation and management of telemetry data. OpenTelemetry empowers projects to collect, transform, and export telemetry data such as logs, traces, and metrics<a id="_idIndexMarker831"/> to the backend systems of choice.</p>
			<p>OpenTelemetry features<a id="_idIndexMarker832"/> the following: </p>
			<ul>
				<li>Instrumentation libraries for the most popular programming languages with both automatic and manual instrumentation</li>
				<li>A single collector binary that can be deployed in a variety of ways</li>
				<li>Pipelines for collecting, transforming, and exporting telemetry data</li>
				<li>A set of open standards to protect against vendor lock-in</li>
			</ul>
			<p>In this section, we will learn about the OpenTelemetry stack and the components we can use to make our complex systems observabl<a id="_idTextAnchor465"/>e.</p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor466"/>Reference architecture for OpenTelemetry</h2>
			<p>Next, let's take a look at a conceptual reference architecture<a id="_idIndexMarker833"/> diagram for <strong class="bold">OpenTelemetry</strong> (<strong class="bold">OTel</strong>):</p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="Images/B17626_09_001.jpg" alt="Figure 9.1 – OpenTelemetry reference architecture &#13;&#10;" width="1351" height="771"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – OpenTelemetry reference architecture </p>
			<p>The preceding reference architecture diagram shows two applications instrumented with the OTel libraries running<a id="_idIndexMarker834"/> on hosts, with the OTel Collector deployed as an agent on the hosts. The OTel Collector agents are collecting traces and metrics from the applications as well as logs from the host. The OTel Collector on the left host is exporting telemetry to Backend 1 and Backend 2. On the right side, the OTel Collector agent is receiving telemetry from the OTel instrumented application, collecting telemetry from the host, and then forwarding the telemetry to an OTel Collector running as a service. The OTel Collector running as a service is exporting telemetry to Backend 1 and Backend 2. This reference architecture illustrates how the OTel Collector can be deployed as both an agent on a host and a service for collecting, transforming, and exporting telemetry data. </p>
			<p>The wire protocol the telemetry is being transmitted on is intentionally missing from the reference architecture diagram, since the OTel Collector is capable of accepting multiple telemetry input formats. For existing applications, accepting existing formats such as Prometheus, Jaeger, and Fluent Bit can make it easier to migrate to OpenTelemetry. For new applications, the OpenTelemetry wire protocol is preferred and simplifies collector configuration for ingesting telemetry da<a id="_idTextAnchor467"/>ta.</p>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor468"/>OpenTelemetry components</h2>
			<p>OpenTelemetry is composed of several components that form the telemetry sta<a id="_idTextAnchor469"/>ck.</p>
			<h3>OpenTelemetry specification</h3>
			<p>The OpenTelemetry specification describes the expectations and requirements for cross-language implementations using the following terms:</p>
			<ul>
				<li><strong class="bold">API</strong>: Defines the data types <a id="_idIndexMarker835"/>and operations for generating and correlating tracing, metrics, and logging.</li>
				<li><strong class="bold">SDK</strong>: Defines the implementation<a id="_idIndexMarker836"/> of the API in a specific languages. This includes configuration, processing, and exporting.</li>
				<li><strong class="bold">Data</strong>: Defines the <strong class="bold">OpenTelemetry Line Protocol</strong> (<strong class="bold">OTLP</strong>), a vendor-agnostic<a id="_idIndexMarker837"/> protocol<a id="_idIndexMarker838"/> for communicating telemetry.</li>
			</ul>
			<p>For more information<a id="_idIndexMarker839"/> about the specification, see <a href="https://opentelemetry.io/docs/reference/specification/">https://opentelemetry.io/docs/reference/specification/</a>.</p>
			<h3>OpenTelemetry Collector</h3>
			<p>The OTel Collector is a vendor-agnostic proxy<a id="_idIndexMarker840"/> that can receive telemetry data in multiple formats, transform and process it, and export it in multiple formats to be consumed by multiple backends (such as Jaeger, Prometheus, other open source backends, and many proprietary backends). The OTel Collector is composed of the following:</p>
			<ul>
				<li><strong class="bold">Receivers</strong>: Push- or pull-based processors <a id="_idIndexMarker841"/>for collecting data</li>
				<li><strong class="bold">Processors</strong>: Responsible for transforming<a id="_idIndexMarker842"/> and filtering data</li>
				<li><strong class="bold">Exporters</strong>: Push- or pull-based processors<a id="_idIndexMarker843"/> for exporting data</li>
			</ul>
			<p>Each of the preceding components is enabled through pipelines described in YAML configurations. To learn<a id="_idIndexMarker844"/> more about data collection, see <a href="https://opentelemetry.io/docs/concepts/data-collection/">https://opentelemetry.io/docs/concepts/data-collection/</a>.</p>
			<h3>Language SDKs and automatic instrumentation</h3>
			<p>Each supported language<a id="_idIndexMarker845"/> in OpenTelemetry offers an SDK that enables<a id="_idIndexMarker846"/> application developers to instrument their applications to emit telemetry data. The SDKs also offer some common components that aid in instrumenting applications. For example, in the Go SDK, there are wrappers for HTTP handlers that will provide instrumentation out of the box. Additionally, some language implementations also offer automatic instrumentation that can take advantage of language-specific features to collect telemetry data, without the need of manually instrumenting application code.</p>
			<p>For more information about instrumenting<a id="_idIndexMarker847"/> applications, see <a href="https://opentelemetry.io/docs/concepts/instrumenting-library/">https://opentelemetry.io/docs/concepts/instrumenting-library/</a>.</p>
			<h3>The correlation of telemetry</h3>
			<p>The correlation of telemetry<a id="_idIndexMarker848"/> is a killer feature for any telemetry stack. The correlation of telemetry<a id="_idIndexMarker849"/> data enables us to determine what events are related to each other across application boundaries and is the key to building insights into complex systems. For example, imagine we have a system composed of multiple interdependent micro-services. Each of these services could be running on multiple different hosts and possibly authored using different languages. We need to be able to correlate a given HTTP request and all subsequent requests across our multiple services. This is what correlation in OpenTelemetry enables. We can rely on OpenTelemetry to establish a correlation ID across these disparate services and provide a holistic view of events taking place within a complex system:</p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="Images/B17626_09_002.jpg" alt="Figure 9.2 – Correlated telemetry &#13;&#10;" width="1411" height="1339"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – Correlated telemetry </p>
			<p>In this section, we have introduced the main concepts in the OpenTelemetry stack. In the next sections, we will learn more about logging, tracing, and metrics and how we can use OpenTelemetry to create an o<a id="_idTextAnchor470"/>bservable system.</p>
			<h1 id="_idParaDest-227"><a id="_idTextAnchor471"/>Logging with context</h1>
			<p>Logging is probably the most familiar form of telemetry. You probably started logging in the first program<a id="_idIndexMarker850"/> you ever authored when you printed <strong class="source-inline">Hello World!</strong> to <strong class="source-inline">STDOUT</strong>. Logging is the most natural first step in providing some data about the internal state of an application to an observer. Think about how many times you have added a print statement to your application to determine the value of a variable. You were logging.</p>
			<p>Printing simple log statements such as <strong class="source-inline">Hello World!</strong> can be helpful for beginners, but it does not provide the critical data we require to operate complex systems. Logs can<a id="_idIndexMarker851"/> be powerful sources of telemetry data when they are enriched with data to provide context for the events they are describing. For example, if our log statements include a correlation ID in the log entry, we can use that data to associate the log entry with other observability data.</p>
			<p>Application or system logs often consist of timestamped text records. These records come in a variety of structures, ranging from completely unstructured text to highly structured schemas with attached metadata. Logs are output in a variety of ways – single files, rotated files, or even to <strong class="source-inline">STDOUT</strong>. We need to be able to gather logs from multiple sources, transform and extract log data in a consumable format, and then export that transformed data for consumption/indexing.</p>
			<p>In this section, we will discuss how to improve our logging, moving from plain text to structured log formats, and how to consume and export various log formats using OpenTelemetry. We will learn using Go, but the concepts presented are applicab<a id="_idTextAnchor472"/>le to any language.</p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor473"/>Our first log statement</h2>
			<p>Let's start by using the standard<a id="_idIndexMarker852"/> Go log and write <strong class="source-inline">Hello World!</strong>:</p>
			<p class="source-code">package main</p>
			<p class="source-code">import "log"</p>
			<p class="source-code">func main() {</p>
			<p class="source-code">     log.Println("Hello World!")</p>
			<p class="source-code">}</p>
			<p class="source-code">// Outputs: 2009/11/10 23:00:00 Hello World!</p>
			<p>The preceding <strong class="source-inline">Println</strong> statement outputs <strong class="source-inline">2009/11/10 23:00:00 Hello World!</strong> when run in <a href="https://go.dev/play/p/XH5JstbL7Ul">https://go.dev/play/p/XH5JstbL7Ul</a>. Observe the plain text structure of the output and think<a id="_idIndexMarker853"/> about what you would need to do to parse the text to extract a structured output. It would be a relatively simple regular expression to parse, but with the addition of new data, the parse structure would change, breaking the parser. Additionally, there is very little context regarding the event or the context in which this event occurred.</p>
			<p>The Go standard library logger has several other functions available, but we will not dive deeply into them here. If you are interested in learning more, I suggest you read <a href="https://pkg.go.dev/log">https://pkg.go.dev/log</a>. For the rest of this section, we will focus on structured and leveled loggers as well as the API described by <a href="https://github.com/go-logr/logr">https://git<span id="_idTextAnchor474"/>hub.com/go-logr/logr</a>.</p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor475"/>Structured and leveled logs with Zap</h2>
			<p>Structured loggers have several benefits over text loggers. Structured logs have a defined schema of keys and values<a id="_idIndexMarker854"/> that can be more easily parsed than plain text. You can take advantage<a id="_idIndexMarker855"/> of the keys and values to embed rich information<a id="_idIndexMarker856"/> such as a correlation ID or other useful<a id="_idIndexMarker857"/> contextual information. Additionally, you can filter out keys that might not be applicable given the log context.</p>
			<p>V-levels are an easy way to control the amount of information in a log. For example, an application may output extremely verbose debug logs at the -1 log level but only critical errors at a log level of 4.</p>
			<p>There has been a movement in the Go community to standardize the structured and leveled log interface via <a href="https://github.com/go-logr/logr">https://github.com/go-logr/logr</a>. There are many libraries that implement the API described in the <strong class="source-inline">logr</strong> project. For our purposes, we'll focus on a single structured logging library, Zap, which also has a <strong class="source-inline">logr</strong> API implementation (<a href="https://github.com/go-logr/zapr">https://github.com/go-logr/zapr</a>).</p>
			<p>Let's take<a id="_idIndexMarker858"/> a look<a id="_idIndexMarker859"/> at the key functions<a id="_idIndexMarker860"/> in the Zap logger<a id="_idIndexMarker861"/> interface:</p>
			<p class="source-code">// Debug will log a Debug level event</p>
			<p class="source-code">func (log *Logger) Debug(msg string, fields ...Field)</p>
			<p class="source-code">// Info will log an Info level event</p>
			<p class="source-code">func (log *Logger) Info(msg string, fields ...Field)</p>
			<p class="source-code">// Error will log an Error level event</p>
			<p class="source-code">func (log *Logger) Error(msg string, fields ...Field)</p>
			<p class="source-code">// With will return a logger that will log the keys and values specified for future log events</p>
			<p class="source-code">func (log *Logger) With(fields ...Field) *Logger</p>
			<p class="source-code">// Named will return a logger with a given name</p>
			<p class="source-code">func (log *Logger) Named(s string) *Logger</p>
			<p>The preceding interface provides an easy-to-use strongly typed set of logging primitives. Let's see an example of structured logging with Zap:</p>
			<p class="source-code">package main</p>
			<p class="source-code">import (</p>
			<p class="source-code">     "time"</p>
			<p class="source-code">     "go.uber.org/zap"</p>
			<p class="source-code">)</p>
			<p class="source-code">func main() {</p>
			<p class="source-code">     logger, _ := zap.NewProduction()</p>
			<p class="source-code">     defer logger.Sync()</p>
			<p class="source-code">     logger = logger.Named("my-app")</p>
			<p class="source-code">     logger.Info</p>
			<p class="source-code">          ("failed to fetch URL",</p>
			<p class="source-code">          zap.String("url", "https://github.com"),</p>
			<p class="source-code">          zap.Int("attempt", 3),</p>
			<p class="source-code">          zap.Duration("backoff", time.Second),</p>
			<p class="source-code">     )</p>
			<p class="source-code">}</p>
			<p class="source-code">// Outputs: {"level":"info","ts":1257894000,"logger":"my</p>
			<p class="source-code">// app","caller":"sandbox4253963123/prog.go:15",</p>
			<p class="source-code">// "msg":"failed to fetch URL",</p>
			<p class="source-code">// "url":"https://github.com","attempt":3,"backoff":1}</p>
			<p>The JSON structured output of the logger provides<a id="_idIndexMarker862"/> helpful, easy-to-parse, and contextual information<a id="_idIndexMarker863"/> through strongly typed<a id="_idIndexMarker864"/> keys and values. In the tracing<a id="_idIndexMarker865"/> section of this chapter, we will use these additional keys and values to embed correlation IDs to link our distributed traces with our logs. If you'd like to give it a go, see <a href="https://go.dev/play/p/EVQPjTdAwX_U">https://go.dev/play/p/EVQPjTdAwX_U</a>.</p>
			<p>We will not dive deeply into where to output logs (such as a filesystem, <strong class="source-inline">STDOUT</strong>, and <strong class="source-inline">STDERR</strong>) but instead assume that the application logs we wish to ingest will have a file representation.</p>
			<p>Now that we are producing structured logs in our application, we can shift gears to ingesting, transforming, and ex<a id="_idTextAnchor476"/>porting logs using OpenTelemetry.</p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor477"/>Ingesting, transforming, and exporting logs using OpenTelemetry</h2>
			<p>In this example<a id="_idIndexMarker866"/> of using OpenTelemetry for<a id="_idIndexMarker867"/> ingesting, transforming, and exporting logs, we<a id="_idIndexMarker868"/> will use <strong class="source-inline">docker-compose</strong> to set<a id="_idIndexMarker869"/> up an environment<a id="_idIndexMarker870"/> that will simulate a Kubernetes<a id="_idIndexMarker871"/> host, with logs stored under <strong class="source-inline">/var/logs/pods/*/*/*.log</strong>. The OTel Collector will act as an agent running on the host. The logs<a id="_idIndexMarker872"/> will be ingested from<a id="_idIndexMarker873"/> the files in the log path, routed<a id="_idIndexMarker874"/> to appropriate operators<a id="_idIndexMarker875"/> in the <strong class="source-inline">filelog</strong> receiver, parsed<a id="_idIndexMarker876"/> per their particular<a id="_idIndexMarker877"/> format, have parsed attributes standardized, and then exported to <strong class="source-inline">STDOUT</strong> through the <strong class="source-inline">logging</strong> exporter.</p>
			<p>For this demo we will using the code at: <a href="https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/9/logging">https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/9/logging</a>. Now let’s take a quick look at the layout of the demo directory:</p>
			<p class="source-code">.</p>
			<p class="source-code">├── README.md</p>
			<p class="source-code">├── docker-compose.yml</p>
			<p class="source-code">├── otel-collector-config.yml</p>
			<p class="source-code">└── varlogpods</p>
			<p class="source-code">    ├── containerd_logs</p>
			<p class="source-code">0_000011112222333344445555666677778888</p>
			<p class="source-code">    │   └── logs</p>
			<p class="source-code">    │       └── 0.log</p>
			<p class="source-code">    ├── crio_logs-0_111122223333444455556666777788889999</p>
			<p class="source-code">    │   └── logs</p>
			<p class="source-code">    │       └── 0.log</p>
			<p class="source-code">    ├── docker_logs-0_222233334444555566667777888899990000</p>
			<p class="source-code">    │   └── logs</p>
			<p class="source-code">    │       └── 0.log</p>
			<p class="source-code">    └── otel_otel_888877776666555544443333222211110000</p>
			<p class="source-code">        └── otel-collector</p>
			<p class="source-code">            └── 0.log</p>
			<p>The <strong class="source-inline">docker-compose.yml</strong> file contains the service definition where we will run the OTel Collector and mount the collector configuration and log files directory, <strong class="source-inline">varlogpods</strong>, to simulate the collector running on a Kubernetes host. Let's take a look at <strong class="source-inline">docker-compose.yml</strong>:</p>
			<p class="source-code">version: "3"</p>
			<p class="source-code">services:</p>
			<p class="source-code">  opentelemetry-collector-contrib:</p>
			<p class="source-code">    image: otelcontribcol</p>
			<p class="source-code">    command: ["--config=/etc/otel-collector-config.yml"]</p>
			<p class="source-code">    volumes:</p>
			<p class="source-code">      - ./otel-collector-config.yml:/etc/otel-collector-config.yml</p>
			<p class="source-code">      - ./varlogpods:/var/log/pods</p>
			<p>To run<a id="_idIndexMarker878"/> this demo, move<a id="_idIndexMarker879"/> to the chapter<a id="_idIndexMarker880"/> source<a id="_idIndexMarker881"/> code, <strong class="source-inline">cd</strong> in<a id="_idTextAnchor478"/>to<a id="_idIndexMarker882"/> the <strong class="source-inline">logging</strong> directory, and<a id="_idIndexMarker883"/> run <strong class="source-inline">docker-compose up</strong>.</p>
			<h3>OTel Collector configuration</h3>
			<p>The OTel Collector configuration file<a id="_idIndexMarker884"/> contains the directives for how the agent is to ingest, process, and export the logs. Let's dive into the configuration and break it down:</p>
			<p class="source-code">receivers:</p>
			<p class="source-code">  filelog:</p>
			<p class="source-code">    include:</p>
			<p class="source-code">      - /var/log/pods/*/*/*.log</p>
			<p class="source-code">    exclude:</p>
			<p class="source-code">      # Exclude logs from all containers named otel-collector</p>
			<p class="source-code">      - /var/log/pods/*/otel-collector/*.log</p>
			<p class="source-code">    start_at: beginning</p>
			<p class="source-code">    include_file_path: true</p>
			<p class="source-code">    include_file_name: false</p>
			<p>The <strong class="source-inline">receivers</strong> section contains a single <strong class="source-inline">filelog</strong> receiver that specifies the directories to include and exclude. The <strong class="source-inline">filelog</strong> receiver will start from the beginning of each log file and include the file path for metadata<a id="_idIndexMarker885"/> extraction in the operators. Next, let's continue to the operators:</p>
			<p class="source-code">    operators:</p>
			<p class="source-code">      # Find out which format is used by kubernetes</p>
			<p class="source-code">      - type: router</p>
			<p class="source-code">        id: get-format</p>
			<p class="source-code">        routes:</p>
			<p class="source-code">          - output: parser-docker</p>
			<p class="source-code">            expr: '$$body matches "^\\{"'</p>
			<p class="source-code">          - output: parser-crio</p>
			<p class="source-code">            expr: '$$body matches "^[^ Z]+ "'</p>
			<p class="source-code">          - output: parser-containerd</p>
			<p class="source-code">            expr: '$$body matches "^[^ Z]+Z"'</p>
			<p>The filelog operators define a series of steps for processing the log files. The initial step is a router operation that will determine, based on the body of the log file, which parser will handle the log body entry specified in the output of the operator. Each parser operator will extract the timestamp from each record, according to the particular format of the log entry. Let's now continue to the parsers to see how the parser will extract information from each log entry once routed:</p>
			<p class="source-code">      # Parse CRI-O format</p>
			<p class="source-code">      - type: regex_parser</p>
			<p class="source-code">        id: parser-crio</p>
			<p class="source-code">        regex: '^(?P&lt;time&gt;[^ Z]+) (?P&lt;stream&gt;stdout|stderr) (?P&lt;logtag&gt;[^ ]*) (?P&lt;log&gt;.*)$'</p>
			<p class="source-code">        output: extract_metadata_from_filepath</p>
			<p class="source-code">        timestamp:</p>
			<p class="source-code">          parse_from: time</p>
			<p class="source-code">          layout_type: gotime</p>
			<p class="source-code">          layout: '2006-01-02T15:04:05.000000000-07:00'</p>
			<p class="source-code">      # Parse CRI-Containerd format</p>
			<p class="source-code">      - type: regex_parser</p>
			<p class="source-code">        id: parser-containerd</p>
			<p class="source-code">        regex: '^(?P&lt;time&gt;[^ ^Z]+Z) (?P&lt;stream&gt;stdout|stderr) (?P&lt;logtag&gt;[^ ]*) (?P&lt;log&gt;.*)$'</p>
			<p class="source-code">        output: extract_metadata_from_filepath</p>
			<p class="source-code">        timestamp:</p>
			<p class="source-code">          parse_from: time</p>
			<p class="source-code">          layout: '%Y-%m-%dT%H:%M:%S.%LZ'</p>
			<p class="source-code">      # Parse Docker format</p>
			<p class="source-code">      - type: json_parser</p>
			<p class="source-code">        id: parser-docker</p>
			<p class="source-code">        output: extract_metadata_from_filepath</p>
			<p class="source-code">        timestamp:</p>
			<p class="source-code">          parse_from: time</p>
			<p class="source-code">          layout: '%Y-%m-%dT%H:%M:%S.%LZ'</p>
			<p class="source-code">      # Extract metadata from file path</p>
			<p class="source-code">      - type: regex_parser</p>
			<p class="source-code">        id: extract_metadata_from_filepath</p>
			<p class="source-code">        regex: '^.*\/(?P&lt;namespace&gt;[^_]+)_(?P&lt;pod_name&gt;[^_]+)_(?P&lt;uid&gt;[a-f0-9\-]{36})\/(?P&lt;container_name&gt;[^\._]+)\/(?P&lt;restart_count&gt;\d+)\.log$'</p>
			<p class="source-code">        parse_from: $$attributes["file.path"]</p>
			<p class="source-code">      # Move out attributes to Attributes</p>
			<p class="source-code">      - type: metadata</p>
			<p class="source-code">        attributes:</p>
			<p class="source-code">          stream: 'EXPR($.stream)'</p>
			<p class="source-code">          k8s.container.name: 'EXPR($.container_name)'</p>
			<p class="source-code">          k8s.namespace.name: 'EXPR($.namespace)'</p>
			<p class="source-code">          k8s.pod.name: 'EXPR($.pod_name)'</p>
			<p class="source-code">          k8s.container.restart_count: 'EXPR($.restart_count)'</p>
			<p class="source-code">          k8s.pod.uid: 'EXPR($.uid)'</p>
			<p class="source-code">      # Clean up log body</p>
			<p class="source-code">      - type: restructure</p>
			<p class="source-code">        id: clean-up-log-body</p>
			<p class="source-code">        ops:</p>
			<p class="source-code">          - move:</p>
			<p class="source-code">              from: log</p>
			<p class="source-code">              to: $</p>
			<p>For example, the <strong class="source-inline">parser-crio</strong> operator will perform<a id="_idIndexMarker886"/> a regular expression on each log entry, parsing a time variable from the entry and specifying the time format for the extracted string. Contrast <strong class="source-inline">parser-crio</strong> with the <strong class="source-inline">parser-docker</strong> operator, which uses a JSON structured log format that has a JSON key of <strong class="source-inline">time</strong> in each log entry. The <strong class="source-inline">parser-docker</strong> operator only provides the key for the JSON entry <a id="_idIndexMarker887"/>and the layout of the string. No regex is needed with the structured log. Each of the parsers outputs to the <strong class="source-inline">extract_metadata_from_filepath</strong>, which extracts attributes from the file path using a regular expression. Following the parsing and extraction of file path information, the <strong class="source-inline">metadata</strong> operation executes adding attributes gathered from the parsing steps to enrich the context for future querying. Finally, the <strong class="source-inline">restructure</strong> operation moves the log key extracted from each parsed log entry to the <strong class="source-inline">Body</strong> attribute for the extracted structure.</p>
			<p>Let's take a look at the CRI-O log format:</p>
			<p class="source-code">2021-02-16T08:59:31.252009327+00:00 stdout F example: 11 Tue Feb 16 08:59:31 UTC 2021</p>
			<p>Now, let's look at the Docker log format:</p>
			<p class="source-code">{"log":"example: 12 Tue Feb 16 09:15:12 UTC</p>
			<p class="source-code">2021\n","stream":"stdout","time":"2021-02-16T09:15:12.50286486Z"}</p>
			<p>When running the example, you should see output like the following:</p>
			<p class="source-code">opentelemetry-collector-contrib_1  | LogRecord #19</p>
			<p class="source-code">opentelemetry-collector-contrib_1  | Timestamp: 2021-02-16 09:15:17.511829776 +0000 UTC</p>
			<p class="source-code">opentelemetry-collector-contrib_1  | Severity:</p>
			<p class="source-code">opentelemetry-collector-contrib_1  | ShortName:</p>
			<p class="source-code">opentelemetry-collector-contrib_1  | Body: example: 17 Tue Feb 16 09:15:17 UTC 2021</p>
			<p class="source-code">opentelemetry-collector-contrib_1  |</p>
			<p class="source-code">opentelemetry-collector-contrib_1  | Attributes:</p>
			<p class="source-code">opentelemetry-collector-contrib_1  |      -&gt; k8s.container.name: STRING(logs)</p>
			<p class="source-code">opentelemetry-collector-contrib_1  |      -&gt; k8s.container.restart_count: STRING(0)</p>
			<p class="source-code">opentelemetry-collector-contrib_1  |      -&gt; k8s.namespace.name: STRING(docker)</p>
			<p class="source-code">opentelemetry-collector-contrib_1  |      -&gt; k8s.pod.name: STRING(logs-0)</p>
			<p class="source-code">opentelemetry-collector-contrib_1  |      -&gt; k8s.pod.uid: STRING(222233334444555566667777888899990000)</p>
			<p class="source-code">opentelemetry-collector-contrib_1  |      -&gt; stream: STRING(stdout)</p>
			<p class="source-code">opentelemetry-collector-contrib_1  | Trace ID:</p>
			<p class="source-code">opentelemetry-collector-contrib_1  | Span ID:</p>
			<p class="source-code">opentelemetry-collector-contrib_1  | Flags: 0</p>
			<p>As you can see from the preceding output, the OTel Collector has extracted the timestamp, body, and specified attributes<a id="_idIndexMarker888"/> from the <strong class="source-inline">metadata</strong> operator, building a normalized structure for the exported logging data, and exported the normalized structure to <strong class="source-inline">STDOUT</strong>. </p>
			<p>We have accomplished our goal of ingesting, transforming, and extracting log telemetry, but you should also be asking yourself how we can build a stronger correlation with this telemetry. As of now, the only correlations we have are time, pod, and container. We would have a difficult time determining the HTTP request or other specific information that led to this log entry. Note that <strong class="source-inline">Trace ID</strong> and <strong class="source-inline">Span ID</strong> are empty in the preceding output. In the next section, we will discuss tracing and see how we can build a stronger correlation betw<a id="_idTextAnchor479"/>een the logs and requests processed in our applications.</p>
			<h1 id="_idParaDest-231"><a id="_idTextAnchor480"/>Instrumenting for distributed tracing</h1>
			<p>Traces track the progression of a single activity<a id="_idIndexMarker889"/> in an application. For example, an activity can be a user making a request in your application. If a trace only tracks the progression of that activity in a single process or a single component of a system composed of many components, its value is limited. However, if a trace can be propagated across multiple components in a system, it becomes much more useful. Traces that can propagate<a id="_idIndexMarker890"/> across components in a system are called <strong class="bold">distributed traces</strong>. Distributed tracing and correlation of activities is a powerful tool for determining causality within a complex system. </p>
			<p>A trace is composed of spans that represent<a id="_idIndexMarker891"/> units of work within an application. Each trace and span can be uniquely identified, and each span contains a context consisting of <strong class="source-inline">Request</strong>, <strong class="source-inline">Error</strong>, and <strong class="source-inline">Duration</strong> metrics. A trace contains a tree of spans with a single root span. For example, imagine a user clicking on the checkout button on your company's commerce site. The root span would encompass the entire request/response cycle as perceived by the user clicking on the checkout button. There would likely be many child spans for that single root span, such as a query for product data, charging a credit card, and updating a database. Perhaps there would also be an error associated with one of the underlying spans within that root span. Each span has metadata associated with it, such as a name, start and end timestamps, events, and status. By creating a tree of spans with this metadata, we are able to deeply inspect the state of complex applications.</p>
			<p>In this section, we will learn to instrument Go applications with OpenTelemetry to emit distributed tracing telemetry, which we will inspect using Jaeger, an open s<a id="_idTextAnchor481"/>ource tool for visualizing and querying distributed traces.</p>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor482"/>The life cycle of a distributed trace</h2>
			<p>Before we get into the code, let's first discuss how distributed tracing<a id="_idIndexMarker892"/> works. Let's imagine we have two services, A and B. Service A serves web pages and makes requests for data from service B. When service A receives a request for a page, the service starts a root span. Service A then requests some data from service B to fulfill the request. Service A encodes the trace and span context in request headers to service B. When service B receives the request, service B extracts the trace and span information from the request headers and creates a child span from the request. If service B received no trace/span headers, it will create a new root span. Service B continues processing the request, creating new child spans along the way as it requests data from a database. After service B has collected the requested information, it responds to service A and sends its spans to the trace aggregator. Service A then receives the response from service B, and service A responds to the user with the page. At the end of the activity, service A marks the root span as complete and sends its spans to the trace aggregator. The trace aggregator builds a tree with the shared correlation of the spans from both service A and service B, and we have<a id="_idIndexMarker893"/> a distributed trace.</p>
			<p>For more details<a id="_idIndexMarker894"/> of the OpenTelemetry tracing specification, see <a href="https://opentelemetry.io/docs/reference/specification/overview/#tracing-signal">https://opentelemetry.io/docs/reference/specification/overview/#tracing-signal</a>.</p>
			<h3>Client/server-distributed tracing with OpenTelemetry</h3>
			<p>In this example, we will deploy<a id="_idIndexMarker895"/> and examine a client/server application<a id="_idIndexMarker896"/> that is instrumented with OpenTelemetry for distributed tracing, and view the distributed traces using Jaeger. The client application sends periodic requests to the server that will populate the traces in Jaeger. The <a href="https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/9/tracing">https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/9/tracing</a> directory contains the following:</p>
			<p class="source-code">.</p>
			<p class="source-code">├── readme.md</p>
			<p class="source-code">├── client</p>
			<p class="source-code">│   ├── Dockerfile</p>
			<p class="source-code">│   ├── go.mod</p>
			<p class="source-code">│   ├── go.sum</p>
			<p class="source-code">│   └── main.go</p>
			<p class="source-code">├── docker-compose.yaml</p>
			<p class="source-code">├── otel-collector-config.yaml</p>
			<p class="source-code">└── server</p>
			<p class="source-code">    ├── Dockerfile</p>
			<p class="source-code">    ├── go.mod</p>
			<p class="source-code">    ├── go.sum</p>
			<p class="source-code">    └── main.go</p>
			<p>To run this<a id="_idIndexMarker897"/> demo, move to the chapter <a id="_idIndexMarker898"/>source code, <strong class="source-inline">cd</strong> into the <strong class="source-inline">tracing</strong> directory, run <strong class="source-inline">docker-compose up -d</strong>, and open <strong class="source-inline">http://localhost:16686</strong> to view the Jaeger-distributed traces.</p>
			<p>Let's explore the <strong class="source-inline">docker-compose.yaml</strong> file first to see each of the services we are deploying:</p>
			<p class="source-code">version: "2"</p>
			<p class="source-code">services:</p>
			<p class="source-code">  # Jaeger</p>
			<p class="source-code">  jaeger-all-in-one:</p>
			<p class="source-code">    image: jaegertracing/all-in-one:latest</p>
			<p class="source-code">    ports:</p>
			<p class="source-code">      - "16686:16686"</p>
			<p class="source-code">      - "14268"</p>
			<p class="source-code">      - "14250"</p>
			<p class="source-code">  # Collector</p>
			<p class="source-code">  otel-collector:</p>
			<p class="source-code">    image: ${OTELCOL_IMG}</p>
			<p class="source-code">    command: ["--config=/etc/otel-collector-config.yaml", "${OTELCOL_ARGS}"]</p>
			<p class="source-code">    volumes:</p>
			<p class="source-code">      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml</p>
			<p class="source-code">    ports:</p>
			<p class="source-code">      - "13133:13133" # health_check extension</p>
			<p class="source-code">    depends_on:</p>
			<p class="source-code">      - jaeger-all-in-one</p>
			<p class="source-code">  demo-client:</p>
			<p class="source-code">    build:</p>
			<p class="source-code">      dockerfile: Dockerfile</p>
			<p class="source-code">      context: ./client</p>
			<p class="source-code">    environment:</p>
			<p class="source-code">      - OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317</p>
			<p class="source-code">      - DEMO_SERVER_ENDPOINT=http://demo-server:7080/hello</p>
			<p class="source-code">    depends_on:</p>
			<p class="source-code">      - demo-server</p>
			<p class="source-code">  demo-server:</p>
			<p class="source-code">    build:</p>
			<p class="source-code">      dockerfile: Dockerfile</p>
			<p class="source-code">      context: ./server</p>
			<p class="source-code">    environment:</p>
			<p class="source-code">      - OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317</p>
			<p class="source-code">    ports:</p>
			<p class="source-code">      - "7080"</p>
			<p class="source-code">    depends_on:</p>
			<p class="source-code">      - otel-collector</p>
			<p>The preceding <strong class="source-inline">docker-compose.yaml</strong> file deploys a Jaeger <em class="italic">all-in-one</em> instance, an OTel Collector, a client Go<a id="_idIndexMarker899"/> application, and a server Go<a id="_idIndexMarker900"/> application. These components are a slight derivation from the OpenTelemetry demo: <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/examples/demo">https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/examples/demo</a>. </p>
			<p>Next, let's take a look at the OTel Collector configuration to get a better understanding of its deployment model<a id="_idIndexMarker901"/> and configured<a id="_idIndexMarker902"/> behaviors:</p>
			<p class="source-code">receivers:</p>
			<p class="source-code">  otlp:</p>
			<p class="source-code">    protocols:</p>
			<p class="source-code">      grpc:</p>
			<p class="source-code">exporters:</p>
			<p class="source-code">  jaeger:</p>
			<p class="source-code">    endpoint: jaeger-all-in-one:14250</p>
			<p class="source-code">    tls:</p>
			<p class="source-code">      insecure: true</p>
			<p class="source-code">processors:</p>
			<p class="source-code">  batch:</p>
			<p class="source-code">service:</p>
			<p class="source-code">  pipelines:</p>
			<p class="source-code">    traces:</p>
			<p class="source-code">      receivers: [otlp]</p>
			<p class="source-code">      processors: [batch]</p>
			<p class="source-code">      exporters: [jaeger]</p>
			<p>The preceding OTel Collector configuration specifies that the collector will listen for <strong class="bold">OpenTelemetry Line Protocol</strong> (<strong class="bold">OTLP</strong>) over gRPC. It will batch the spans and export them to a Jaeger service running on port <strong class="source-inline">14250</strong>.</p>
			<p>Next, let's break down the significant parts of the client <strong class="source-inline">main.go</strong>:</p>
			<p class="source-code">func main() {</p>
			<p class="source-code">     shutdown := initTraceProvider()</p>
			<p class="source-code">     defer shutdown()</p>
			<p class="source-code">     </p>
			<p class="source-code">     continuouslySendRequests()</p>
			<p class="source-code">}</p>
			<p><strong class="source-inline">func main()</strong> initializes the tracing<a id="_idIndexMarker903"/> provider, which returns<a id="_idIndexMarker904"/> a shutdown function that is deferred until <strong class="source-inline">func main()</strong> exits. The <strong class="source-inline">main()</strong> func then calls <strong class="source-inline">continuouslySendRequests</strong> to send a continuous, periodic stream of requests to the server application. Next, let's look at the <strong class="source-inline">initTraceProvider</strong> function:</p>
			<p class="source-code">func initTraceProvider() func() {</p>
			<p class="source-code">	ctx := context.Background()</p>
			<p class="source-code">	cancel = context.CancelFunc</p>
			<p class="source-code">	timeout := 1 * time.Second</p>
			<p class="source-code">	endPointEnv := "OTEL_EXPORTER_OTLP_ ENDPOINT"</p>
			<p class="source-code">	otelAgentAddr, ok := os.LookupEnv(endPointEnv)</p>
			<p class="source-code">	if !ok {</p>
			<p class="source-code">		otelAgentAddr = "0.0.0.0:4317"</p>
			<p class="source-code">	}</p>
			<p class="source-code">	closeTraces := initTracer(ctx, otelAgentAddr)</p>
			<p class="source-code">	return func() {</p>
			<p class="source-code">		ctx, cancel = context.WithTimeout(ctx, time.Second)</p>
			<p class="source-code">		defer cancel()</p>
			<p class="source-code">		// pushes any last exports to the receiver</p>
			<p class="source-code">		closeTraces(doneCtx)</p>
			<p class="source-code">	}</p>
			<p class="source-code">}</p>
			<p><strong class="source-inline">initTraceProvider()</strong> looks up the OTLP trace<a id="_idIndexMarker905"/> endpoint from an environment variable or defaults to <strong class="source-inline">0.0.0.0:4317</strong>. After setting<a id="_idIndexMarker906"/> up the trace endpoint address, the code calls <strong class="source-inline">initTracer</strong> to initialize the tracer, returning a function named <strong class="source-inline">closeTraces</strong>, which will be used to shut down the tracer. Finally, the <strong class="source-inline">initTraceProvider()</strong> returns a function that can be used to flush and close the tracer. Next, let's look at what is happening in <strong class="source-inline">initTracer()</strong>:</p>
			<p class="source-code">func initTracer(ctx context.Context, otelAgentAddr string) func(context.Context) {</p>
			<p class="source-code">     traceClient := otlptracegrpc.NewClient(</p>
			<p class="source-code">          otlptracegrpc.WithInsecure(),</p>
			<p class="source-code">          otlptracegrpc.WithEndpoint(otelAgentAddr),</p>
			<p class="source-code">          otlptracegrpc.WithDialOption(grpc.WithBlock()))</p>
			<p class="source-code">     traceExp, err := otlptrace.New(ctx, traceClient)</p>
			<p class="source-code">     handleErr(err, "Failed to create the collector trace exporter")</p>
			<p class="source-code">     res, err := resource.New(</p>
			<p class="source-code">          ctx,</p>
			<p class="source-code">          resource.WithFromEnv(),</p>
			<p class="source-code">          resource.WithProcess(),</p>
			<p class="source-code">          resource.WithTelemetrySDK(),</p>
			<p class="source-code">          resource.WithHost(),</p>
			<p class="source-code">          resource.WithAttributes(</p>
			<p class="source-code">               semconv.ServiceNameKey.String("demo-client"),</p>
			<p class="source-code">          ),</p>
			<p class="source-code">     )</p>
			<p class="source-code">     handleErr(err, "failed to create resource")</p>
			<p class="source-code">     bsp := sdktrace.NewBatchSpanProcessor(traceExp)</p>
			<p class="source-code">     tracerProvider := sdktrace.NewTracerProvider(</p>
			<p class="source-code">          sdktrace.WithSampler(sdktrace.AlwaysSample()),</p>
			<p class="source-code">          sdktrace.WithResource(res),</p>
			<p class="source-code">          sdktrace.WithSpanProcessor(bsp),</p>
			<p class="source-code">     )</p>
			<p class="source-code">     // set global propagator to tracecontext (the default is no-op).</p>
			<p class="source-code">     otel.SetTextMapPropagator(propagation.TraceContext{})</p>
			<p class="source-code">     otel.SetTracerProvider(tracerProvider)</p>
			<p class="source-code">     return func(doneCtx context.Context) {</p>
			<p class="source-code">          if err := traceExp.Shutdown(doneCtx); err != nil {</p>
			<p class="source-code">               otel.Handle(err)</p>
			<p class="source-code">          }</p>
			<p class="source-code">     }</p>
			<p class="source-code">}</p>
			<p><strong class="source-inline">initTracer()</strong> builds a trace client that connects to the OTLP endpoint over gRPC. The trace client<a id="_idIndexMarker907"/> is then used to build a trace<a id="_idIndexMarker908"/> exporter, which is used to batch process and export spans. The batch span processor is then used to create a trace provider, configured to trace all spans, and is identified with the <strong class="source-inline">"demo-client"</strong> resource. Trace providers can be configured to sample stochastically or with custom sampling strategies. The trace provider is then added to the global OTel context. Finally, a function is returned that will shut down and flush the trace exporter.</p>
			<p>Now that we have explored how to set up a tracer, let's move on to sending and tracing requests in the <strong class="source-inline">continuouslySendRequests</strong> func:</p>
			<p class="source-code">func continuouslySendRequests() {</p>
			<p class="source-code">     tracer := otel.Tracer("demo-client-tracer")</p>
			<p class="source-code">     for {</p>
			<p class="source-code">          ctx, span := tracer.Start(context.Background(), "ExecuteRequest")</p>
			<p class="source-code">          makeRequest(ctx)</p>
			<p class="source-code">          span.End()</p>
			<p class="source-code">          time.Sleep(time.Duration(1) * time.Second)</p>
			<p class="source-code">     }</p>
			<p class="source-code">}</p>
			<p>As the name suggests, the <strong class="source-inline">continuouslySendRequests</strong> func creates a named tracer from the global OTel context, which we initialized earlier in the chapter. The <strong class="source-inline">otel.Tracer</strong> interface only has one function, <strong class="source-inline">Start(ctx context.Context, spanName string, opts ...SpanStartOption) (context.Context, Span)</strong>, which is used to start a new span if one does not already exist in the <strong class="source-inline">context.Context</strong> values bag. The <strong class="source-inline">for</strong> loop in main will continue infinitely creating a new span, making a request to the server, doing a bit of work, and finally, sleeping for 1 second:</p>
			<p class="source-code">func makeRequest(ctx context.Context) {</p>
			<p class="source-code">     demoServerAddr, ok := os.LookupEnv("DEMO_SERVER_ENDPOINT")</p>
			<p class="source-code">     if !ok {</p>
			<p class="source-code">          demoServerAddr = "http://0.0.0.0:7080/hello"</p>
			<p class="source-code">     }</p>
			<p class="source-code">     // Trace an HTTP client by wrapping the transport</p>
			<p class="source-code">     client := http.Client{</p>
			<p class="source-code">          Transport: otelhttp.NewTransport(http.DefaultTransport),</p>
			<p class="source-code">     }</p>
			<p class="source-code">     // Make sure we pass the context to the request to avoid broken traces.</p>
			<p class="source-code">     req, err := http.NewRequestWithContext(ctx, "GET", demoServerAddr, nil)</p>
			<p class="source-code">     if err != nil {</p>
			<p class="source-code">          handleErr(err, "failed to http request")</p>
			<p class="source-code">     }</p>
			<p class="source-code">     // All requests made with this client will create spans.</p>
			<p class="source-code">     res, err := client.Do(req)</p>
			<p class="source-code">     if err != nil {</p>
			<p class="source-code">          panic(err)</p>
			<p class="source-code">     }</p>
			<p class="source-code">     res.Body.Close()</p>
			<p class="source-code">}</p>
			<p><strong class="source-inline">makeRequest()</strong> should look pretty familiar<a id="_idIndexMarker909"/> to those of you who have used<a id="_idIndexMarker910"/> the Go <strong class="source-inline">http</strong> library. There is one significant difference from non-OTel instrumented HTTP requests: the transport for the <strong class="source-inline">client</strong> has been wrapped with <strong class="source-inline">otelhttp.NewTransport()</strong>. The <strong class="source-inline">otelhttp</strong> transport uses <strong class="source-inline">request.Context()</strong> in the <strong class="source-inline">Roundtrip</strong> implementation to extract the existing span from the context, and then the <strong class="source-inline">otelhttp.Transport</strong> adds the span information to the HTTP headers to enable the propagation of span data to the server application.</p>
			<p>Now that we have covered the client, let's see the server <strong class="source-inline">main.go</strong>. The code for this section can be found here: <a href="https://github.com/PacktPublishing/Go-for-DevOps/blob/rev0/chapter/9/tracing/server/main.go">https://github.com/PacktPublishing/Go-for-DevOps/blob/rev0/chapter/9/tracing/server/main.go</a>:</p>
			<p class="source-code">func main() { </p>
			<p class="source-code">    shutdown := initTraceProvider() </p>
			<p class="source-code">    defer shutdown()</p>
			<p class="source-code">     handler := handleRequestWithRandomSleep()</p>
			<p class="source-code">     wrappedHandler := otelhttp.NewHandler(handler, "/hello")</p>
			<p class="source-code">     http.Handle("/hello", wrappedHandler)</p>
			<p class="source-code">     http.ListenAndServe(":7080", nil)</p>
			<p class="source-code">}</p>
			<p><strong class="source-inline">func main.go</strong> calls <strong class="source-inline">initTraceProvider</strong> and <strong class="source-inline">shutdown</strong> in a similar manner to the client <strong class="source-inline">main.go</strong>. After initializing <a id="_idIndexMarker911"/>the trace provider, the<a id="_idIndexMarker912"/> server <strong class="source-inline">main.go</strong> code creates an HTTP server, handling requests to <strong class="source-inline">"/hello"</strong> on port <strong class="source-inline">7080</strong>. The significant bit is <strong class="source-inline">wrappedHandler := otelhttp.NewHandler(handler, "/hello")</strong>. <strong class="source-inline">wrappedHandler()</strong> extracts the span context from the HTTP headers and populates the request <strong class="source-inline">context.Context</strong> with a span derived from the client span. Within <strong class="source-inline">handleRequestWithRandomSleep()</strong>, the code uses the propagated span context to continue the distributed trace. Let's explore <strong class="source-inline">handleRequestWithRandomSleep()</strong>:</p>
			<p class="source-code">func handleRequestWithRandomSleep() http.HandlerFunc {</p>
			<p class="source-code">     commonLabels := []attribute.KeyValue{</p>
			<p class="source-code">          attribute.String("server-attribute", "foo"),</p>
			<p class="source-code">     }</p>
			<p class="source-code">     return func(w http.ResponseWriter, req *http.Request) {</p>
			<p class="source-code">          //  random sleep to simulate latency</p>
			<p class="source-code">          var sleep int64</p>
			<p class="source-code">          switch modulus := time.Now().Unix() % 5; modulus {</p>
			<p class="source-code">          case 0:</p>
			<p class="source-code">               sleep = rng.Int63n(2000)</p>
			<p class="source-code">          case 1:</p>
			<p class="source-code">               sleep = rng.Int63n(15)</p>
			<p class="source-code">          case 2:</p>
			<p class="source-code">               sleep = rng.Int63n(917)</p>
			<p class="source-code">          case 3:</p>
			<p class="source-code">               sleep = rng.Int63n(87)</p>
			<p class="source-code">          case 4:</p>
			<p class="source-code">               sleep = rng.Int63n(1173)</p>
			<p class="source-code">          }</p>
			<p class="source-code">          time.Sleep(time.Duration(sleep) * time.Millisecond)</p>
			<p class="source-code">          ctx := req.Context()</p>
			<p class="source-code">          span := trace.SpanFromContext(ctx)</p>
			<p class="source-code">          span.SetAttributes(commonLabels...)</p>
			<p class="source-code">          w.Write([]byte("Hello World"))</p>
			<p class="source-code">     }</p>
			<p class="source-code">}</p>
			<p>In <strong class="source-inline">handleRequestWithRandomSleep()</strong>, the request is handled, introducing<a id="_idIndexMarker913"/> a random sleep to simulate<a id="_idIndexMarker914"/> latency. <strong class="source-inline">trace.SpanFromContext(ctx)</strong> uses the span populated by <strong class="source-inline">wrappedHandler</strong> to then set attributes on the distributed span.</p>
			<p>The viewable result in Jaeger at <strong class="source-inline">http://localhost:16686</strong> is the following:</p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="Images/B17626_09_003.jpg" alt="Figure 9.3 – The Jaeger client/server-distributed trace&#13;&#10;" width="1175" height="940"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3 – The Jaeger client/server-distributed trace</p>
			<p>In the preceding screenshot, you can see the distributed trac<a id="_idIndexMarker915"/>e between the client and the server, including each span that was<a id="_idIndexMarker916"/> created in the request/response cycle. This is a simple example, but you can imagine how this simple example can be extrapolated into a more complex system to provide insight into the difficult-to-debug scenarios. The trace provides the inform<a id="_idTextAnchor483"/>ation needed to gain insight into errors as well as more subtle performance issues.</p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor484"/>Correlating traces and logs</h2>
			<p>In the <em class="italic">Logging with context</em> section, we discussed the correlation<a id="_idIndexMarker917"/> of log entries with activities. Without<a id="_idIndexMarker918"/> correlation to a given trace and span, you would not be able to determine which log events originated from a specific activity. Remember, log entries do not contain the trace and span data that enables us to build correlated trace views, as we see in Jaeger. However, we can extend<a id="_idIndexMarker919"/> our log entries to include this data<a id="_idIndexMarker920"/> and enable robust correlation with a specific activity:</p>
			<p class="source-code">func WithCorrelation(span trace.Span, log *zap.Logger) *zap.Logger {</p>
			<p class="source-code">     return log.With(</p>
			<p class="source-code">          zap.String("span_id", convertTraceID(span.SpanContext().SpanID().String())),</p>
			<p class="source-code">          zap.String("trace_id", convertTraceID(span.SpanContext().TraceID().String())),</p>
			<p class="source-code">     )</p>
			<p class="source-code">}</p>
			<p class="source-code">func convertTraceID(id string) string {</p>
			<p class="source-code">     if len(id) &lt; 16 {</p>
			<p class="source-code">          return ""</p>
			<p class="source-code">     }</p>
			<p class="source-code">     if len(id) &gt; 16 {</p>
			<p class="source-code">          id = id[16:]</p>
			<p class="source-code">     }</p>
			<p class="source-code">     intValue, err := strconv.ParseUint(id, 16, 64)</p>
			<p class="source-code">     if err != nil {</p>
			<p class="source-code">          return ""</p>
			<p class="source-code">     }</p>
			<p class="source-code">     return strconv.FormatUint(intValue, 10)</p>
			<p class="source-code">}</p>
			<p>In the preceding code, we use the <strong class="source-inline">zap</strong> structured logger to add the span and trace IDs to the logger, so each log entry<a id="_idIndexMarker921"/> written by a logger enhan<a id="_idTextAnchor485"/>ced<a id="_idIndexMarker922"/> with <strong class="source-inline">WithCorrelation()</strong> will contain a strong correlation to a given activity.</p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor486"/>Adding log entries to spans</h2>
			<p>Correlating logs with traces<a id="_idIndexMarker923"/> is effective for building correlations<a id="_idIndexMarker924"/> of logs with activities, but you can take it a step<a id="_idIndexMarker925"/> further. You can add your log events directly to the spans, instead of or in combination with correlating logs:</p>
			<p class="source-code">func SuccessfullyFinishedRequestEvent(span trace.Span, opts ...trace.EventOption) {</p>
			<p class="source-code">     opts = append(opts, trace.WithAttributes(attribute.String("someKey", "someValue")))</p>
			<p class="source-code">     span.AddEvent("successfully finished request operation", opts...)</p>
			<p class="source-code">}</p>
			<p><strong class="source-inline">SuccessfullyFinishedRequestEvent()</strong> will decorate the span with an event entry that shows as a log entry in Jaeger. If we were to call this function in the client's <strong class="source-inline">main.go</strong> after we complete the request, a log event would be added to the client request span:</p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="Images/B17626_09_004.jpg" alt="Figure 9.4 – The Jaeger client/server-distributed trace with the log entry&#13;&#10;" width="1216" height="656"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4 – The Jaeger client/server-distributed trace with the log entry</p>
			<p>As you can see, the log entry is embedded within the span visualized in Jaeger. Adding log entries to spans adds even<a id="_idIndexMarker926"/> more context to your distributed <a id="_idIndexMarker927"/>traces, making it easier to understand what is happening<a id="_idIndexMarker928"/> with your application.</p>
			<p>In the next section, we will instrumen<a id="_idTextAnchor487"/>t this example with metrics to provide an aggregated view of the application using Prometheus.</p>
			<h1 id="_idParaDest-235"><a id="_idTextAnchor488"/>Instrumenting for metrics</h1>
			<p>Metrics are measurements at a given moment of a particular aspect of an application during runtime. An individual capture is called a <strong class="bold">metric event</strong> and consists of a timestamp, a measurement, and associated <a id="_idIndexMarker929"/>metadata. Metric events are used to provide an aggregated view of the behavior of an application at runtime. For example, a metric event can be a counter incremented by 1 when a request is handled by a service. The individual<a id="_idIndexMarker930"/> event is not especially useful. However, when aggregated into a sum of requests over a period of time, you can see how many requests are made to a service over that period of time. </p>
			<p>The OpenTelemetry API does not allow for custom aggregations but does provide some common aggregations, such as sum, count, last value, and histograms, which are supported by backend visualization<a id="_idIndexMarker931"/> and analysis software such as Prometheus.</p>
			<p>To give you a better idea<a id="_idIndexMarker932"/> of when metrics are useful, here are some example scenarios:</p>
			<ul>
				<li>Providing the aggregate total number of bits read or written in a process</li>
				<li>Providing CPU or memory utilization</li>
				<li>Providing the number of requests over a period of time</li>
				<li>Providing the number of errors over a period of time</li>
				<li>Providing the duration of requests to form a statistical distribution of the request processing time</li>
			</ul>
			<p>OpenTelemetry offers three types<a id="_idIndexMarker933"/> of metrics:</p>
			<ul>
				<li><strong class="source-inline">counter</strong>: To count a value over time, such as the number of requests</li>
				<li><strong class="source-inline">measure</strong>: To sum or otherwise aggregate a value over a period of time, such as how many bytes are read per minute</li>
				<li><strong class="source-inline">observer</strong>: To periodically capture a value, such as memory utilization every minute</li>
			</ul>
			<p>In this section, we will learn to instrument Go applications with OpenTelemetry to emit metrics telemetry,<a id="_idTextAnchor489"/> which we will inspect using Prometheus, an open source tool for visualizing and analyzing metrics.</p>
			<h2 id="_idParaDest-236"><a id="_idTextAnchor490"/>The life cycle of a metric</h2>
			<p>Before we get into the code, let's first discuss<a id="_idIndexMarker934"/> how metrics are defined and used. Before you can record or observe a metric, it must be defined. For example, a histogram of request latency would be defined as follows:</p>
			<p class="source-code">meter := global.Meter("demo-client-meter")</p>
			<p class="source-code">requestLatency := metric.Must(meter).NewFloat64Histogram(</p>
			<p class="source-code">	"demo_client/request_latency",</p>
			<p class="source-code">	metric.WithDescription(</p>
			<p class="source-code">		"The latency of requests processed"</p>
			<p class="source-code">	),</p>
			<p class="source-code">)</p>
			<p class="source-code">requestCount := metric.Must(meter).NewInt64Counter(</p>
			<p class="source-code">	"demo_client/request_counts",</p>
			<p class="source-code">	metric.WithDescription("The number of requests processed"),</p>
			<p class="source-code">)</p>
			<p>The preceding code fetches a global meter named <strong class="source-inline">demo-client-meter</strong> and then registers a new histogram<a id="_idIndexMarker935"/> instrument named <strong class="source-inline">demo_client/reqeust_latency</strong> and <strong class="source-inline">demo_client/request_counts</strong>, a counter instrument, both of which have a description of what is being collected. It's important to provide descriptive names and descriptions for your metrics, as it can become confusing later when analyzing your data.</p>
			<p>Once the instrument has been defined, it can be used to record measurements, as follows:</p>
			<p class="source-code">meter.RecordBatch(</p>
			<p class="source-code">    ctx,</p>
			<p class="source-code">    commonLabels,</p>
			<p class="source-code">    requestLatency.Measurement(latencyMs),</p>
			<p class="source-code">    requestCount.Measurement(1),</p>
			<p class="source-code">)</p>
			<p>The preceding code uses the global meter we defined previously to record two measurements, the request latency and an increment for the number of requests. Note that <strong class="source-inline">ctx</strong> was included, which will contain correlation information to correlate the activity to the measurement.</p>
			<p>After events have been <a id="_idTextAnchor491"/>recorded, they will be<a id="_idIndexMarker936"/> exported based on the configuration of <strong class="source-inline">MeterProvider</strong>, which which we will explore next.</p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor492"/>Client/server metrics with OpenTelemetry</h2>
			<p>We will extend the same client/server application<a id="_idIndexMarker937"/> described in the <em class="italic">Instrumenting for distributed tracing </em>section. Code<a id="_idIndexMarker938"/> for this section can<a id="_idIndexMarker939"/> be found here: <a href="https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/9/metrics">https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/9/metrics</a>. The directory has the following layout:</p>
			<p class="source-code">.</p>
			<p class="source-code">├── readme.md</p>
			<p class="source-code">├── client</p>
			<p class="source-code">│   ├── Dockerfile</p>
			<p class="source-code">│   ├── go.mod</p>
			<p class="source-code">│   ├── go.sum</p>
			<p class="source-code">│   └── main.go</p>
			<p class="source-code">├── .env</p>
			<p class="source-code">├── docker-compose.yaml</p>
			<p class="source-code">├── otel-collector-config.yaml</p>
			<p class="source-code">├── prometheus.yaml</p>
			<p class="source-code">└── server</p>
			<p class="source-code">    ├── Dockerfile</p>
			<p class="source-code">    ├── go.mod</p>
			<p class="source-code">    ├── go.sum</p>
			<p class="source-code">    └── main.go</p>
			<p>The only addition to the preceding is the <strong class="source-inline">prometheus.yaml</strong> file, which contains the following:</p>
			<p class="source-code">scrape_configs:</p>
			<p class="source-code">  - job_name: 'otel-collector'</p>
			<p class="source-code">    scrape_interval: 10s</p>
			<p class="source-code">    static_configs:</p>
			<p class="source-code">      - targets: ['otel-collector:8889']</p>
			<p class="source-code">      - targets: ['otel-collector:8888']</p>
			<p>The preceding configuration informs<a id="_idIndexMarker940"/> Prometheus of the endpoint<a id="_idIndexMarker941"/> to scrape to gather metrics<a id="_idIndexMarker942"/> data from the OTel Collector. Let's next look at the updates needed to add Prometheus to the <strong class="source-inline">docker-compose.yaml</strong> file:</p>
			<p class="source-code">version: "2"</p>
			<p class="source-code">services:</p>
			<p class="source-code">  # omitted Jaeger config</p>
			<p class="source-code">  # Collector</p>
			<p class="source-code">  otel-collector:</p>
			<p class="source-code">    image: ${OTELCOL_IMG}</p>
			<p class="source-code">    command: ["--config=/etc/otel-collector-config.yaml", "${OTELCOL_ARGS}"]</p>
			<p class="source-code">    volumes:</p>
			<p class="source-code">      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml</p>
			<p class="source-code">    ports:</p>
			<p class="source-code">      - "8888:8888"   # Prometheus metrics exposed by the collector</p>
			<p class="source-code">      - "8889:8889"   # Prometheus exporter metrics</p>
			<p class="source-code">      - "4317"        # OTLP gRPC receiver</p>
			<p class="source-code">    depends_on:</p>
			<p class="source-code">      - jaeger-all-in-one</p>
			<p class="source-code">  # omitted demo-client and demo-server</p>
			<p class="source-code">  prometheus:</p>
			<p class="source-code">    container_name: prometheus</p>
			<p class="source-code">    image: prom/prometheus:latest</p>
			<p class="source-code">    volumes:</p>
			<p class="source-code">      - ./prometheus.yaml:/etc/prometheus/prometheus.yml</p>
			<p class="source-code">    ports:</p>
			<p class="source-code">      - "9090:9090"</p>
			<p>As you can see from<a id="_idIndexMarker943"/> the preceding, we have added some additional ports<a id="_idIndexMarker944"/> for Prometheus to scrape on the OTel Collector, and the Prometheus<a id="_idIndexMarker945"/> service with <strong class="source-inline">prometheus.yaml</strong> mounted in the container. Next, let's take a look at the updated OTel Collector configuration:</p>
			<p class="source-code">receivers:</p>
			<p class="source-code">  otlp:</p>
			<p class="source-code">    protocols:</p>
			<p class="source-code">      grpc:</p>
			<p class="source-code">exporters:</p>
			<p class="source-code">  prometheus:</p>
			<p class="source-code">    endpoint: "0.0.0.0:8889"</p>
			<p class="source-code">    const_labels:</p>
			<p class="source-code">      label1: value1</p>
			<p class="source-code">  logging:</p>
			<p class="source-code">  # omitted jaeger exporter</p>
			<p class="source-code">processors:</p>
			<p class="source-code">  batch:</p>
			<p class="source-code">service:</p>
			<p class="source-code">  pipelines:</p>
			<p class="source-code">    # omitted tracing pipeline</p>
			<p class="source-code">    metrics:</p>
			<p class="source-code">      receivers: [otlp]</p>
			<p class="source-code">      processors: [batch]</p>
			<p class="source-code">      exporters: [logging, prometheus]</p>
			<p>The preceding configuration has omitted<a id="_idIndexMarker946"/> the Jaeger config used in the <em class="italic">Instrumenting for distributed tracing</em> section for brevity. The additions<a id="_idIndexMarker947"/> are the exporter for Prometheus<a id="_idIndexMarker948"/> as well as the metrics pipeline. The Prometheus exporter will expose port <strong class="source-inline">8889</strong> so that Prometheus can scrape metrics data collected by the OTel Collector.</p>
			<p>Next, let's break down the significant parts of the client <strong class="source-inline">main.go</strong>:</p>
			<p class="source-code">func main() {</p>
			<p class="source-code">     shutdown := initTraceAndMetricsProvider()</p>
			<p class="source-code">     defer shutdown()</p>
			<p class="source-code">     continuouslySendRequests()</p>
			<p class="source-code">}</p>
			<p>The only difference between the tracing version we explored earlier in the chapter is that instead of calling <strong class="source-inline">initTraceProvider</strong>, the code now calls <strong class="source-inline">initTraceAndMetricsProvdier</strong> to initialize both the trace and metrics providers. Next, let's explore <strong class="source-inline">initTraceAndMetricsProvider()</strong>:</p>
			<p class="source-code">func initTraceAndMetricsProvider() func() {</p>
			<p class="source-code">	ctx := context.Background()</p>
			<p class="source-code">	var cancel context.CancelFunc</p>
			<p class="source-code">	timeout := 1 * time.Second</p>
			<p class="source-code">	endpoint := "OTEL_EXPORTER_OTLP_ ENDPOINT"</p>
			<p class="source-code">	otelAgentAddr, ok := os.LookupEnv(endpoint)</p>
			<p class="source-code">	if !ok {</p>
			<p class="source-code">		otelAgentAddr = "0.0.0.0:4317"</p>
			<p class="source-code">	}</p>
			<p class="source-code">	closeMetrics := initMetrics(ctx, otelAgentAddr)</p>
			<p class="source-code">	closeTraces := initTracer(ctx, otelAgentAddr)</p>
			<p class="source-code">	return func() {</p>
			<p class="source-code">		ctx, cancel = context.WithTimeout(ctx, timeout)</p>
			<p class="source-code">		defer cancel()</p>
			<p class="source-code">		closeTraces(doneCtx)</p>
			<p class="source-code">		closeMetrics(doneCtx)</p>
			<p class="source-code">	}</p>
			<p class="source-code">}</p>
			<p>The code in <strong class="source-inline">initTraceAndMetricsProvider</strong> establishes the OTel agent address and goes on to initialize<a id="_idIndexMarker949"/> the metrics and tracing<a id="_idIndexMarker950"/> providers. Finally, a function to close and flush both <a id="_idIndexMarker951"/>metrics and traces is returned. Next, let's explore <strong class="source-inline">initMetrics()</strong>:</p>
			<p class="source-code">func initMetrics(ctx context.Context, otelAgentAddr string) func(context.Context) {</p>
			<p class="source-code">     metricClient := otlpmetricgrpc.NewClient(</p>
			<p class="source-code">          otlpmetricgrpc.WithInsecure(),</p>
			<p class="source-code">          otlpmetricgrpc.WithEndpoint(otelAgentAddr))</p>
			<p class="source-code">     metricExp, err := otlpmetric.New(ctx, metricClient)</p>
			<p class="source-code">     handleErr(err, "Failed to create the collector metric exporter")</p>
			<p class="source-code">     pusher := controller.New(</p>
			<p class="source-code">          processor.NewFactory(</p>
			<p class="source-code">               simple.NewWithHistogramDistribution(),</p>
			<p class="source-code">               metricExp,</p>
			<p class="source-code">          ),</p>
			<p class="source-code">          controller.WithExporter(metricExp),</p>
			<p class="source-code">          controller.WithCollectPeriod(2*time.Second),</p>
			<p class="source-code">     )</p>
			<p class="source-code">     global.SetMeterProvider(pusher)</p>
			<p class="source-code">     err = pusher.Start(ctx)</p>
			<p class="source-code">     handleErr(err, "Failed to start metric pusher")</p>
			<p class="source-code">     return func(doneCtx context.Context) {</p>
			<p class="source-code">          // pushes any last exports to the receiver</p>
			<p class="source-code">          if err := pusher.Stop(doneCtx); err != nil {</p>
			<p class="source-code">               otel.Handle(err)</p>
			<p class="source-code">          }</p>
			<p class="source-code">     }</p>
			<p class="source-code">}</p>
			<p>In <strong class="source-inline">initMetrics()</strong>, we create a new <strong class="source-inline">metricClient</strong> to transmit metrics from the client to the OTel Collector<a id="_idIndexMarker952"/> in the OTLP format. After setting up the <strong class="source-inline">metricClient</strong>, we then create <strong class="source-inline">pusher</strong> to manage the export of the metrics<a id="_idIndexMarker953"/> to the OTel Collector, register <strong class="source-inline">pusher</strong> as the global <strong class="source-inline">MeterProvider</strong>, and start <strong class="source-inline">pusher</strong> to export metrics to the OTel Collector. Finally, we create a closure<a id="_idIndexMarker954"/> to shut down <strong class="source-inline">pusher</strong>. Now, let's move on to explore <strong class="source-inline">continuouslySendRequests()</strong> from client's <strong class="source-inline">main.go</strong>:</p>
			<p class="source-code">func continuouslySendRequests() {</p>
			<p class="source-code">     var (</p>
			<p class="source-code">          meter        = global.Meter("demo-client-meter")</p>
			<p class="source-code">          instruments  = NewClientInstruments(meter)</p>
			<p class="source-code">          commonLabels = []attribute.KeyValue{</p>
			<p class="source-code">               attribute.String("method", "repl"),</p>
			<p class="source-code">               attribute.String("client", "cli"),</p>
			<p class="source-code">          }</p>
			<p class="source-code">          rng = rand.New(rand.NewSource(time.Now().UnixNano()))</p>
			<p class="source-code">     )</p>
			<p class="source-code">     for {</p>
			<p class="source-code">          startTime := time.Now()</p>
			<p class="source-code">          ctx, span := tracer.Start(context.Background(), "ExecuteRequest")</p>
			<p class="source-code">          makeRequest(ctx)</p>
			<p class="source-code">          span.End()</p>
			<p class="source-code">          latencyMs := float64(time.Since(startTime)) / 1e6</p>
			<p class="source-code">          nr := int(rng.Int31n(7))</p>
			<p class="source-code">          for i := 0; i &lt; nr; i++ {</p>
			<p class="source-code">               randLineLength := rng.Int63n(999)</p>
			<p class="source-code">               meter.RecordBatch(</p>
			<p class="source-code">                    ctx,</p>
			<p class="source-code">                    commonLabels,</p>
			<p class="source-code">                    instruments.LineCounts.Measurement(1),</p>
			<p class="source-code">                    instruments.LineLengths.Measurement(</p>
			<p class="source-code">  randLineLength</p>
			<p class="source-code">),</p>
			<p class="source-code">               )</p>
			<p class="source-code">               fmt.Printf("#%d: LineLength: %dBy\n", i, randLineLength)</p>
			<p class="source-code">          }</p>
			<p class="source-code">          meter.RecordBatch(</p>
			<p class="source-code">               ctx,</p>
			<p class="source-code">               commonLabels,</p>
			<p class="source-code">               instruments.RequestLatency.Measurement(</p>
			<p class="source-code">  latencyMs</p>
			<p class="source-code">),</p>
			<p class="source-code">               instruments.RequestCount.Measurement(1),</p>
			<p class="source-code">          )</p>
			<p class="source-code">          fmt.Printf("Latency: %.3fms\n", latencyMs)</p>
			<p class="source-code">          time.Sleep(time.Duration(1) * time.Second)</p>
			<p class="source-code">     }</p>
			<p class="source-code">}</p>
			<p>We first create a metrics meter with the name <strong class="source-inline">demo-client-meter</strong>, metric instruments to be used to measure<a id="_idIndexMarker955"/> metrics in this function, and a set of common<a id="_idIndexMarker956"/> labels to be added to the metrics<a id="_idIndexMarker957"/> collected. These labels enable scoped querying of metrics. After initializing the random number generator for artificial latency, the client enters the <strong class="source-inline">for</strong> loop, stores the start time of the request, makes a request to the server, and stores the duration of <strong class="source-inline">makeRequest</strong> as the latency in milliseconds. Following the execution of <strong class="source-inline">makeRequest</strong>, the client executes a random number of iterations between 0 and 7 to generate a random line length, recording a batch of metric events during each iteration, and measuring the count of executions and the random line length. Finally, the client records a batch of metric events, measuring the latency of <strong class="source-inline">makeRequest</strong> and a count for one request. </p>
			<p>So, how did we define the instruments used in the preceding code? Let's explore <strong class="source-inline">NewClientInstruments</strong> and learn how to define counter and histogram instruments:</p>
			<p class="source-code">func NewClientInstruments(meter metric.Meter) </p>
			<p class="source-code">ClientInstruments {</p>
			<p class="source-code">     return ClientInstruments{</p>
			<p class="source-code">          RequestLatency: metric.Must(meter).</p>
			<p class="source-code">               NewFloat64Histogram(</p>
			<p class="source-code">                    "demo_client/request_latency",</p>
			<p class="source-code">                    metric.WithDescription("The latency of requests processed"),</p>
			<p class="source-code">               ),</p>
			<p class="source-code">          RequestCount: metric.Must(meter).</p>
			<p class="source-code">               NewInt64Counter(</p>
			<p class="source-code">                    "demo_client/request_counts",</p>
			<p class="source-code">                    metric.WithDescription("The number of requests processed"),</p>
			<p class="source-code">               ),</p>
			<p class="source-code">          LineLengths: metric.Must(meter).</p>
			<p class="source-code">               NewInt64Histogram(</p>
			<p class="source-code">                    "demo_client/line_lengths",</p>
			<p class="source-code">                    metric.WithDescription("The lengths of the various lines in"),</p>
			<p class="source-code">               ),</p>
			<p class="source-code">          LineCounts: metric.Must(meter).</p>
			<p class="source-code">               NewInt64Counter(</p>
			<p class="source-code">                    "demo_client/line_counts",</p>
			<p class="source-code">                    metric.WithDescription("The counts of the lines in"),</p>
			<p class="source-code">               ),</p>
			<p class="source-code">     }</p>
			<p class="source-code">}</p>
			<p><strong class="source-inline">NewClientInstruments()</strong> takes a meter and returns a struct of instruments used by the client. An instrument<a id="_idIndexMarker958"/> is used to record and aggregate<a id="_idIndexMarker959"/> measurements. This func sets<a id="_idIndexMarker960"/> up the two <strong class="source-inline">Int64Counter</strong> and <strong class="source-inline">Int64Histogram</strong> instruments. Each instrument is defined with a well-described name for easier analysis in the backend metric system. The <strong class="source-inline">Int64Counter</strong> instrument will monotonically increase and <strong class="source-inline">Int64Histogram</strong> will record <strong class="source-inline">int64</strong> the values and pre-aggregate values before pushing to the metrics backend. </p>
			<p>Now that we have covered the client, let's look at the server's <strong class="source-inline">main.go</strong>:</p>
			<p class="source-code">func main() {</p>
			<p class="source-code">     shutdown := initProvider()</p>
			<p class="source-code">     defer shutdown()</p>
			<p class="source-code">     // create a handler wrapped in OpenTelemetry instrumentation</p>
			<p class="source-code">     handler := handleRequestWithRandomSleep()</p>
			<p class="source-code">     wrappedHandler := otelhttp.NewHandler(handler, "/hello")</p>
			<p class="source-code">     http.Handle("/hello", wrappedHandler)</p>
			<p class="source-code">     http.ListenAndServe(":7080", nil)</p>
			<p class="source-code">}</p>
			<p>The server's <strong class="source-inline">main.go</strong> calls <strong class="source-inline">initProvider()</strong> and <strong class="source-inline">shutdown()</strong> in a similar manner to the client's <strong class="source-inline">main.go</strong>. The interesting<a id="_idIndexMarker961"/> metric measures<a id="_idIndexMarker962"/> happen within <strong class="source-inline">handleRequestWithRandomSleep()</strong>. Next, let's export<a id="_idIndexMarker963"/> <strong class="source-inline">handleRequestWithRandomSleep()</strong>:</p>
			<p class="source-code">func handleRequestWithRandomSleep() http.HandlerFunc {</p>
			<p class="source-code">     var (</p>
			<p class="source-code">          meter        = global.Meter("demo-server-meter")</p>
			<p class="source-code">          instruments  = NewServerInstruments(meter)</p>
			<p class="source-code">          commonLabels = []attribute.KeyValue{</p>
			<p class="source-code">               attribute.String("server-attribute", "foo"),</p>
			<p class="source-code">          }</p>
			<p class="source-code">     )</p>
			<p class="source-code">     return func(w http.ResponseWriter, req *http.Request) {</p>
			<p class="source-code">          var sleep int64</p>
			<p class="source-code">          switch modulus := time.Now().Unix() % 5; modulus {</p>
			<p class="source-code">          case 0:</p>
			<p class="source-code">               sleep = rng.Int63n(2000)</p>
			<p class="source-code">          case 1:</p>
			<p class="source-code">               sleep = rng.Int63n(15)</p>
			<p class="source-code">          case 2:</p>
			<p class="source-code">               sleep = rng.Int63n(917)</p>
			<p class="source-code">          case 3:</p>
			<p class="source-code">               sleep = rng.Int63n(87)</p>
			<p class="source-code">          case 4:</p>
			<p class="source-code">               sleep = rng.Int63n(1173)</p>
			<p class="source-code">          }</p>
			<p class="source-code">          time.Sleep(time.Duration(sleep) * time.Millisecond)</p>
			<p class="source-code">          ctx := req.Context()</p>
			<p class="source-code">          meter.RecordBatch(</p>
			<p class="source-code">               ctx,</p>
			<p class="source-code">               commonLabels,</p>
			<p class="source-code">               instruments.RequestCount.Measurement(1),</p>
			<p class="source-code">          )</p>
			<p class="source-code">          span := trace.SpanFromContext(ctx)</p>
			<p class="source-code">          span.SetAttributes(commonLabels...)</p>
			<p class="source-code">          w.Write([]byte("Hello World"))</p>
			<p class="source-code">     }</p>
			<p class="source-code">}</p>
			<p>In the preceding code, <strong class="source-inline">handleRequestWithRandomSleep()</strong> creates a named meter from the global OTel context, initializes the server<a id="_idIndexMarker964"/> instruments in a similar way<a id="_idIndexMarker965"/> to the client example, and defines<a id="_idIndexMarker966"/> a slice of custom attributes. Finally, the function returns a handler function, which introduces a random sleep and records the request count.</p>
			<p>The result is viewable in Prometheus at <strong class="source-inline">http://localhost:9090/graph?g0.expr=rate(demo_server_request_counts%5B2m%5D)&amp;g0.tab=0&amp;g0.stacked=0&amp;g0.show_exemplars=0&amp;g0.range_input=1h</strong>:</p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="Images/B17626_09_005.jpg" alt="Figure 9.5 – The Prometheus server request rate&#13;&#10;" width="1346" height="871"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5 – The Prometheus server request rate</p>
			<p>In the preceding screenshot, you can see the average<a id="_idIndexMarker967"/> requests per second for the server<a id="_idIndexMarker968"/> application in Prometheus. At the bottom<a id="_idIndexMarker969"/> of the screenshot, you will see the common labels and other associated metadata that was added in the server <strong class="source-inline">main.go</strong>. Prometheus provides a powerful query language to analyze and alert on metrics. Take some time and explore what you can do in the Prometheus UI. If you'd like to learn more about Prometheus, see <a href="https://prometheus.io/docs/introduction/overview/">https://prometheus.io/docs/introduction/overview/</a>.</p>
			<p>In this section, we learned how to instrument a Go application, export metrics to the OTel Collector, configure Prometheus to scrape metrics from the OTel Collector, and start to analyze metrics telemetry in Prometheus. With these newly gained skills, you will be able to understand more about the runtime characteristics of your applications.</p>
			<p>Next up, let’s look at how you can add alerting when your metrics are showing abnormalities that could indicate a problem.</p>
			<h1 id="_idParaDest-238"><a id="_idTextAnchor493"/>Alerting on metrics abnormalities</h1>
			<p>Metrics provide time-series measurements<a id="_idIndexMarker970"/> of the behavior of our applications and infrastructure, but they provide no notification when those measurements deviate from the expected behavior of our applications. To be able to react to abnormal behaviors in our applications, we need to establish rules about what is normal behavior in our applications and how we can be notified when our applications deviate from that behavior. </p>
			<p>Alerting on metrics enables us to define behavioral norms and specify how we should be notified when our applications exhibit abnormal behavior. For example, if we expect HTTP responses from our application to respond in under 100 milliseconds and we observe a time span of 5 minutes when our application is responding in greater than 100 milliseconds, we would want to be notified of the deviation from the expected behavior.</p>
			<p>In this section, we will learn how to extend<a id="_idIndexMarker971"/> our current configuration of services to include an Alertmanager (<a href="https://prometheus.io/docs/alerting/latest/alertmanager/">https://prometheus.io/docs/alerting/latest/alertmanager/</a>) service to provide alerts when observed behavior deviates from expected norms. We'll learn how to define alerting rules and specify where to send those notifications when our application experiences abnormal behaviors.</p>
			<p>The code for this section is here: <a href="https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/9/alerting">https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/9/alerting</a>.</p>
			<h2 id="_idParaDest-239"><a id="_idTextAnchor494"/>Adding and configuring Alertmanager</h2>
			<p>We will start by<a id="_idIndexMarker972"/> adding the Alertmanager service to the <strong class="source-inline">docker-compose.yaml</strong> file. Let's look at the updates<a id="_idIndexMarker973"/> needed to add Prometheus to the <strong class="source-inline">docker-compose.yaml</strong> file:</p>
			<p class="source-code">version: "2"</p>
			<p class="source-code">services:</p>
			<p class="source-code">  # omitted previous configurations</p>
			<p class="source-code">  prometheus:</p>
			<p class="source-code">    container_name: prometheus</p>
			<p class="source-code">    image: prom/prometheus:latest</p>
			<p class="source-code">    volumes:</p>
			<p class="source-code">      - ./prometheus.yaml:/etc/prometheus/prometheus.yml</p>
			<p class="source-code">      - ./rules:/etc/prometheus/rules</p>
			<p class="source-code">    ports:</p>
			<p class="source-code">      - "9090:9090"</p>
			<p class="source-code">  alertmanager:</p>
			<p class="source-code">    container_name: alertmanager</p>
			<p class="source-code">    image: prom/alertmanager:latest</p>
			<p class="source-code">    restart: unless-stopped</p>
			<p class="source-code">    ports:</p>
			<p class="source-code">      - "9093:9093"</p>
			<p class="source-code">    volumes:</p>
			<p class="source-code">      - ./alertmanager.yml:/config/alertmanager.yaml</p>
			<p class="source-code">      - alertmanager-data:/data</p>
			<p class="source-code">    command: --config.file=/config/alertmanager.yaml -- log.level=debug</p>
			<p class="source-code">volumes:</p>
			<p class="source-code">  alertmanager-data:</p>
			<p>As you can see from the preceding, we have added<a id="_idIndexMarker974"/> a <strong class="source-inline">rules</strong> folder to the <strong class="source-inline">prometheus</strong> service, a new service<a id="_idIndexMarker975"/> called <strong class="source-inline">alertmanager</strong>, and a volume to store the <strong class="source-inline">alertmanager</strong> data called <strong class="source-inline">alertmanager-data</strong>. We will discuss the Prometheus <strong class="source-inline">./rules</strong> volume mount and contents later in this section, but for now, know that it contains our alerting rules for Prometheus. The new <strong class="source-inline">alertmanager</strong> service exposes an HTTP endpoint at <strong class="source-inline">http://localhost:9093</strong> and mounts an <strong class="source-inline">alertmanager.yml</strong> configuration as well as a data directory. Next, let's explore the contents of the <strong class="source-inline">alertmanager.yml</strong> file to see how Alertmanager is configured:</p>
			<p class="source-code">route:</p>
			<p class="source-code">  receiver: default</p>
			<p class="source-code">  group_by: [ alertname ]</p>
			<p class="source-code">  routes:</p>
			<p class="source-code">    - match:</p>
			<p class="source-code">        exported_job: demo-server</p>
			<p class="source-code">      receiver: demo-server</p>
			<p class="source-code">receivers:</p>
			<p class="source-code">  - name: default</p>
			<p class="source-code">    pagerduty_configs:</p>
			<p class="source-code">      - service_key: "**Primary-Integration-Key**"</p>
			<p class="source-code">  - name: demo-server</p>
			<p class="source-code">    pagerduty_configs:</p>
			<p class="source-code">      - service_key: "**Server-Team-Integration-Key**"</p>
			<p>Alertmanager configuration<a id="_idIndexMarker976"/> consists mainly of routes and receivers. A route describes<a id="_idIndexMarker977"/> where to send an alert based on it either being default or by some criteria. For example, we have a default route and a specialized route in the preceeding Alertmanager configuration. The default route will send alerts to the default receiver if they do not match <strong class="source-inline">exported_job</strong> attribute with the value "<strong class="source-inline">demo-server"</strong>. If alerts match the <strong class="source-inline">exported_job</strong> attribute with value <strong class="source-inline">"demo-server"</strong>, they are routed to the <strong class="source-inline">demo-server</strong> receiver, described in the receivers section. </p>
			<p>In this example of Alertmanager receivers, we<a id="_idIndexMarker978"/> are using PagerDuty (<a href="https://www.pagerduty.com">https://www.pagerduty.com</a>), but there are many other receivers that can be configured. For example, you can configure receivers for Slack, Teams, Webhooks, and so on. Note that the <strong class="source-inline">service_key</strong> values for each of the receivers requires a PagerDuty integration key, which can be set up by following the docs for integrating Prometheus with PagerDuty (<a href="https://www.pagerduty.com/docs/guides/prometheus-integration-guide/">https://www.pagerduty.com/docs/guides/prometheus-integration-guide/</a>). If you wish to use<a id="_idIndexMarker979"/> another receiver such as email, feel free to mutate the receivers with email by following<a id="_idIndexMarker980"/> the Prometheus guide for email configuration (<a href="https://prometheus.io/docs/alerting/latest/configuration/#email_config">https://prometheus.io/docs/alerting/latest/configuration/#email_config</a>).</p>
			<p>Next, we will look at the changes that we need to make to the Prometheus configuration in <strong class="source-inline">./prometheus.yaml</strong> to make Prometheus aware of the Alertmanager service and the rules for sending alerts to the Alertmanager service:</p>
			<p class="source-code">scrape_configs:</p>
			<p class="source-code">  - job_name: 'otel-collector'</p>
			<p class="source-code">    scrape_interval: 10s</p>
			<p class="source-code">    static_configs:</p>
			<p class="source-code">      - targets: ['otel-collector:8889']</p>
			<p class="source-code">      - targets: ['otel-collector:8888']</p>
			<p class="source-code">alerting:</p>
			<p class="source-code">  alertmanagers:</p>
			<p class="source-code">    - scheme: http</p>
			<p class="source-code">      static_configs:</p>
			<p class="source-code">        - targets: [ 'alertmanager:9093' ]</p>
			<p class="source-code">rule_files:</p>
			<p class="source-code">  - /etc/prometheus/rules/*</p>
			<p>In the preceding <strong class="source-inline">./prometheus.yaml</strong>, we see the original <strong class="source-inline">scrape_config</strong> and two new keys, <strong class="source-inline">alerting</strong> and <strong class="source-inline">rule_files</strong>. The <strong class="source-inline">alerting</strong> key describes the <strong class="source-inline">alertmanager</strong> services to send alerts and the connection<a id="_idIndexMarker981"/> details for connecting to those services. The <strong class="source-inline">rules_files</strong> key describes<a id="_idIndexMarker982"/> the glob rules for selecting files containing alerting rules. These rules can be set up in the Prometheus UI, but it is good practice to define these rules declaratively in code so that they are clear and visible to the rest of your team as source code. </p>
			<p>Next, let's look at the <strong class="source-inline">rules</strong> file and see how we describe rules for alerting in <strong class="source-inline">./rules/demo-server.yml</strong>:</p>
			<p class="source-code">groups:</p>
			<p class="source-code">  - name: demo-server</p>
			<p class="source-code">    rules:</p>
			<p class="source-code">      - alert: HighRequestLatency</p>
			<p class="source-code">        expr: |</p>
			<p class="source-code">          histogram_quantile(0.5, rate(http_server_duration_bucket{exported_job="demo-server"}[5m])) &gt; 200000</p>
			<p class="source-code">        labels:</p>
			<p class="source-code">          severity: page</p>
			<p class="source-code">        annotations:</p>
			<p class="source-code">          summary: High request latency</p>
			<p>Rules in <strong class="source-inline">rule_files</strong> are categorized into groups. In the preceding example, we can see a single group named <strong class="source-inline">demo-server</strong> specifying a single rule named <strong class="source-inline">HighRequestLatency</strong>. The rule specifies an expression, which is a Prometheus query. The preceding query triggers when the mean request latency is exceeding 200,000 microseconds, or 0.2 seconds. The alert is triggered with a severity<a id="_idIndexMarker983"/> label of <strong class="source-inline">page</strong> and an annotation<a id="_idIndexMarker984"/> summary of <strong class="source-inline">High request latency</strong>. </p>
			<p>Now, let's run the following to start the services:</p>
			<p class="source-code">$ docker-compose up -d</p>
			<p>After the services start, we should see the following in Prometheus at <strong class="source-inline">http://localhost:9090/alerts</strong>:</p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="Images/B17626_09_006.jpg" alt="Figure 9.6 – The Prometheus alert for HighRequestLatency&#13;&#10;" width="828" height="442"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6 – The Prometheus alert for HighRequestLatency</p>
			<p>The preceding screenshot shows the alert rules registered in Prometheus. As you can see, the <strong class="source-inline">HighRequestLatency</strong> alert is registered with the command we configured in the <strong class="source-inline">./rules/demo-server</strong> file.</p>
			<p>After roughly 5 minutes of running the service, you should see the following:</p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="Images/B17626_09_007.jpg" alt="Figure 9.7 – The Prometheus alert for HighRequestLatency triggered&#13;&#10;" width="1046" height="384"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – The Prometheus alert for HighRequestLatency triggered</p>
			<p>In the preceding screenshot, you can see the triggered alert for <strong class="source-inline">HighRequestLatency</strong>. This is Prometheus<a id="_idIndexMarker985"/> triggering the alert for the mean request latency<a id="_idIndexMarker986"/> rising above 0.2 seconds. This will then trigger an alert that is sent to the Alertmanager which delegates to the appropriate receiver. The receiver will then send the alert on to the service configured to notify PagerDuty or, perhaps, another receiver you have configured. You have now established a flow for alerting yourself or others on your team that your application has entered into an aberrant state of behavior.</p>
			<p>In this section, you learned to configure Prometheus alerting rules, deploy Alertmanager, and configure Alertmanager to send alerts to the notification service of your choice. With this knowledge, you should be able to establish rules for defining the normative behavior of your applications and alert you or your team when an application is behaving outside of those bounds.</p>
			<p>Alerting is a key component of reacting to aberrant behaviors in applications. With proper metrics in place, you are now empowered to proactively respond when your applications are not meeting expectations, rather than responding to customer complaints.</p>
			<h1 id="_idParaDest-240"><a id="_idTextAnchor495"/>Summary</h1>
			<p>In this chapter, we explored the basics of OpenTelemetry, how to instrument your applications and infrastructure, and how to export that telemetry into backend visualization and analysis tools such as Jaeger and Prometheus. We also extended the benefits of metrics by integrating alerting rules to proactively notify us when an application is operating outside of expected behavioral parameters. With the application of what you have learned, you will never be caught blind during a support call. You will have the data to diagnose and resolve issues in your complex system. Better yet, you will know about these problems before issues are raised by your customers.</p>
			<p>We also established some relatively simple metrics, traces, and alerts. With this knowledge, you will be able to implement your own traces, metrics, and alerts to empower you and your team to react quickly and efficiently to failures in production.</p>
			<p>In the next chapter, we will discuss how to automate workflows with GitHub Actions. We will learn about the basics of GitHub actions and build upon that to create our own Go-based GitHub actions to empower you to author any automation allowable by a Turing-complete language. </p>
		</div>
	</div></body></html>