<html><head></head><body>
		<div id="_idContainer140">
			<h1 id="_idParaDest-149" class="hapter-number"><a id="_idTextAnchor204"/>11</h1>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor205"/>MLOps and DataOps</h1>
			<p class="author-quote">The future will soon be a thing of the past.</p>
			<p class="author-quote">– George Carlin</p>
			<p>I was going to use a bit of generative AI to write the content of this chapter but that would make it more of an autobiography than a technical book, since then the generative AI would become a co-author. I will try to be a very objective observer and do some justice to this hot topic. I hope this will give me a favorable position in a possible future ruled by AI. There are many reasons why there has been such great progress in the field of machine learning and AI in the past century: Noam Chomsky, Alan Turing, the creation of computers themselves, science fiction novels, and man’s eternal longing for new life in the universe. But in the past few years, the delivery of these concepts as viable products and services has required the DevOps touch. After all, how else do you think <strong class="bold">ChatGPT</strong> manages to instantly answer your four-paragraph questions in about five seconds and does it for a million more people <span class="No-Break">every minute?</span></p>
			<p>Now, all of the chapters and concepts in this book have gotten us back to one fact, which is that DevOps is about delivering value and making things work. And that is no different when the focus of DevOps is data. In such cases, Python becomes even more useful because it is the language of the data operator. Most people and development environments for data usually default to Python these days because of the existence of the necessary tools for data processing and analysis. Most effective <strong class="bold">DataOps</strong> workloads will use Python in some capacity. A lot of them will use Python both on the running script as well as any supporting operational scripts that they may need to write. We will also talk about <strong class="bold">MLOps</strong> and the operations that help deliver and optimize machine learning models and algorithms. We will talk about all of this and more in this chapter after you have read through the <em class="itali">Technical </em><span class="No-Break"><em class="itali">requirements</em></span><span class="No-Break"> section.</span></p>
			<p>To summarize, in this chapter, you will learn <span class="No-Break">the following:</span></p>
			<ul>
				<li>The difference in the approach taken when DataOps/MLOps is in play as opposed to <span class="No-Break">regular DevOps</span></li>
				<li>The approaches to deal with a variety of different <span class="No-Break">data-based </span><span class="No-Break"><a id="_idIndexMarker351"/></span><span class="No-Break">challenges</span></li>
				<li>The Ops behind the delivery of ChatGPT to your <span class="No-Break">computer screen</span></li>
			</ul>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor206"/>Technical requirements</h1>
			<p>Here are some requirements that will help you follow along with this <span class="No-Break">chapter’s activities:</span></p>
			<ul>
				<li>A GitHub account and access to this book’s Git <span class="No-Break">repository (</span><a href="https://github.com/PacktPublishing/Hands-On-Python-for-DevOps"><span class="No-Break">https://github.com/PacktPublishing/Hands-On-Python-for-DevOps</span></a><span class="No-Break">)</span></li>
				<li>A Google account to use <span class="No-Break">Google Colab</span></li>
				<li>A usable Python <span class="No-Break">environment somewhere</span></li>
				<li>A nice cup of your <span class="No-Break">favorite beverage</span></li>
			</ul>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor207"/>How MLOps and DataOps differ from regular DevOps</h1>
			<p>A question that we <a id="_idIndexMarker352"/>often encounter in any sort of technical industry<a id="_idIndexMarker353"/> in general is: what is the difference between a data role and a non-data role? What would be the difference between a software and data engineer, a data analyst and an accountant, or a DJ and a music composer? It is something employers ask a lot; people speculate on whether one is a subgroup of another or whether they are completely different. Even in the Swedish language, <em class="itali">dator</em> means “computer,” science is translated as <em class="itali">vetenskap</em>, and computer science is referred to as <em class="itali">datavetenskap,</em> so at some point whatever entity that designs and updates the Swedish language thought that there was very little to distinguish between <span class="No-Break">the two.</span></p>
			<p>We will now explain this through a couple of common DevOps use cases that can be applied and used in these more narrowed fields of DataOps and MLOps. For DataOps, we will go through a method that is simple but has saved me a lot of data concatenation operations in Python when using JSON files. For MLOps, we will focus on the GPU side, which is the primary hardware that an MLOps engineer may have to <span class="No-Break">work with.</span></p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor208"/>DataOps use case – JSON concatenation</h2>
			<p>This is quite a simple little trick, but one that is not as commonly known as you think. I honestly think if I can help even one person working with data with this section, I will have succeeded. The manipulation of JSON is a very important aspect of data operations that is very prominent, especially <a id="_idIndexMarker354"/>in NoSQL use cases, but also in a number of other cases. The ability to naturally manipulate JSON gives Python a major advantage over a lot of other programming languages. One of the most useful applications of this is the pipe (<strong class="sour e-inline">|</strong>) operator. This little operator can be used to perform concatenations, unions, and even bitwise operations on numbers. It is one of the many ways in which Python has made it easier to perform these small data operations for ease <span class="No-Break">of use.</span></p>
			<p>So, we will begin with just the function for the concatenation of one JSON <span class="No-Break">with another:</span></p>
			<pre class="sour e- ode">a = {"one":1, "two":2}b = {"one":"one", "two":2, "three":3}print(a|b)</pre>
			<p>That’s it. That’s the code, and here’s the output of <span class="No-Break">that code:</span></p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B21320_11_1.jpg" alt="Figure 11.1 – Output of JSON concatenation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Output of JSON concatenation</p>
			<p>You’ll see that the second JSON’s value for a key overrides the value from the first JSON and that if they have the same common values, they will stay the same, and any additional values are combined into the overall JSON. So, with all that in mind, whenever you encounter such a problem with JSON combination (and it can come up quite often), you will have this little trick in your toolbelt. Now, let’s move on to another trick, one that will certainly help all you gaming hardware addicts out there. It’ll help the rest of you out too, but I like mentioning hardware addicts because they make the most YouTube videos and I’m hoping to get some of that <span class="No-Break">sweet exposure.</span></p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor209"/>MLOps use case – overclocking a GPU</h2>
			<p>In this modern age of AI art, image <a id="_idIndexMarker355"/>generation at the highest levels can require a lot of processing power. For any kind of graphical rendering, CPUs are only used when no other options are available and are usually not recommended for larger renderings. For machine learning TensorFlow algorithms, Google’s proprietary TPUs are the norm. But again, for anything concerning image generation or manipulation, a good GPU is good to have. And if the rare case comes up where that GPU needs a bit of extra juice to get things done, overclocking can <span class="No-Break">be necessary.</span></p>
			<p>A lot of the time, GPU processors have their own drivers and with their drivers come their own command-line tools. Executing these before and after the use of overclocking or another GPU feature can be a hassle. Instead, using Python’s in-built <strong class="sour e-inline">subprocess</strong> module, we can automatically overclock or perform any other GPU processes that we would like. For this example, we are going to use the CLI tools for NVIDIA, which is probably the most popular GPU brand available at the moment. NVIDIA has a command-line tool called <strong class="bold">nvidia-smi</strong>, which also contains an <a id="_idIndexMarker356"/>overclocking feature and is what we are going to invoke. Now, let’s write the code block that will help us overclock <span class="No-Break">our GPUs:</span></p>
			<pre class="sour e- ode">import subprocessdef overclock_gpu():    # Set the new clock frequency for memory and graphics    new_clock_memory= &lt;your_clock_frequency_in_MHz&gt;    new_clock_graphics= &lt;your_clock_frequency_in_MHz&gt;    # Run NVIDIA command to overclock GPU    command = "nvidia-smi –i 0 --applications-clocks {new_clock_memory},{new_clock_graphics}"    subprocess.run(command, shell=True)if __name__ == "__main__":    overclock_gpu()</pre>
			<p>The preceding code, when run, will overclock whichever NVIDIA GPU has been set up on your device. This, in turn, will make processes such as image processing and generation faster. This can be useful when there is a higher demand for these resources, and it isn’t possible to shift those demands to other resources. So, this code can be used to temporarily overclock a GPU based on some condition that may cause it to be called. Once it has been overclocked, you <a id="_idIndexMarker357"/>can set it back to its default by running the following command (in or out <span class="No-Break">of script):</span></p>
			<pre class="onsole">nvidia-smi –- reset-applications-clocks</pre>
			<p>So, this is how you would manipulate GPUs using Python. A lot of this section has involved learning how to manipulate data and the aspects surrounding data. However, the data itself can be difficult to work with for a variety of other reasons as well. One of the primary reasons can be just how much data is there, which can be a lot. The next section will be all about finding ways to not be overwhelmed by all of the data that comes from various sources that you may have to <span class="No-Break">deal with.</span></p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor210"/>Dealing with velocity, volume, and variety</h1>
			<p>When given any tutorial on how to process data, you are usually given a quick introduction to the <strong class="bold">three Vs</strong> (<strong class="bold">velocity</strong>, <strong class="bold">volume</strong>, and <strong class="bold">variety</strong>). These are the three ways in which the complexity of data can scale. Each of them presents a singularly unique problem when dealing with data, and a<a id="_idIndexMarker358"/> lot of data that you would have to deal with can be a combination of all three. Velocity is the speed of data coming in over a period of time, volume is the amount of data, and variety is the diversity of the data <span class="No-Break">being presented.</span></p>
			<p>So, this section will be divided according to the three Vs, and in each subsection, there will be a solution for a common problem that may arise with them. This way, you will get to see how Python can help in dealing with such massive amounts of data. Let’s start with volume as it is the simplest and probably<a id="_idIndexMarker359"/> the first thing that comes to people’s minds when it comes to <span class="No-Break"><strong class="bold">big data</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor211"/>Volume</h2>
			<p>The volume of data is a pretty<a id="_idIndexMarker360"/> simple thing. It represents a certain quantity of data, most, if not all, of which will be of the same type. If we are going to deal with a large volume of data, it will require understanding the time sensitivity of data as well as the resources that we wo<a id="_idTextAnchor212"/>uld have on hand. The volume of data that is usually processed differs based on whether the data is massive based on width or length (i.e., whether there are a lot of fields for one row of data or there is a massive number of data rows). Both of these require different solutions, even specialized databases sometimes. There is also the possibility of datasets not being numbers and letters at all but instead being files of audio or video. In this section, we will use an example that will be very useful when we have a database or data file that contains a large number <span class="No-Break">of fields/columns.</span></p>
			<p>To start, we will need a high-volume <a id="_idIndexMarker361"/>dataset, so we will use an app called <strong class="bold">Mockaroo</strong>, which allows you to<a id="_idIndexMarker362"/> generate data fields and sample data using generative AI (very fitting in this chapter). Let’s go to the Mockaroo site and generate a few fields for our <span class="No-Break">sample data:</span></p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/B21320_11_2.jpg" alt="Figure 11.2 – Mockaroo schema"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Mockaroo schema</p>
			<p>The dataset we produced with Mockaroo looks like <span class="No-Break">the following:</span></p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/B21320_11_3.jpg" alt="Figure 11.3 – Sample CSV created by Mockaroo"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – Sample CSV created by Mockaroo</p>
			<p>The preceding figure shows just a small piece of it; it’s 20 very large fields for 1,000 rows. Let’s write the script to parse <span class="No-Break">through it:</span></p>
			<pre class="sour e- ode">import csvdef read_large_csv(file_path):    with open(file_path, 'r') as csv_file:        csv_reader = csv.reader(csv_file)        next(csv_reader, None)        for row in csv_reader:            yield rowcsv_file_path = 'MOCK_DATA.csv'for row in read_large_csv(csv_file_path):    print(row)</pre>
			<p>The script may seem a little redundant in terms of reading the CSV file, but the reason it is like this is so that all of the rows in the CSV aren’t loaded into the memory of the OS at the same time. This method <a id="_idIndexMarker363"/>will reduce the load on the memory of the data and is a great way to read large amounts of data in a system where the memory can’t hold a lot of data. What it does is that it reads one row of the data and then releases that data from the memory before reading the other rows. This is efficient management of memory during the reading of a high volume of data, which in turn makes the reading a lot faster and smoother, as demonstrated in <span class="No-Break">this diagram:</span></p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/B21320_11_4.jpg" alt="Figure 11.4 – Workflow behind a generator"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – Workflow behind a generator</p>
			<p>Now, that was simple enough, but <a id="_idIndexMarker364"/>what happens when it’s just one row at a time, but constant, such as streaming data? All of it needs to be processed live as it comes in. How would we achieve this? Let’s <span class="No-Break">find out.</span></p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor213"/>Velocity</h2>
			<p>Tackling the velocity of data is a<a id="_idIndexMarker365"/> legitimate billion-dollar question. Even today, the biggest video streamers struggle to send out livestream data consistently. Of course, there are a number of reasons for this, but the fact is a lot of solutions don’t have the right combination of budget and quality to be consistent all of the time. We can get pretty <span class="No-Break">close, though.</span></p>
			<p>In this exercise, we will be using something that a lot of people call the future, and perhaps the present, of data <a id="_idIndexMarker366"/>streaming: <strong class="bold">Apache Flink</strong>. This is a stream and batch processing framework developed by the Apache Software Foundation for a smooth, fast data flow. Unlike a lot of frameworks managed by the Apache Software Foundation, this one was created with the express intent of being maintained by them as opposed to a project created by a company and made open source for <span class="No-Break">easier maintenance.</span></p>
			<p>Flink itself does not offer any data storage solutions and is instead simply supposed to process incoming data into<a id="_idIndexMarker367"/> a storage location. It has APIs in Java, Python, and Scala, and support on all <span class="No-Break">cloud platforms.</span></p>
			<p>To start with Python, you will need to install <strong class="sour e-inline">pyflink</strong> using the <span class="No-Break">following command:</span></p>
			<pre class="onsole">pip install apache-flink</pre>
			<p>Also install pandas if you <span class="No-Break">have not:</span></p>
			<pre class="onsole">pip install pandas</pre>
			<p>Alright, now let’s write some code to stream data from a bunch of JSON rows to a CSV table. This is just a sample program to show Flink’s workflow, but it does serve that purpose <span class="No-Break">rather effectively:</span></p>
			<pre class="sour e- ode">from pyflink.common import Rowfrom pyflink.datastream import StreamExecutionEnvironmentfrom pyflink.table import StreamTableEnvironment, DataTypesfrom pyflink.table.descriptors import FileSystem, Json, Schemaimport pandas as pd#Function to usedef flink_input(input_data):    # Set up the Flink environment    env = StreamExecutionEnvironment.get_execution_environment()    t_env = StreamTableEnvironment.create(env)    # Define the CSV file to output to along with temporary table name    t_env.connect(FileSystem().path('output.csv')) \        .with_format(Json().fail_on_missing_field(True)) \        .with_schema(Schema().field('data', DataTypes.STRING())) \        .create_temporary_table('output_table')    # Convert multiple JSON values into PyFlink CSV rows    input_rows = [Row(json.dumps(json_obj)) for json_obj in input_data]    df = pd.DataFrame([r[0] for r in input_rows], columns=['data'])    # Insert the rows into the output table which in turn inserts them into the CSV file    t_env.from_pandas(df).insert_into('output_table')    # Execute the Flink job    env.execute('CSVJob')input_data = [{'key1': 'value1'}, {'key2': 'value2'}, {'key3': 'value3'}]flink_input(input_data)</pre>
			<p>In this code, you’ll see that<a id="_idIndexMarker368"/> the JSON rows are inserted into a CSV using a temporary table as a holdover for insertion. This temporary table, when inserted, also inserts the data into the <span class="No-Break">CSV file.</span></p>
			<p>This is a rather simple explanation of the capabilities of Flink, whose job is to work with essentially the same context, but for millions of bits of streaming data at the same time. So, a scaled-up version of the code looks similar, and essentially performs the same function, except it would perform those operations on a larger amount of data. There are a lot of other operations that Flink can perform, an absolute vast quantity (one of the reasons it is so popular), and they all follow a <a id="_idIndexMarker369"/>similar pattern and can be integrated with most available <span class="No-Break">data sources.</span></p>
			<p>Now, we will move on to deal with a complication in data that is far too often experienced, and indeed one that always needs to be dealt with in some form. The next section is <span class="No-Break">about variety.</span></p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor214"/>Variety</h2>
			<p>Variety is interesting and is probably the most complicated burden that most people who work with data deal with. Data <a id="_idIndexMarker370"/>can come in all shapes and sizes and often comes in the most expected ways. Many hackers attempt SQL injection attacks by adding valid SQL queries as form fields, which can then cause those queries to run if the data input matches properly. A good quality assurance tester always attempts a variety of tests that try to befuddle a lot of applications by using data types that they should not be able to input into certain fields. But often – when just regular people are given access to a keyboard – what happens is that people will find some way to break a lot of the safety measures placed in a system by pure accident, showing previously unknown system bugs <span class="No-Break">or vulnerabilities.</span></p>
			<p>So, now we are going to go into an example where such a thing can happen, and this is especially prominent in a lot of looser NoSQL databases that may not have all of the standard data formatting built into them. We are going to attempt to insert emojis into a JSON file. Emojis are usually covered under the <strong class="sour e-inline">UTF-8</strong> format, but this format, while readily available on web pages, usually needs to be set on most databases for when more unconventional formats <span class="No-Break">are used.</span></p>
			<p>We will be using Google Colab for this exercise because it is more efficient for something that is a concise proof of concept such as this. Let’s start by adding a JSON variable containing <span class="No-Break">an emoji:</span></p>
			<pre class="sour e- ode">user_data = {'username': 'user_with_emoji😊',}</pre>
			<p>Now, we are going to insert <a id="_idIndexMarker371"/>it into a file, first without any <span class="No-Break"><strong class="sour e-inline">UTF-8</strong></span><span class="No-Break"> formatting:</span></p>
			<pre class="sour e- ode">import jsonwith open('sample.json', 'w') as file:json.dump(user_data, file)</pre>
			<p>This produces sample JSON with the format <span class="No-Break">shown here:</span></p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/B21320_11_5.jpg" alt="Figure 11.5 – Storage of emoji without UTF-8 format"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – Storage of emoji without UTF-8 format</p>
			<p>This JSON, when converted back for a web page, will require an extra parsing step that may slow down the web page. So, in order to avoid that, we can find a way to store the emoji in the way that it was input. The final output of the code will look more correct, like <span class="No-Break">the following:</span></p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/B21320_11_6.jpg" alt="Figure 11.6 – Storage of emoji with UTF-8 format"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – Storage of emoji with UTF-8 format</p>
			<p>There, that is much better and will be more sustainable in the long term as well. The overall code for this in Google Colab will look <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B21320_11_7.jpg" alt="Figure 11.7 – Colab notebook for UTF-8-based storage of emoji"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7 – Colab notebook for UTF-8-based storage of emoji</p>
			<p>Those were a few simple examples to get you started on optimizing your work with big data. We have talked quite a bit <a id="_idIndexMarker372"/>about data and a bit about machine learning. But let’s round all of this out with the hottest topic of all: ChatGPT. We will now talk about how the DevOps behind ChatGPT works and how similar open source systems are widely <span class="No-Break">available currently.</span></p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor215"/>The Ops behind ChatGPT</h1>
			<p>ChatGPT – in the time that I have been writing this book – has gone from being a hot topic to just being a fact of life, sometimes almost second nature as a tool for information. The way it handles<a id="_idIndexMarker373"/> data and the very nature of it have been topics that have brought on a lot of controversy. But one of the things that I get asked very often by my friends who aren’t in the industry is, how does it work? They see that it delivers information nearly seamlessly on whatever the whim of the user is and then retains that information historically in that chat for future questions. It also does so very quickly. So, one does wonder how it <span class="No-Break">all works.</span></p>
			<p>Let’s start with what ChatGPT is: it is a <strong class="bold">large language model</strong> (<strong class="bold">LLM</strong>), which is a very large neural network that verges on<a id="_idIndexMarker374"/> general language understanding (i.e., the ability to understand questions or queries and deliver back responses that would be appropriate to the solution). While ChatGPT is the biggest deal at the moment, the technology itself has been around for a few years, mostly used in more domain-specific chatbots. However, the newest LLMs have been made so that they can talk about pretty much anything, with slight knowledge specializations in certain fields. Even then, the concept behind ChatGPT is pretty simple: the more data you feed into it that it can contain, the better <span class="No-Break">it becomes.</span></p>
			<p>The current free commercial model GPT-3.5 is made up of about 175 billion parameters spread over 96 neural network layers. How it did so was by inputting over 500 billion words as tokens (numbers) and using its neural network to find associations between these tokens in a way that simulates human language. The set used as a reference for these tokens is just the internet. That’s it, it takes all the text and data from the internet and uses that to recreate human interaction and creativity. Now, most of you have probably seen what GPT-3/3.5 can do, and GPT-4 ramps that concept up even further, using a total of 1.7 trillion<a id="_idIndexMarker375"/> data points. As we can see in the following figure, it is a case of adding some parameters to a neural network until it creates a <span class="No-Break">coherent output:</span></p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/B21320_11_8.jpg" alt="Figure 11.8 – The workflow behind ChatGPT"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8 – The workflow behind ChatGPT</p>
			<p>As seen in the diagram, you put the prompt in and get an answer generated by the trained neural network. It’s that simple. Now, you may be wondering, what happens in between? The answer to that is fascinating, but can be boiled down to a concise statement: we <span class="No-Break">don’t know.</span></p>
			<p>Truly, neural networks are a mystery because they are built and modeled around our own neurons, so they aren’t trained by humans; they train themselves for the best possible success, similar to the way a human would find their best method of study for themselves when trying to<a id="_idIndexMarker376"/> pass a test. So, we don’t really know what is at the core of these neural networks; we just know we can train them to become good at having <span class="No-Break">a conversation.</span></p>
			<p>You can train a similar one at home, too. Some companies have developed more compressed versions of LLMs that can be placed on smaller servers, such as Meta’s <strong class="bold">LLaMA</strong>. But even <a id="_idIndexMarker377"/>besides that, you can find a never-ending amount of generative AI models on any cloud provider of your preference and on open source sites such as Hugging Face, which you can plug and play to try and <span class="No-Break">understand better.</span></p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor216"/>Summary</h1>
			<p>The journey of a DataOps or MLOps engineer is just a DevOps engineer who has gotten some understanding of data and machine learning concepts. That’s pretty much it. But, as we saw in this chapter, the usage of those concepts is a pretty <span class="No-Break">useful thing.</span></p>
			<p>First, we talked about the differences and similarities between DevOps and these associated fields and how they are connected with each other. Using that, we managed to produce a couple of practical use cases that can come in handy when using Python with DataOps <span class="No-Break">and MLOps.</span></p>
			<p>Next, we talked about handling the proverbial big data. We talked about the aspects that make the data so big and how to tackle each of these aspects individually using a use case <span class="No-Break">for each.</span></p>
			<p>Finally, we talked about ChatGPT and how it works in delivering all the things that it delivers to users around the world. We discussed the simplicity of its complexity and its mystery, as well as the new age of open source LLMs that has accelerated the development of <span class="No-Break">generative AI.</span></p>
			<p>In the next chapter, we will get into perhaps the most powerful tool in the DevOps arsenal, <strong class="bold">Infrastructure as Code</strong> (<strong class="bold">IaC</strong>), and how Python is used in <span class="No-Break">this realm.</span></p>
		</div>
		<div id="_idContainer141" class="Content">
			<p class="hidden"><a id="_idTextAnchor217"/></p>
		</div>
	</body></html>