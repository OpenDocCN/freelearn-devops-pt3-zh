- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying Kubernetes Apps Like a True Cloud Native
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When engineers start hearing about Kubernetes or want to start implementing
    it, the typical reason is from a Dev perspective of managing and deploying applications.
    The whole premise around Kubernetes making engineering teams’ lives easier, regardless
    of whether it’s Dev or Ops, is based on application deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying applications is at the forefront of every business’s mind, whether
    it’s a website, some mobile application, or an internal app in any company, from
    a software company to an auto-parts company to a beer manufacturer. Regardless
    of the industry, almost every company deploys some type of application and some
    type of software. As all engineers know, deploying and maintaining an application
    successfully isn’t an easy task. Whether you’re running an app on bare metal,
    on a VM, in the cloud, or even in a container, that app could be (and most likely
    is) the make or break between a successful business and a bankrupt company.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, you’re going to notice that a lot of topics covered
    will remind you of how other application deployments work. From the actual deployment
    to scaling and upgrading, the overall concepts are the same. For example, scaling
    an application is scaling an application. There’s no magical new methodology with
    Kubernetes. However, what Kubernetes does give you is the ease of scalability.
    With that being said, the major thing you’ll notice throughout this chapter is
    that Kubernetes isn’t reinventing the wheel. It’s making what we’ve been doing
    for 30+ years easier.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud-native apps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controllers, controller deployments, and Pods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segregation and namespaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stateless and stateful apps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upgrading deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To follow along with this chapter, you should have already deployed a Kubernetes
    app via a Kubernetes manifest. This chapter is going to break down the process
    of things such as deploying apps and what a Kubernetes manifest is, but to fully
    grasp the chapter, you should be familiar with the deployment process. Think of
    it like this – you should be at a beginner/mid level with the Kubernetes deployment
    process, and this chapter will get you to the production level.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch5](https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch5)'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding cloud-native apps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although the whole *cloud-native* thing can feel a bit buzzword-ish in today’s
    world, there is some merit behind the idea of building cloud-native apps. The
    way an application is architected matters as it relates to how it can be deployed,
    managed, and maintained later. The way a platform is built matters because that’s
    the starting point for how an application can be deployed and how it can be run.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the years of technology’s existence, there have been multiple different
    methodologies around how applications are architected and built. The original
    methods were formed around on-premises systems, such as mainframes and servers.
    After that, applications started to be architected for virtualized hardware platforms,
    such as ESXi, and other virtualization products, with the idea in mind of utilizing
    more of the server, but for different workloads instead of just one workload running
    like in the bare-metal days. After virtualization, there was architecture and
    planning for apps around cloud workloads, which started to introduce the idea
    of cloud native and how applications would work if they only ran in the cloud.
    Considerations such as bandwidth, size of servers, and overall cost consumed a
    lot of conversations around cloud workloads, and still do.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we’re faced with the *fourth phase*, which is containerized workloads.
    Containerized workloads really kicked off the focus around cloud-native applications
    and deployments, and for good reason, especially with the idea of microservices
    starting to become a real thing for many organizations that would’ve thought it
    wasn’t possible just 5 years ago.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you’re going to learn what cloud-native applications are and
    a brief history of application architecture, cloud deployments, and microservices.
  prefs: []
  type: TYPE_NORMAL
- en: What’s a cloud-native app?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before deploying applications in a cloud-native way, let’s take a step back
    and think about a core computer science concept – distributing computing.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed computing is a field that studies distributed systems, and distributed
    systems are systems that have components located on different network-connected
    computers. Those different network-connected computers then communicate with each
    other to send data, or packets, back and forth.
  prefs: []
  type: TYPE_NORMAL
- en: The important part here is this – distributed systems equal multiple software
    components that are on multiple systems but run as a single system. Distributed
    computing sounds like microservices, right? (More on microservices later.)
  prefs: []
  type: TYPE_NORMAL
- en: Cloud native takes the concept of distributed computing and expands it to a
    whole other level. Think about it from an AWS or Azure perspective. AWS and Azure
    are by definition distributed systems. When you log in to the AWS portal, there
    are a ton of services at your fingertips – EC2, databases, storage, and a lot
    more. All of those services that you interact with are from a *single system*,
    but the network components that make up the *single system* span hundreds of thousands
    of servers, across multiple data centers across the entire world. Cloud native
    doesn’t just mean the public cloud, however. Remember, the cloud is a distribution
    of services. Something that’s *cloud native* can also be, for example, an entire
    OpenStack server farm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Combining the whole idea of cloud native and distributed computing, you have
    a major concept – cloud-native applications. Cloud-native apps aim to give you
    the ability to design and build apps that are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Easily scalable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resilient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elastic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The important thing to remember is that these concepts aren’t any different
    than what we’ve already had in the engineering world. We’ve had distributed computing
    for a long time. We’ve had distributed applications for a while. What we didn’t
    always have is the ability to easily implement distributed computing. Thinking
    about the AWS or Azure example from previously, how long would it take us to build
    the same infrastructure as Azure or AWS? Then, think about how many people it
    would take to manage and maintain it. With distributed computing at the cloud
    level, all of the *day-one* configurations are abstracted away from you, leaving
    you with only worrying about the *day-two* complexities of building a cloud-native/distrusted
    computing application.
  prefs: []
  type: TYPE_NORMAL
- en: In the cloud, if you want to scale your application, you click a few buttons,
    write a few lines of code, and boom, you have autoscaling groups. If you want
    to build resilient applications, you point and click on what data centers you
    want your apps to run in instead of having to physically build out those data
    centers. Again, the concept of distributed computing is the same thing as cloud
    native and cloud-native apps. The difference is that you don’t have to worry about
    building out the data center. You just have to worry about scaling the app.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud-specific cloud native
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One major point to keep in mind, whether it’s with a standard app deployment
    in the cloud or an app deployment in Kubernetes, is *cloud native* doesn’t just
    mean *the cloud*. It’s more or less the overall concept, but again, the whole
    idea of cloud native is distributed computing without the need to focus on the
    day-one implementation and configuration.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s take OpenStack. OpenStack is a private cloud. You can deploy
    OpenStack in your data center and interact with it just like you would with any
    other cloud service. However, here’s the catch – some teams may see it as cloud
    native and others may see it as general distributed computing. The infrastructure
    teams that are building out OpenStack will see the *behind-the-scenes* configurations,
    such as building out the hardware and scaling it across multiple data centers.
    To them, it’s no different than a standard distributed computing environment.
    Same for the infrastructure engineers that are building, managing, and maintaining
    the infrastructure for AWS or Azure. Then, there are the teams that interact with
    OpenStack after it’s already built. They’re logging into the UI and communicating
    with OpenStack via the CLI and other API methods, so they’re getting the full
    satisfaction of a true cloud-native environment just like many engineers are getting
    the full satisfaction of interacting with AWS and Azure without needing to worry
    about the infrastructure and services on-premises.
  prefs: []
  type: TYPE_NORMAL
- en: Another example is the hybrid cloud. If you’re running Azure Stack HCI on-premises,
    that means you’re utilizing some server that runs the Azure Stack HCI operating
    system, which interacts with the Azure cloud. The engineers that are managing
    Azure Stack HCI see what’s happening behind the scenes. Other engineers that are
    simply interacting with **Azure Kubernetes Service** (**AKS**) don’t see the underlying
    infrastructure. They just know that they go to a specific location to create a
    new Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of where a platform or app is deployed, it could be considered cloud
    native to some and standard distributed computing to others. You could be an engineer
    that’s building a cloud-native platform so others can interact with it in a cloud-native
    fashion.
  prefs: []
  type: TYPE_NORMAL
- en: What are microservices?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Taking the idea of distributed computing and cloud native to the next level
    gives us a microservice. By definition, a microservice is a loosely coupled architecture
    that has components that have no dependencies on each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say you have five pieces that make up your application: three backend APIs,
    some middleware to connect the backend and frontend, and a frontend that consists
    of a website with multiple paths. In a monolithic-style environment, you would
    take that entire application, package it up, deploy it to a server, and run the
    binary. Then, if you had to update or upgrade any part of that application, such
    as the one backend API, you would have to take down the rest of the application.
    This not only brings down production but also would slow down the ability to get
    new updates and features out because you’d have to specify a specific window to
    bring down the entire platform.'
  prefs: []
  type: TYPE_NORMAL
- en: Microservices allow you to take those five pieces of the application and split
    them out into their own individual pieces. Then, you can manage those *pieces*
    separately instead of having to worry about combining them to deploy and have
    a working platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a Kubernetes environment, you would have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: One container image for backend API 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One container image for backend API 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One container image for backend API 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One container image for the middleware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One container image for the frontend
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, each of those container images can be updated, upgraded, deployed, and
    managed separately.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that microservices aren’t just for containers and Kubernetes.
    The same concepts talked about previously can work just as well on five different
    Ubuntu VMs. It’s just easier to manage a container from a microservice perspective
    than it is to manage it from a VM perspective. Way less automation and repeatable
    practices are needed to do the same thing you would have to do on a VM inside
    of Kubernetes. It’s possible and 100% doable, but it takes more effort.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you’re going to take what you learned in this section and
    start applying it to Kubernetes-based scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about Kubernetes app deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When engineers are first getting started with deploying an application to a
    Kubernetes cluster, it looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a Kubernetes manifest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run a command such as `kubectl apply -f` or `kubectl create -f` against the
    manifest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure that the application has Pods running.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Access your app to ensure it’s running the way you were expecting it to run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Although this is a great approach to getting started with deploying applications
    to Kubernetes, we must dive a little bit deeper to fully understand how the deployment
    process of an app occurs, why it works the way that it does, how manifests interact
    with Kubernetes to ensure an application is deployed, and how Kubernetes keeps
    the desired state of Pods running.
  prefs: []
  type: TYPE_NORMAL
- en: It seems like how Kubernetes deploys apps is simply magic that occurs on the
    platform because that’s how it’s built and that’s the way it’s supposed to be,
    but there’s a ridiculous number of pieces built that allow Kubernetes to appear
    to be the magical deployment platform that it makes itself out to be.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you’re going to learn from start to finish how app deployments
    work inside of Kubernetes and the internals of everything that’s needed to make
    a successful deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes manifests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before actually deploying an application, you’ll need to learn the ins and outs
    of how most applications are deployed to Kubernetes – a **Kubernetes manifest**.
    The idea is that you already know what a Kubernetes manifest is, but perhaps you
    don’t know the breakdown of the internals of a Kubernetes manifest.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Kubernetes manifest is a YAML- or JSON-based configuration that interacts
    differently with the Kubernetes API. The Kubernetes manifest is where you specify
    what API you want to work with and what resource you want to work with from that
    API. There are two groups of APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`/api/v1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/apis/$GROUP_NAME/$VERSION`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, the following is a code snippet showcasing that the Deployment
    resource is in the `/``api/v1` group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is another example of an Ingress controller, which you can see
    is in `/apis/networking.k8s.io/v1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`apiVersion` is the Kubernetes API you’re utilizing, and `kind` is what Kubernetes
    resource you’re creating, updating, or deleting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A Kubernetes manifest consists of four key parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '`apiVersion`: Which version of the Kubernetes API you’re using to create the
    object/resource'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Kind`: What kind of object you want to create, update, or delete'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Metadata`: Data that helps uniquely identify the resource/object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Spec`: What you want the resource to look like'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is a Kubernetes manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let’s break that down.
  prefs: []
  type: TYPE_NORMAL
- en: First, you have the API version. You can see that the API version indicates
    that it’s utilizing a resource in the core group. Next, there’s `kind`, which
    specifies what resource you’re creating/updating/deleting. Then there’s `metadata`,
    which is specifying a name for the deployment to uniquely identify it via metadata.
    Finally, there’s `spec`, which indicates how you want your containerized app to
    look. For example, the spec shown earlier indicates that the manifest is using
    the latest version of the Nginx container image and utilizing port `80`.
  prefs: []
  type: TYPE_NORMAL
- en: To wrap up this section, something you should know about Kubernetes manifests,
    and the way Kubernetes works in general, is that it’s declarative. Declarative
    means “*tell me what to do, not how to do it*.” Imperative means “*tell me what
    to do and how to* *do it*.”
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say you were teaching someone how to bake a cake. If it was
    imperative, you would be telling the person what ingredients to use, the size
    for each ingredient, and how to do it step by step, ultimately leading them to
    the finished product. Declarative would mean you tell them what ingredients they
    need and they figure out how to do it on their own.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes manifests are declarative because you tell Kubernetes what resource
    you want to create, including the name of the resource, ports, volumes, and so
    on, but you don’t tell Kubernetes how to make that resource. You simply define
    what you want, but not how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: The common way but not the only way
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Nine times out of ten, when you’re deploying a resource to Kubernetes, you’ll
    most likely be using a Kubernetes manifest. However, as you’ve learned throughout
    this book so far, the core of Kubernetes is an API. Because it’s an API, you can
    utilize any programmatic approach to interact with it.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the following is a code snippet using Pulumi, a popular IaaS platform
    to create an Nginx deployment inside of Kubernetes. This code requires more to
    run it, so don’t try to run it. This is just a pseudo example.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s no YAML and no configuration language. It’s raw Go (Golang) code interacting
    with the Kubernetes API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Although you won’t see this too often, you should know that this type of method
    exists and you can create any resource you want in Kubernetes, in any programmatic
    fashion, as long as you do it via the Kubernetes API.
  prefs: []
  type: TYPE_NORMAL
- en: Controllers and operators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes comes out of the box with a way to ensure that the current state
    of a deployed application is the desired state. For example, let’s say you deploy
    a Kubernetes deployment that is supposed to have two replicas. Then, for whatever
    reason, one of the Pods goes away. The deployment controller would see that and
    do whatever it needs to do to ensure a second replica/Pod gets created. All resources
    that can be created in Kubernetes (Pods, Services, Ingress, Secrets, and so on)
    have a **controller**.
  prefs: []
  type: TYPE_NORMAL
- en: An **operator** is a special form of a controller. Operators implement the controller
    pattern, and their primary job is to move the resources inside of the Kubernetes
    cluster to the desired state.
  prefs: []
  type: TYPE_NORMAL
- en: Operators also add the Kubernetes API extendibility to use **CustomResourceDefinitions**
    (**CRDs**), which are a way that engineers can utilize the existing Kubernetes
    API without having to build an entire controller and, instead, just use the CRD
    controller. You’ll see a lot of products/platforms that tie into Kubernetes to
    create a CRD.
  prefs: []
  type: TYPE_NORMAL
- en: 'A popular way of building your own operator and controller is with Kubebuilder:
    [https://book.kubebuilder.io/](https://book.kubebuilder.io/).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Controllers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_05_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – Controllers
  prefs: []
  type: TYPE_NORMAL
- en: You’ll hear the terms *operator* and *controller* used interchangeably. To give
    a frame of reference, just remember that the operator is like the big boss working
    at a high level, ensuring that things are going well for the organization, and
    the operator is like the engineer doing the hands-on work to make sure that the
    organization gets what it needs.
  prefs: []
  type: TYPE_NORMAL
- en: Another form of this that’s gaining increased popularity is GitOps. GitOps looks
    at the desired state of a Kubernetes manifest that’s in source control as opposed
    to what controllers do, which is look at what’s actively deployed on a Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Different ways to deploy with higher-level controllers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you deploy a Pod by itself, the manifest can look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The problem with this method is you have no high-level controller that manages
    the Pod(s) for you. Without the higher-level controller, like a Deployment or
    DaemonSet, if the Pod fails, the kubelet watches the static Pod and restarts it
    if it fails. There’s also no management of the current state and desired state.
    From a production perspective, you never want to deploy a Pod resource by itself.
    It’s fine if you want to test a container image, but that’s about it. It should
    be used for testing/development purposes only.
  prefs: []
  type: TYPE_NORMAL
- en: 'Not always, but most of the time, you’ll see the following production-level
    controllers that manage Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deployments**: A deployment is one of the highest-level controllers in the
    Core API group. It gives you the ability to control one Pod or multiple replicas
    and scale out across the cluster. Deployments also give you the ability to self-heal
    and confirm that the current state of a deployed containerized application is
    the desired state. The following code is an example deployment resource:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**DaemonSets**: This is like a deployment resource but is cluster wide. It
    ensures that either all nodes or the nodes you choose run a copy/replica of the
    Pod. A key difference you’ll see in a DaemonSet is that there’s no field for replicas.
    That’s because the Pod can’t run more replicas than the number of worker nodes,
    meaning you can’t have five Pod replicas if you only have three worker nodes.
    In that case, you’d only be able to have three Pods. The following code is an
    example DaemonSet:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`StatefulSet`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In terms of which controller to use, it’s going to depend on your use case.
    There’s no right or wrong answer here unless it’s something straightforward, for
    example, if you have a containerized app that needs to hold on to its network
    ID, in which case you’d use a StatefulSet.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the key components out of the box with Kubernetes is its ability to easily
    scale both horizontally and vertically. From a production perspective, this takes
    a ton of load off of your back. In a standard VM environment, you would have to
    worry about deploying a new server, installing the operating system, getting packages
    up to date, and deploying the application binary, and finally, the app would be
    running.
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal Pod autoscaling is the most common, which means more Pods get created
    to handle load. Vertical autoscaling means the CPU/memory of a Pod gets raised.
    Vertical Pod autoscaling is not all that common, but possible.
  prefs: []
  type: TYPE_NORMAL
- en: When you’re scaling, you can have standard ReplicaSets, but in production, the
    number may not be so cut and dry. For example, if you have three replicas, but
    you may need four or ten, you need a way to account for that. The best thing that
    you can do is start with at least three to four replicas, and if needed, work
    your way up. If you have to scale up to five or ten, you can update the Kubernetes
    manifest and redeploy it with a GitOps solution or in another repeatable fashion
    using the `kubectl apply -f` `name_of_manifest.yaml` command.
  prefs: []
  type: TYPE_NORMAL
- en: When you’re scaling a Pod, or for that matter, when you’re doing anything for
    a Pod deployment, never use commands such as `kubectl patch` or any of the other
    quick fixes on the command line. If you do, any time the Pod gets redeployed,
    your configurations won’t exist because you did them ad hoc/manually on the command
    line. Always make changes in a Kubernetes manifest and deploy them properly (remember,
    current state versus desired state).
  prefs: []
  type: TYPE_NORMAL
- en: How to horizontally scale Pods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When you’re scaling Pods horizontally, it’s all about replica count. For example,
    let’s say you have a Kubernetes manifest like the following, which contains two
    replicas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'You run `kubectl apply -f nginx.yaml` on the preceding manifest, and then you
    come to realize that due to user load on the Nginx frontend, you need to bump
    the replicas from two to four. At that point, you can update the Kubernetes manifest
    to go from two replicas to four:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: This method won’t recreate anything as you’re using `kubectl apply -f` instead
    of `kubectl create -f`. `create` is for creating net-new resources and `apply`
    is for updating/patching a resource.
  prefs: []
  type: TYPE_NORMAL
- en: How to vertically scale Pods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Vertically scaling Pods, as discussed, is not a common practice. However, it
    is doable. The typical method is to use the `VerticalPodAutoscaler` resource from
    the `autoscaling.k8s.io` API. It gives you the ability to point to an existing
    deployment so that deployment is managed by the autoscaler. For example, the following
    Kubernetes manifest shows a target reference of a deployment called `nginxdeployment`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Please note that with the vertical Pod autoscaler turned to `Auto` for the
    update mode, it’ll have the ability to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Delete Pods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjust the CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjust the memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a new Pod
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It requires a restart of the application running inside the Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-container Pods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Sidecars**, sometimes called **multi-container Pods**, are a way to tightly
    couple containers into one Pod. Typically, and especially from an application
    perspective, one Pod runs one container. However, there may be use cases where
    you want to run sidecars. The biggest use case is when you’re running some type
    of log collector/aggregator for your Pods. A lot of engineers will put the container
    running the log collector into the same Pod where the application is running.
    That way, it’s straightforward to communicate with the application and pull the
    logs from it as containers inside of a Pod share the same IP address but are reachable
    on different ports.'
  prefs: []
  type: TYPE_NORMAL
- en: One thing you should absolutely never do is run multiple applications in a Pod.
    For example, you never want to put the frontend app and the backend app inside
    of the same Pod. That defeats the whole purpose of containers and microservices.
    Sidecars are only for very specific use cases and if it’s absolutely necessary.
    A personal belief of mine is that you should never use sidecars unless you absolutely
    have to. Other engineers will disagree with me, but I believe a Pod should run
    one workload. That’s the purpose of a microservice architecture. The only time
    that I see it absolutely necessary is when you’re running a service mesh and you
    need the service mesh proxy inside of the Pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is what a Kubernetes manifest would look like if you have multiple
    containers inside of a Pod. Notice how under `spec.containers`, there’s `container1`
    and `container2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: Liveness and readiness probes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whenever you’re deploying any type of application, whether it’s containerized
    or not, you want to ensure that the application is running as expected. Within
    a containerized environment, it’s no different. Let’s say you have a Pod that’s
    running an Nginx frontend. The Pod could be up and running, have all of the appropriate
    resources, and so on. However, that doesn’t mean that the binary running inside
    the Pod is working as expected. To ensure that the actual application is running
    as expected, you can use **liveness probes** and **readiness probes**.
  prefs: []
  type: TYPE_NORMAL
- en: A liveness probe indicates whether a container is running. It helps Kubernetes
    understand the overall health of the Pod. The kubelet continuously sends a *ping*
    of sorts to the container to ensure that it’s running as expected. If the liveness
    probe deems a container unhealthy, the kubelet restarts the Pod.
  prefs: []
  type: TYPE_NORMAL
- en: A readiness probe indicates whether the container is ready to receive requests.
    Readiness probes are a bit more important from an application perspective because
    they tell Kubernetes whether or not to route service traffic to Pods. If a service
    is trying to route traffic to a Pod and that Pod is down or unhealthy, the application
    won’t be reachable. The readiness probe tells the service which Pods are ready
    to receive requests and which aren’t.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a readiness probe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is an example of a liveness probe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: All production-level Kubernetes deployments should use readiness probes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you’re going to dive into an important topic, which is
    segregating your containerized apps, and a few different ways of doing it.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring segregation and namespaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once applications are deployed, engineers wipe the sweat off their foreheads,
    give high fives to their team, and rejoice in their victory. However, what comes
    after the deployment? Better yet, what if you have to deploy the applications
    again? Or other types of applications? Or to a different location or segregation
    point? (Segregation will be discussed later in this chapter.) Getting an application
    up and running is a mental workout in itself, but the *what-comes-next* questions
    you ask yourself are typically the most important. These are things such as the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Will the next deployment be automated and repeatable?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have to deploy the application again, will it be an effective deployment?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can (or should) the apps run right next to each other?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which engineers should have access to what apps and why?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying an application is a great victory but designing how and where an application
    should run is the difference between a successful and a broken-down production
    environment. Questions around application segregation and multi-tenancy keep engineers
    up at night because it’s less *engineering* work and more planning/architecture
    work. It’s less hands-on-keyboard and more critical thinking at a higher level
    compared to being down in the trenches in the code.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you’re going to learn about a few of the most popular segregation
    techniques. Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Namespaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first level of segregation, typically, is a **namespace**. When you’re deploying
    Pods, the last thing that you want to do is deploy everything and anything to
    the default namespace. Instead, you want to ensure that applications have their
    own namespaces. At a network level, Pods within one namespace can communicate
    with another namespace. However, if you have a service account that’s used for
    Pod deployments in one namespace and a service account that’s used to deploy Pods
    in another namespace, that means the same service account cannot be used to manage
    all of the Pods. That gives you a bit more segregation from a Pod management perspective
    and ensures that there’s proper authentication and authorization. But from a network
    perspective, Pods can still communicate with other Pods in separate namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice in *Figure 5**.2* that there are three namespaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '`argoCD`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-system`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`monitoring`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Kubernetes resources'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_05_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 – Kubernetes resources
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot shows that everything in the `argocd` namespace is
    segregated/isolated from everything in the `kube-system` namespace. If an engineer
    were to run `kubectl get pods`, they would only see the Pods in the namespace
    that they have access to.
  prefs: []
  type: TYPE_NORMAL
- en: Single tenancy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Taking segregation and isolation a step further, there are **tenancy models**.
    First, let’s start with **single tenancy**, but before diving in, let’s talk about
    what tenancy models mean in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Isolating via tenancy could be anything from users to engineers to applications
    and all different resources. For example, single tenancy could mean running one
    containerized application across a cluster, or it could mean ensuring that one
    engineer has access to a cluster that no one else has access to it, but they can
    run as many applications as they want.
  prefs: []
  type: TYPE_NORMAL
- en: A typical scenario of single tenancy is isolating development environments.
    Let’s say you’re a developer and you need a Kubernetes cluster to test an application
    stack. The scenario would be that the platform engineering team, or whichever
    team manages Kubernetes clusters, gives you your own Kubernetes cluster to test
    the application stack. This is a great way to perform single tenancy as it allows
    all engineers working on different tech stacks to test their code without it compromising
    or interfering with other application stacks.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-tenancy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On the flip side is **multi-tenancy**. Multi-tenancy is where you have multiple
    engineers, users, or applications running on the same Kubernetes cluster. If you
    take a look again at *Figure 5**.2* showing the namespaces, you’ll see that Prometheus,
    ArgoCD, and Nginx are running on the same cluster. That would be considered a
    multi-tenancy cluster. Single tenancy would be if ArgoCD, Nginx, and Prometheus
    were all running on separate clusters.
  prefs: []
  type: TYPE_NORMAL
- en: In the real world, rarely do you see applications running on different clusters,
    or rather, an application per cluster. Instead, you usually see the multi-tenancy
    model for applications and the single-tenancy model for developers testing application
    stacks, and once the application stack is tested, it moves into the Kubernetes
    cluster with the rest of the applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you’re going to learn how to think about stateless apps,
    stateful apps, and volumes.
  prefs: []
  type: TYPE_NORMAL
- en: Investigating stateless and stateful apps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At a high level, applications come in two forms – apps that need data stored
    and apps that don’t care whether the state of the data is stored. Let’s think
    about two different scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: When you log in to your Gmail account, or another email service provider, everything
    stays where it’s supposed to be. You can see the emails in your inbox, the sent
    messages, the emails in your trash bin, and so on. The application/platform stays
    how it’s supposed to be because the data is stateful. Now, on the opposite side
    of the spectrum, let’s take `www.google.com` into consideration. When you go to
    `www.google.com` in a web browser, you always have a fresh start. The entry box
    to type in your question is there, but the results to the previous question that
    you asked Google isn’t there. It’s always a fresh, clean slate. That’s because
    `www.google.com` is stateless, as in, it doesn’t just hold on to your data (well,
    it does… but that’s a separate discussion) and keep it in the web browser after
    every search.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, stateless versus stateful is a much deeper discussion, but that’s
    a high-level definition of how you can think about the two different types of
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you’re going to learn about the different deployment methods
    for stateless and stateful applications inside of Kubernetes, along with resource
    considerations including limits, quotas, and requests for Pods to ensure that
    the production-level environment you’re running is sustainable.
  prefs: []
  type: TYPE_NORMAL
- en: Stateful versus stateless
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the opening of this section, I shared the Gmail example, which essentially
    shows what a stateful app is and what a stateless app is. From a Kubernetes perspective,
    the key difference is that a stateless application doesn’t need to store data.
    Stateful applications require backend storage, such as volumes. Another key difference
    is that stateful applications require keeping unique IDs, so if a Pod goes down,
    the Pod that comes up and replaces it must have the same unique ID. A stateless
    app doesn’t need to keep unique IDs.
  prefs: []
  type: TYPE_NORMAL
- en: A common misconception is that stateless apps never use volumes, and that’s
    not the case. You can have a stateless application that, for example, requires
    a backend database or a volume/hard drive to store values.
  prefs: []
  type: TYPE_NORMAL
- en: Volumes and hard drives aren’t what make a stateful application. The unique
    ID is what makes a stateful application.
  prefs: []
  type: TYPE_NORMAL
- en: Container Storage Interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For Kubernetes to interact with outside components that aren’t native, there
    must be a plugin of sorts. In the previous chapters, you learned about **Container
    Network Interface** (**CNI**), which is a plugin to use different network frameworks
    in Kubernetes. **Container Storage Interface** (**CSI**) is the same thing, but
    for storage devices. For example, you can have a CSI for NetApp, AWS S3, Azure
    Storage, and a ton of other storage providers.
  prefs: []
  type: TYPE_NORMAL
- en: Before these interfaces, organizations had to put the code to connect the resources
    that aren’t native in the core Kubernetes code. Just as an example, let’s say
    that Azure wanted to allow Kubernetes engineers to utilize Azure Storage inside
    of Kubernetes for storing the output of a Pod. Before CSI, Azure would’ve had
    to put the code to make it all possible inside of the core Kubernetes code. That
    was a major hassle because not only did Azure have to wait for a new release of
    the Kubernetes API to push the feature out, but if there was a bug or a new update
    that they wanted to push out, they would’ve had to wait for the next Kubernetes
    API release.
  prefs: []
  type: TYPE_NORMAL
- en: CSI, and interfaces/plugins across Kubernetes in general, ensures that organizations
    can create plugins for Kubernetes separately from the core Kubernetes code.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to see an example of CSI, you can check it out on GitHub: [https://github.com/kubernetes-sigs/azuredisk-csi-driver.](https://github.com/kubernetes-sigs/azuredisk-csi-driver%0D)'
  prefs: []
  type: TYPE_NORMAL
- en: Volumes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Volumes** are hard drives, plain and simple.'
  prefs: []
  type: TYPE_NORMAL
- en: With a volume, you give a Pod, or multiple Pods, the ability to store data in
    a location. That location could be Azure, AWS, NetApp, some other storage provider,
    or even the worker node that the Pod is running on (definitely not recommended.
    Just an example).
  prefs: []
  type: TYPE_NORMAL
- en: 'When you’re creating a volume for a Pod, there are typically three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**StorageClass**: A storage class is a way to ask some storage vendor (dynamically)
    for a hard drive. For example, you can create a storage class that connects to
    EBS. Then, you can call upon that storage class later with a volume (which you’ll
    learn about in a minute) and utilize the connection to the storage. You can do
    the same thing in Azure, GCP, and all of the other cloud providers, including
    most of the storage providers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**PersistentVolume**: A persistent volume is created manually by an engineer
    that uses the storage class to utilize storage from an available source. For example,
    the Persistent Volume would connect to the EBS storage class from the previous
    example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**PersistentVolumeClaim**: The last piece is the persistent volume claim, which
    is a request made by a user, usually in a Kubernetes manifest that’s creating
    a Pod, to use some of the storage that’s available in the storage class. The engineer
    can say “hey, I want 10 GB of storage from this storage class for my Pod”:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: At this point, you may be wondering “well, why do I need a persistent volume
    if I can just automatically request some storage with a claim?”, and that’s a
    good question. The answer is going to depend on your environment. If you’re using
    NetApp storage, and you have 1,000 GB of storage, you want an engineer to create
    a persistent volume and manage those volumes because you only have 1,000 GB of
    storage. If you attempt to go over that 1,000 GB, failures will start to occur,
    so having someone manage it makes sense. On the flip side, if you’re using cloud
    storage, such as in Azure or AWS, that storage is *unlimited* to a user (you of
    course have to pay for it), so going straight to a persistent volume claim instead
    of having an engineer create a persistent volume would make sense.
  prefs: []
  type: TYPE_NORMAL
- en: Resource requests and limits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In any production environment, you have Kubernetes clusters that are running
    on servers, regardless of whether it’s on-premises or a managed Kubernetes service.
    Because they are running on servers, those servers have hardware resources, and
    all servers have a limit. There’s no *unlimited CPU* on a server or *unlimited
    memory*. There are limits to a server’s resources and servers can reach 100% capacity.
  prefs: []
  type: TYPE_NORMAL
- en: Because of that, when you’re creating Pods, you should specify limits and requests.
    You never want to give anything, whether it’s a virtualized VM or a containerized
    app, open reign in an environment to take as much CPU and memory as it wants.
    If you don’t control resources, such as memory and CPU, every application could
    take whatever resources it wanted to take.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s think about a basic example. Say you have an application that has a memory
    leak. If you containerize it, the Pod that it’s running in will continue to take
    more and more memory until the worker node eventually fails and/or the application
    crashes, and you’ll only know when it’s too late.
  prefs: []
  type: TYPE_NORMAL
- en: Before diving in, let’s define the difference between a **limit** and a **request**.
  prefs: []
  type: TYPE_NORMAL
- en: A limit is telling a Pod “you cannot go above this.” For example, if you specify
    *X* amount of CPU or memory on a Pod, that Pod cannot go above that limit. It’s
    completely blocked.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a limit. As you can see, the Nginx app is limited
    to 128 Mi of memory. Anything above that and Kubernetes will say “nope, you can’t
    have it”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: A request is what the Pod is guaranteed to get. If a Pod requests a resource,
    Kubernetes will only schedule it on a worker node that can give it that resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a request. In this example, Kubernetes will
    say “alright, you want 64 Mi of memory and 250m of CPU. Let me schedule you onto
    a worker node that can handle this”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is an entire manifest example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: Which should you choose?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There’s some confusion around how requests and limits work.
  prefs: []
  type: TYPE_NORMAL
- en: When Pods are done using memory, they give that memory back to the worker node
    and it goes back into the pool for other Pods to use. With the CPU, it does not.
    The Pod will hold on to that CPU. Because of that, it’s not a best practice to
    let the Pod just hold on to the CPU until it gets deleted because it may not always
    need that amount of CPU. It’s essentially wasting CPU resources.
  prefs: []
  type: TYPE_NORMAL
- en: So, which should you choose?
  prefs: []
  type: TYPE_NORMAL
- en: In every production environment, you should always set up requests, but you
    should only limit CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Namespace quotas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When it comes to limits and requests, one of the really awesome things that
    you can do is set them up for namespaces. For example, you can have a namespace
    that has a limit of 1,000 Mi and a request of 512 Mi. That way, all nodes running
    in that namespace automatically get limited to the required resources, which means
    you don’t have to put limits and requests into every single Kubernetes Pod manifest.
    The following code block showcases the resource quota:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: In the next and final section, you’re going to learn how to upgrade apps and
    different types of update methods.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading Kubernetes apps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Throughout this chapter, you learned some very important lessons:'
  prefs: []
  type: TYPE_NORMAL
- en: How to deploy an app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to deploy different types of apps on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to ensure apps are properly scaled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to ensure apps are running as you expected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you get an application to where you’d like it to be, it’s a great accomplishment.
    Then, before you know it, it’s time to upgrade or update the application and you
    have to start on the journey all over again. You must test out the new version
    of the app, get it deployed without taking down the entire production environment,
    and retest all the components to ensure it’s running as expected.
  prefs: []
  type: TYPE_NORMAL
- en: There may also be times, which is extremely common, when you must roll back
    an update or upgrade to a previous application version. Perhaps it wasn’t properly
    tested in the staging environment, or something popped up that the QA/regression
    testing didn’t catch. In any case, you need a solid plan and methodology on how
    to do a rollback.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you’re going to learn a few different ways to test out application
    updates and upgrades, how you can upgrade and update applications running in Kubernetes,
    and how you can roll back updates and upgrades when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Types of upgrades
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, let’s break down the typical types of upgrades in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: A/B testing is a way to have a set of users on one version of the application
    and a set of users on another version of the application. For example, let’s say
    you’re testing out two versions of an app, v1.1 and v1.2\. A set of users would
    get v1.1 and another set of users would get v1.2\. At that point, you can test
    things such as performance, how the users are interacting with the new version
    of the app, bugs, and issues. This type of test is a controlled experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Canary deployments are pretty much identical to A/B testing except they’re done
    with real users. Taking the previous example, let’s say you had v1.1 and v1.2
    of an app. You would roll out v1.2 in production and put a set of users on v1.2
    but keep a set of users on v1.1\. That way, you can see how users interact with
    the new version in production.
  prefs: []
  type: TYPE_NORMAL
- en: Blue/green testing is when you have two production environments, one on v1.1
    and one on v1.2\. All the users are still on v1.1, but you slowly start to migrate
    all of the users to v1.2\. All users are moved over to v1.2 once it’s confirmed
    to be working.
  prefs: []
  type: TYPE_NORMAL
- en: In Kubernetes, the most popular upgrade method is a **rolling update**, which,
    based on the preceding explanations, is a blue/green deployment.
  prefs: []
  type: TYPE_NORMAL
- en: What happens to an app being upgraded?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you’re upgrading a container image in a Pod, what happens is the new Pod
    comes up and is tested and the old Pod then gets deleted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take the example from the previous section regarding v1.1 and v.1.2 with
    the help of the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Rolling update'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_05_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 – Rolling update
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding architecture diagram, what’s happening is v1.1 is running on
    a Pod with an IP address of 10.0.0.5\. Then, the new Pod running v1.2 comes up
    and is running at the same time as the old Pod. Once the deployment confirms that
    v1.2 of the Pod is working properly and as expected, the users will begin to move
    over to the new Pod. Once all users are on the new Pod running v1.2, the old Pod
    running v1.1 gets deleted.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling updates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What was explained in the previous section was a rolling update. Let’s take
    a look at it from a code perspective. The following is a Kubernetes manifest that’s
    running a deployment spec with a containerized Nginx image using v1.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the time comes to upgrade the containerized app. To upgrade the app with
    `RollingUpdate` (blue/green deployment), you would swap out the `nginx:1.1` container
    image version with `nginx:1.2`. The `RollingUpdate` configuration contains a `progressDeadlineSeconds`
    and `minReadySeconds` configuration to confirm that the new version of the containerized
    app comes up appropriately. Within the strategy map, you specify a `RollingUpdate`
    type and ensure that one replica is always running the old containerized app version
    as the update occurs. That way, users aren’t kicked off the app. The following
    code will perform the proper rolling update action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: You would then run `kubectl apply -f` against the Kubernetes manifest, and the
    rolling update would begin.
  prefs: []
  type: TYPE_NORMAL
- en: Rollbacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’d like to roll back `RollingUpdate`, you’ll need two commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, get the revision number that you want to roll back to from the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, undo `RollingUpdate`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: Not only are updates and rollbacks important to understand from an educational
    perspective, but you’ll most likely see this a fair amount as your organization
    moves to a more microservice-driven approach.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many types of resource deployments when it comes to Kubernetes and
    often, there’s no right or wrong answer to which you choose. The only time that
    there’s a true right or wrong answer is depending on the deployment. If you have
    a stateful application, you want to use a StatefulSet. There’s no mystery as to
    which controller you should be using and there’s no *good or bad*. It simply depends
    on the type of application and workload you need to deploy and manage.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll be diving a little bit deeper into different types
    of deployments from a more advanced perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Kubernetes – An Enterprise Guide* by Marc Boorshtein and Scott Surovich: [https://www.packtpub.com/product/kubernetes-an-enterprise-guide-second-edition/9781803230030](https://www.packtpub.com/product/kubernetes-an-enterprise-guide-second-edition/9781803230030)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
