- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: MLOps and DataOps
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLOps和DataOps
- en: The future will soon be a thing of the past.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 未来很快将成为过去
- en: – George Carlin
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: – 乔治·卡林
- en: 'I was going to use a bit of generative AI to write the content of this chapter
    but that would make it more of an autobiography than a technical book, since then
    the generative AI would become a co-author. I will try to be a very objective
    observer and do some justice to this hot topic. I hope this will give me a favorable
    position in a possible future ruled by AI. There are many reasons why there has
    been such great progress in the field of machine learning and AI in the past century:
    Noam Chomsky, Alan Turing, the creation of computers themselves, science fiction
    novels, and man’s eternal longing for new life in the universe. But in the past
    few years, the delivery of these concepts as viable products and services has
    required the DevOps touch. After all, how else do you think **ChatGPT** manages
    to instantly answer your four-paragraph questions in about five seconds and does
    it for a million more people every minute?'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我本来打算使用一些生成性AI来编写本章节的内容，但那样的话，内容会更像是自传而非技术书籍，因为生成性AI就变成了共同作者。我会尽量做一个非常客观的观察者，并公正地处理这个热门话题。我希望这能让我在未来可能由AI主宰的世界中占据一个有利位置。在过去的一个世纪里，机器学习和AI领域取得如此巨大的进展有很多原因：诺姆·乔姆斯基、艾伦·图灵、计算机本身的发明、科幻小说，以及人类对宇宙中新生命的永恒渴望。但在过去的几年里，这些概念作为可行产品和服务的交付，离不开DevOps的支持。毕竟，你觉得**ChatGPT**是怎么在大约五秒钟内，瞬间回答你四段问题，并且每分钟为一百万多人做到这一点的呢？
- en: Now, all of the chapters and concepts in this book have gotten us back to one
    fact, which is that DevOps is about delivering value and making things work. And
    that is no different when the focus of DevOps is data. In such cases, Python becomes
    even more useful because it is the language of the data operator. Most people
    and development environments for data usually default to Python these days because
    of the existence of the necessary tools for data processing and analysis. Most
    effective **DataOps** workloads will use Python in some capacity. A lot of them
    will use Python both on the running script as well as any supporting operational
    scripts that they may need to write. We will also talk about **MLOps** and the
    operations that help deliver and optimize machine learning models and algorithms.
    We will talk about all of this and more in this chapter after you have read through
    the *Technical* *requirements* section.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，本书中的所有章节和概念都让我们回到一个事实，那就是DevOps的核心是提供价值并使事情能够正常运行。当DevOps的焦点转向数据时，这一点并没有改变。在这种情况下，Python变得更加有用，因为它是数据操作员的语言。如今，大多数人和数据开发环境默认使用Python，因为它提供了进行数据处理和分析所需的工具。大多数高效的**DataOps**工作负载都会在某种程度上使用Python。很多人会在运行脚本和任何需要编写的支持性操作脚本中都使用Python。我们还将讨论**MLOps**以及帮助交付和优化机器学习模型和算法的操作。所有这些内容将在你阅读完*技术*
    *要求*部分后，在本章节中进行讲解。
- en: 'To summarize, in this chapter, you will learn the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，在本章节中，你将学到以下内容：
- en: The difference in the approach taken when DataOps/MLOps is in play as opposed
    to regular DevOps
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当涉及到DataOps/MLOps时，与常规DevOps的做法相比，存在着哪些不同？
- en: The approaches to deal with a variety of different data-based challenges
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理各种不同数据挑战的做法
- en: The Ops behind the delivery of ChatGPT to your computer screen
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ChatGPT交付到你电脑屏幕背后的操作
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Here are some requirements that will help you follow along with this chapter’s
    activities:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些帮助你跟上本章活动的要求：
- en: A GitHub account and access to this book’s Git repository ([https://github.com/PacktPublishing/Hands-On-Python-for-DevOps](https://github.com/PacktPublishing/Hands-On-Python-for-DevOps))
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个GitHub帐户，并能够访问本书的Git仓库（[https://github.com/PacktPublishing/Hands-On-Python-for-DevOps](https://github.com/PacktPublishing/Hands-On-Python-for-DevOps)）
- en: A Google account to use Google Colab
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Google Colab所需的Google帐户
- en: A usable Python environment somewhere
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某处可用的Python环境
- en: A nice cup of your favorite beverage
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一杯你最喜欢的饮品
- en: How MLOps and DataOps differ from regular DevOps
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLOps和DataOps与常规DevOps的区别
- en: 'A question that we often encounter in any sort of technical industry in general
    is: what is the difference between a data role and a non-data role? What would
    be the difference between a software and data engineer, a data analyst and an
    accountant, or a DJ and a music composer? It is something employers ask a lot;
    people speculate on whether one is a subgroup of another or whether they are completely
    different. Even in the Swedish language, *dator* means “computer,” science is
    translated as *vetenskap*, and computer science is referred to as *datavetenskap,*
    so at some point whatever entity that designs and updates the Swedish language
    thought that there was very little to distinguish between the two.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在任何技术行业中经常会遇到的一个问题是：数据角色和非数据角色之间的区别是什么？软件工程师和数据工程师、数据分析师和会计师，或者DJ和音乐作曲家之间的区别是什么？这是雇主经常提问的问题；人们猜测是否有一种是另一种的子集，或者它们是否完全不同。即使在瑞典语中，*dator*表示“计算机”，科学被翻译为*vetenskap*，而计算机科学则被称为*datavetenskap*，因此在某个时候，设计并更新瑞典语言的实体认为这两者之间几乎没有什么区别。
- en: We will now explain this through a couple of common DevOps use cases that can
    be applied and used in these more narrowed fields of DataOps and MLOps. For DataOps,
    we will go through a method that is simple but has saved me a lot of data concatenation
    operations in Python when using JSON files. For MLOps, we will focus on the GPU
    side, which is the primary hardware that an MLOps engineer may have to work with.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在通过几个常见的DevOps用例来解释这些内容，这些用例可以应用到数据操作（DataOps）和机器学习操作（MLOps）这两个更具体的领域。在DataOps中，我们将介绍一个简单的方法，这个方法帮助我在使用JSON文件时，节省了很多Python中的数据连接操作。在MLOps中，我们将重点关注GPU方面，这是MLOps工程师可能需要处理的主要硬件。
- en: DataOps use case – JSON concatenation
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataOps用例——JSON连接
- en: This is quite a simple little trick, but one that is not as commonly known as
    you think. I honestly think if I can help even one person working with data with
    this section, I will have succeeded. The manipulation of JSON is a very important
    aspect of data operations that is very prominent, especially in NoSQL use cases,
    but also in a number of other cases. The ability to naturally manipulate JSON
    gives Python a major advantage over a lot of other programming languages. One
    of the most useful applications of this is the pipe (`|`) operator. This little
    operator can be used to perform concatenations, unions, and even bitwise operations
    on numbers. It is one of the many ways in which Python has made it easier to perform
    these small data operations for ease of use.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的小技巧，但它并不像你想象的那样为人所知。我真心认为，如果我能通过这一部分帮助到一个正在处理数据的人，那么我就算成功了。JSON的操作是数据操作中非常重要的一个方面，尤其在NoSQL用例中非常显著，但在其他一些场景中也很常见。自然地操作JSON给Python带来了相较于许多其他编程语言的重大优势。它最有用的应用之一就是管道（`|`）操作符。这个小操作符可以用于执行连接、联合，甚至是数字的按位操作。它是Python使这些小数据操作变得更加易用的许多方式之一。
- en: 'So, we will begin with just the function for the concatenation of one JSON
    with another:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们将从仅仅一个连接JSON的函数开始：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'That’s it. That’s the code, and here’s the output of that code:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。这就是代码，下面是代码的输出：
- en: '![Figure 11.1 – Output of JSON concatenation](img/B21320_11_1.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图11.1 – JSON连接的输出](img/B21320_11_1.jpg)'
- en: Figure 11.1 – Output of JSON concatenation
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – JSON连接的输出
- en: You’ll see that the second JSON’s value for a key overrides the value from the
    first JSON and that if they have the same common values, they will stay the same,
    and any additional values are combined into the overall JSON. So, with all that
    in mind, whenever you encounter such a problem with JSON combination (and it can
    come up quite often), you will have this little trick in your toolbelt. Now, let’s
    move on to another trick, one that will certainly help all you gaming hardware
    addicts out there. It’ll help the rest of you out too, but I like mentioning hardware
    addicts because they make the most YouTube videos and I’m hoping to get some of
    that sweet exposure.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到第二个JSON的键值会覆盖第一个JSON中的值，如果它们有相同的公共值，那么它们会保持不变，任何额外的值会被合并到整体JSON中。所以，记住这一点，每当你遇到JSON合并的问题时（而且这个问题可能会很常见），你就可以使用这个小技巧。现在，我们来看看另一个技巧，它肯定能帮助所有游戏硬件爱好者。它也会帮助其他人，但我特别提到硬件爱好者，因为他们拍了最多的YouTube视频，我希望能得到一些曝光。
- en: MLOps use case – overclocking a GPU
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLOps用例——超频GPU
- en: In this modern age of AI art, image generation at the highest levels can require
    a lot of processing power. For any kind of graphical rendering, CPUs are only
    used when no other options are available and are usually not recommended for larger
    renderings. For machine learning TensorFlow algorithms, Google’s proprietary TPUs
    are the norm. But again, for anything concerning image generation or manipulation,
    a good GPU is good to have. And if the rare case comes up where that GPU needs
    a bit of extra juice to get things done, overclocking can be necessary.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个AI艺术的现代时代，最高水平的图像生成可能需要大量的处理能力。对于任何类型的图形渲染，只有在没有其他选择的情况下才会使用CPU，并且通常不建议用于较大的渲染。对于机器学习的TensorFlow算法，Google的专有TPU是常见的选择。但同样，对于任何涉及图像生成或处理的任务，拥有一块好的GPU是非常有用的。如果出现了需要GPU额外能量才能完成任务的罕见情况，超频可能是必需的。
- en: 'A lot of the time, GPU processors have their own drivers and with their drivers
    come their own command-line tools. Executing these before and after the use of
    overclocking or another GPU feature can be a hassle. Instead, using Python’s in-built
    `subprocess` module, we can automatically overclock or perform any other GPU processes
    that we would like. For this example, we are going to use the CLI tools for NVIDIA,
    which is probably the most popular GPU brand available at the moment. NVIDIA has
    a command-line tool called **nvidia-smi**, which also contains an overclocking
    feature and is what we are going to invoke. Now, let’s write the code block that
    will help us overclock our GPUs:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 很多时候，GPU处理器都有自己的驱动程序，而这些驱动程序也带有各自的命令行工具。在使用超频或其他GPU功能之前和之后执行这些命令可能会很麻烦。相反，通过使用Python内建的`subprocess`模块，我们可以自动超频或执行任何其他GPU相关的任务。对于这个例子，我们将使用NVIDIA的命令行工具，它可能是目前最流行的GPU品牌。NVIDIA有一个命令行工具叫做**nvidia-smi**，它也包含了一个超频功能，这正是我们要调用的工具。现在，让我们写出能够帮助我们超频GPU的代码块：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding code, when run, will overclock whichever NVIDIA GPU has been
    set up on your device. This, in turn, will make processes such as image processing
    and generation faster. This can be useful when there is a higher demand for these
    resources, and it isn’t possible to shift those demands to other resources. So,
    this code can be used to temporarily overclock a GPU based on some condition that
    may cause it to be called. Once it has been overclocked, you can set it back to
    its default by running the following command (in or out of script):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上面的代码后，将会对设备上设置的任何NVIDIA GPU进行超频。这将使得图像处理和生成等过程变得更快。这在对这些资源有更高需求时很有用，并且无法将这些需求转移到其他资源上时。所以下面的代码可以根据某些条件临时对GPU进行超频。一旦超频完成，你可以通过运行以下命令（无论是在脚本中还是外部）将其恢复到默认设置：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: So, this is how you would manipulate GPUs using Python. A lot of this section
    has involved learning how to manipulate data and the aspects surrounding data.
    However, the data itself can be difficult to work with for a variety of other
    reasons as well. One of the primary reasons can be just how much data is there,
    which can be a lot. The next section will be all about finding ways to not be
    overwhelmed by all of the data that comes from various sources that you may have
    to deal with.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是如何使用Python操作GPU的方式。这个部分的内容大多是学习如何操作数据及其相关方面。然而，数据本身也因为多种原因可能变得难以处理。一个主要的原因可能是数据的数量，可能会非常庞大。接下来的部分将会讲述如何找到应对来自各种来源的数据的方法，避免被大量数据压倒。
- en: Dealing with velocity, volume, and variety
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理速度、体积和种类
- en: When given any tutorial on how to process data, you are usually given a quick
    introduction to the **three Vs** (**velocity**, **volume**, and **variety**).
    These are the three ways in which the complexity of data can scale. Each of them
    presents a singularly unique problem when dealing with data, and a lot of data
    that you would have to deal with can be a combination of all three. Velocity is
    the speed of data coming in over a period of time, volume is the amount of data,
    and variety is the diversity of the data being presented.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何关于如何处理数据的教程中，通常都会简单介绍**三维（three Vs）**（**速度（velocity）**、**体积（volume）**和**种类（variety）**）。这三种方式描述了数据复杂性的扩展方式。在处理数据时，每种方式都会提出独特的问题，而你需要处理的大量数据可能是这三者的组合。速度指的是数据在一段时间内的流入速度，体积是数据的数量，种类则是呈现的数据的多样性。
- en: So, this section will be divided according to the three Vs, and in each subsection,
    there will be a solution for a common problem that may arise with them. This way,
    you will get to see how Python can help in dealing with such massive amounts of
    data. Let’s start with volume as it is the simplest and probably the first thing
    that comes to people’s minds when it comes to **big data**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这一部分将根据三个V进行划分，每个小节中都将为可能出现的共同问题提供解决方案。这样一来，您将了解Python如何帮助处理如此大量的数据。让我们从容量开始，因为这是最简单的，而且当谈到**大数据**时，可能是人们首先想到的。
- en: Volume
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容量
- en: The volume of data is a pretty simple thing. It represents a certain quantity
    of data, most, if not all, of which will be of the same type. If we are going
    to deal with a large volume of data, it will require understanding the time sensitivity
    of data as well as the resources that we would have on hand. The volume of data
    that is usually processed differs based on whether the data is massive based on
    width or length (i.e., whether there are a lot of fields for one row of data or
    there is a massive number of data rows). Both of these require different solutions,
    even specialized databases sometimes. There is also the possibility of datasets
    not being numbers and letters at all but instead being files of audio or video.
    In this section, we will use an example that will be very useful when we have
    a database or data file that contains a large number of fields/columns.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的量是一件相当简单的事情。它表示一定量的数据，其中大多数（如果不是全部）将是相同类型的数据。如果我们要处理大量数据，将需要理解数据的时间敏感性以及我们手头的资源。通常处理的数据量根据数据是基于宽度还是长度而不同（即一行数据是否有很多字段或者数据行数非常多）。这两种情况需要不同的解决方案，有时甚至需要专门的数据库。还有可能数据集根本不是数字和字母，而是音频或视频文件。在本节中，我们将使用一个示例，在拥有包含大量字段/列的数据库或数据文件时会非常有用。
- en: 'To start, we will need a high-volume dataset, so we will use an app called
    **Mockaroo**, which allows you to generate data fields and sample data using generative
    AI (very fitting in this chapter). Let’s go to the Mockaroo site and generate
    a few fields for our sample data:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，我们需要一个大容量数据集，因此我们将使用一个名为**Mockaroo**的应用程序，该应用程序允许您使用生成式AI生成数据字段和样本数据（在本章非常合适）。让我们转到Mockaroo网站，为我们的样本数据生成几个字段：
- en: '![Figure 11.2 – Mockaroo schema](img/B21320_11_2.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.2 – Mockaroo模式](img/B21320_11_2.jpg)'
- en: Figure 11.2 – Mockaroo schema
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – Mockaroo模式
- en: 'The dataset we produced with Mockaroo looks like the following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用Mockaroo生成的数据集如下所示：
- en: '![Figure 11.3 – Sample CSV created by Mockaroo](img/B21320_11_3.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.3 – Mockaroo创建的样本CSV](img/B21320_11_3.jpg)'
- en: Figure 11.3 – Sample CSV created by Mockaroo
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – Mockaroo创建的样本CSV
- en: 'The preceding figure shows just a small piece of it; it’s 20 very large fields
    for 1,000 rows. Let’s write the script to parse through it:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图示仅显示了其一小部分；它是1,000行中有20个非常大字段。让我们编写脚本来解析它：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The script may seem a little redundant in terms of reading the CSV file, but
    the reason it is like this is so that all of the rows in the CSV aren’t loaded
    into the memory of the OS at the same time. This method will reduce the load on
    the memory of the data and is a great way to read large amounts of data in a system
    where the memory can’t hold a lot of data. What it does is that it reads one row
    of the data and then releases that data from the memory before reading the other
    rows. This is efficient management of memory during the reading of a high volume
    of data, which in turn makes the reading a lot faster and smoother, as demonstrated
    in this diagram:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本在读取CSV文件方面可能看起来有点多余，但之所以这样是为了确保CSV中的所有行不会同时加载到操作系统的内存中。这种方法将减少数据内存的负荷，是在内存无法容纳大量数据的系统中读取大量数据的绝佳方式。其原理是读取一行数据，然后释放内存中的数据再读取其他行。这是在读取高容量数据时有效管理内存的方式，从而使读取速度更快、更顺畅，正如本图所示：
- en: '![Figure 11.4 – Workflow behind a generator](img/B21320_11_4.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.4 – 生成器背后的工作流程](img/B21320_11_4.jpg)'
- en: Figure 11.4 – Workflow behind a generator
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 – 生成器背后的工作流程
- en: Now, that was simple enough, but what happens when it’s just one row at a time,
    but constant, such as streaming data? All of it needs to be processed live as
    it comes in. How would we achieve this? Let’s find out.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这已经足够简单了，但是当每次只有一行数据时，但是持续不断，例如流数据时会发生什么？所有这些都需要在其传入时进行实时处理。我们该如何实现这一点？让我们找出答案。
- en: Velocity
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 速度
- en: Tackling the velocity of data is a legitimate billion-dollar question. Even
    today, the biggest video streamers struggle to send out livestream data consistently.
    Of course, there are a number of reasons for this, but the fact is a lot of solutions
    don’t have the right combination of budget and quality to be consistent all of
    the time. We can get pretty close, though.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数据速度是一个值得数十亿美元投资的问题。即使在今天，最大的在线视频流平台也在努力持续地发送直播数据。当然，造成这一现象的原因有很多，但事实是，许多解决方案没有正确的预算和质量组合，因此无法始终保持一致。不过，我们可以做到非常接近。
- en: 'In this exercise, we will be using something that a lot of people call the
    future, and perhaps the present, of data streaming: **Apache Flink**. This is
    a stream and batch processing framework developed by the Apache Software Foundation
    for a smooth, fast data flow. Unlike a lot of frameworks managed by the Apache
    Software Foundation, this one was created with the express intent of being maintained
    by them as opposed to a project created by a company and made open source for
    easier maintenance.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用很多人称之为数据流的未来，甚至可能是现在的技术：**Apache Flink**。这是一个由 Apache 软件基金会开发的流式和批处理框架，旨在实现平滑快速的数据流。与许多由
    Apache 软件基金会管理的框架不同，它是专门为基金会维护而创建的，而不是由某公司创建并开源以便于维护的项目。
- en: Flink itself does not offer any data storage solutions and is instead simply
    supposed to process incoming data into a storage location. It has APIs in Java,
    Python, and Scala, and support on all cloud platforms.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Flink 本身不提供任何数据存储解决方案，它的工作是将传入的数据处理后存储到指定位置。它提供 Java、Python 和 Scala 的 API，并且支持所有云平台。
- en: 'To start with Python, you will need to install `pyflink` using the following
    command:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用 Python，你需要通过以下命令安装 `pyflink`：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Also install pandas if you have not:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有安装 pandas，请同时安装：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Alright, now let’s write some code to stream data from a bunch of JSON rows
    to a CSV table. This is just a sample program to show Flink’s workflow, but it
    does serve that purpose rather effectively:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在让我们编写一些代码，将来自一堆 JSON 行的数据流转存到 CSV 表格中。这只是一个展示 Flink 工作流的示例程序，但它确实有效地实现了这个目的：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In this code, you’ll see that the JSON rows are inserted into a CSV using a
    temporary table as a holdover for insertion. This temporary table, when inserted,
    also inserts the data into the CSV file.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，你会看到 JSON 行通过临时表插入到 CSV 中，临时表作为插入数据的过渡。当插入临时表时，它也会将数据插入到 CSV 文件中。
- en: This is a rather simple explanation of the capabilities of Flink, whose job
    is to work with essentially the same context, but for millions of bits of streaming
    data at the same time. So, a scaled-up version of the code looks similar, and
    essentially performs the same function, except it would perform those operations
    on a larger amount of data. There are a lot of other operations that Flink can
    perform, an absolute vast quantity (one of the reasons it is so popular), and
    they all follow a similar pattern and can be integrated with most available data
    sources.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这对 Flink 功能的解释相当简单，它的工作是处理基本相同的上下文，但同时处理数百万条流数据。所以，代码的扩展版本看起来相似，本质上执行相同的功能，唯一不同的是，它将在更多数据上执行这些操作。Flink
    可以执行许多其他操作，数量庞大（这也是它如此受欢迎的原因之一），它们都遵循类似的模式，并可以与大多数现有的数据源集成。
- en: Now, we will move on to deal with a complication in data that is far too often
    experienced, and indeed one that always needs to be dealt with in some form. The
    next section is about variety.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将处理数据中的一个常见复杂问题，事实上，这是一个总是需要以某种形式处理的问题。下一节讲的是多样性。
- en: Variety
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多样性
- en: Variety is interesting and is probably the most complicated burden that most
    people who work with data deal with. Data can come in all shapes and sizes and
    often comes in the most expected ways. Many hackers attempt SQL injection attacks
    by adding valid SQL queries as form fields, which can then cause those queries
    to run if the data input matches properly. A good quality assurance tester always
    attempts a variety of tests that try to befuddle a lot of applications by using
    data types that they should not be able to input into certain fields. But often
    – when just regular people are given access to a keyboard – what happens is that
    people will find some way to break a lot of the safety measures placed in a system
    by pure accident, showing previously unknown system bugs or vulnerabilities.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 多样性是有趣的，可能也是大多数数据工作者面临的最复杂的负担。数据可以以各种形状和大小出现，并且通常以最出人意料的方式出现。许多黑客尝试通过将有效的 SQL
    查询作为表单字段添加，从而发起 SQL 注入攻击，如果数据输入正确匹配，这些查询就会被执行。一个优质的质量保证测试员总是尝试各种测试，试图通过使用他们不应能输入到某些字段中的数据类型，来让很多应用程序陷入困境。但通常——当普通人有机会使用键盘时——会发生的是，人们会以某种方式意外地打破系统中的许多安全措施，从而暴露出之前未知的系统漏洞或问题。
- en: So, now we are going to go into an example where such a thing can happen, and
    this is especially prominent in a lot of looser NoSQL databases that may not have
    all of the standard data formatting built into them. We are going to attempt to
    insert emojis into a JSON file. Emojis are usually covered under the `UTF-8` format,
    but this format, while readily available on web pages, usually needs to be set
    on most databases for when more unconventional formats are used.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在我们将进入一个可能会发生这种情况的示例，这在许多较为宽松的 NoSQL 数据库中尤为明显，这些数据库可能没有内建所有标准的数据格式化功能。我们将尝试在
    JSON 文件中插入表情符号。表情符号通常使用 `UTF-8` 格式表示，但这种格式虽然在网页上很常见，通常在使用更不常规格式的数据库时，仍需要手动设置。
- en: 'We will be using Google Colab for this exercise because it is more efficient
    for something that is a concise proof of concept such as this. Let’s start by
    adding a JSON variable containing an emoji:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Google Colab 来进行这个练习，因为它对于像这样的简洁概念验证更加高效。让我们从添加一个包含表情符号的 JSON 变量开始：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we are going to insert it into a file, first without any `UTF-8` formatting:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将首先不使用任何 `UTF-8` 格式，将其插入到一个文件中：
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This produces sample JSON with the format shown here:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成如下格式的示例 JSON：
- en: '![Figure 11.5 – Storage of emoji without UTF-8 format](img/B21320_11_5.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.5 – 使用非 UTF-8 格式存储表情符号](img/B21320_11_5.jpg)'
- en: Figure 11.5 – Storage of emoji without UTF-8 format
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 – 使用非 UTF-8 格式存储表情符号
- en: 'This JSON, when converted back for a web page, will require an extra parsing
    step that may slow down the web page. So, in order to avoid that, we can find
    a way to store the emoji in the way that it was input. The final output of the
    code will look more correct, like the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 JSON 在转换回网页时，将需要额外的解析步骤，这可能会减慢网页的加载速度。为了避免这种情况，我们可以找到一种方法，以表情符号输入的方式存储它。代码的最终输出将看起来更正确，如下所示：
- en: '![Figure 11.6 – Storage of emoji with UTF-8 format](img/B21320_11_6.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.6 – 使用 UTF-8 格式存储表情符号](img/B21320_11_6.jpg)'
- en: Figure 11.6 – Storage of emoji with UTF-8 format
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6 – 使用 UTF-8 格式存储表情符号
- en: 'There, that is much better and will be more sustainable in the long term as
    well. The overall code for this in Google Colab will look like this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这样就好多了，从长远来看，也会更加可持续。在 Google Colab 中的整体代码看起来如下：
- en: '![Figure 11.7 – Colab notebook for UTF-8-based storage of emoji](img/B21320_11_7.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.7 – 用于基于 UTF-8 存储表情符号的 Colab 笔记本](img/B21320_11_7.jpg)'
- en: Figure 11.7 – Colab notebook for UTF-8-based storage of emoji
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 – 用于基于 UTF-8 存储表情符号的 Colab 笔记本
- en: 'Those were a few simple examples to get you started on optimizing your work
    with big data. We have talked quite a bit about data and a bit about machine learning.
    But let’s round all of this out with the hottest topic of all: ChatGPT. We will
    now talk about how the DevOps behind ChatGPT works and how similar open source
    systems are widely available currently.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是一些简单的示例，帮助你入门如何优化大数据工作。我们已经讨论了很多关于数据的内容，也提到了机器学习的一些内容。但现在让我们把这一切总结一下，讨论一下最热门的话题：ChatGPT。接下来，我们将讨论
    ChatGPT 背后的 DevOps 是如何工作的，以及当前类似的开源系统是如何广泛可用的。
- en: The Ops behind ChatGPT
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatGPT 背后的操作
- en: ChatGPT – in the time that I have been writing this book – has gone from being
    a hot topic to just being a fact of life, sometimes almost second nature as a
    tool for information. The way it handles data and the very nature of it have been
    topics that have brought on a lot of controversy. But one of the things that I
    get asked very often by my friends who aren’t in the industry is, how does it
    work? They see that it delivers information nearly seamlessly on whatever the
    whim of the user is and then retains that information historically in that chat
    for future questions. It also does so very quickly. So, one does wonder how it
    all works.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT – 在我写这本书的过程中 – 从一个热门话题变成了生活中的一部分，有时几乎成为了信息工具的第二天性。它处理数据的方式以及它的本质一直是引发大量争议的话题。但我经常被我的一些不在行业内的朋友问到的是，它是怎么工作的？他们看到它几乎无缝地提供信息，无论用户的任何需求是什么，并且在聊天中将这些信息作为历史记录保留下来，以便以后回答问题。它还做得非常迅速。所以，大家都想知道它是怎么做到的。
- en: 'Let’s start with what ChatGPT is: it is a **large language model** (**LLM**),
    which is a very large neural network that verges on general language understanding
    (i.e., the ability to understand questions or queries and deliver back responses
    that would be appropriate to the solution). While ChatGPT is the biggest deal
    at the moment, the technology itself has been around for a few years, mostly used
    in more domain-specific chatbots. However, the newest LLMs have been made so that
    they can talk about pretty much anything, with slight knowledge specializations
    in certain fields. Even then, the concept behind ChatGPT is pretty simple: the
    more data you feed into it that it can contain, the better it becomes.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从ChatGPT是什么开始：它是一个**大型语言模型**（**LLM**），这是一个非常大的神经网络，几乎接近于通用语言理解（即理解问题或查询并返回适当的解决方案的能力）。虽然ChatGPT目前是最大的热点，但这项技术实际上已经存在了几年，最初主要用于更具领域特定的聊天机器人。然而，最新的LLM已经被开发得可以讨论几乎任何话题，并在某些领域有轻微的知识专长。即便如此，ChatGPT背后的概念非常简单：你给它提供的更多数据，它能容纳得更多，它就会变得更好。
- en: 'The current free commercial model GPT-3.5 is made up of about 175 billion parameters
    spread over 96 neural network layers. How it did so was by inputting over 500
    billion words as tokens (numbers) and using its neural network to find associations
    between these tokens in a way that simulates human language. The set used as a
    reference for these tokens is just the internet. That’s it, it takes all the text
    and data from the internet and uses that to recreate human interaction and creativity.
    Now, most of you have probably seen what GPT-3/3.5 can do, and GPT-4 ramps that
    concept up even further, using a total of 1.7 trillion data points. As we can
    see in the following figure, it is a case of adding some parameters to a neural
    network until it creates a coherent output:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的免费商业模型GPT-3.5由大约1750亿个参数组成，分布在96个神经网络层中。它是通过将超过5000亿个单词作为标记（数字）输入，并使用神经网络以模拟人类语言的方式找出这些标记之间的关联来实现的。用于这些标记的参考集就是互联网。就是这么简单，它将互联网的所有文本和数据拿来，利用这些来重建人类的互动和创造力。现在，你们中的大多数人可能已经看过GPT-3/3.5能做什么，而GPT-4则进一步提升了这个概念，使用了总计1.7万亿的数据点。正如我们在下图中所看到的，这就是向神经网络添加一些参数，直到它生成一个连贯的输出：
- en: '![Figure 11.8 – The workflow behind ChatGPT](img/B21320_11_8.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.8 – ChatGPT的工作流程](img/B21320_11_8.jpg)'
- en: Figure 11.8 – The workflow behind ChatGPT
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 – ChatGPT的工作流程
- en: 'As seen in the diagram, you put the prompt in and get an answer generated by
    the trained neural network. It’s that simple. Now, you may be wondering, what
    happens in between? The answer to that is fascinating, but can be boiled down
    to a concise statement: we don’t know.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，你输入提示并获得由训练好的神经网络生成的答案。就是这么简单。现在，你可能会想，发生了什么呢？这个问题的答案非常有趣，但可以归结为一句简洁的话：我们不知道。
- en: Truly, neural networks are a mystery because they are built and modeled around
    our own neurons, so they aren’t trained by humans; they train themselves for the
    best possible success, similar to the way a human would find their best method
    of study for themselves when trying to pass a test. So, we don’t really know what
    is at the core of these neural networks; we just know we can train them to become
    good at having a conversation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，神经网络是一个谜，因为它们是围绕我们自己的神经元构建和建模的，因此它们不是由人类训练的；它们自我训练，以实现最佳的成功，类似于人类在通过考试时会找到最适合自己的学习方法。因此，我们并不真正知道这些神经网络的核心是什么；我们只知道我们可以训练它们，让它们变得擅长于对话。
- en: You can train a similar one at home, too. Some companies have developed more
    compressed versions of LLMs that can be placed on smaller servers, such as Meta’s
    **LLaMA**. But even besides that, you can find a never-ending amount of generative
    AI models on any cloud provider of your preference and on open source sites such
    as Hugging Face, which you can plug and play to try and understand better.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在家里训练一个类似的模型。一些公司已经开发出了更压缩版本的 LLM，可以部署在较小的服务器上，比如 Meta 的 **LLaMA**。但即便如此，你也可以在任何你偏好的云服务提供商和像
    Hugging Face 这样的开源网站上找到源源不断的生成型 AI 模型，随时使用并尝试，以帮助你更好地理解。
- en: Summary
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: The journey of a DataOps or MLOps engineer is just a DevOps engineer who has
    gotten some understanding of data and machine learning concepts. That’s pretty
    much it. But, as we saw in this chapter, the usage of those concepts is a pretty
    useful thing.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: DataOps 或 MLOps 工程师的旅程其实就是一个 DevOps 工程师，他对数据和机器学习概念有所了解。就这么简单。但正如我们在这一章中看到的，这些概念的应用确实非常有用。
- en: First, we talked about the differences and similarities between DevOps and these
    associated fields and how they are connected with each other. Using that, we managed
    to produce a couple of practical use cases that can come in handy when using Python
    with DataOps and MLOps.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们讨论了 DevOps 和这些相关领域之间的异同，以及它们如何相互连接。通过这一点，我们成功地展示了一些实际的用例，这些用例在将 Python
    与 DataOps 和 MLOps 配合使用时非常有用。
- en: Next, we talked about handling the proverbial big data. We talked about the
    aspects that make the data so big and how to tackle each of these aspects individually
    using a use case for each.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论了如何处理所谓的大数据。我们分析了哪些因素使得数据如此庞大，并且通过每个用例逐一解决这些方面。
- en: Finally, we talked about ChatGPT and how it works in delivering all the things
    that it delivers to users around the world. We discussed the simplicity of its
    complexity and its mystery, as well as the new age of open source LLMs that has
    accelerated the development of generative AI.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们讨论了 ChatGPT 以及它如何在全球范围内为用户提供所有服务。我们讨论了它复杂性背后的简单性和神秘感，以及加速生成型 AI 发展的开源大型语言模型（LLMs）的新时代。
- en: In the next chapter, we will get into perhaps the most powerful tool in the
    DevOps arsenal, **Infrastructure as Code** (**IaC**), and how Python is used in
    this realm.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨 DevOps 工具箱中或许最强大的工具——**基础设施即代码**（**IaC**），以及 Python 在这一领域中的应用。
