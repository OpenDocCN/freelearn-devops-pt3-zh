- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLOps and DataOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The future will soon be a thing of the past.
  prefs: []
  type: TYPE_NORMAL
- en: – George Carlin
  prefs: []
  type: TYPE_NORMAL
- en: 'I was going to use a bit of generative AI to write the content of this chapter
    but that would make it more of an autobiography than a technical book, since then
    the generative AI would become a co-author. I will try to be a very objective
    observer and do some justice to this hot topic. I hope this will give me a favorable
    position in a possible future ruled by AI. There are many reasons why there has
    been such great progress in the field of machine learning and AI in the past century:
    Noam Chomsky, Alan Turing, the creation of computers themselves, science fiction
    novels, and man’s eternal longing for new life in the universe. But in the past
    few years, the delivery of these concepts as viable products and services has
    required the DevOps touch. After all, how else do you think **ChatGPT** manages
    to instantly answer your four-paragraph questions in about five seconds and does
    it for a million more people every minute?'
  prefs: []
  type: TYPE_NORMAL
- en: Now, all of the chapters and concepts in this book have gotten us back to one
    fact, which is that DevOps is about delivering value and making things work. And
    that is no different when the focus of DevOps is data. In such cases, Python becomes
    even more useful because it is the language of the data operator. Most people
    and development environments for data usually default to Python these days because
    of the existence of the necessary tools for data processing and analysis. Most
    effective **DataOps** workloads will use Python in some capacity. A lot of them
    will use Python both on the running script as well as any supporting operational
    scripts that they may need to write. We will also talk about **MLOps** and the
    operations that help deliver and optimize machine learning models and algorithms.
    We will talk about all of this and more in this chapter after you have read through
    the *Technical* *requirements* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, in this chapter, you will learn the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The difference in the approach taken when DataOps/MLOps is in play as opposed
    to regular DevOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The approaches to deal with a variety of different data-based challenges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Ops behind the delivery of ChatGPT to your computer screen
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some requirements that will help you follow along with this chapter’s
    activities:'
  prefs: []
  type: TYPE_NORMAL
- en: A GitHub account and access to this book’s Git repository ([https://github.com/PacktPublishing/Hands-On-Python-for-DevOps](https://github.com/PacktPublishing/Hands-On-Python-for-DevOps))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Google account to use Google Colab
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A usable Python environment somewhere
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A nice cup of your favorite beverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How MLOps and DataOps differ from regular DevOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A question that we often encounter in any sort of technical industry in general
    is: what is the difference between a data role and a non-data role? What would
    be the difference between a software and data engineer, a data analyst and an
    accountant, or a DJ and a music composer? It is something employers ask a lot;
    people speculate on whether one is a subgroup of another or whether they are completely
    different. Even in the Swedish language, *dator* means “computer,” science is
    translated as *vetenskap*, and computer science is referred to as *datavetenskap,*
    so at some point whatever entity that designs and updates the Swedish language
    thought that there was very little to distinguish between the two.'
  prefs: []
  type: TYPE_NORMAL
- en: We will now explain this through a couple of common DevOps use cases that can
    be applied and used in these more narrowed fields of DataOps and MLOps. For DataOps,
    we will go through a method that is simple but has saved me a lot of data concatenation
    operations in Python when using JSON files. For MLOps, we will focus on the GPU
    side, which is the primary hardware that an MLOps engineer may have to work with.
  prefs: []
  type: TYPE_NORMAL
- en: DataOps use case – JSON concatenation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is quite a simple little trick, but one that is not as commonly known as
    you think. I honestly think if I can help even one person working with data with
    this section, I will have succeeded. The manipulation of JSON is a very important
    aspect of data operations that is very prominent, especially in NoSQL use cases,
    but also in a number of other cases. The ability to naturally manipulate JSON
    gives Python a major advantage over a lot of other programming languages. One
    of the most useful applications of this is the pipe (`|`) operator. This little
    operator can be used to perform concatenations, unions, and even bitwise operations
    on numbers. It is one of the many ways in which Python has made it easier to perform
    these small data operations for ease of use.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we will begin with just the function for the concatenation of one JSON
    with another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s it. That’s the code, and here’s the output of that code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Output of JSON concatenation](img/B21320_11_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Output of JSON concatenation
  prefs: []
  type: TYPE_NORMAL
- en: You’ll see that the second JSON’s value for a key overrides the value from the
    first JSON and that if they have the same common values, they will stay the same,
    and any additional values are combined into the overall JSON. So, with all that
    in mind, whenever you encounter such a problem with JSON combination (and it can
    come up quite often), you will have this little trick in your toolbelt. Now, let’s
    move on to another trick, one that will certainly help all you gaming hardware
    addicts out there. It’ll help the rest of you out too, but I like mentioning hardware
    addicts because they make the most YouTube videos and I’m hoping to get some of
    that sweet exposure.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps use case – overclocking a GPU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this modern age of AI art, image generation at the highest levels can require
    a lot of processing power. For any kind of graphical rendering, CPUs are only
    used when no other options are available and are usually not recommended for larger
    renderings. For machine learning TensorFlow algorithms, Google’s proprietary TPUs
    are the norm. But again, for anything concerning image generation or manipulation,
    a good GPU is good to have. And if the rare case comes up where that GPU needs
    a bit of extra juice to get things done, overclocking can be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'A lot of the time, GPU processors have their own drivers and with their drivers
    come their own command-line tools. Executing these before and after the use of
    overclocking or another GPU feature can be a hassle. Instead, using Python’s in-built
    `subprocess` module, we can automatically overclock or perform any other GPU processes
    that we would like. For this example, we are going to use the CLI tools for NVIDIA,
    which is probably the most popular GPU brand available at the moment. NVIDIA has
    a command-line tool called **nvidia-smi**, which also contains an overclocking
    feature and is what we are going to invoke. Now, let’s write the code block that
    will help us overclock our GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code, when run, will overclock whichever NVIDIA GPU has been
    set up on your device. This, in turn, will make processes such as image processing
    and generation faster. This can be useful when there is a higher demand for these
    resources, and it isn’t possible to shift those demands to other resources. So,
    this code can be used to temporarily overclock a GPU based on some condition that
    may cause it to be called. Once it has been overclocked, you can set it back to
    its default by running the following command (in or out of script):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: So, this is how you would manipulate GPUs using Python. A lot of this section
    has involved learning how to manipulate data and the aspects surrounding data.
    However, the data itself can be difficult to work with for a variety of other
    reasons as well. One of the primary reasons can be just how much data is there,
    which can be a lot. The next section will be all about finding ways to not be
    overwhelmed by all of the data that comes from various sources that you may have
    to deal with.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with velocity, volume, and variety
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When given any tutorial on how to process data, you are usually given a quick
    introduction to the **three Vs** (**velocity**, **volume**, and **variety**).
    These are the three ways in which the complexity of data can scale. Each of them
    presents a singularly unique problem when dealing with data, and a lot of data
    that you would have to deal with can be a combination of all three. Velocity is
    the speed of data coming in over a period of time, volume is the amount of data,
    and variety is the diversity of the data being presented.
  prefs: []
  type: TYPE_NORMAL
- en: So, this section will be divided according to the three Vs, and in each subsection,
    there will be a solution for a common problem that may arise with them. This way,
    you will get to see how Python can help in dealing with such massive amounts of
    data. Let’s start with volume as it is the simplest and probably the first thing
    that comes to people’s minds when it comes to **big data**.
  prefs: []
  type: TYPE_NORMAL
- en: Volume
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The volume of data is a pretty simple thing. It represents a certain quantity
    of data, most, if not all, of which will be of the same type. If we are going
    to deal with a large volume of data, it will require understanding the time sensitivity
    of data as well as the resources that we would have on hand. The volume of data
    that is usually processed differs based on whether the data is massive based on
    width or length (i.e., whether there are a lot of fields for one row of data or
    there is a massive number of data rows). Both of these require different solutions,
    even specialized databases sometimes. There is also the possibility of datasets
    not being numbers and letters at all but instead being files of audio or video.
    In this section, we will use an example that will be very useful when we have
    a database or data file that contains a large number of fields/columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we will need a high-volume dataset, so we will use an app called
    **Mockaroo**, which allows you to generate data fields and sample data using generative
    AI (very fitting in this chapter). Let’s go to the Mockaroo site and generate
    a few fields for our sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Mockaroo schema](img/B21320_11_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Mockaroo schema
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset we produced with Mockaroo looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – Sample CSV created by Mockaroo](img/B21320_11_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – Sample CSV created by Mockaroo
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding figure shows just a small piece of it; it’s 20 very large fields
    for 1,000 rows. Let’s write the script to parse through it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The script may seem a little redundant in terms of reading the CSV file, but
    the reason it is like this is so that all of the rows in the CSV aren’t loaded
    into the memory of the OS at the same time. This method will reduce the load on
    the memory of the data and is a great way to read large amounts of data in a system
    where the memory can’t hold a lot of data. What it does is that it reads one row
    of the data and then releases that data from the memory before reading the other
    rows. This is efficient management of memory during the reading of a high volume
    of data, which in turn makes the reading a lot faster and smoother, as demonstrated
    in this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – Workflow behind a generator](img/B21320_11_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – Workflow behind a generator
  prefs: []
  type: TYPE_NORMAL
- en: Now, that was simple enough, but what happens when it’s just one row at a time,
    but constant, such as streaming data? All of it needs to be processed live as
    it comes in. How would we achieve this? Let’s find out.
  prefs: []
  type: TYPE_NORMAL
- en: Velocity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tackling the velocity of data is a legitimate billion-dollar question. Even
    today, the biggest video streamers struggle to send out livestream data consistently.
    Of course, there are a number of reasons for this, but the fact is a lot of solutions
    don’t have the right combination of budget and quality to be consistent all of
    the time. We can get pretty close, though.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this exercise, we will be using something that a lot of people call the
    future, and perhaps the present, of data streaming: **Apache Flink**. This is
    a stream and batch processing framework developed by the Apache Software Foundation
    for a smooth, fast data flow. Unlike a lot of frameworks managed by the Apache
    Software Foundation, this one was created with the express intent of being maintained
    by them as opposed to a project created by a company and made open source for
    easier maintenance.'
  prefs: []
  type: TYPE_NORMAL
- en: Flink itself does not offer any data storage solutions and is instead simply
    supposed to process incoming data into a storage location. It has APIs in Java,
    Python, and Scala, and support on all cloud platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with Python, you will need to install `pyflink` using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Also install pandas if you have not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Alright, now let’s write some code to stream data from a bunch of JSON rows
    to a CSV table. This is just a sample program to show Flink’s workflow, but it
    does serve that purpose rather effectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this code, you’ll see that the JSON rows are inserted into a CSV using a
    temporary table as a holdover for insertion. This temporary table, when inserted,
    also inserts the data into the CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: This is a rather simple explanation of the capabilities of Flink, whose job
    is to work with essentially the same context, but for millions of bits of streaming
    data at the same time. So, a scaled-up version of the code looks similar, and
    essentially performs the same function, except it would perform those operations
    on a larger amount of data. There are a lot of other operations that Flink can
    perform, an absolute vast quantity (one of the reasons it is so popular), and
    they all follow a similar pattern and can be integrated with most available data
    sources.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will move on to deal with a complication in data that is far too often
    experienced, and indeed one that always needs to be dealt with in some form. The
    next section is about variety.
  prefs: []
  type: TYPE_NORMAL
- en: Variety
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Variety is interesting and is probably the most complicated burden that most
    people who work with data deal with. Data can come in all shapes and sizes and
    often comes in the most expected ways. Many hackers attempt SQL injection attacks
    by adding valid SQL queries as form fields, which can then cause those queries
    to run if the data input matches properly. A good quality assurance tester always
    attempts a variety of tests that try to befuddle a lot of applications by using
    data types that they should not be able to input into certain fields. But often
    – when just regular people are given access to a keyboard – what happens is that
    people will find some way to break a lot of the safety measures placed in a system
    by pure accident, showing previously unknown system bugs or vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: So, now we are going to go into an example where such a thing can happen, and
    this is especially prominent in a lot of looser NoSQL databases that may not have
    all of the standard data formatting built into them. We are going to attempt to
    insert emojis into a JSON file. Emojis are usually covered under the `UTF-8` format,
    but this format, while readily available on web pages, usually needs to be set
    on most databases for when more unconventional formats are used.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using Google Colab for this exercise because it is more efficient
    for something that is a concise proof of concept such as this. Let’s start by
    adding a JSON variable containing an emoji:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are going to insert it into a file, first without any `UTF-8` formatting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces sample JSON with the format shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – Storage of emoji without UTF-8 format](img/B21320_11_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – Storage of emoji without UTF-8 format
  prefs: []
  type: TYPE_NORMAL
- en: 'This JSON, when converted back for a web page, will require an extra parsing
    step that may slow down the web page. So, in order to avoid that, we can find
    a way to store the emoji in the way that it was input. The final output of the
    code will look more correct, like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6 – Storage of emoji with UTF-8 format](img/B21320_11_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – Storage of emoji with UTF-8 format
  prefs: []
  type: TYPE_NORMAL
- en: 'There, that is much better and will be more sustainable in the long term as
    well. The overall code for this in Google Colab will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Colab notebook for UTF-8-based storage of emoji](img/B21320_11_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Colab notebook for UTF-8-based storage of emoji
  prefs: []
  type: TYPE_NORMAL
- en: 'Those were a few simple examples to get you started on optimizing your work
    with big data. We have talked quite a bit about data and a bit about machine learning.
    But let’s round all of this out with the hottest topic of all: ChatGPT. We will
    now talk about how the DevOps behind ChatGPT works and how similar open source
    systems are widely available currently.'
  prefs: []
  type: TYPE_NORMAL
- en: The Ops behind ChatGPT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ChatGPT – in the time that I have been writing this book – has gone from being
    a hot topic to just being a fact of life, sometimes almost second nature as a
    tool for information. The way it handles data and the very nature of it have been
    topics that have brought on a lot of controversy. But one of the things that I
    get asked very often by my friends who aren’t in the industry is, how does it
    work? They see that it delivers information nearly seamlessly on whatever the
    whim of the user is and then retains that information historically in that chat
    for future questions. It also does so very quickly. So, one does wonder how it
    all works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with what ChatGPT is: it is a **large language model** (**LLM**),
    which is a very large neural network that verges on general language understanding
    (i.e., the ability to understand questions or queries and deliver back responses
    that would be appropriate to the solution). While ChatGPT is the biggest deal
    at the moment, the technology itself has been around for a few years, mostly used
    in more domain-specific chatbots. However, the newest LLMs have been made so that
    they can talk about pretty much anything, with slight knowledge specializations
    in certain fields. Even then, the concept behind ChatGPT is pretty simple: the
    more data you feed into it that it can contain, the better it becomes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The current free commercial model GPT-3.5 is made up of about 175 billion parameters
    spread over 96 neural network layers. How it did so was by inputting over 500
    billion words as tokens (numbers) and using its neural network to find associations
    between these tokens in a way that simulates human language. The set used as a
    reference for these tokens is just the internet. That’s it, it takes all the text
    and data from the internet and uses that to recreate human interaction and creativity.
    Now, most of you have probably seen what GPT-3/3.5 can do, and GPT-4 ramps that
    concept up even further, using a total of 1.7 trillion data points. As we can
    see in the following figure, it is a case of adding some parameters to a neural
    network until it creates a coherent output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8 – The workflow behind ChatGPT](img/B21320_11_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – The workflow behind ChatGPT
  prefs: []
  type: TYPE_NORMAL
- en: 'As seen in the diagram, you put the prompt in and get an answer generated by
    the trained neural network. It’s that simple. Now, you may be wondering, what
    happens in between? The answer to that is fascinating, but can be boiled down
    to a concise statement: we don’t know.'
  prefs: []
  type: TYPE_NORMAL
- en: Truly, neural networks are a mystery because they are built and modeled around
    our own neurons, so they aren’t trained by humans; they train themselves for the
    best possible success, similar to the way a human would find their best method
    of study for themselves when trying to pass a test. So, we don’t really know what
    is at the core of these neural networks; we just know we can train them to become
    good at having a conversation.
  prefs: []
  type: TYPE_NORMAL
- en: You can train a similar one at home, too. Some companies have developed more
    compressed versions of LLMs that can be placed on smaller servers, such as Meta’s
    **LLaMA**. But even besides that, you can find a never-ending amount of generative
    AI models on any cloud provider of your preference and on open source sites such
    as Hugging Face, which you can plug and play to try and understand better.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The journey of a DataOps or MLOps engineer is just a DevOps engineer who has
    gotten some understanding of data and machine learning concepts. That’s pretty
    much it. But, as we saw in this chapter, the usage of those concepts is a pretty
    useful thing.
  prefs: []
  type: TYPE_NORMAL
- en: First, we talked about the differences and similarities between DevOps and these
    associated fields and how they are connected with each other. Using that, we managed
    to produce a couple of practical use cases that can come in handy when using Python
    with DataOps and MLOps.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we talked about handling the proverbial big data. We talked about the
    aspects that make the data so big and how to tackle each of these aspects individually
    using a use case for each.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we talked about ChatGPT and how it works in delivering all the things
    that it delivers to users around the world. We discussed the simplicity of its
    complexity and its mystery, as well as the new age of open source LLMs that has
    accelerated the development of generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will get into perhaps the most powerful tool in the
    DevOps arsenal, **Infrastructure as Code** (**IaC**), and how Python is used in
    this realm.
  prefs: []
  type: TYPE_NORMAL
