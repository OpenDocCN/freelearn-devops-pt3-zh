<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer028">&#13;
			<h1 id="_idParaDest-39" class="chapter-number"><a id="_idTextAnchor038"/>2</h1>&#13;
			<h1 id="_idParaDest-40"><a id="_idTextAnchor039"/>Getting the Ball Rolling with Kubernetes and the Top Three Cloud Platforms</h1>&#13;
			<p>When starting your Kubernetes journey, the typical first step is to create a Kubernetes cluster to work with. The reason why is that if you, for example, start by creating a Kubernetes Manifest (more on this in later chapters), you’ll have nowhere to deploy the Manifest to because you don’t have a Kubernetes cluster. The other reality when it comes to Kubernetes is there’s a ton of cloud-native operations management – things such as monitoring a cluster, automating the deployment of a cluster, and scaling a cluster. Because of that, understanding cluster creation is a crucial step in your <span class="No-Break">Kubernetes journey.</span></p>&#13;
			<p>In the previous chapter, you learned not only about why Kubernetes is important but also the backstory of why engineers want to use orchestration in today’s world. In this chapter, you’re going to hit the ground running by creating and managing your very own Kubernetes clusters in the three major clouds – Azure, <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>), and <strong class="bold">Google Cloud </strong><span class="No-Break"><strong class="bold">Platform</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">GCP</strong></span><span class="No-Break">).</span></p>&#13;
			<p>By the end of this chapter, you’ll be able to fully create, deploy, manage, and automate Kubernetes clusters running in the three major clouds. The skills that you will pick up from this chapter will also translate across other Kubernetes cluster deployments. For example, you’ll be using Terraform to automate the Kubernetes cluster creation, and you can use Terraform to deploy Kubernetes clusters in other clouds and <span class="No-Break">on-premises environments.</span></p>&#13;
			<p>In this chapter, we’re going to cover the <span class="No-Break">following topics:</span></p>&#13;
			<ul>&#13;
				<li>Azure <span class="No-Break">Kubernetes Service</span></li>&#13;
				<li><span class="No-Break">AWS EKS</span></li>&#13;
				<li><span class="No-Break">GKE</span></li>&#13;
			</ul>&#13;
			<p>With each of the topics, you’ll learn how to properly run them in a production-level environment. Throughout the rest of the chapter, you’ll be working in depth with various amounts of hands-on-driven labs, creating resources automatically <span class="No-Break">and manually.</span></p>&#13;
			<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/>Technical requirements</h1>&#13;
			<p>For the purpose of this chapter, you should already know how to navigate through each cloud portal and have a general understanding of how you can automate the creation of cloud infrastructure. Although it would be great to dive into those topics in this book, these topics are huge and there are whole books out there dedicated just to them. Because of that, you should know about automated workflows, Terraform, and the cloud prior to <span class="No-Break">getting started.</span></p>&#13;
			<p>To work inside the cloud, you will need the following, all of which you can sign up for and get <span class="No-Break">free credit:</span></p>&#13;
			<ul>&#13;
				<li>An <span class="No-Break">Azure account</span></li>&#13;
				<li>An <span class="No-Break">AWS account</span></li>&#13;
				<li>A <span class="No-Break">GCP account</span></li>&#13;
				<li>An infrastructure automation tool such <span class="No-Break">as Terraform</span></li>&#13;
			</ul>&#13;
			<p>The code for this chapter is in the GitHub repository or directory found <span class="No-Break">here: </span><a href="https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch2"><span class="No-Break">https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch2</span></a><span class="No-Break">.</span></p>&#13;
			<h1 id="_idParaDest-42"><a id="_idTextAnchor041"/>Azure Kubernetes Service</h1>&#13;
			<p>When you’re using <a id="_idIndexMarker036"/>Microsoft Azure, you have a few options to choose from when using containers <span class="No-Break">and Kubernetes:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Azure Kubernetes </strong><span class="No-Break"><strong class="bold">Service</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">AKS</strong></span><span class="No-Break">)</span></li>&#13;
				<li>Azure <span class="No-Break">Container Instances</span></li>&#13;
				<li><strong class="bold">Azure Container </strong><span class="No-Break"><strong class="bold">Apps</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">ACA</strong></span><span class="No-Break">)</span></li>&#13;
			</ul>&#13;
			<p>AKS is the primary way to run Kubernetes <a id="_idIndexMarker037"/>workloads inside Azure. You do not have to worry about managing the Control Plane or API Server and instead, simply handle deploying your apps, scaling, and managing or maintaining the cloud infrastructure. However, there is still maintenance and management that you need to do for worker nodes – for example, if you want to scale Kubernetes clusters, utilize a multi-cloud model, or implement some sort of hybrid-cloud model, you would be solely responsible for implementing that setup. AKS<a id="_idIndexMarker038"/> abstracts the need to worry about managing and scaling the Control Plane or API Server, but you’re responsible for everything else (scaling workloads, monitoring, <span class="No-Break">and observability).</span></p>&#13;
			<p class="callout-heading">Important note</p>&#13;
			<p class="callout">There’s a new service that<a id="_idIndexMarker039"/> recently went <strong class="bold">Generally Available</strong> (<strong class="bold">GA</strong>) at Microsoft Build 2022 called ACA. Although we won’t be going into detail about ACA in this book, you should know that it’s essentially <em class="italic">serverless Kubernetes</em>. It’s drastically different in comparison to AKS, so if you’re planning on using ACA, ensure that you learn about those tech <span class="No-Break">spaces prior.</span></p>&#13;
			<p>In the following section, you’re going to learn how to create an AKS cluster manually first. After that, you’ll take what you learned from a manual perspective and learn how to automate it with Terraform. Then, you’ll learn about scaling AKS clusters from a vertical-autoscaling perspective. Finally, you’ll wrap up with serverless Kubernetes. Let’s dive <span class="No-Break">right in!</span></p>&#13;
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>Creating an AKS cluster manually</h2>&#13;
			<p>Before managing an<a id="_idIndexMarker040"/> AKS cluster, you have to learn how to create one. In today’s world, you’ll most likely never do this process manually because of the need for every organization to strive toward an automated and repeatable mindset. However, because you cannot automate something without doing it manually first, you’ll learn how to do that in <span class="No-Break">this section:</span></p>&#13;
			<ol>&#13;
				<li value="1">Log in to the <span class="No-Break">Azure portal.</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer012" class="IMG---Figure">&#13;
					<img src="Images/B19116_02_01.jpg" alt="Figure 2.1 – The Azure portal&#13;&#10;" width="930" height="247"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – The Azure portal</p>&#13;
			<ol>&#13;
				<li value="2">Search <a id="_idIndexMarker041"/>for <strong class="source-inline">azure </strong><span class="No-Break"><strong class="source-inline">kubernetes services</strong></span><span class="No-Break">:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer013" class="IMG---Figure">&#13;
					<img src="Images/B19116_02_02.jpg" alt="Figure 2.2 – Searching for AKS&#13;&#10;" width="1107" height="395"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Searching for AKS</p>&#13;
			<ol>&#13;
				<li value="3">Click on the <strong class="bold">Create</strong> dropdown and choose the <strong class="bold">Create a Kubernetes </strong><span class="No-Break"><strong class="bold">cluster</strong></span><span class="No-Break"> option:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer014" class="IMG---Figure">&#13;
					<img src="Images/B19116_02_03.jpg" alt="Figure 2.3 – Creating an AKS cluster&#13;&#10;" width="1111" height="640"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Creating an AKS cluster</p>&#13;
			<ol>&#13;
				<li value="4">Choose the <a id="_idIndexMarker042"/>options for your Kubernetes cluster, including the name of the cluster and your Azure <span class="No-Break">resource group:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer015" class="IMG---Figure">&#13;
					<img src="Images/B19116_02_04.jpg" alt="Figure 2.4 – Adding cluster details before its creation&#13;&#10;" width="904" height="973"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Adding cluster details before its creation</p>&#13;
			<ol>&#13;
				<li value="5">Under the <strong class="bold">Primary node pool</strong> section, you can choose what <strong class="bold">Virtual Machine</strong> (<strong class="bold">VM</strong>) size you want for your Kubernetes worker nodes, how many should be available, and <a id="_idIndexMarker043"/>whether or not you want to autoscale. One of the biggest powers behind cloud Kubernetes services such as AKS is autoscaling. In production, it’s recommended to autoscale when needed. However, you also have to understand that it comes with a cost, as extra VMs will be provisioned. Leave everything as the default for now and scroll down to the <strong class="bold">Primary node </strong><span class="No-Break"><strong class="bold">pool</strong></span><span class="No-Break"> section:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer016" class="IMG---Figure">&#13;
					<img src="Images/B19116_02_05.jpg" alt="Figure 2.5 – Specifying the worker node size, node count, and scale method&#13;&#10;" width="1087" height="511"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – Specifying the worker node size, node count, and scale method</p>&#13;
			<ol>&#13;
				<li value="6">Once you have chosen<a id="_idIndexMarker044"/> your options, which in a dev environment could be one node, but will vary in production, click the blue <strong class="bold">Review + create</strong> button. Your AKS cluster is <span class="No-Break">now created.</span></li>&#13;
			</ol>&#13;
			<p>Now that you know how to create an AKS cluster manually, it’s time to learn how to create it with Terraform so you can ensure repeatable processes throughout <span class="No-Break">your environment.</span></p>&#13;
			<h2 id="_idParaDest-44"><a id="_idTextAnchor043"/>Creating an AKS cluster with automation</h2>&#13;
			<p>In many <a id="_idIndexMarker045"/>production-level cases, you’ll run the following Terraform code within a CI/CD pipeline to ensure repeatability. For the purpose of this section, you can run it locally. You’ll first see the <strong class="source-inline">main.tf</strong> configuration and then you’ll take a look <span class="No-Break">at </span><span class="No-Break"><strong class="source-inline">variables.tf</strong></span><span class="No-Break">.</span></p>&#13;
			<p>Let’s break down <span class="No-Break">the code.</span></p>&#13;
			<p>First, there’s the Terraform provider itself. The <strong class="source-inline">azurerm</strong> Terraform provider is used to make API calls to <span class="No-Break">Azure programmatically:</span></p>&#13;
			<pre class="source-code">&#13;
terraform {&#13;
  required_providers {&#13;
    azurerm = {&#13;
      source  = "hashicorp/azurerm"&#13;
    }&#13;
  }&#13;
}&#13;
provider "azurerm" {&#13;
  features {}&#13;
}</pre>&#13;
			<p>Next, there’s the <strong class="source-inline">azurerm_kubernetes_cluster</strong> Terraform resource block, which is used to<a id="_idIndexMarker046"/> create the AKS cluster. There are a few key parameters, including the name and <strong class="bold">Domain Name System</strong> (<strong class="bold">DNS</strong>) <strong class="bold">Fully Qualified Domain Name</strong> (<strong class="bold">FQDN</strong>). The Kubernetes<a id="_idIndexMarker047"/> worker nodes are created via the <strong class="source-inline">default_node_pool</strong> parameter block. You can specify<a id="_idIndexMarker048"/> the VM size, node count, and name of the <span class="No-Break">node pool:</span></p>&#13;
			<pre class="source-code">&#13;
resource "azurerm_kubernetes_cluster" "k8squickstart" {&#13;
  name                = var.name&#13;
  location            = var.location&#13;
  resource_group_name = var.resource_group_name&#13;
  dns_prefix          = "${var.name}-dns01"&#13;
  default_node_pool {&#13;
    name       = "default"&#13;
    node_count = var.node_count&#13;
    vm_size    = "Standard_A2_v2"&#13;
  }&#13;
  identity {&#13;
    type = "SystemAssigned"&#13;
  }&#13;
  tags = {&#13;
    Environment = "Production"&#13;
  }&#13;
}</pre>&#13;
			<p>Putting it all together, you’ll have a Terraform configuration that creates an AKS cluster <span class="No-Break">in Azure.</span></p>&#13;
			<p>Now that you have the<a id="_idIndexMarker049"/> Terraform configuration, you’ll need variables to pass in. The variables allow your code to stay repeatable – in accordance with the <strong class="bold">Don’t Repeat Yourself</strong> (<strong class="bold">DRY</strong>) principle – so that you don’t have to <a id="_idIndexMarker050"/>continuously change hardcoded values or create new configurations for <span class="No-Break">each environment.</span></p>&#13;
			<p>There are <span class="No-Break">four variables:</span></p>&#13;
			<ul>&#13;
				<li><strong class="source-inline">name</strong>: Name of the <span class="No-Break">AKS cluster</span></li>&#13;
				<li><strong class="source-inline">resource_group_name</strong>: The resource group that AKS will <span class="No-Break">reside in</span></li>&#13;
				<li><strong class="source-inline">location</strong>: The region that the AKS cluster will <span class="No-Break">reside in</span></li>&#13;
				<li><strong class="source-inline">node_count</strong>: How many Kubernetes worker nodes will be in the <span class="No-Break">AKS cluster</span></li>&#13;
			</ul>&#13;
			<p>These variables can be seen in the following <span class="No-Break">code block:</span></p>&#13;
			<pre class="source-code">&#13;
variable "name" {&#13;
  type = <strong class="bold">string</strong>&#13;
  default = "aksenvironment01"&#13;
}&#13;
variable "resource_group_name" {&#13;
  type = <strong class="bold">string</strong>&#13;
  default = "devrelasaservice"&#13;
}&#13;
variable "location" {&#13;
  type = <strong class="bold">string</strong>&#13;
  default = "eastus"&#13;
}&#13;
variable "node_count" {&#13;
  type = <strong class="bold">string</strong>&#13;
  default = 3&#13;
}</pre>&#13;
			<p>Putting both the <strong class="source-inline">main.tf</strong> and <strong class="source-inline">variables.tf</strong> configuration files in the same directory will create a Terraform<a id="_idIndexMarker051"/> module for creating an AKS cluster. You can use this for almost any environment, change configurations (such as the node count) depending on your needs, and make your <span class="No-Break">process repeatable.</span></p>&#13;
			<h2 id="_idParaDest-45"><a id="_idTextAnchor044"/>Scaling an AKS cluster</h2>&#13;
			<p>Scaling an AKS cluster is<a id="_idIndexMarker052"/> made possible by implementing the Kubernetes Cluster Autoscaler. Much like autoscaling groups for Azure VMs, AKS decides on how and why to scale the cluster based on the worker node load, which is the Azure VMs in the background. The Cluster Autoscaler is typically deployed to the Kubernetes cluster with the <strong class="source-inline">cluster-autoscaler</strong> <span class="No-Break">container image.</span></p>&#13;
			<p>Log in to the Azure portal and go to the AKS service. Once there, go to <strong class="bold">Settings</strong> | <span class="No-Break"><strong class="bold">Node pools</strong></span><span class="No-Break">:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer017" class="IMG---Figure">&#13;
					<img src="Images/B19116_02_06.jpg" alt="Figure 2.6 – Node pools settings&#13;&#10;" width="250" height="505"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – Node pools settings</p>&#13;
			<p>Click on the three dots as <a id="_idIndexMarker053"/>per the following screenshot and choose the <strong class="bold">Scale node </strong><span class="No-Break"><strong class="bold">pool</strong></span><span class="No-Break"> option:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer018" class="IMG---Figure">&#13;
					<img src="Images/B19116_02_07.jpg" alt="Figure 2.7 – Scaling node pools&#13;&#10;" width="1092" height="252"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7 – Scaling node pools</p>&#13;
			<p>The <strong class="bold">Scale node pool</strong> pane will come up and you’ll see the option to automatically scale the node pool or manually scale it and choose how many nodes you want to <span class="No-Break">make available:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer019" class="IMG---Figure">&#13;
					<img src="Images/B19116_02_08.jpg" alt="Figure 2.8 – Specifying the node count and scale method&#13;&#10;" width="550" height="449"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8 – Specifying the node count and scale method</p>&#13;
			<p>From an automation<a id="_idIndexMarker054"/> and repeatability standpoint, you can do the same thing. The following is an example of creating the <strong class="source-inline">azurerm_kubernetes_cluster_node_pool</strong> Terraform resource with the <strong class="source-inline">enable_auto_scaling</strong> parameter set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">true</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
resource "azurerm_kubernetes_cluster_node_pool" "example" {&#13;
  name                  = "internal"&#13;
  kubernetes_cluster_id = azurerm_kubernetes_cluster.example.id&#13;
  vm_size               = "Standard_DS2_v2"&#13;
  node_count            = 1&#13;
  enable_auto_scaling = true&#13;
  tags = {&#13;
    Environment = "Production"&#13;
  }&#13;
}</pre>&#13;
			<p>Node pools are simply Azure VMs that run as Kubernetes worker nodes. When thinking about autoscaling, remember that horizontal autoscaling comes at a cost. Although it’s very much needed, you should limit the<a id="_idIndexMarker055"/> amount of Kubernetes worker nodes that are available. That way, you can keep track of costs and how many resources your containerized <span class="No-Break">apps need.</span></p>&#13;
			<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/>AKS and Virtual Kubelet</h2>&#13;
			<p>To wrap things up with AKS, there is<a id="_idIndexMarker056"/> Virtual Kubelet. Virtual Kubelet isn’t AKS-specific. Virtual Kubelet allows you to take Kubernetes and connect it to other APIs. A<a id="_idIndexMarker057"/> kubelet is the node agent that runs on each node. It’s responsible for registering the node with Kubernetes. AKS Virtual Kubelet registers serverless <span class="No-Break">container platforms.</span></p>&#13;
			<p>In Azure, it’s <strong class="bold">Azure Container Instances</strong> (<strong class="bold">ACI</strong>). ACI is a way to run containers without using Kubernetes. If<a id="_idIndexMarker058"/> someone using Kubernetes doesn’t want to scale out worker nodes due to cost or management, they can use ACI bursting, which uses Virtual Kubelet. It essentially tells Kubernetes to send Deployments, Pods, and other workloads to ACI instead of keeping them on the local <span class="No-Break">Kubernetes cluster.</span></p>&#13;
			<p>Now that ACA is GA, chances are you’ll see less of this type of implementation. However, it’s still a great use case for teams that want to scale, but don’t want the overhead of managing large <span class="No-Break">AKS clusters.</span></p>&#13;
			<h2 id="_idParaDest-47"><a id="_idTextAnchor046"/>Managing and maintaining AKS clusters</h2>&#13;
			<p>Once a Kubernetes cluster is created and running, the mindset shift moves from day-one Ops to day-two Ops. Day-two Ops <a id="_idIndexMarker059"/>will be focused on <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li>Managing <span class="No-Break">the cluster</span></li>&#13;
				<li>Monitoring and maintaining <span class="No-Break">the cluster</span></li>&#13;
				<li>Deploying applications and getting <span class="No-Break">services running</span></li>&#13;
			</ul>&#13;
			<p>When managing an AKS cluster, the biggest thing to think about is where the configuration exists and what tools you’re using to manage it. For example, the Terraform configuration could be in GitHub and you could be managing the cluster via Azure Monitor and the rest of the Azure configurations that are available in the AKS cluster. Day-two Ops is about making sure the cluster and your configurations are running as you expect. The focus is really on the question “<em class="italic">is my environment working and performing </em><span class="No-Break"><em class="italic">as intended?</em></span><span class="No-Break">”</span></p>&#13;
			<p>When it comes to<a id="_idIndexMarker060"/> monitoring, alerting, and overall observability, there are several options. Azure Monitor and Azure Insights are built into Azure, but if you have a multi-cloud or a hybrid-cloud environment, you may want to look at other options. That’s where a combination of Prometheus and Grafana can come into play. Whichever tool you choose (because there are several) isn’t important. What’s important is what you monitor. You’ll need a combination of monitoring the cluster itself and the Kubernetes resources (for example, Pods, Services, or Ingresses) inside of <span class="No-Break">the cluster.</span></p>&#13;
			<p>Because managing Kubernetes clusters doesn’t differ all that much (other than the native cloud tools), it’s safe to<a id="_idIndexMarker061"/> assume that whether you’re using AKS, EKS, or GKE, the path<a id="_idIndexMarker062"/> forward will be<a id="_idTextAnchor047"/><a id="_idTextAnchor048"/> <span class="No-Break">the same.</span></p>&#13;
			<h1 id="_idParaDest-48"><a id="_idTextAnchor049"/>AWS EKS</h1>&#13;
			<p>When you’re using AWS, you have a <a id="_idIndexMarker063"/>few options to choose from when using containers <span class="No-Break">and Kubernetes:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break">EKS</span></li>&#13;
				<li>EKS with <span class="No-Break">Fargate profiles</span></li>&#13;
				<li><strong class="bold">Elastic Container </strong><span class="No-Break"><strong class="bold">Service</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">ECS</strong></span><span class="No-Break">)</span></li>&#13;
			</ul>&#13;
			<p>EKS is the primary way to run Kubernetes workloads inside AWS. If you don’t want to go the Kubernetes route but still want scalability, you can use ECS, which gives you the ability to scale and create reliable microservices but <span class="No-Break">without Kubernetes.</span></p>&#13;
			<p>As with AKS, you don’t have to worry about managing the Control Plane or API Server when it comes to EKS. You only have to worry about managing and scaling worker nodes. If you want to, you can even take it a step further and implement EKS with Fargate profiles, which abstracts the Control Plane or API Server and the worker nodes to ensure a fully <em class="italic">serverless </em><span class="No-Break"><em class="italic">Kubernetes</em></span><span class="No-Break"> experience.</span></p>&#13;
			<p>As with AKS, in the following few sections, you’re going to learn how to create an EKS cluster manually first. After that, you’ll take what you learned from a manual perspective and learn how to <a id="_idIndexMarker064"/>automate it with Terraform. Then, you’ll learn about scaling EKS clusters from a vertical-autoscaling perspective. Finally, you’ll wrap up with <span class="No-Break">serverless Kubernetes.</span></p>&#13;
			<h2 id="_idParaDest-49"><a id="_idTextAnchor050"/>Creating an EKS cluster manually</h2>&#13;
			<p>Much like AKS clusters, before <a id="_idIndexMarker065"/>creating EKS clusters from an automated perspective, you must learn how to manually deploy them. In this section, you’ll learn how to deploy an EKS cluster with a node group in the <span class="No-Break">AWS Console:</span></p>&#13;
			<ol>&#13;
				<li value="1">Log in to the AWS portal and search for the <span class="No-Break">EKS service:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer020" class="IMG---Figure">&#13;
					<img src="Images/B19116_02_09.jpg" alt="Figure 2.9 – The AWS portal&#13;&#10;" width="884" height="220"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.9 – The AWS portal</p>&#13;
			<ol>&#13;
				<li value="2">Click the orange <strong class="bold">Add cluster</strong> button and choose the <span class="No-Break"><strong class="bold">Create</strong></span><span class="No-Break"> option:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer021" class="IMG---Figure">&#13;
					<img src="Images/B19116_02_10.jpg" alt="Figure 2.10 – Adding a cluster&#13;&#10;" width="1112" height="362"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.10 – Adding a cluster</p>&#13;
			<ol>&#13;
				<li value="3">When configuring an EKS cluster, you’ll have to provide a few options to uniquely identify it, which include <span class="No-Break">the following:</span><ul><li>The EKS <span class="No-Break">cluster name</span></li><li>The Kubernetes <span class="No-Break">API version</span></li><li>The <span class="No-Break">IAM role</span></li></ul></li>&#13;
			</ol>&#13;
			<p>The IAM role is very<a id="_idIndexMarker066"/> important because there are specific policies that must be attached to the role that you’re assigning to the EKS cluster. Those policies include <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break"><strong class="source-inline">AmazonEC2ContainerRegistryReadOnly</strong></span></li>&#13;
				<li><span class="No-Break"><strong class="source-inline">AmazonEKSClusterPolicy</strong></span></li>&#13;
			</ul>&#13;
			<p>Without the proceeding policies, the EKS cluster will not work <span class="No-Break">as expected:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer022" class="IMG---Figure">&#13;
					<img src="Images/B19116_02_11.jpg" alt="Figure 2.11 – Configuring a cluster&#13;&#10;" width="820" height="887"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.11 – Configuring a cluster</p>&#13;
			<ol>&#13;
				<li value="4">Next, you’ll need to set up networking. The absolute minimum amount of subnets that you<a id="_idIndexMarker067"/> want to use is two public subnets with different CIDRs in different availability zones. For a full list of recommendations, check out the AWS <span class="No-Break">docs (</span><a href="https://docs.aws.amazon.com/eks/latest/userguide/network_reqs.html"><span class="No-Break">https://docs.aws.amazon.com/eks/latest/userguide/network_reqs.html</span></a><span class="No-Break">):</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer023" class="IMG---Figure">&#13;
					<img src="Images/B19116_02_12.jpg" alt="Figure 2.12 – Specifying the network configuration&#13;&#10;" width="799" height="768"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.12 – Specifying the network configuration</p>&#13;
			<ol>&#13;
				<li value="5">When configuring the<a id="_idIndexMarker068"/> cluster endpoint access, you have <span class="No-Break">three options:</span><ul><li><strong class="bold">Public</strong> means the EKS cluster is essentially open to <span class="No-Break">the world</span></li><li><strong class="bold">Public and private</strong> means the API Server or Control Plane is open to the outside world, but worker node traffic will <span class="No-Break">remain internal</span></li><li><strong class="bold">Private</strong> means the EKS cluster is only available inside the AWS <strong class="bold">Virtual Private </strong><span class="No-Break"><strong class="bold">Cloud</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">VPC</strong></span><span class="No-Break">):</span></li></ul></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer024" class="IMG---Figure">&#13;
					<img src="Images/B19116_02_13.jpg" alt="Figure 2.13 – Configuring cluster API access&#13;&#10;" width="754" height="236"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.13 – Configuring cluster API access</p>&#13;
			<ol>&#13;
				<li value="6">The last piece from a networking <a id="_idIndexMarker069"/>perspective is choosing the <strong class="bold">Container Networking Interface</strong> (<strong class="bold">CNI</strong>) and the version of CoreDNS. Choosing the latest typically makes the <span class="No-Break">most sense:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer025" class="IMG---Figure">&#13;
					<img src="Images/B19116_02_14.jpg" alt="Figure 2.14 – Network add-ons&#13;&#10;" width="809" height="657"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.14 – Network add-ons</p>&#13;
			<ol>&#13;
				<li value="7">Click the orange <span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break"> button.</span></li>&#13;
				<li>The final piece when<a id="_idIndexMarker070"/> creating the EKS cluster is the API logging. Regardless of where you plan to keep logs, traces, and metrics from an observability perspective, you must turn this option <em class="italic">on</em> if you want your cluster to record any type <span class="No-Break">of logs:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer026" class="IMG---Figure">&#13;
					<img src="Images/B19116_02_15.jpg" alt="Figure 2.15 – Configuring observability&#13;&#10;" width="818" height="545"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.15 – Configuring observability</p>&#13;
			<ol>&#13;
				<li value="9">After you choose your logging options, click the orange <strong class="bold">Next</strong> button and you’ll be at the last page to review and create your <span class="No-Break">EKS cluster.</span></li>&#13;
			</ol>&#13;
			<p>Now that you know how to<a id="_idIndexMarker071"/> create an EKS cluster manually, it’s time to learn how to create it with Terraform so you can ensure repeatable processes throughout <span class="No-Break">your environment.</span></p>&#13;
			<h2 id="_idParaDest-50"><a id="_idTextAnchor051"/>Creating an EKS cluster with Terraform</h2>&#13;
			<p>In many production-level<a id="_idIndexMarker072"/> cases, you’ll run the following Terraform code within a<a id="_idIndexMarker073"/> CI/CD pipeline to ensure repeatability. For the purposes of this section, you can run <span class="No-Break">it locally.</span></p>&#13;
			<p>First, you’ll see the <strong class="source-inline">main.tf</strong> configuration and then you’ll take a look <span class="No-Break">at </span><span class="No-Break"><strong class="source-inline">variables.tf</strong></span><span class="No-Break">.</span></p>&#13;
			<p>Because the <strong class="source-inline">main.tf</strong> configuration for AWS EKS is much longer than EKS, let’s break it down into chunks for an <span class="No-Break">easier explanation:</span></p>&#13;
			<ol>&#13;
				<li value="1">First, there’s the Terraform provider block. To ensure repeatability throughout your team, you can use an S3 bucket backend for storing your <strong class="source-inline">TFSTATE</strong>. The Terraform block also includes the AWS <span class="No-Break">Terraform provider:</span><pre class="console">&#13;
terraform {</pre><pre class="console">&#13;
  backend "s3" {</pre><pre class="console">&#13;
    bucket = "terraform-state-k8senv"</pre><pre class="console">&#13;
    key    = "eks-terraform-workernodes.tfstate"</pre><pre class="console">&#13;
    region = "us-east-1"</pre><pre class="console">&#13;
  }</pre><pre class="console">&#13;
  required_providers {</pre><pre class="console">&#13;
    aws = {</pre><pre class="console">&#13;
      source = "hashicorp/aws"</pre><pre class="console">&#13;
    }</pre><pre class="console">&#13;
  }</pre><pre class="console">&#13;
}</pre></li>&#13;
				<li>Next, the first<a id="_idIndexMarker074"/> resource is created. The resources <a id="_idIndexMarker075"/>allow an IAM role to be attached to the EKS cluster. For EKS to access various components and services from AWS, plus worker nodes, there are a few policies that need to <span class="No-Break">be attached:</span><pre class="console">&#13;
resource "aws_iam_role" "eks-iam-role" {</pre><pre class="console">&#13;
  name = "k8squickstart-eks-iam-role"</pre><pre class="console">&#13;
  path = "/"</pre><pre class="console">&#13;
  assume_role_policy = &lt;&lt;EOF</pre><pre class="console">&#13;
{</pre><pre class="console">&#13;
  "Version": "2012-10-17",</pre><pre class="console">&#13;
  "Statement": [</pre><pre class="console">&#13;
    {</pre><pre class="console">&#13;
      "Effect": "Allow",</pre><pre class="console">&#13;
      "Principal": {</pre><pre class="console">&#13;
        "Service": "eks.amazonaws.com"</pre><pre class="console">&#13;
      },</pre><pre class="console">&#13;
      "Action": "sts:AssumeRole"</pre><pre class="console">&#13;
    }</pre><pre class="console">&#13;
  ]</pre><pre class="console">&#13;
}</pre><pre class="console">&#13;
EOF</pre><pre class="console">&#13;
}</pre></li>&#13;
				<li>Following the IAM<a id="_idIndexMarker076"/> role are the IAM policies that have <a id="_idIndexMarker077"/>to be attached to the role. The two policies that you’ll need for a successful EKS deployment are <span class="No-Break">the following:</span><ul><li><strong class="source-inline">AmazonEKSClusterPolicy</strong>: This provides Kubernetes with the permissions it requires to manage resources on <span class="No-Break">your behalf:</span><pre class="console">&#13;
resource "aws_iam_role_policy_attachment" "AmazonEKSClusterPolicy" {</pre><pre class="console">&#13;
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"</pre><pre class="console">&#13;
  role       = aws_iam_role.eks-iam-role.name</pre><pre class="console">&#13;
}</pre></li><li><strong class="source-inline">AmazonEC2ContainerRegistryReadOnly</strong>: This provides read-only access to Elastic Container Registry if you decide to put your container <span class="No-Break">images there:</span><pre class="console">&#13;
resource "aws_iam_role_policy_attachment" "AmazonEC2ContainerRegistryReadOnly-EKS" {</pre><pre class="console">&#13;
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"</pre><pre class="console">&#13;
  role       = aws_iam_role.eks-iam-role.name</pre><pre class="console">&#13;
}</pre></li></ul></li>&#13;
				<li>Once the IAM role and<a id="_idIndexMarker078"/> policies are defined, it’s time to <a id="_idIndexMarker079"/>create the EKS cluster itself. The EKS cluster resource will create EKS itself, enable logging, and attach the IAM role that you <span class="No-Break">created earlier:</span><pre class="console">&#13;
resource "aws_eks_cluster" "k8squickstart-eks" {</pre><pre class="console">&#13;
  name = "k8squickstart-cluster"</pre><pre class="console">&#13;
  role_arn = aws_iam_role.eks-iam-role.arn</pre><pre class="console">&#13;
  enabled_cluster_log_types = ["api", "audit", "scheduler", "controllerManager"]</pre><pre class="console">&#13;
  vpc_config {</pre><pre class="console">&#13;
    subnet_ids = [var.subnet_id_1, var.subnet_id_2]</pre><pre class="console">&#13;
  }</pre><pre class="console">&#13;
  depends_on = [</pre><pre class="console">&#13;
    aws_iam_role.eks-iam-role,</pre><pre class="console">&#13;
  ]</pre><pre class="console">&#13;
}</pre></li>&#13;
				<li>The next resource is another IAM role, which is for the worker nodes. When creating an EKS cluster, you’ll have<a id="_idIndexMarker080"/> multiple resources that are created because you’re creating two sets <span class="No-Break">of services:</span><ul><li>The EKS cluster itself with <a id="_idIndexMarker081"/>all of its permissions and policies that <span class="No-Break">are needed</span></li><li>The Kubernetes worker nodes with all of the permissions and <span class="No-Break">policies needed:</span><pre class="console">&#13;
resource "aws_iam_role" "workernodes" {</pre><pre class="console">&#13;
  name = "eks-node-group-example"</pre><pre class="console">&#13;
  assume_role_policy = jsonencode({</pre><pre class="console">&#13;
    Statement = [{</pre><pre class="console">&#13;
      Action = "sts:AssumeRole"</pre><pre class="console">&#13;
      Effect = "Allow"</pre><pre class="console">&#13;
      Principal = {</pre><pre class="console">&#13;
        Service = "ec2.amazonaws.com"</pre><pre class="console">&#13;
      }</pre><pre class="console">&#13;
    }]</pre><pre class="console">&#13;
    Version = "2012-10-17"</pre><pre class="console">&#13;
  })</pre><pre class="console">&#13;
}</pre></li></ul></li>&#13;
				<li>Once the IAM role for the worker nodes is created, there are a few policies that you’ll need <span class="No-Break">to attach:</span><ul><li><strong class="source-inline">AmazonEKSWorkerNodePolicy</strong>: This provides Kubernetes the permissions it requires to manage resources on <span class="No-Break">your behalf:</span><pre class="console">&#13;
resource "aws_iam_role_policy_attachment" "AmazonEKSWorkerNodePolicy" {</pre><pre class="console">&#13;
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"</pre><pre class="console">&#13;
  role       = aws_iam_role.workernodes.name</pre><pre class="console">&#13;
}</pre></li><li><strong class="source-inline">AmazonEKS_CNI_Policy</strong>: This <a id="_idIndexMarker082"/>attaches the CNI policy for Kubernetes internal <a id="_idIndexMarker083"/><span class="No-Break">networking (kubeproxy):</span><pre class="console">&#13;
resource "aws_iam_role_policy_attachment" "AmazonEKS_CNI_Policy" {</pre><pre class="console">&#13;
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"</pre><pre class="console">&#13;
  role       = aws_iam_role.workernodes.name</pre><pre class="console">&#13;
}</pre></li><li><strong class="source-inline">EC2InstanceProfileForImageBuilderECRContainerBuilds</strong>: EC2 Image Builder uses a service-linked role to grant permissions to other AWS services on <span class="No-Break">your behalf:</span><pre class="console">&#13;
resource "aws_iam_role_policy_attachment" "EC2InstanceProfileForImageBuilderECRContainerBuilds" {</pre><pre class="console">&#13;
  policy_arn = "arn:aws:iam::aws:policy/EC2InstanceProfileForImageBuilderECRContainerBuilds"</pre><pre class="console">&#13;
  role       = aws_iam_role.workernodes.name</pre><pre class="console">&#13;
}</pre></li><li><strong class="source-inline">AmazonEC2ContainerRegistryReadOnly</strong>: This provides read-only access to Elastic Container Registry if you decide to put your<a id="_idIndexMarker084"/> container <span class="No-Break">images</span><span class="No-Break"><a id="_idIndexMarker085"/></span><span class="No-Break"> there:</span><pre class="console">&#13;
resource "aws_iam_role_policy_attachment" "AmazonEC2ContainerRegistryReadOnly" {</pre><pre class="console">&#13;
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"</pre><pre class="console">&#13;
  role       = aws_iam_role.workernodes.name</pre><pre class="console">&#13;
}</pre></li><li><strong class="source-inline">CloudWatchAgentServerPolicy</strong>: This allows the worker nodes to run the CloudWatch agent for monitoring, logging, tracing, <span class="No-Break">and metrics:</span><pre class="console">&#13;
resource "aws_iam_role_policy_attachment" "CloudWatchAgentServerPolicy-eks" {</pre><pre class="console">&#13;
  policy_arn = "arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy"</pre><pre class="console">&#13;
  role       = aws_iam_role.workernodes.name</pre><pre class="console">&#13;
}</pre></li></ul></li>&#13;
				<li>The final part once the IAM role and policies have been created is to create the EKS node group resource, which is the Kubernetes worker nodes. You’ll define the following: <ul><li>The IAM role and <span class="No-Break">subnet IDs:</span><pre class="console">&#13;
resource "aws_eks_node_group" "worker-node-group" {</pre><pre class="console">&#13;
  cluster_name    = aws_eks_cluster.k8squickstart-eks.name</pre><pre class="console">&#13;
  node_group_name = "k8squickstart-workernodes"</pre><pre class="console">&#13;
  node_role_arn   = aws_iam_role.workernodes.arn</pre><pre class="console">&#13;
  subnet_ids      = [var.subnet_id_1, var.subnet_id_2]</pre><pre class="console">&#13;
  instance_types = ["t3.xlarge"]</pre></li><li>The desired scale size <a id="_idIndexMarker086"/><span class="No-Break">for autoscaling:</span><pre class="console">&#13;
  scaling_config {</pre><pre class="console">&#13;
    desired_size = 3</pre><pre class="console">&#13;
    max_size     = 4</pre><pre class="console">&#13;
    min_size     = 2</pre><pre class="console">&#13;
  }</pre></li><li>The policies that the <a id="_idIndexMarker087"/>resource <span class="No-Break">depends on:</span><pre class="console">&#13;
  depends_on = [</pre><pre class="console">&#13;
    aws_iam_role_policy_attachment.AmazonEKSWorkerNodePolicy,</pre><pre class="console">&#13;
    aws_iam_role_policy_attachment.AmazonEKS_CNI_Policy,</pre><pre class="console">&#13;
    </pre><pre class="console">&#13;
  ]</pre><pre class="console">&#13;
}</pre></li></ul></li>&#13;
				<li>Now that you have the Terraform configuration, you’ll need variables to pass in. The variables allow your code to stay repeatable, so you don’t have to continuously change hardcoded values or create new configurations for <span class="No-Break">each environment.</span></li>&#13;
			</ol>&#13;
			<p>The two variables you’ll need are for the subnet IDs in the VPC of your choosing that will work with EKS. You<a id="_idIndexMarker088"/> can pass in two public subnet IDs that are in different <span class="No-Break">availability zones:</span></p>&#13;
			<pre class="console">&#13;
variable "subnet_id_1" {&#13;
  type = string&#13;
  default = ""&#13;
}&#13;
variable "subnet_id_2" {&#13;
  type = string&#13;
  default = ""&#13;
}</pre>&#13;
			<p>Putting it all together, you’ll have<a id="_idIndexMarker089"/> a Terraform configuration that creates an <span class="No-Break">AKS cluster.</span></p>&#13;
			<h2 id="_idParaDest-51"><a id="_idTextAnchor052"/>Scaling an EKS cluster</h2>&#13;
			<p>Scaling an EKS cluster is <a id="_idIndexMarker090"/>made possible by implementing the Kubernetes Cluster Autoscaler. Much like autoscaling EC2 instances, EKS decides on how and why to scale the cluster based on a load perspective. The Cluster Autoscaler is typically deployed to the Kubernetes cluster using the <strong class="source-inline">cluster-autoscaler</strong> <span class="No-Break">container image.</span></p>&#13;
			<p>Inside the Kubernetes GitHub repo, under the <strong class="source-inline">cluster-autoscaler</strong> directory, there’s a list of cloud providers. One of those cloud providers is AWS. Inside the AWS directory, there’s an example Kubernetes Manifest called <strong class="source-inline">cluster-autoscaler-autodiscover.yaml</strong>, which shows that it’s using the <strong class="source-inline">cluster-autoscaler</strong> container image. It runs as a Kubernetes Deployment on your cluster and listens for certain resource limits. To autoscale the cluster, you’ll need an IAM role with the <strong class="source-inline">AmazonEKSClusterAutoscalerPolicy</strong> policy attached <span class="No-Break">to it.</span></p>&#13;
			<p>Now that you know about scaling an EKS cluster and how it’s possible with <strong class="source-inline">cluster-autoscaler</strong>, let’s talk about serverless Kubernetes with AWS Fargate profiles and how they can help automate<a id="_idIndexMarker091"/> <span class="No-Break">day-one Ops.</span></p>&#13;
			<h2 id="_idParaDest-52"><a id="_idTextAnchor053"/>EKS Fargate profiles</h2>&#13;
			<p>The content around Fargate profiles is pretty similar to AKS Virtual Kubelet and ACI bursting. However, you <a id="_idIndexMarker092"/>don’t need to deploy Virtual Kubelet manually as you do in AKS. Instead, you can set up Fargate profiles to act as your Kubernetes worker nodes. Virtual Kubelet is still running on Fargate to interact with the EKS API Server or Control Plane, but it’s sort of <span class="No-Break">done automatically.</span></p>&#13;
			<p>The biggest difference <a id="_idIndexMarker093"/>here is that you don’t have to manage the worker nodes. Instead, Fargate profiles are like serverless Kubernetes. You deploy the EKS cluster, which is the API Server or Control Plane. Then, you deploy a Fargate profile, which is where your Kubernetes resources (for example, Deployments, Pods, and Services) run. You don’t have to worry about cluster management or maintaining EC2 instances that would otherwise be running as your Kubernetes <span class="No-Break">worker nodes.</span></p>&#13;
			<p>To add a Fargate profile on your EKS cluster, you go into the <strong class="bold">Compute</strong> tab of the EKS cluster and you’ll see an option for adding or creating a Fargate profile, as seen in the <span class="No-Break">following screenshot:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer027" class="IMG---Figure">&#13;
					<img src="Images/B19116_02_16.jpg" alt="Figure 2.16 – Fargate profiles and compute&#13;&#10;" width="1209" height="754"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.16 – Fargate profiles and compute</p>&#13;
			<p>Now that you know how<a id="_idIndexMarker094"/> to create an EKS cluster manually and automatically, and <a id="_idIndexMarker095"/>are also familiar with the day-two Ops considerations with autoscaling and serverless Kubernetes, it’s time to learn about the final <em class="italic">big 3</em> Kubernetes service – <span class="No-Break">GKE.</span></p>&#13;
			<h1 id="_idParaDest-53"><a id="_idTextAnchor054"/>GKE</h1>&#13;
			<p>When you’re using GCP, you have a<a id="_idIndexMarker096"/> few options to choose from when using containers <span class="No-Break">and Kubernetes:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break">GKE</span></li>&#13;
				<li><span class="No-Break">GKE Autopilot</span></li>&#13;
				<li>Google <span class="No-Break">Cloud Run</span></li>&#13;
			</ul>&#13;
			<p>GKE is the primary way to run Kubernetes workloads inside of GCP. If you don’t want to go the Kubernetes route but still want scalability, you can use Google Cloud Run. Cloud Run gives you the ability to scale and create reliable microservices, but without Kubernetes. It supports Node.js, Go, Java, Kotlin, Scala, Python, .NET, <span class="No-Break">and Docker.</span></p>&#13;
			<p>As with AKS and EKS, you <a id="_idIndexMarker097"/>don’t have to worry about managing the Control Plane or API Server when it comes to GKE. You only have to worry about managing and scaling worker nodes. If you want to, you can even take it a step further and implement GKE Autopilot, which abstracts both the Control Plane or API Server and the worker nodes to ensure a fully <em class="italic">serverless </em><span class="No-Break"><em class="italic">Kubernetes</em></span><span class="No-Break"> experience.</span></p>&#13;
			<p>There have been many debates inside of container and DevOps communities around which Kubernetes service in the cloud is the superior choice. Although we’re not here to pick sides, a lot of engineers love GKE and believe it’s a spectacular way to implement Kubernetes. Since Kubernetes originated at Google, it makes sense that the GKE service would be incredibly reliable with well-thought-out features <span class="No-Break">and implementations.</span></p>&#13;
			<p>In the following section, you will learn about creating a GKE cluster automatically using Terraform and how to think about serverless Kubernetes using <span class="No-Break">GKE Autopilot.</span></p>&#13;
			<p class="callout-heading">Important note</p>&#13;
			<p class="callout">We’re skipping a section on scaling the GKE cluster because it’s the same concept as the other clouds. It uses the Kubernetes Autoscaler in the background. All the autoscalers are considered horizontal autoscalers, as they create new worker nodes or VMs to run <span class="No-Break">Kubernetes workloads.</span></p>&#13;
			<h2 id="_idParaDest-54"><a id="_idTextAnchor055"/>Creating a GKE cluster with Terraform</h2>&#13;
			<p>Throughout this chapter, you’ve learned <a id="_idIndexMarker098"/>several manual ways of creating a Kubernetes cluster in the cloud. Instead of continuing down the<a id="_idIndexMarker099"/> manual road, let’s jump right into automating the repeatable process of creating a GKE cluster <span class="No-Break">with Terraform.</span></p>&#13;
			<p>What you’ll find with GKE is that it’s much less code compared to EKS, for example. You’ll see the <strong class="source-inline">main.tf</strong> configuration first and then you’ll take a look at <strong class="source-inline">variables.tf</strong>. Let’s break down the <span class="No-Break">following code:</span></p>&#13;
			<ol>&#13;
				<li value="1">First, you have the Google Terraform provider, for which you’ll need to specify the GCP project ID and the region in which you want to deploy the <span class="No-Break">GKE cluster:</span><pre class="console">&#13;
provider "google" {</pre><pre class="console">&#13;
  project     = var.project_id</pre><pre class="console">&#13;
  region      = var.region</pre><pre class="console">&#13;
}</pre></li>&#13;
				<li>Next, you’ll create the <strong class="source-inline">google_container_cluster</strong> resource, which is the GKE cluster. It’ll specify<a id="_idIndexMarker100"/> the cluster name, region, and worker <span class="No-Break">node</span><span class="No-Break"><a id="_idIndexMarker101"/></span><span class="No-Break"> count:</span><pre class="console">&#13;
resource "google_container_cluster" "primary" {</pre><pre class="console">&#13;
  name     = var.cluster_name</pre><pre class="console">&#13;
  location = var.region</pre><pre class="console">&#13;
  </pre><pre class="console">&#13;
  remove_default_node_pool = true</pre><pre class="console">&#13;
  initial_node_count       = 1</pre><pre class="console">&#13;
  network    = var.vpc_name</pre><pre class="console">&#13;
  subnetwork = var.subnet_name</pre><pre class="console">&#13;
}</pre></li>&#13;
				<li>The final resource to create is the <strong class="source-inline">google_container_node_pool</strong> resource, which is for creating the Kubernetes worker nodes. Here is where you <span class="No-Break">can specify:</span><ul><li> The worker <span class="No-Break">node count:</span><pre class="console">&#13;
resource "google_container_node_pool" "nodes" {</pre><pre class="console">&#13;
  name       = "${google_container_cluster.primary.name}-node-pool"</pre><pre class="console">&#13;
  location   = var.region</pre><pre class="console">&#13;
  cluster    = google_container_cluster.primary.name</pre><pre class="console">&#13;
  node_count = var.node_count</pre></li><li>The GCP <a id="_idIndexMarker102"/>scopes (or services) that you want<a id="_idIndexMarker103"/> GKE to have <span class="No-Break">access to:</span><pre class="console">&#13;
  node_config {</pre><pre class="console">&#13;
    oauth_scopes = [</pre><pre class="console">&#13;
      "https://www.googleapis.com/auth/logging.write",</pre><pre class="console">&#13;
      "https://www.googleapis.com/auth/monitoring",</pre><pre class="console">&#13;
    ]</pre><pre class="console">&#13;
    labels = {</pre><pre class="console">&#13;
      env = var.project_id</pre><pre class="console">&#13;
    }</pre></li><li>The VM type <span class="No-Break">or size:</span><pre class="console">&#13;
    machine_type = "n1-standard-1"</pre><pre class="console">&#13;
    tags         = ["gke-node", "${var.project_id}-gke"]</pre><pre class="console">&#13;
    metadata = {</pre><pre class="console">&#13;
      disable-legacy-endpoints = "true"</pre><pre class="console">&#13;
    }</pre><pre class="console">&#13;
  }</pre><pre class="console">&#13;
}</pre></li></ul></li>&#13;
			</ol>&#13;
			<p>Putting it all together, you’ll have a <strong class="source-inline">main.tf</strong> configuration that you can use to set up a GKE cluster. </p>&#13;
			<ol>&#13;
				<li value="4">Next, let’s <a id="_idIndexMarker104"/>take a look at <strong class="source-inline">variables.tf</strong>, which will contain <span class="No-Break">the following:</span><ul><li>The GCP <span class="No-Break">project ID:</span><pre class="console">&#13;
variable "project_id" {</pre><pre class="console">&#13;
  type = string</pre><pre class="console">&#13;
  default = "gold-mode-297211"</pre><pre class="console">&#13;
}</pre></li><li>The <span class="No-Break">GCP region:</span><pre class="console">&#13;
variable "region" {</pre><pre class="console">&#13;
  type = string</pre><pre class="console">&#13;
  default = "us-east1"</pre><pre class="console">&#13;
}</pre></li><li>The GCP VPC name that GKE will <span class="No-Break">exist in:</span><pre class="console">&#13;
variable "vpc_name" {</pre><pre class="console">&#13;
  type = string</pre><pre class="console">&#13;
  default = "default"</pre><pre class="console">&#13;
}</pre></li><li>The subnet <a id="_idIndexMarker105"/>name inside of the VPC that you want GKE to be <span class="No-Break">attached to:</span><pre class="console">&#13;
variable "subnet_name" {</pre><pre class="console">&#13;
  type = string</pre><pre class="console">&#13;
  default = "default"</pre><pre class="console">&#13;
}</pre></li><li>The code count (Kubernetes <span class="No-Break">worker nodes):</span><pre class="console">&#13;
variable "node_count" {</pre><pre class="console">&#13;
  type = string</pre><pre class="console">&#13;
  default = 2</pre><pre class="console">&#13;
}</pre></li><li>The GKE <a id="_idIndexMarker106"/><span class="No-Break">cluster name:</span><pre class="console">&#13;
variable "cluster_name" {</pre><pre class="console">&#13;
  type = string</pre><pre class="console">&#13;
  default = "gkek8senv"</pre><pre class="console">&#13;
}</pre></li></ul></li>&#13;
			</ol>&#13;
			<p>You’re now ready to put the <a id="_idIndexMarker107"/>proceeding code into the appropriate <strong class="source-inline">main.tf</strong> and <strong class="source-inline">variables.tf</strong> configuration files to create your <span class="No-Break">GKE environment.</span></p>&#13;
			<h2 id="_idParaDest-55"><a id="_idTextAnchor056"/>GKE Autopilot</h2>&#13;
			<p>To wrap up the <em class="italic">GKE</em> section, let’s quickly talk about GKE Autopilot. Autopilot is the same concept as EKS Fargate. It’s serverless<a id="_idIndexMarker108"/> Kubernetes, which means you don’t have to worry about managing <a id="_idIndexMarker109"/>the worker nodes for your GKE cluster. Instead, you only have to worry about deploying the application(s) and setting up any monitoring, logging, traces, alerts, and metrics you’d like to capture from the <span class="No-Break">GKE cluster.</span></p>&#13;
			<h2 id="_idParaDest-56"><a id="_idTextAnchor057"/>A quick note on multi-cloud</h2>&#13;
			<p>Many engineers just <a id="_idIndexMarker110"/>getting started with Kubernetes may not come across it too much, but multi-cloud is very much a reality. Just as organizations didn’t want to rely on one data center for redundancy, some organizations don’t want only one cloud for redundancy. Instead, they want to think about the multi-cloud approach – for example, scaling out Kubernetes workloads from AKS <span class="No-Break">to GKE.</span></p>&#13;
			<p>This implementation can be rather advanced and require a ton of security-related permissions, authentication and authorization capabilities between clouds, and heavy networking knowledge to ensure Kubernetes clusters between clouds can communicate with each other. Because <a id="_idIndexMarker111"/>of that, it’s highly recommended to do extensive research before implementing this and ensure that all of the proper testing went <span class="No-Break">as expected.</span></p>&#13;
			<h1 id="_idParaDest-57"><a id="_idTextAnchor058"/>Summary</h1>&#13;
			<p>Although a multi-cloud approach may not be at the forefront of everyone’s mind, it’s still super crucial to understand how the three clouds work with Kubernetes. The reason why is that chances are, throughout your Kubernetes journey, you’ll work in one cloud, but when the need arises to work in other clouds, you should <span class="No-Break">be prepared.</span></p>&#13;
			<p>In this chapter, you learned about setting up, managing, and maintaining Kubernetes clusters across Azure, AWS, and GCP. One of the biggest takeaways is that at the end of the day, the setup of Kubernetes across the clouds isn’t really so different. They’re all sort of doing the same thing with different <span class="No-Break">service names.</span></p>&#13;
			<h1 id="_idParaDest-58"><a id="_idTextAnchor059"/>Further reading</h1>&#13;
			<ul>&#13;
				<li><em class="italic">Building Google Cloud Platform Solutions</em> by Ted Hunter, Steven Porter, and Legorie <span class="No-Break">Rajan PS:</span></li>&#13;
			</ul>&#13;
			<p><a href="https://www.packtpub.com/product/building-google-cloud-platform-solutions/9781838647438"><span class="No-Break">https://www.packtpub.com/product/building-google-cloud-platform-solutions/9781838647438</span></a></p>&#13;
			<ul>&#13;
				<li><em class="italic">Hands-On Kubernetes on Azure – Second Edition</em> by Nills Franssens, Shivakumar Gopalakrishnan, and <span class="No-Break">Gunther Lenz:</span></li>&#13;
			</ul>&#13;
			<p><a href="https://www.packtpub.com/product/hands-on-kubernetes-on-azure-second-edition/9781800209671"><span class="No-Break">https://www.packtpub.com/product/hands-on-kubernetes-on-azure-second-edition/9781800209671</span></a></p>&#13;
			<ul>&#13;
				<li><em class="italic">Learning AWS – Second Edition</em> by Aurobindo Sarkar and <span class="No-Break">Amit Shah:</span></li>&#13;
			</ul>&#13;
			<p><a href="https://www.packtpub.com/product/learning-aws-second-edition/9781787281066"><span class="No-Break">https://www.packtpub.com/product/learning-aws-second-edition/9781787281066</span></a></p>&#13;
		</div>&#13;
	</div></body></html>