<html><head></head><body>
		<div id="_idContainer059">
			<h1 id="_idParaDest-251" class="chapter-number"><a id="_idTextAnchor321"/>13</h1>
			<h1 id="_idParaDest-252"><a id="_idTextAnchor322"/>Taking Puppet Server Further</h1>
			<p>This chapter will look at how you can monitor, tune, and integrate your Puppet infrastructure with third-party sources. You will understand how to find the logs of the various services we have discussed in previous chapters and how to find the current status APIs. You will then learn how these logs and statuses can be integrated into services such as <strong class="bold">logstash </strong>to provide greater visibility and alerting options. Then, we’ll review the metrics provided by Puppet, along with how these can be integrated with dashboarding tools such as Splunk and Grafana to provide <strong class="bold">monitoring </strong>and <strong class="bold">observability </strong>for Puppet’s infrastructure. We will set up a lab for both <strong class="bold">Splunk </strong>and <strong class="bold">Grafana </strong>as part of the Puppet Operational Dashboard to show these dashboards. Using these metrics, you will learn how the various components of Puppet’s infrastructure can be tuned and scaled to deal with common issues and problems as Puppet grows. After, you’ll learn how the external provider pattern can allow for facts, classification, and Hiera data to be fed from external data sources into Puppet and to allow Puppet platform teams to provide self-service with Puppet data without requiring full knowledge of Puppet or the environment release procedures. Various third-party implementations, including <strong class="bold">ServiceNow </strong>and <strong class="bold">1Password</strong>, will be shown. The <strong class="bold">Puppet Data Service </strong>(<strong class="bold">PDS</strong>) will be implemented in this chapter’s lab to demonstrate <span class="No-Break">this pattern.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Logging <span class="No-Break">and status</span></li>
				<li>Metrics, tuning, <span class="No-Break">and scaling</span></li>
				<li>Identifying and avoiding <span class="No-Break">common issues</span></li>
				<li>External <span class="No-Break">data sources</span></li>
			</ul>
			<h1 id="_idParaDest-253"><a id="_idTextAnchor323"/>Technical requirements</h1>
			<p>Clone the control repository from <a href="https://github.com/puppetlabs/control-repo ">https://github.com/puppetlabs/control-repo </a>to your GitHub account (<strong class="source-inline">controlrepo-chapter13</strong>) and update the following files in <span class="No-Break">this repository:</span></p>
			<ul>
				<li><strong class="source-inline">Puppetfile</strong> with <a href="https://github.com/PacktPublishing/Puppet-8-for-DevOps-Engineers/blob/main/ch13/Puppetfile">https://github.com/PacktPublishing/Puppet-8-for-DevOps-Engineers/blob/main/ch13/Puppetfile</a> </li>
				<li><strong class="source-inline">hiera.yaml</strong> <span class="No-Break">with </span><a href="https://github.com/PacktPublishing/Puppet-8-for-DevOps-Engineers/blob/main/ch13/hiera.yaml"><span class="No-Break">https://github.com/PacktPublishing/Puppet-8-for-DevOps-Engineers/blob/main/ch13/hiera.yaml</span></a></li>
				<li><strong class="source-inline">manifests/site.pp</strong> <span class="No-Break">with </span><a href="https://github.com/PacktPublishing/Puppet-8-for-DevOps-Engineers/blob/main/ch13/site.pp"><span class="No-Break">https://github.com/PacktPublishing/Puppet-8-for-DevOps-Engineers/blob/main/ch13/site.pp</span></a></li>
			</ul>
			<p>Build a large cluster with three compilers and three clients by downloading the <strong class="source-inline">params.json </strong>file from <a href="https://github.com/PacktPublishing/Puppet-8-for-DevOps-Engineers/blob/main/ch13/params.json">https://github.com/PacktPublishing/Puppet-8-for-DevOps-Engineers/blob/main/ch13/params.json</a> and update it with the location of your control repository and your SSH key for the control repository. Then, run the following command from your <span class="No-Break"><strong class="source-inline">pecdm </strong></span><span class="No-Break">directory:</span></p>
			<pre class="source-code">
bolt --verbose plan run pecdm::provision --params @params.json</pre>
			<p>First, we will look at where to find the logs and current status of the Puppet services and infrastructure. This will be fundamental to how you will need to tune and <span class="No-Break">troubleshoot Puppet.</span></p>
			<h1 id="_idParaDest-254"><a id="_idTextAnchor324"/>Logging and status</h1>
			<p>When we <a id="_idIndexMarker946"/>discussed different Puppet components previously in this book, we listed logging directories, but it is useful to have a single reference point for <span class="No-Break">these logs.</span></p>
			<h2 id="_idParaDest-255"><a id="_idTextAnchor325"/>Exploring log locations</h2>
			<p>This section <a id="_idIndexMarker947"/>provides a list of these logs, titled with the core function, the containing directory, and the list of logs in <span class="No-Break">that directory:</span></p>
			<ul>
				<li><strong class="bold">Primary </strong><span class="No-Break"><strong class="bold">server logs</strong></span><span class="No-Break">:</span><ul><li><strong class="source-inline">/var/log/puppetlabs/puppetserver/</strong>: The<a id="_idIndexMarker948"/> primary server <span class="No-Break">logging directory</span></li><li><strong class="source-inline">puppetserver.log</strong>: The primary server which logs <span class="No-Break">its activity</span></li><li><strong class="source-inline">puppetserver-access.log</strong>: Requests to <span class="No-Break">access endpoints</span></li><li><strong class="source-inline">puppetserver-daemon.log</strong>: Crash reports and <span class="No-Break">fatal errors</span></li><li><strong class="source-inline">puppetserver-status.log</strong>: Debug status logging for <span class="No-Break">the service</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Database logs</strong></span><span class="No-Break">:</span><ul><li><strong class="source-inline">/var/log/puppetlabs/postgresql/&lt;version&gt;</strong>: PostgreSQL <span class="No-Break">logging </span><span class="No-Break"><a id="_idIndexMarker949"/></span><span class="No-Break">directory</span></li><li><strong class="source-inline">pgstartup.log</strong>: <span class="No-Break">Start-up logs</span></li><li><strong class="source-inline">postgresql-&lt;Mon – Sun&gt;.log</strong>: Daily <span class="No-Break">debugging logs</span></li><li><strong class="source-inline">/var/log/puppetlabs/puppetdb/</strong>: PuppetDB <span class="No-Break">logging directory</span></li><li><strong class="source-inline">puppetdb.log</strong>: The PuppetDB service <span class="No-Break">activity log</span></li><li><strong class="source-inline">puppetdb-access.log</strong>: Requests to <span class="No-Break">access endpoints</span></li><li><strong class="source-inline">puppetdb-status.log</strong>: Debug status logging for <span class="No-Break">the service</span></li></ul></li>
				<li><strong class="bold">Primary server logs </strong>(Puppet <span class="No-Break">Enterprise only):</span><ul><li><strong class="source-inline">/var/log/puppetlabs/puppetserver/</strong>: The <a id="_idIndexMarker950"/>primary server <span class="No-Break">logging directory</span></li><li><strong class="source-inline">code-manager-access.log</strong>: Requests to access endpoints of the <span class="No-Break">code manager</span></li><li><strong class="source-inline">file-sync-access.log</strong>: Requests to access endpoints of <span class="No-Break">file sync</span></li><li><strong class="source-inline">pcp-broker.log</strong>: Puppet Communications Protocol brokers <span class="No-Break">on compilers</span></li></ul></li>
				<li><strong class="bold">Console and console services logs </strong>(Puppet <span class="No-Break">Enterprise only):</span><ul><li><strong class="source-inline">/var/log/puppetlabs/console-services/</strong>: Puppet Enterprise console <a id="_idIndexMarker951"/>service <span class="No-Break">logging directory</span></li><li><strong class="source-inline">console-services.log</strong>: Console service <span class="No-Break">activity logs</span></li><li><strong class="source-inline">console-services-api-access.log</strong>: Requests to access the console service <span class="No-Break">API endpoints</span></li><li><strong class="source-inline">console-services-access.log</strong>: Requests to access the console <span class="No-Break">service endpoint</span></li><li><strong class="source-inline">console-services-daemon.log</strong>: Crash reports and fatal <span class="No-Break">errors logged</span></li><li><strong class="source-inline">/var/log/puppetlabs/nginx/</strong>: nginx <span class="No-Break">logging directory</span></li><li><strong class="source-inline">access.log</strong>: Requests to <span class="No-Break">nginx endpoints</span></li><li><strong class="source-inline">error.log</strong>: nginx <a id="_idIndexMarker952"/>errors and general <span class="No-Break">console errors</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Agent logs:</strong></span></li>
			</ul>
			<p>The<a id="_idIndexMarker953"/> agent output that you see on your screen when you run Puppet manually is logged to a location based on the <strong class="source-inline">logdest </strong>and <strong class="source-inline">logdir </strong>settings in the <strong class="source-inline">puppet.conf </strong>file. The <strong class="source-inline">logdest </strong>parameter can be set to <strong class="source-inline">syslog </strong>(to be sent to the POSIX syslog service), <strong class="source-inline">eventlog </strong>(to be sent to the Windows event log), <strong class="source-inline">console </strong>(for logs to be sent to the console), or a filename so that they’re outputted to a file of this name in the location set by <strong class="source-inline">logdest</strong>. <strong class="source-inline">syslog </strong>is the default for Unix-based systems, while <strong class="source-inline">eventlog </strong>is the default for Windows. The defaults for <strong class="source-inline">logdest </strong>are <strong class="source-inline">/var/log/puppetlabs/puppet </strong>for Unix and <strong class="source-inline">C:\ProgramData\PuppetLabs\puppet\var\log </strong><span class="No-Break">for Windows.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">It is possible to turn on server profiling, which can generate detailed catalog logging information. This can then be graphed to show in-depth debugging information about the catalog compilation. It is beyond the scope of this book to dive into this. More information can be found in Puppet’s documentation <span class="No-Break">at </span><a href="https://github.com/puppetlabs/puppet/blob/main/docs/profiling.md"><span class="No-Break">https://github.com/puppetlabs/puppet/blob/main/docs/profiling.md</span></a><span class="No-Break">.</span></p>
			<p>Having examined <a id="_idIndexMarker954"/>the various locations of logging, it becomes clear that it would be useful to forward these server logs to specialized tools so that they can be indexed <span class="No-Break">and processed.</span></p>
			<h2 id="_idParaDest-256"><a id="_idTextAnchor326"/>Forwarding server logs</h2>
			<p>As the<a id="_idIndexMarker955"/> number of Puppet clients grows, the log tracking exercise, which we completed in <a href="B18492_10.xhtml#_idTextAnchor252"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, simply becomes impractical. In this scenario, more <a id="_idIndexMarker956"/>specialized <strong class="bold">log tooling </strong>should be used to filter and view events. Previously, we saw that Puppet uses the <strong class="bold">Logback </strong>library, <a href="http://logback.qos.ch/">http://logback.qos.ch/</a>, for<a id="_idIndexMarker957"/> logging Java services on servers. This can be configured to output logging in JSON format in <strong class="source-inline">logback.xml</strong>, which can be sent to a logging backend such as Elastic’s Logstash or Grafana’s Loki. The <strong class="source-inline">logback.xml </strong>file contains <strong class="source-inline">appender </strong>definitions, which are the Logback components for <span class="No-Break">writing logs.</span></p>
			<p>By observing the <strong class="source-inline">appender</strong> configuration for <strong class="source-inline">puppetserver.log</strong>, we will see the <span class="No-Break">current configuration:</span></p>
			<pre class="source-code">
    &lt;appender name="F1" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt;
&lt;file&gt;/var/log/puppetlabs/puppetserver/puppetserver.log&lt;/file&gt;
        &lt;append&gt;true&lt;/append&gt;
        &lt;rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy"&gt;
&lt;fileNamePattern&gt;/var/log/puppetlabs/puppetserver/puppetserver-%d{yyyy-MM-dd}.%i.log.gz&lt;/fileNamePattern&gt;
            &lt;!-- each file should be at most 200MB, keep 90 days worth of history, but at most 1GB total --&gt;
            &lt;maxFileSize&gt;200MB&lt;/maxFileSize&gt;
            &lt;maxHistory&gt;90&lt;/maxHistory&gt;
            &lt;totalSizeCap&gt;1GB&lt;/totalSizeCap&gt;
        &lt;/rollingPolicy&gt;
        &lt;encoder&gt;
            &lt;pattern&gt;%d{yyyy-MM-dd'T'HH:mm:ss.SSSXXX} %-5p [%t] [%c{2}] %m%n&lt;/pattern&gt;
        &lt;/encoder&gt;
    &lt;/appender&gt;</pre>
			<p>Here, we can<a id="_idIndexMarker958"/> see how it appends to the log, the filename pattern it will use, along with dates for rolling the log, and that the file size will not exceed 200 MB per file, no more than 1 GB, and that logs will not be retained for more than 90 days. The encoder shows how the log entries should <span class="No-Break">be formed.</span></p>
			<p>To add a JSON version of the log, we could make a similar entry that consists of just 5 days of logging. Here, the encoder is the Logstash encoder to output in JSON. Here’s the code for <span class="No-Break">this </span><span class="No-Break"><strong class="source-inline">appender</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
&lt;appender name="server_JSON" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt;
    &lt;file&gt;/var/log/puppetlabs/puppetserver/puppetserver.log.json&lt;/file&gt;
    &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt;
        &lt;fileNamePattern&gt;/var/log/puppetlabs/puppetserver/puppetserver.log.json.%d{yyyy-MM-dd}&lt;/fileNamePattern&gt;
        &lt;maxHistory&gt;5&lt;/maxHistory&gt;
    &lt;/rollingPolicy&gt;
    &lt;encoder class="net.logstash.logback.encoder.LogstashEncoder"/&gt;
&lt;/appender&gt;</pre>
			<p>To <a id="_idIndexMarker959"/>enable this <strong class="source-inline">appender </strong>toward the bottom of the <strong class="source-inline">logback.xml </strong>file, add the <span class="No-Break">following definitions:</span></p>
			<pre class="source-code">
    &lt;root level="info"&gt;
        &lt;!--&lt;appender-ref ref="STDOUT"/&gt;--&gt;
        &lt;appender-ref ref="${logappender:-DUMMY}" /&gt;
        &lt;appender-ref ref="F1"/&gt;
    &lt;/root&gt;</pre>
			<p>Adding <strong class="source-inline">&lt;appender-ref ref="server_JSON"/&gt; </strong>within this root section would enable our JSON <strong class="source-inline">appender</strong>. Restarting the <strong class="source-inline">puppetserver </strong>service would enable the <span class="No-Break">new </span><span class="No-Break"><strong class="source-inline">appender</strong></span><span class="No-Break">.</span></p>
			<p>To set up <strong class="source-inline">server-access.log</strong>, you can use the code at <a href="https://github.com/PacktPublishing/Puppet-8-for-DevOps-Engineers/blob/main/ch13/appender_example.xml">https://github.com/PacktPublishing/Puppet-8-for-DevOps-Engineers/blob/main/ch13/appender_example.xml</a>, which will add an <strong class="source-inline">appender </strong>that will configure the JSON output with an <span class="No-Break">appropriate pattern.</span></p>
			<p>You will need to consider disk space in such cases. It’s possible to run this with JSON logging. Logback is a powerful library, but it’s beyond the scope of this book to go through the full options. These can be reviewed at <a href="http://logback.qos.ch/manual/configuration.html">http://logback.qos.ch/manual/configuration.html</a> <span class="No-Break">and </span><a href="https://logback.qos.ch/manual/appenders.html"><span class="No-Break">https://logback.qos.ch/manual/appenders.html</span></a><span class="No-Break">.</span></p>
			<p>Now that logfiles exist in JSON, a tool such as <a id="_idIndexMarker960"/>Grafana’s <strong class="bold">Promtail </strong>or Elastic’s <strong class="bold">Filebeat </strong>could<a id="_idIndexMarker961"/> be configured to forward the log files to a service such as Elastic’s Logstash or Grafana’s Loki. The Ruby logs managed by <strong class="bold">Logrotate </strong>can<a id="_idIndexMarker962"/> also be gathered, but it would require more work to put suitable patterns on them to <span class="No-Break">be processed.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Puppet Enterprise services, <strong class="source-inline">console-services</strong>, PuppetDB, and orchestration services all use Logback and can have their logs forwarded <span class="No-Break">like this.</span></p>
			<p>Having reviewed how logs can be sent to external services to be processed, we will now see how the reports that are generated from applying Puppet catalogs can also be sent to external tools using <span class="No-Break">report processors.</span></p>
			<h2 id="_idParaDest-257"><a id="_idTextAnchor327"/>Report processors</h2>
			<p>As well as <a id="_idIndexMarker963"/>server logging, as shown in <a href="B18492_10.xhtml#_idTextAnchor252"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, every catalog run generates reports. In <a href="B18492_10.xhtml#_idTextAnchor252"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, you saw how this was configured by <strong class="source-inline">peadm </strong>to be stored in <strong class="source-inline">puppetdb </strong>using <strong class="source-inline">reports = puppetdb </strong>in the <strong class="source-inline">master/server </strong>section of <strong class="source-inline">puppet.conf</strong>. Setting this <strong class="source-inline">reports </strong>value told the server to use a report processor, which is a Ruby script that’s run when Puppet Server receives a report. The script then performs actions to pass it on to a target. In the case of PuppetDB, this is to send reports to be stored in PuppetDB. There are three built-in report processors: <strong class="source-inline">http</strong>, <strong class="source-inline">log</strong>,<strong class="source-inline"> </strong>and <strong class="source-inline">store</strong>. <strong class="source-inline">http, which </strong>sends the report to an HTTP address set by the <strong class="source-inline">reporturl </strong>setting in <strong class="source-inline">puppet.conf </strong>in YAML format. <strong class="source-inline">log </strong>sends the report output to the logging file specified in <strong class="source-inline">logdest </strong>and <strong class="source-inline">logdir </strong>in <strong class="source-inline">puppet.conf</strong>, and <strong class="source-inline">store </strong>puts the report’s output into files specified by the <strong class="source-inline">reportdir </strong>setting that’s set in <strong class="source-inline">puppet.conf</strong>. Other custom report processors are available in Puppet Forge, including the Splunk integration module, as described at <a href="https://forge.puppet.com/modules/puppetlabs/splunk_hec">https://forge.puppet.com/modules/puppetlabs/splunk_hec</a>, and the Datadog agent module, as described at <a href="https://forge.puppet.com/modules/datadog/datadog_agent">https://forge.puppet.com/modules/datadog/datadog_agent</a>, which allows report data to be viewed in those third-party services. The instructions vary, depending on the module, but normally, the minimum actions required to add a report processor is for Puppet Server to have the module deployed in an environment and for the reports to have the name of the forwarder set. Writing custom report processors is beyond the scope of this book but details can be found <span class="No-Break">at </span><a href="https://puppet.com/docs/puppet/latest/reporting_write_processors.html&#13;"><span class="No-Break">https://puppet.com/docs/puppet/latest/reporting_write_processors.html.</span></a></p>
			<p>In addition to logs and reports, we can see what condition the current Puppet Infrastructure is by calling the <span class="No-Break">status APIs.</span></p>
			<h2 id="_idParaDest-258"><a id="_idTextAnchor328"/>Accessing status APIs</h2>
			<p>Puppet <a id="_idIndexMarker964"/>provides a status endpoint that can be called at <strong class="source-inline">GET /status/v1/services</strong>. This endpoint returns the status of all known services on the server. This access is controlled by <strong class="source-inline">auth.conf </strong>and can be accessed locally via the Puppet CA certificates, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
cert="$(puppet config print hostcert)"
cacert="$(puppet config print localcacert)"
key="$(puppet config print hostprivkey)"
uri="https://$(puppet config print server):8140/status/v1/services"
curl --cert "$cert" --cacert "$cacert" --key "$key" "$uri" | jq</pre>
			<p>The final pipe to JQ (a command-line JSON processor) is optional but makes the output more readable. Here’s an example of the output Puppet Server’s status <span class="No-Break">would provide:</span></p>
			<pre class="source-code">
{
  "server": {
    "service_version": "7.6.0",
    "service_status_version": 1,
    "detail_level": "info",
    "state": "running",
    "status": {},
    "active_alerts": []
  },</pre>
			<p>It is possible to target an individual service by adding the service name to the URL – for example,  <strong class="source-inline">GET/status/v1/services/server</strong>. For PE installations that have additional services, the specific port for each service should also be called. PE services will be discussed in <a href="B18492_14.xhtml#_idTextAnchor340"><span class="No-Break"><em class="italic">Chapter 14</em></span></a><span class="No-Break">.</span></p>
			<p>The<a id="_idIndexMarker965"/> return code from these calls will be <strong class="source-inline">200 </strong>for all services running, <strong class="source-inline">404 </strong>when a service is not found, or <strong class="source-inline">503 </strong>when a service state is in any state other <span class="No-Break">than running.</span></p>
			<p>The server state can be <strong class="source-inline">running </strong>when all services are running, <strong class="source-inline">error </strong>if any service reports an error, <strong class="source-inline">starting </strong>or <strong class="source-inline">stopping </strong>if any services are in those states, and <strong class="source-inline">unknown </strong>if any service reports an <span class="No-Break">unknown state.</span></p>
			<p>Puppet Enterprise has an extra command-line option to call APIs via the <strong class="source-inline">puppet infrastructure status </strong>command, which produces an output similar to <span class="No-Break">the following:</span></p>
			<pre class="source-code">
Puppet Server: Running, checked via https://pe-server-davidsand-0-cffe02.tq2kpafq5bsehkpub4ur5a35ya.xx.internal.cloudapp.net:8140/status/v1/services
  PuppetDB: Running, checked via<strong class="bold"> </strong><a href="https://pe-server-davidsand-0-cffe02.tq2kpafq5bsehkpub4ur5a35ya.xx.internal.cloudapp.net:8081/status/v1/services&#13;"><strong class="bold">https://pe-server-davidsand-0-cffe02.tq2kpafq5bsehkpub4ur5a35ya.xx.internal.cloudapp.net:8081/status/v1/services</strong></a></pre>
			<p>In the web console, you can find the Puppet Service’s status by clicking the <strong class="bold">Puppet Services status </strong>button, as per the <span class="No-Break">following figure:</span></p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B18492_13_01.jpg" alt="Figure 13.1 – Puppet Services status in the web console"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.1 – Puppet Services status in the web console</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <strong class="source-inline">puppet status </strong>command, which was depreciated in Puppet 5, was removed in Puppet 7. It didn’t use <span class="No-Break">API endpoints.</span></p>
			<p>The logs, reports, and statuses we have viewed so far allow us to observe what the Puppet infrastructure and clients are doing, but they don’t tell us about the overall performance<a id="_idIndexMarker966"/> of the infrastructure and its clients. Next, we will look at the metrics that Puppet supplies and how they can be used to monitor the performance and capacity <span class="No-Break">of infrastructure.</span></p>
			<h1 id="_idParaDest-259"><a id="_idTextAnchor329"/>Metrics, tuning, and scaling</h1>
			<p>To provide <a id="_idIndexMarker967"/>more detailed data on the performance and health of Puppet services via the services status API, the <strong class="source-inline">level </strong>flag can be set to <strong class="source-inline">debug</strong>; this will return metrics. For example, to <a id="_idIndexMarker968"/>return the metrics for Puppet Server and filter them using JQ, the following commands can <span class="No-Break">be run:</span></p>
			<pre class="source-code">
cert="$(puppet config print hostcert)"
cacert="$(puppet config print localcacert)"
key="$(puppet config print hostprivkey)"
uri="https://$(puppet config print server):8140/status/v1/services/server?level=debug"
curl --cert "$cert" --cacert "$cacert" --key "$key" "$uri" | jq ".status.experimental"</pre>
			<p>This would output data such as the following metric for the <span class="No-Break"><strong class="source-inline">puppet-v3-catalog </strong></span><span class="No-Break">endpoint:</span></p>
			<pre class="source-code">
{
  "http-metrics": [
    {
      "route-id": "puppet-v3-catalog-/*/",
      "count": 41,
      "mean": 4459,
      "aggregate": 182819
    },</pre>
			<p>This gives us a <strong class="source-inline">count </strong>of how many calls have been made to the endpoint since the service last restarted. <strong class="source-inline">mean </strong>is the average response time over 5 minutes, while <strong class="source-inline">aggregate </strong>is the total time spent since the <span class="No-Break">service started.</span></p>
			<p>There are many metrics <a id="_idIndexMarker969"/>across all the different services. To find the definitions of these metrics, the API services can be viewed in the documentation (for example, <a href="https://puppet.com/docs/pe/2021.7/status_api.html">https://puppet.com/docs/pe/2021.7/status_api.html</a>). However, overall, they are poorly documented and may take some exploration or you asking questions on Puppet’s Slack channels and support channels if you have a contract. Do not be concerned by most of the metrics having <em class="italic">experimental </em>in their title – most of the metrics have been <a id="_idIndexMarker970"/>available for years; they just haven’t had the experimental tag removed <span class="No-Break">by Puppet.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">In-depth details explaining how the underlying metrics library works for Puppet are available at <a href="https://www.youtube.com/watch?v=czes-oa0yik&amp;t=0s">https://www.youtube.com/watch?v=czes-oa0yik&amp;t=0s</a>, provided by the author of the <span class="No-Break">metrics library.</span></p>
			<p>Now, let’s take a look at the dashboards that are used to display <span class="No-Break">metrics data.</span></p>
			<h2 id="_idParaDest-260"><a id="_idTextAnchor330"/>Exploring metrics dashboards</h2>
			<p>Puppet <a id="_idIndexMarker971"/>provides three implementations that automate the process of gathering and displaying <span class="No-Break">metrics data:</span></p>
			<ul>
				<li><strong class="bold">Puppet Operational Dashboards</strong>, available at <a href="https://forge.puppet.com/modules/puppetlabs/puppet_operational_dashboards">https://forge.puppet.com/modules/puppetlabs/puppet_operational_dashboards</a>, is a<a id="_idIndexMarker972"/> supported Puppet module from <a id="_idIndexMarker973"/>Puppet Forge that implements Telegraf, InfluxDB, and Grafana to provide mechanisms to send the API metric data, store it in a time series database, and visualize the data in preconfigured dashboards. Operational Dashboards supports both Puppet Enterprise and open source Puppet. An example of such a dashboard can be seen in the <span class="No-Break">following figure:</span></li>
			</ul>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B18492_13_02.jpg" alt="Figure 13.2 – Grafana Puppetserver performance dashboard"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.2 – Grafana Puppetserver performance dashboard</p>
			<ul>
				<li>The <strong class="bold">Splunk Plugin</strong>, an<a id="_idIndexMarker974"/> app <a id="_idIndexMarker975"/>on the Splunk store, available<a id="_idIndexMarker976"/> at <a href="https://splunkbase.splunk.com/app/4413/#/overview">https://splunkbase.splunk.com/app/4413/#/overview</a>, can be added to your Splunk setup to provide preconfigured dashboards. The Splunk <strong class="bold">HTTP event collector </strong>(<strong class="bold">HEC</strong>) module<a id="_idIndexMarker977"/> can be found at <a href="https://forge.puppet.com/modules/puppetlabs/splunk_hec">https://forge.puppet.com/modules/puppetlabs/splunk_hec</a>. <a href="https://forge.puppet.com/modules/puppetlabs/pe_event_forwarding">https://forge.puppet.com/modules/puppetlabs/pe_event_forwarding</a> can be combined with this module to send the metrics over HTTP to the HEC module. An example of a Splunk dashboard is shown in the following figure, with Puppet Server <span class="No-Break">Memory graphed:</span></li>
			</ul>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B18492_13_03.jpg" alt="Figure 13.3 – Splunk Puppet Server Memory dashboard"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.3 – Splunk Puppet Server Memory dashboard</p>
			<p>Puppet teams work together to keep the Splunk and Grafana <span class="No-Break">dashboards consistent.</span></p>
			<p>For <a id="_idIndexMarker978"/>Puppet Enterprise, there is also the <strong class="bold">Puppet Metrics collector </strong>module (<a href="https://forge.puppet.com/modules/puppetlabs/puppet_metrics_collector">https://forge.puppet.com/modules/puppetlabs/puppet_metrics_collector</a>) installed by default <a id="_idIndexMarker979"/>on Puppet Enterprise 2019.8.7 onwards. This module gathers metrics from the APIs and outputs them to JSON files in the <strong class="source-inline">/opt/puppetlabs/puppet-metrics-collector </strong>directory. These JSON files can then be searched using commands such as <strong class="source-inline">grep </strong>or <strong class="source-inline">JQ </strong>(assuming the terminal was in the metric collector directory). Two common queries, which will be explained in detail in the <strong class="bold">Identifying and avoiding common issues</strong> section, are <strong class="source-inline">average-free-jrubies </strong>and <strong class="source-inline">queue_depth</strong>. These can be added <span class="No-Break">like this:</span></p>
			<pre class="source-code">
<strong class="bold">grep -oP '"average-free-jrubies.*?,' puppetserver/primary.example.com/*.json puppetserver/pe-server-davidsand-0-cffe02.tq2kpafq5bsehkpub4ur5a35ya.xx.internal.cloudapp.net/*.json</strong>
<strong class="bold">"puppetserver/pe-server-davidsand-0-cffe02.tq2kpafq5bsehkpub4ur5a35ya.xx.internal.cloudapp.net/20220731T220502Z.json":"average-free-jrubies":3</strong>
<strong class="bold">jq '.. |."queue_depth "? | select(. != null)| input_filename , .' -- puppetdb/pe-server-davidsand-0-cffe02.tq2kpafq5bsehkpub4ur5a35ya.xx.internal.cloudapp.net/*.json</strong>
<strong class="bold">"puppetdb/pe-server-davidsand-0-cffe02.tq2kpafq5bsehkpub4ur5a35ya.xx.internal.cloudapp.net/20220731T221001Z.json"</strong>
<strong class="bold">0</strong></pre>
			<p>To make<a id="_idIndexMarker980"/> it easier to share this data, it can be archived by running the <strong class="source-inline">/opt/puppetlabs/puppet-metrics-collector/scripts/create-metrics-archive </strong>command, which will produce <a id="_idIndexMarker981"/>a <strong class="bold">tarball file</strong>. It contains the <strong class="source-inline">–r </strong>flag if you wish to archive a set number <span class="No-Break">of days.</span></p>
			<p>Having looked at how to gather and display metrics, we will now discuss some common performance and capacity issues and how to manage them <span class="No-Break">using metrics.</span></p>
			<h2 id="_idParaDest-261"><a id="_idTextAnchor331"/>Identifying and avoiding common issues</h2>
			<p>Having <a id="_idIndexMarker982"/>set up logging, statuses, and metrics, we need to consider what to look for and how to examine our Puppet infrastructure. Normal monitoring of CPU, memory, and disk usage should be in place but there are some key functional areas to focus on. We will discuss these in the <span class="No-Break">following sections.</span></p>
			<h3>Catalog compilation</h3>
			<p>In <a href="B18492_10.xhtml#_idTextAnchor252"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, we<a id="_idIndexMarker983"/> learned how each <strong class="bold">catalog compilation </strong>required<a id="_idIndexMarker984"/> a JRuby instance to compile a catalog for each Puppet request and that the Puppet Primary or Compiler Servers provided this JRuby capacity. To calculate the necessary number of JRuby instances to handle the load for infrastructure, we can <a id="_idIndexMarker985"/>take the <strong class="bold">run interval </strong>(how often servers will check in) and divide this by the average length of compilation. This will sum up how many JRuby instances are required per server. We can take the number of Puppet Clients we expect the infrastructure to have and divide this by the previous figure to provide an estimate of the total required <span class="No-Break">JRuby instances:</span></p>
			<p>Add this here: <em class="italic">Total JRuby Instances = Number of Puppet clients / ( run interval / average </em><span class="No-Break"><em class="italic">compilation length</em></span></p>
			<p>Choosing the sizing for your infrastructure can be complicated. The number of JRuby instances on a primary server or compiler can be set by running <strong class="source-inline">max-active-instances </strong>in the <strong class="source-inline">puppetserver.conf </strong>file; this defaults to the number of CPUs – 1 for a range of 1 to 4. Each JRuby instance will require memory in the JVM stack. For Puppet Enterprise, this file is controlled by setting the <strong class="source-inline">hiera </strong>value <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">puppet_enterprise::master::puppetserver::jruby_max_active_instances:</strong></span><span class="No-Break">.</span></p>
			<p>The total JVM stack memory is allocated by the Puppet Server startup script, which depending on the operating system, will be at <strong class="source-inline">/etc/sysconfig/puppetserver </strong>or <strong class="source-inline">/etc/defaults/puppetserver</strong>. This is set by the <strong class="source-inline">xmx </strong>argument, which can be calculated as each JRuby instance requiring 512 MB of memory by default and leaving 512 MB of headroom for other <span class="No-Break">Java tasks:</span></p>
			<p>Add this here: <em class="italic">Total stack heap size required = 512mb + maximum active instances * </em><span class="No-Break"><em class="italic">512MB</em></span></p>
			<p>It is recommended that you never exceed 32 GB of stack size for JVM. As seen from various field experiments, the maximum effective number of JRuby instances appears to be between 11 and 13. These maximum figures tend to be for much larger estates and concern should be given to allow for compiler failures. In this case, it would be unwise to focus entirely on horizontal scaling; instead, it should be <a id="_idIndexMarker986"/>balanced with <strong class="bold">vertical scaling </strong>(having more <span class="No-Break">compiler servers).</span></p>
			<p>Sizing<a id="_idIndexMarker987"/> recommendations can be difficult when you’re just starting – it can be very unclear what your average compilation time will be and seconds in compilation can have a big impact, so it is wise to monitor both JRuby usage and catalog compilation time as the estate grows and look for outliers as they appear. Puppet has some guidelines for Puppet Enterprise sizing <span class="No-Break">at </span><a href="https://puppet.com/docs/pe/2021.7/tuning_infrastructure.html"><span class="No-Break">https://puppet.com/docs/pe/2021.7/tuning_infrastructure.html</span></a><span class="No-Break">.</span></p>
			<p>When monitoring catalog performance, we have some key concerns. The <strong class="source-inline">jruby.num-jrubies </strong>and <strong class="source-inline">jruby.num-free-jrubies </strong>metrics show how many JRuby instances are on a server and how many are free. When looking at these metrics, the average used capacity of the infrastructure should be calculated. It is recommended that you avoid going beyond 80% usage as performance tends not to scale beyond this. You should also confirm that there are no issues with load balancers and that the free JRuby instance usage is even across compilers. One issue that can occur is known as <strong class="bold">Thundering Herd</strong>, where<a id="_idIndexMarker988"/> many servers request catalog compilations at the same time. This can be seen in the metrics as large spikes in JRuby instance usage. If you experience this, you<a id="_idIndexMarker989"/> can use the <strong class="bold">puppet run scheduler </strong>module at <a href="https://forge.puppet.com/modules/reidmv/puppet_run_scheduler">https://forge.puppet.com/modules/reidmv/puppet_run_scheduler</a> to distribute the scheduling of Puppet <span class="No-Break">agent runs.</span></p>
			<p>If the capacity of the JRuby pool is exceeded, then requests will queue and timeout after a default of 10 seconds. The <strong class="source-inline">borrow-timeout-count </strong>metric provides a count of the number of requests that have timed out while waiting for a JRuby instance to <span class="No-Break">become available.</span></p>
			<h3>Catalog runtimes</h3>
			<p>As<a id="_idIndexMarker990"/> highlighted previously, the catalog’s runtime has a huge impact on the number of JRuby instances that are required in the infrastructure. Looking at the <strong class="source-inline">metrics-time-total </strong>metric, which shows the compile time for report events sent, we can look at the average time to compile to help with our capacity calculations. We can also look at the distribution of these figures to see if we have any extreme outliers we would want to investigate in that catalog and its <span class="No-Break">Puppet code.</span></p>
			<p>Some key areas to check can be seen in the <span class="No-Break">following table:</span></p>
			<table id="table001-5" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Metric Definition</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Metric Name</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Catalog <span class="No-Break">compilation time</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">Metrics-time-config_retrieval</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Time to apply <span class="No-Break">the catalog</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">Metrics-time.catalog_application</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Number of resources in <span class="No-Break">the catalog</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">Metrics-resources-total</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Time to <span class="No-Break">generate facts</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">Metrics-changes-total</strong></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 12.1 – Catalog metrics</p>
			<p>Within<a id="_idIndexMarker991"/> each of these measures, you should ensure that there was a reason for it to be an outlier. If the code or fact is complex or any particular resource is known to be slow, this may be normal, or it may be inefficient code that can be reviewed before it is applied to more servers, thus saving <span class="No-Break">infrastructure capacity.</span></p>
			<h3>PuppetDB and PostgreSQL tuning</h3>
			<p>For <a id="_idIndexMarker992"/>PuppetD, the best metrics to monitor are <strong class="source-inline">jvm-metrics.heap-memory.committed </strong>and <strong class="source-inline">jvm-metrics.heap-memory.used</strong>. If the used memory’s size is regularly approaching the committed memory’s size, then it’s best to increase the stack’s size. Similar to compilers, this involves updating the <strong class="source-inline">puppetdb </strong>or <strong class="source-inline">pe-puppetdb </strong>config file at <strong class="source-inline">/etc/sysconfig/ </strong>or <strong class="source-inline">/etc/default/puppetdb</strong>, respectively, depending on your operating system, and updating the <strong class="source-inline">JAVA_ARGS </strong>argument. For example, if you found that <strong class="source-inline">jvm-metrics.heap-memory.committed </strong>was set to 512 MB but <strong class="source-inline">jvm-metrics.heap-memory.used </strong>was approaching this limit regularly, the maximum heap size could be updated to <strong class="bold">1 GB</strong> by changing <strong class="source-inline">JAVA_ARGS ="-Xmx512m"</strong> to <strong class="source-inline">JAVA_ARGS="-Xmx1g"  </strong>in the config file. After doing this, you would need to restart the PuppetDB service. However, note that all jobs that have been queued due to them running out of memory would just continue after a restart. For Puppet Enterprise, this file can be controlled by setting the <strong class="source-inline">hiera </strong>value <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">puppet_enterprise<a id="_idTextAnchor332"/>::profile::puppetdb::java_args:</strong></span><span class="No-Break">.</span></p>
			<p>Another good indication of performance is the queue depth, which is represented by the <strong class="source-inline">puppetdb-status.status.queue_depth </strong>metric. If this is high and there are free CPUs, it would be beneficial to increase the number of CPU threads available to PuppetDB. This can be done in the PuppetDB configuration file at <strong class="source-inline">/etc/puppetlabs/puppetdb/conf.d</strong>. If PuppetDB has been installed by a package in the <strong class="source-inline">[command-processing] </strong>section with the <strong class="source-inline">threads </strong>key or if the Puppet <a id="_idIndexMarker993"/>PuppetDB module has been used, as will be the case in Puppet Enterprise, the class should be adjusted using the module’s settings. In Puppet Enterprise, this can be done in the <strong class="bold">node groups </strong>section in the web console. Any changes that are made to threads will require you to restart the <span class="No-Break">PuppetDB service.</span></p>
			<p>The reverse scenario, where CPU usage is high and throttling but the PuppetDB queue is low, should allow threads to be released to improve the throughput of <span class="No-Break">other services.</span></p>
			<h3>Tuning sizing</h3>
			<p>To assist <a id="_idIndexMarker994"/>in getting the right server settings based on available hardware, Puppet Enterprise has the <strong class="source-inline">puppet infrastructure </strong><span class="No-Break"><strong class="source-inline">tune </strong></span><span class="No-Break">command.</span></p>
			<p>This calculates the optimal settings to apply for your servers. The following example output extracts only the suggested Hiera settings printed by <span class="No-Break">the command:</span></p>
			<pre class="source-code">
puppet_enterprise::profile::database::shared_buffers: 3176MB
puppet_enterprise::puppetdb::command_processing_threads: 1
puppet_enterprise::profile::puppetdb::java_args:
  Xms: 1588m
  Xmx: 1588m
puppet_enterprise::profile::orchestrator::jruby_max_active_instances: 2
puppet_enterprise::profile::orchestrator::java_args:
  Xms: 1588m
  Xmx: 1588m
puppet_enterprise::profile::console::java_args:
  Xms: 1024m
  Xmx: 1024m
puppet_enterprise::master::puppetserver::jruby_max_active_instances: 2
puppet_enterprise::profile::master::java_args:
  Xms: 1536m
  Xmx: 1536m
puppet_enterprise::master::puppetserver::reserved_code_cache: 192m</pre>
			<p>This<a id="_idIndexMarker995"/> extracted data could be put in a Hiera file and classified against the primary server. Note that if the RAM is high enough, it will recommend configurations for heap sizes above 32 GB. This is sub-optimal, as we discussed when we looked at compilation sizing <span class="No-Break">and issues.</span></p>
			<p>The following is <span class="No-Break">general advice:</span></p>
			<p>It may seem difficult to process the number of metrics, especially given the lack of clear definitions, but if the Splunk plugin or Operational dashboard is used, this can give you a view that’s consistent with what Puppet Support teams use and monitor. Learning how your estate normally behaves in these values and looking for spikes in the graph and relating them to others can go some way to <span class="No-Break">finding issues.</span></p>
			<p>Using Puppet’s knowledge base, which has been open to any user since April 2021, can help you search for issues. Looking at collections of articles such as <a href="https://support.puppet.com/hc/en-us/sections/360000926413-Performance-tuning">https://support.puppet.com/hc/en-us/sections/360000926413-Performance-tuning</a> can assist you in gaining a deeper understanding of any issues that <span class="No-Break">you experience.</span></p>
			<h2 id="_idParaDest-262"><a id="_idTextAnchor333"/>Lab – configuring metric dashboards</h2>
			<p>Having <a id="_idIndexMarker996"/>discussed metrics and Puppet’s two options for viewing them, we can configure the Splunk dashboard and the Puppet Operational dashboard to see the dashboards provided. Using the issues that were described in the previous section around PuppetDB and compiler capacity, find the graphs that would assist in <span class="No-Break">your investigation.</span></p>
			<p>Configure the Puppet <span class="No-Break">operational dashboard:</span></p>
			<ol>
				<li>Choose one of your nodes to host the Puppet Operational dashboard and classify <strong class="source-inline">puppet_operational_dashboards </strong>on the web console as a <span class="No-Break">node group.</span></li>
				<li>In the node group’s PE Infrastructure Agent, add  <strong class="source-inline">puppet_operational_dashboards::enterprise_infrastructure </strong>to the list of classes on the <span class="No-Break"><strong class="bold">Classes </strong></span><span class="No-Break">tab.</span></li>
				<li>Run <strong class="source-inline">puppet </strong>on all the nodes until the servers are <span class="No-Break">showing clean.</span></li>
				<li>Log in at <strong class="source-inline">https://&lt;public_ip_of_operational_dashboard_node&gt;:3000  </strong><span class="No-Break"><strong class="source-inline">user=admin password=admin</strong></span><span class="No-Break">.</span></li>
			</ol>
			<p><span class="No-Break">Configure Splunk:</span></p>
			<ol>
				<li>Sign up for a Splunk account at <a href="https://www.splunk.com/en_us/sign-up.html?301=/page/sign_up">https://www.splunk.com/en_us/sign-up.html?301=/page/sign_up</a> (this <span class="No-Break">is free).</span></li>
				<li>Choose a different node to host Splunk Enterprise by classifying <strong class="source-inline">splunk::enterprise </strong>as a node group on the web console and pinning the <span class="No-Break">chosen node.</span></li>
				<li>Install the Puppet <span class="No-Break">report viewer:</span><ol><li>Log in and <span class="No-Break">download </span><a href="https://splunkbase.splunk.com/app/4413/"><span class="No-Break">https://splunkbase.splunk.com/app/4413/</span></a><span class="No-Break">.</span></li><li>Log in to <strong class="source-inline">https://&lt;public_ip_of_splunk_server&gt;:8000</strong><span class="hidden"> </span><strong class="source-inline">username</strong>=<strong class="source-inline">admin </strong><span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">password</strong></span><span class="No-Break">=</span><span class="No-Break"><strong class="source-inline">changeme</strong></span><span class="No-Break">.</span></li><li>Select the cog to the top left of the <span class="No-Break"><strong class="bold">Apps </strong></span><span class="No-Break">bar.</span></li><li>On the next screen, select <strong class="bold">upload app from file </strong>at the <span class="No-Break">top right.</span></li></ol></li>
				<li>Set the license to free <span class="No-Break">in Splunk:</span><ol><li>Click <strong class="bold">Settings </strong>at the <span class="No-Break">top right.</span></li><li>In the dropdown under system, <span class="No-Break">select </span><span class="No-Break"><strong class="bold">Licensing</strong></span><span class="No-Break">.</span></li><li>Select <strong class="bold">change </strong><span class="No-Break"><strong class="bold">license group</strong></span><span class="No-Break">.</span></li><li>Select <strong class="bold">free license </strong>and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Save</strong></span><span class="No-Break">.</span></li></ol></li>
				<li>Create an HEC token <span class="No-Break">in Splunk:</span><ol><li>Navigate to <strong class="bold">Settings </strong>| <strong class="bold">Data Input </strong>in your <span class="No-Break">Splunk console.</span></li><li>Add a new HTTP Event Collector with a name of <span class="No-Break">your choice.</span></li><li>Ensure <strong class="bold">Indexer acknowledgement </strong>is <span class="No-Break">not enabled.</span></li><li>Click <strong class="bold">Next </strong>and set <strong class="bold">source type </strong><span class="No-Break">to </span><span class="No-Break"><strong class="bold">Automatic</strong></span><span class="No-Break">.</span></li><li>Ensure <strong class="bold">App Context </strong>is set to <strong class="bold">Puppet </strong><span class="No-Break"><strong class="bold">Report Viewer</strong></span><span class="No-Break">.</span></li><li>Add the <span class="No-Break">main index.</span></li><li>Set <strong class="bold">Default Index </strong><span class="No-Break">to </span><span class="No-Break"><strong class="bold">main</strong></span><span class="No-Break">.</span></li><li>Click <strong class="bold">Review </strong>and <span class="No-Break">then </span><span class="No-Break"><strong class="bold">Submit</strong></span><span class="No-Break">.</span></li></ol></li>
				<li>Classify <strong class="source-inline">puppet_metrics_collector </strong>with the <strong class="source-inline">metrics_server_type </strong>parameter set to <strong class="source-inline">splunk_hec </strong>on the PE Infrastructure <a id="_idIndexMarker997"/><span class="No-Break">node group.</span></li>
				<li>Classify <strong class="source-inline">splunk_hec </strong>on the <strong class="bold">PE HA Master </strong>node group with the <span class="No-Break">following parameters:</span><ul><li><strong class="source-inline">enable_reports </strong>set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">true</strong></span></li><li><strong class="source-inline">events_reporting_enabled </strong>set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">true</strong></span></li><li><strong class="source-inline">manage_routes </strong>set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">true</strong></span></li><li><strong class="source-inline">token </strong>set to <strong class="source-inline">&lt;token number of generated in </strong><span class="No-Break"><strong class="source-inline">step 5&gt;</strong></span></li><li><strong class="source-inline">url </strong>set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">https://&lt;public_ip_of_splunk_server&gt;:8088/services/collector</strong></span></li></ul></li>
				<li>Log in at <strong class="source-inline">https://&lt;public_ip_of_splunk_server&gt;:8000</strong> and run <strong class="source-inline">index=* sourcetype=puppet:summary </strong>to ensure data is being gathered (it may take some time <span class="No-Break">to start).</span></li>
			</ol>
			<p>Now that both <a id="_idIndexMarker998"/>the Puppet operational dashboard and Splunk have been configured, tour the various graphs and panels to find the graphs that are relevant to catalog capacity and <span class="No-Break">PuppetDB performance.</span></p>
			<p>If possible, leave Splunk set up for the next section as the inventory and inventory trend views are the dashboard views of the Facter <span class="No-Break">terminus output.</span></p>
			<p>Now that you know how to integrate Puppet’s status, logging, and metrics, we can look at a pattern that allows us to integrate Puppet with other services and provide self-service to <span class="No-Break">Puppet users.</span></p>
			<h1 id="_idParaDest-263"><a id="_idTextAnchor334"/>External data provider pattern</h1>
			<p>Puppet will<a id="_idIndexMarker999"/> not be the only source of configuration and information in your estate. There are likely to be numerous sources for <strong class="bold">Configuration Management Databases </strong>(<strong class="bold">CMDBs</strong>) such<a id="_idIndexMarker1000"/> as ServiceNow or internally developed systems that are used by application teams to store their information. Several of your colleagues and internal customers will want to be able to create exceptions and customizations without having to understand Puppet code and the workflow for deployment. There will also be demands to be able to feed Puppet data back into external systems. The external data provider pattern allows this to happen by allowing you to do <span class="No-Break">the following:</span></p>
			<ul>
				<li>Make changes in <span class="No-Break">the classification</span></li>
				<li>Add and change <span class="No-Break">trusted facts</span></li>
				<li>Feed existing data in as a fact or <span class="No-Break">Hiera data</span></li>
				<li>Send Facter data to <span class="No-Break">external sources</span></li>
			</ul>
			<p>Having introduced the core concept of the external data provider pattern, we will now look at each of the technical components used <span class="No-Break">within it.</span></p>
			<h2 id="_idParaDest-264"><a id="_idTextAnchor335"/>Understanding external data provider components</h2>
			<p>The <a id="_idIndexMarker1001"/>underlying components of this pattern are shown in the <span class="No-Break">following figure:</span></p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B18492_13_04.jpg" alt="Figure 13.4 – Core components of the external data provider pattern"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.4 – Core components of the external data provider pattern</p>
			<p>We will run through each to show how this pattern works. It is beyond the scope of this book to show you how to write each of these components, but we will detail where documentation exists for this. In the following section, sample implementations will <span class="No-Break">be referenced:</span></p>
			<p>A <strong class="bold">backend storage service </strong>(<strong class="bold">BSS</strong>) allows <a id="_idIndexMarker1002"/>you to store data for consumption. The technical solution for the BSS is not important, but it must be resilient and provide high throughput <span class="No-Break">on reads.</span></p>
			<p>This throughput can be calculated at <em class="italic">2 + &lt;number of Hiera levels&gt; </em>reads per Puppet agent run. To show how this would be calculated if an estate of 10,000 servers used the default agent run time of 30 minutes and had a 5-level Hiera setup, this would be calculated as <em class="italic">(2+5) * 10,000 / 1,800 = 39 queries per second </em>(<span class="No-Break">rounded up).</span></p>
			<p>Tools such as CMDB or internal applications can be directly queried and act as the BSS, but the tool can deal with <span class="No-Break">the workload.</span></p>
			<p>The <strong class="source-inline">trusted </strong>external command, which was introduced in Puppet 6.11 and 7.0, allows a script to be run during Puppet runs and gather facts and classification information from external sources. This script should take the <strong class="source-inline">certname </strong>property of the client as its argument, return a JSON hash of facts, and exit with an error code for any unknown <strong class="source-inline">certname</strong>. This script can be configured by using the <strong class="source-inline">trusted_external_command </strong>setting in the <strong class="source-inline">master/server </strong>section of <strong class="source-inline">puppet.conf </strong>on each primary and compiler server. The facts that are returned by this command will be contained under <strong class="source-inline">trusted.external.basename</strong>, where <strong class="source-inline">basename </strong>is the name of the script. Since Puppet 6.17 and 7.0, it is also possible to use multiple<a id="_idIndexMarker1003"/> trusted external commands by setting <strong class="source-inline">trusted_external_command </strong>to be a directory containing multiple scripts. This can be useful for querying multiple sources. Each source would then get a different base name. In the external data provider pattern, it is used to query <span class="No-Break">the BSS.</span></p>
			<p>The Hiera backend uses functions written in Ruby or Puppet to query APIs or other sources when Hiera lookups are performed. In the external data provider pattern, the backend queries the BSS for values. Documentation on this is available <span class="No-Break">at </span><a href="https://puppet.com/docs/puppet/latest/hiera_custom_backends.html"><span class="No-Break">https://puppet.com/docs/puppet/latest/hiera_custom_backends.html</span></a><span class="No-Break">.</span></p>
			<p>Puppet allows pluggable backends<a id="_idIndexMarker1004"/> known as <strong class="bold">termini </strong>and uses indirectors, as discussed in <a href="B18492_10.xhtml#_idTextAnchor252"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, to allow Ruby scripts to access key-value pairs at endpoints. Fact termini are Ruby scripts that access the fact endpoints and allow the data to be sent on to other external systems. Further details on this are available at <a href="https://puppet.com/docs/puppet/latest/indirection.html">https://puppet.com/docs/puppet/latest/indirection.html </a><span class="No-Break">and </span><a href="https://puppet.com/docs/puppet/latest/man/facts.html"><span class="No-Break">https://puppet.com/docs/puppet/latest/man/facts.html</span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-265"><a id="_idTextAnchor336"/>External data provider implementations</h2>
			<p>At the time<a id="_idIndexMarker1005"/> of writing, no single implementation of the external data provider pattern implements all parts, but they can be used together to integrate multiple systems and purposes. The list of examples in this section is not meant to be exhaustive but should show the breadth of integrations that can be investigated. We will also provide documentation examples, which can be expanded on <span class="No-Break">if necessary.</span></p>
			<h3>Satellite</h3>
			<p>Red Hat Satellite <a id="_idIndexMarker1006"/>can receive reports from <a id="_idIndexMarker1007"/>Puppet Server through a report processor while using the module available at <a href="https://forge.puppet.com/modules/puppetlabs/satellite_pe_tools">https://forge.puppet.com/modules/puppetlabs/satellite_pe_tools</a>. However, it is also possible to use the <strong class="source-inline">puppetserver_foreman </strong>module to configure a trusted external command to gather the various configuration data from Satellite, such as smart parameters and organization as facts. With Puppet being removed as the default configuration management choice and instead used as an optional plugin, and Puppet versions not keeping up with development in the Satellite platform, as per <a href="https://www.redhat.com/en/blog/upcoming-changes-puppet-functionality-red-hat-satellite">https://www.redhat.com/en/blog/upcoming-changes-puppet-functionality-red-hat-satellite</a>, the use of this trusted external command allows Puppet Server’s functionality to be migrated as a separate Puppet infrastructure while the configuration is maintained in the Foreman component of Satellite. See the files/Satellite at <a href="https://github.com/theforeman/puppet-puppetserver_foreman">https://github.com/theforeman/puppet-puppetserver_foreman</a> to see the trusted <span class="No-Break">external command.</span></p>
			<h3>ServiceNow</h3>
			<p>Several ServiceNow integrations <a id="_idIndexMarker1008"/>have<a id="_idIndexMarker1009"/> been developed for use with Puppet; the CMDB integration allows a trusted command provided by the module available <span class="No-Break">at </span><a href="https://forge.puppet.com/modules/puppetlabs/servicenow_cmdb_integration"><span class="No-Break">https://forge.puppet.com/modules/puppetlabs/servicenow_cmdb_integration</span></a><span class="No-Break">.</span></p>
			<p>ServiceNow should only be used with BSS on a smaller scale since using a large number of nodes could overwhelm ServiceNow with queries. It provides a useful example of using the <span class="No-Break">trusted command.</span></p>
			<p>A better approach has been developed to ensure scaling where the ServiceNow graph connector connects to the Puppet API and gathers the necessary <span class="No-Break">data: </span><a href="https://store.servicenow.com/sn_appstore_store.do#!/store/application/42ae987a1b832c10fa34a8233a4bcb0b"><span class="No-Break">https://store.servicenow.com/sn_appstore_store.do#!/store/application/42ae987a1b832c10fa34a8233a4bcb0b</span></a><span class="No-Break">.</span></p>
			<h3>Azure Key Vault</h3>
			<p>Azure Key Vault’s integration<a id="_idIndexMarker1010"/> is a <a id="_idIndexMarker1011"/>function that calls <strong class="source-inline">azure_key_vault::secret </strong>in Puppet code. This can be used with a Hiera backend to access Azure Key Vault secrets. It is an approved <span class="No-Break">module (</span><a href="https://forge.puppet.com/modules/tragiccode/azure_key_vault"><span class="No-Break">https://forge.puppet.com/modules/tragiccode/azure_key_vault</span></a><span class="No-Break">).</span></p>
			<h3>1Password</h3>
			<p>The<a id="_idIndexMarker1012"/> 1Password integration<a id="_idIndexMarker1013"/> is a Hiera backend that allows lookup calls to be made for secrets in the 1Password <span class="No-Break">setup: </span><a href="https://forge.puppet.com/modules/bryxxit/onepassword_lookup"><span class="No-Break">https://forge.puppet.com/modules/bryxxit/onepassword_lookup</span></a><span class="No-Break">.</span></p>
			<h3>Vault</h3>
			<p>The two<a id="_idIndexMarker1014"/> Vault solutions (server-side and client-side) were discussed and demonstrated in <a href="B18492_09.xhtml#_idTextAnchor233"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, but to<a id="_idIndexMarker1015"/> recap, it is the server-side Vault lookup that implements a Hiera backend lookup function called <strong class="source-inline">hiera_vault </strong>in the module. As discussed at <a href="https://forge.puppet.com/modules/petems/hiera_vault">https://forge.puppet.com/modules/petems/hiera_vault</a>, this allows secrets from Vault to be called via Hiera and compiled <span class="No-Break">into code.</span></p>
			<h3>Puppet Data Service</h3>
			<p><strong class="bold">Puppet Data Service </strong>(<strong class="bold">PDS</strong>) provides <a id="_idIndexMarker1016"/>one of the most complete implementations of the external data provider pattern, except<a id="_idIndexMarker1017"/> it implements a fact terminus. The following components are a part <span class="No-Break">of PDS:</span></p>
			<ul>
				<li>A REST API and CLI that allow user and <span class="No-Break">application interaction</span></li>
				<li>A pluggable backend database to provide a BSS (at the time of writing, only PostgreSQL <span class="No-Break">is supported)</span></li>
				<li>A Hiera backend to query <span class="No-Break">the BSS</span></li>
				<li>A trusted external command to query <span class="No-Break">the BSS</span></li>
			</ul>
			<p>PDS <a id="_idIndexMarker1018"/>was<a id="_idIndexMarker1019"/> designed to be less focused on a particular set of integrations and allow Puppet platform teams to leverage the external data provider pattern to provide self-service and reduce operational burdens. PDS has an install module (<a href="https://github.com/puppetlabs/puppetlabs-puppet_data_service">https://github.com/puppetlabs/puppetlabs-puppet_data_service</a>). The code that makes up the application and API (<a href="https://github.com/puppetlabs/puppet-data-service">https://github.com/puppetlabs/puppet-data-service</a>) is packaged in <strong class="source-inline">deb </strong>and <strong class="source-inline">rpm</strong>, which are used by the <strong class="source-inline">install </strong>module. At the time of writing, both modules are only designed for a Puppet Enterprise installation, but nothing within the underlying setup limits the application to Puppet Enterprise, which means it can be adapted to open <span class="No-Break">source Puppet.</span></p>
			<h3>Splunk</h3>
			<p>In the <em class="italic">Logging and status </em>section <a id="_idIndexMarker1020"/>of this chapter, we discussed and demonstrated how the <strong class="source-inline">splunk_hec </strong>module<a id="_idIndexMarker1021"/> provided communication from Puppet Server to the Splunk Hec URL on a Splunk server using the same modules fact <span class="No-Break">terminus (</span><a href="https://github.com/puppetlabs/puppetlabs-splunk_hec/blob/main/lib/puppet/indirector/facts/splunk_hec.rb"><span class="No-Break">https://github.com/puppetlabs/puppetlabs-splunk_hec/blob/main/lib/puppet/indirector/facts/splunk_hec.rb</span></a><span class="No-Break">).</span></p>
			<p>Facts from Puppet runs can be sent to Splunk, which can then be viewed in the Splunk app (<a href="https://splunkbase.splunk.com/app/4413/">https://splunkbase.splunk.com/app/4413/</a>) in terms of inventory and <span class="No-Break">inventory trends.</span></p>
			<h2 id="_idParaDest-266"><a id="_idTextAnchor337"/>Lab – hands-on with Splunk and Puppet Data Service</h2>
			<p>Having discussed several integrations, if you left the Splunk <a id="_idIndexMarker1022"/>installation or reinstallation as per the previous lab, you can log into Splunk and view the <strong class="bold">inventory </strong>and <strong class="bold">inventory trend </strong>tabs. Here, you will see the Facter terminus output and can experiment with viewing the data from <span class="No-Break">your nodes.</span></p>
			<p>In this part of the lab, you will see how PDS can be used to classify nodes, update Hiera data, and add <span class="No-Break">trusted facts.</span></p>
			<p>To install PDS, you<a id="_idIndexMarker1023"/> will need to perform the <span class="No-Break">following tasks:</span></p>
			<ol>
				<li>Observe the <strong class="source-inline">hiera.yaml </strong>and <strong class="source-inline">site.pp </strong>file in the control repository you cloned and see how PDS will <span class="No-Break">use them.</span></li>
				<li>Configure the two required <span class="No-Break">application roles.</span></li>
				<li>For <a id="_idIndexMarker1024"/>the database server, do <span class="No-Break">the following:</span><ol><li>Add a new node group from the <span class="No-Break">PE console:</span></li></ol><pre class="source-code">
Parent name: PE Infrastructure
Group name: PDS Database
Environment: production</pre><ol><li value="2">Add the <strong class="source-inline">puppet_data_service::database </strong>class to the PDS database group you created in the <span class="No-Break">previous step.</span></li><li>Add your existing primary server to the group using the Rules tab node before following <span class="No-Break">these steps.</span></li><li>Commit <span class="No-Break">your changes.</span></li></ol></li>
				<li>For the PDS API servers, do <span class="No-Break">the following</span><ol><li>Select the <strong class="bold">PE Master </strong><span class="No-Break">node group.</span></li><li>Select the <span class="No-Break"><strong class="bold">classes </strong></span><span class="No-Break">tab.</span></li><li>Add the new <span class="No-Break"><strong class="source-inline">puppet_data_service::server </strong></span><span class="No-Break">class</span></li><li>Include the <strong class="source-inline">database_host: &lt;FQDN of your primary </strong><span class="No-Break"><strong class="source-inline">server&gt; </strong></span><span class="No-Break">parameter</span></li><li>Select the <strong class="bold">Configuration </strong><span class="No-Break">data tab</span></li><li>Configure the <strong class="source-inline">sensitive pds_token </strong>parameter. You can use <a href="https://www.uuidgenerator.net/">https://www.uuidgenerator.net/</a> to generate <span class="No-Break">a token</span></li><li>Commit <span class="No-Break">your changes.</span></li></ol></li>
				<li>Run Puppet on all nodes until reports <span class="No-Break">show unchanged.</span></li>
				<li>Create<a id="_idIndexMarker1025"/> an SSH session to the primary server and one of the nodes in separate <span class="No-Break">terminal windows.</span></li>
				<li>On the primary server, run the <strong class="source-inline">pds-cli node upsert &lt;fqdn_of_node&gt; -c motd -e </strong><span class="No-Break"><strong class="source-inline">production </strong></span><span class="No-Break">command.</span></li>
				<li>On the node, run <strong class="source-inline">puppet agent –t</strong>. In the output of the command, you will see that <strong class="source-inline">motd </strong>has been applied with <span class="No-Break">default settings.</span></li>
				<li>On the primary server, run <strong class="source-inline">pds-cli hiera upsert nodes/&lt;fqdn_of_node&gt; motd::content -v '"Hello world </strong><span class="No-Break"><strong class="source-inline">its PDS\n"'</strong></span><span class="No-Break">.</span></li>
				<li>On the node, run <strong class="source-inline">puppet agent –t</strong>. In the output of the command, you will see that <strong class="source-inline">motd </strong>has been applied with the Hiera override <span class="No-Break">we set.</span></li>
				<li>On the primary, run <strong class="source-inline"> pds-cli node upsert &lt;fqdn_of_node&gt; -c motd -d '{"status": "Testing"}' -e production </strong>and <strong class="source-inline">pds-cli hiera upsert nodes/&lt;fqdn_of_name&gt; motd::content -v '"Hello world, I am a PDS %{</strong><span class="No-Break"><strong class="source-inline">trusted.external.pds.data.status} Server\n"'</strong></span><span class="No-Break">.</span></li>
				<li>On the node, run <strong class="source-inline">puppet agent –t</strong>. Observe that it applies the new <strong class="source-inline">motd </strong>with the Hiera override and value <strong class="source-inline">testing </strong>set for the <span class="No-Break">trusted fact.</span></li>
				<li>Log into the console and look at the node and its facts to see if it has a trusted fact, <strong class="source-inline">pds.data.status</strong>, set <span class="No-Break">to testing.</span></li>
			</ol>
			<h1 id="_idParaDest-267"><a id="_idTextAnchor338"/>Summary</h1>
			<p>In this chapter, we summarized the various log locations and showed you how logs could be turned into JSON and exported so that they can be handled in logging toolsets such as Elastic or Grafana, which can better index them for viewing and analysis. We learned how report processors can be used on Puppet Server to allow the reports to be generated by applying catalogs on clients. This allows them to be sent to tools such as Splunk and allows for advanced visualizations and searches. The available status APIs were discussed, indicating how an API call could be made to find the status of all running services or a particular service. Puppet Enterprise was shown to have a command line (<strong class="source-inline">Puppet Infrastructure status</strong>) and web console option to call this API. Using these mechanisms, you learned how to access critical logging and metrics to understand the current state of <span class="No-Break">the system.</span></p>
			<p>To use this information and understand the performance of the services in depth, you learned how Puppet metrics become available upon using the <strong class="source-inline">debug </strong>flag of the status API and how tools such as the Puppet Operational Dashboard and the Puppet plugin for Splunk could be used to gather this data and visualize it. Puppet Enterprise was noted as having the Metrics Collector module, which gathers metrics locally in JSON files, which can be viewed manually <span class="No-Break">or exported.</span></p>
			<p>To better understand how these metrics and dashboards can be used, we reviewed some common issues, looking at how to size the infrastructure for catalog compilation and avoid issues such as Thundering Herd as servers squeeze demand and how PuppetDB could be adjusted as demand increases or decreases. Various infrastructure tuning tools were shown to be an option in PE to optimize settings for <span class="No-Break">deployed hardware.</span></p>
			<p>Then, we covered the external data provider pattern, which provides mechanisms for self-service and access to Puppet data on external services so that it can be integrated better. The core components of a backend storage service were shown to provide a store for data that could cope with the level of queries Puppet would make while trusted external commands and the Hiera backend were shown as ways to query that data. Fact termini were shown to be ways to export data from the BSS to <span class="No-Break">external services.</span></p>
			<p>Various implementations of these components were shown when using various Hiera backends, with 1Password, Azure Key Vault, and Vault being shown as ways to access external secret managers, while Satellite and ServiceNow were shown to have trusted commands that allowed data within those applications to be fed into <span class="No-Break">Puppet code.</span></p>
			<p>Puppet Data Service was shown to be one of the most complete implementations of the pattern and provides a solid design to allow for self-service of internal customers who would be able to access suitably exposed Puppet options without requiring full knowledge of the Git flow and <span class="No-Break">Puppet language.</span></p>
			<p>This coverage of the external data provider pattern showed you how powerful integrations can be made with Puppet Enterprise to feed data into and out of different tools and work toward building a platform with Puppet as a <span class="No-Break">vital component.</span></p>
			<p>Having covered the components of Puppet Server how to monitor performance at scale and integrate it, the next chapter will look at Puppet Enterprise-specific services and their components. It will describe what Puppet Enterprise is, how it differs from the open source version, what extra services it provides, and the reference architectures provided by Puppet to allow for easier scaling and tooling to automate a deployment and its status. Projects and integrations specifically for Puppet Enterprise will also <span class="No-Break">be discussed.</span></p>
		</div>
	

		<div id="_idContainer060" class="Content">
			<h1 id="_idParaDest-268"><a id="_idTextAnchor339"/>Part 4 – Puppet Enterprise and Approaches to the Adoption of Puppet</h1>
			<p>This part will look at Puppet Enterprise and how it differs from open source. It will review some Puppet-related products that can extend Puppet Enterprise and some specific integrations for Puppet Enterprise. We will then discuss approaches that can help organizations successfully adopt Puppet. We will look at correctly scoping use cases to benefit from regular delivery, and how Puppet can work within platform engineering as well as with heritage estates, and even in highly regulated, <span class="No-Break">change-managed estates.</span></p>
			<p>This part has the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B18492_14.xhtml#_idTextAnchor340"><em class="italic">Chapter 14</em></a>, <em class="italic">A Brief Overview of Puppet Enterprise</em></li>
				<li><a href="B18492_15.xhtml#_idTextAnchor359"><em class="italic">Chapter 15</em></a>, <em class="italic">Approaches to Adoption</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer061" class="Basic-Graphics-Frame">
			</div>
		</div>
		<div>
			<div id="_idContainer062">
			</div>
		</div>
	</body></html>