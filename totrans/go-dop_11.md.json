["```\npackage main\nimport \"log\"\nfunc main() {\n     log.Println(\"Hello World!\")\n}\n// Outputs: 2009/11/10 23:00:00 Hello World!\n```", "```\n// Debug will log a Debug level event\nfunc (log *Logger) Debug(msg string, fields ...Field)\n// Info will log an Info level event\nfunc (log *Logger) Info(msg string, fields ...Field)\n// Error will log an Error level event\nfunc (log *Logger) Error(msg string, fields ...Field)\n// With will return a logger that will log the keys and values specified for future log events\nfunc (log *Logger) With(fields ...Field) *Logger\n// Named will return a logger with a given name\nfunc (log *Logger) Named(s string) *Logger\n```", "```\npackage main\nimport (\n     \"time\"\n     \"go.uber.org/zap\"\n)\nfunc main() {\n     logger, _ := zap.NewProduction()\n     defer logger.Sync()\n     logger = logger.Named(\"my-app\")\n     logger.Info\n          (\"failed to fetch URL\",\n          zap.String(\"url\", \"https://github.com\"),\n          zap.Int(\"attempt\", 3),\n          zap.Duration(\"backoff\", time.Second),\n     )\n}\n// Outputs: {\"level\":\"info\",\"ts\":1257894000,\"logger\":\"my\n// app\",\"caller\":\"sandbox4253963123/prog.go:15\",\n// \"msg\":\"failed to fetch URL\",\n// \"url\":\"https://github.com\",\"attempt\":3,\"backoff\":1}\n```", "```\n.\n├── README.md\n├── docker-compose.yml\n├── otel-collector-config.yml\n└── varlogpods\n    ├── containerd_logs\n0_000011112222333344445555666677778888\n    │   └── logs\n    │       └── 0.log\n    ├── crio_logs-0_111122223333444455556666777788889999\n    │   └── logs\n    │       └── 0.log\n    ├── docker_logs-0_222233334444555566667777888899990000\n    │   └── logs\n    │       └── 0.log\n    └── otel_otel_888877776666555544443333222211110000\n        └── otel-collector\n            └── 0.log\n```", "```\nversion: \"3\"\nservices:\n  opentelemetry-collector-contrib:\n    image: otelcontribcol\n    command: [\"--config=/etc/otel-collector-config.yml\"]\n    volumes:\n      - ./otel-collector-config.yml:/etc/otel-collector-config.yml\n      - ./varlogpods:/var/log/pods\n```", "```\nreceivers:\n  filelog:\n    include:\n      - /var/log/pods/*/*/*.log\n    exclude:\n      # Exclude logs from all containers named otel-collector\n      - /var/log/pods/*/otel-collector/*.log\n    start_at: beginning\n    include_file_path: true\n    include_file_name: false\n```", "```\n    operators:\n      # Find out which format is used by kubernetes\n      - type: router\n        id: get-format\n        routes:\n          - output: parser-docker\n            expr: '$$body matches \"^\\\\{\"'\n          - output: parser-crio\n            expr: '$$body matches \"^[^ Z]+ \"'\n          - output: parser-containerd\n            expr: '$$body matches \"^[^ Z]+Z\"'\n```", "```\n      # Parse CRI-O format\n      - type: regex_parser\n        id: parser-crio\n        regex: '^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) (?P<log>.*)$'\n        output: extract_metadata_from_filepath\n        timestamp:\n          parse_from: time\n          layout_type: gotime\n          layout: '2006-01-02T15:04:05.000000000-07:00'\n      # Parse CRI-Containerd format\n      - type: regex_parser\n        id: parser-containerd\n        regex: '^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) (?P<log>.*)$'\n        output: extract_metadata_from_filepath\n        timestamp:\n          parse_from: time\n          layout: '%Y-%m-%dT%H:%M:%S.%LZ'\n      # Parse Docker format\n      - type: json_parser\n        id: parser-docker\n        output: extract_metadata_from_filepath\n        timestamp:\n          parse_from: time\n          layout: '%Y-%m-%dT%H:%M:%S.%LZ'\n      # Extract metadata from file path\n      - type: regex_parser\n        id: extract_metadata_from_filepath\n        regex: '^.*\\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\\-]{36})\\/(?P<container_name>[^\\._]+)\\/(?P<restart_count>\\d+)\\.log$'\n        parse_from: $$attributes[\"file.path\"]\n      # Move out attributes to Attributes\n      - type: metadata\n        attributes:\n          stream: 'EXPR($.stream)'\n          k8s.container.name: 'EXPR($.container_name)'\n          k8s.namespace.name: 'EXPR($.namespace)'\n          k8s.pod.name: 'EXPR($.pod_name)'\n          k8s.container.restart_count: 'EXPR($.restart_count)'\n          k8s.pod.uid: 'EXPR($.uid)'\n      # Clean up log body\n      - type: restructure\n        id: clean-up-log-body\n        ops:\n          - move:\n              from: log\n              to: $\n```", "```\n2021-02-16T08:59:31.252009327+00:00 stdout F example: 11 Tue Feb 16 08:59:31 UTC 2021\n```", "```\n{\"log\":\"example: 12 Tue Feb 16 09:15:12 UTC\n2021\\n\",\"stream\":\"stdout\",\"time\":\"2021-02-16T09:15:12.50286486Z\"}\n```", "```\nopentelemetry-collector-contrib_1  | LogRecord #19\nopentelemetry-collector-contrib_1  | Timestamp: 2021-02-16 09:15:17.511829776 +0000 UTC\nopentelemetry-collector-contrib_1  | Severity:\nopentelemetry-collector-contrib_1  | ShortName:\nopentelemetry-collector-contrib_1  | Body: example: 17 Tue Feb 16 09:15:17 UTC 2021\nopentelemetry-collector-contrib_1  |\nopentelemetry-collector-contrib_1  | Attributes:\nopentelemetry-collector-contrib_1  |      -> k8s.container.name: STRING(logs)\nopentelemetry-collector-contrib_1  |      -> k8s.container.restart_count: STRING(0)\nopentelemetry-collector-contrib_1  |      -> k8s.namespace.name: STRING(docker)\nopentelemetry-collector-contrib_1  |      -> k8s.pod.name: STRING(logs-0)\nopentelemetry-collector-contrib_1  |      -> k8s.pod.uid: STRING(222233334444555566667777888899990000)\nopentelemetry-collector-contrib_1  |      -> stream: STRING(stdout)\nopentelemetry-collector-contrib_1  | Trace ID:\nopentelemetry-collector-contrib_1  | Span ID:\nopentelemetry-collector-contrib_1  | Flags: 0\n```", "```\n.\n├── readme.md\n├── client\n│   ├── Dockerfile\n│   ├── go.mod\n│   ├── go.sum\n│   └── main.go\n├── docker-compose.yaml\n├── otel-collector-config.yaml\n└── server\n    ├── Dockerfile\n    ├── go.mod\n    ├── go.sum\n    └── main.go\n```", "```\nversion: \"2\"\nservices:\n  # Jaeger\n  jaeger-all-in-one:\n    image: jaegertracing/all-in-one:latest\n    ports:\n      - \"16686:16686\"\n      - \"14268\"\n      - \"14250\"\n  # Collector\n  otel-collector:\n    image: ${OTELCOL_IMG}\n    command: [\"--config=/etc/otel-collector-config.yaml\", \"${OTELCOL_ARGS}\"]\n    volumes:\n      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml\n    ports:\n      - \"13133:13133\" # health_check extension\n    depends_on:\n      - jaeger-all-in-one\n  demo-client:\n    build:\n      dockerfile: Dockerfile\n      context: ./client\n    environment:\n      - OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317\n      - DEMO_SERVER_ENDPOINT=http://demo-server:7080/hello\n    depends_on:\n      - demo-server\n  demo-server:\n    build:\n      dockerfile: Dockerfile\n      context: ./server\n    environment:\n      - OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317\n    ports:\n      - \"7080\"\n    depends_on:\n      - otel-collector\n```", "```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\nexporters:\n  jaeger:\n    endpoint: jaeger-all-in-one:14250\n    tls:\n      insecure: true\nprocessors:\n  batch:\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [jaeger]\n```", "```\nfunc main() {\n     shutdown := initTraceProvider()\n     defer shutdown()\n\n     continuouslySendRequests()\n}\n```", "```\nfunc initTraceProvider() func() {\n\tctx := context.Background()\n\tcancel = context.CancelFunc\n\ttimeout := 1 * time.Second\n\tendPointEnv := \"OTEL_EXPORTER_OTLP_ ENDPOINT\"\n\totelAgentAddr, ok := os.LookupEnv(endPointEnv)\n\tif !ok {\n\t\totelAgentAddr = \"0.0.0.0:4317\"\n\t}\n\tcloseTraces := initTracer(ctx, otelAgentAddr)\n\treturn func() {\n\t\tctx, cancel = context.WithTimeout(ctx, time.Second)\n\t\tdefer cancel()\n\t\t// pushes any last exports to the receiver\n\t\tcloseTraces(doneCtx)\n\t}\n}\n```", "```\nfunc initTracer(ctx context.Context, otelAgentAddr string) func(context.Context) {\n     traceClient := otlptracegrpc.NewClient(\n          otlptracegrpc.WithInsecure(),\n          otlptracegrpc.WithEndpoint(otelAgentAddr),\n          otlptracegrpc.WithDialOption(grpc.WithBlock()))\n     traceExp, err := otlptrace.New(ctx, traceClient)\n     handleErr(err, \"Failed to create the collector trace exporter\")\n     res, err := resource.New(\n          ctx,\n          resource.WithFromEnv(),\n          resource.WithProcess(),\n          resource.WithTelemetrySDK(),\n          resource.WithHost(),\n          resource.WithAttributes(\n               semconv.ServiceNameKey.String(\"demo-client\"),\n          ),\n     )\n     handleErr(err, \"failed to create resource\")\n     bsp := sdktrace.NewBatchSpanProcessor(traceExp)\n     tracerProvider := sdktrace.NewTracerProvider(\n          sdktrace.WithSampler(sdktrace.AlwaysSample()),\n          sdktrace.WithResource(res),\n          sdktrace.WithSpanProcessor(bsp),\n     )\n     // set global propagator to tracecontext (the default is no-op).\n     otel.SetTextMapPropagator(propagation.TraceContext{})\n     otel.SetTracerProvider(tracerProvider)\n     return func(doneCtx context.Context) {\n          if err := traceExp.Shutdown(doneCtx); err != nil {\n               otel.Handle(err)\n          }\n     }\n}\n```", "```\nfunc continuouslySendRequests() {\n     tracer := otel.Tracer(\"demo-client-tracer\")\n     for {\n          ctx, span := tracer.Start(context.Background(), \"ExecuteRequest\")\n          makeRequest(ctx)\n          span.End()\n          time.Sleep(time.Duration(1) * time.Second)\n     }\n}\n```", "```\nfunc makeRequest(ctx context.Context) {\n     demoServerAddr, ok := os.LookupEnv(\"DEMO_SERVER_ENDPOINT\")\n     if !ok {\n          demoServerAddr = \"http://0.0.0.0:7080/hello\"\n     }\n     // Trace an HTTP client by wrapping the transport\n     client := http.Client{\n          Transport: otelhttp.NewTransport(http.DefaultTransport),\n     }\n     // Make sure we pass the context to the request to avoid broken traces.\n     req, err := http.NewRequestWithContext(ctx, \"GET\", demoServerAddr, nil)\n     if err != nil {\n          handleErr(err, \"failed to http request\")\n     }\n     // All requests made with this client will create spans.\n     res, err := client.Do(req)\n     if err != nil {\n          panic(err)\n     }\n     res.Body.Close()\n}\n```", "```\nfunc main() { \n    shutdown := initTraceProvider() \n    defer shutdown()\n     handler := handleRequestWithRandomSleep()\n     wrappedHandler := otelhttp.NewHandler(handler, \"/hello\")\n     http.Handle(\"/hello\", wrappedHandler)\n     http.ListenAndServe(\":7080\", nil)\n}\n```", "```\nfunc handleRequestWithRandomSleep() http.HandlerFunc {\n     commonLabels := []attribute.KeyValue{\n          attribute.String(\"server-attribute\", \"foo\"),\n     }\n     return func(w http.ResponseWriter, req *http.Request) {\n          //  random sleep to simulate latency\n          var sleep int64\n          switch modulus := time.Now().Unix() % 5; modulus {\n          case 0:\n               sleep = rng.Int63n(2000)\n          case 1:\n               sleep = rng.Int63n(15)\n          case 2:\n               sleep = rng.Int63n(917)\n          case 3:\n               sleep = rng.Int63n(87)\n          case 4:\n               sleep = rng.Int63n(1173)\n          }\n          time.Sleep(time.Duration(sleep) * time.Millisecond)\n          ctx := req.Context()\n          span := trace.SpanFromContext(ctx)\n          span.SetAttributes(commonLabels...)\n          w.Write([]byte(\"Hello World\"))\n     }\n}\n```", "```\nfunc WithCorrelation(span trace.Span, log *zap.Logger) *zap.Logger {\n     return log.With(\n          zap.String(\"span_id\", convertTraceID(span.SpanContext().SpanID().String())),\n          zap.String(\"trace_id\", convertTraceID(span.SpanContext().TraceID().String())),\n     )\n}\nfunc convertTraceID(id string) string {\n     if len(id) < 16 {\n          return \"\"\n     }\n     if len(id) > 16 {\n          id = id[16:]\n     }\n     intValue, err := strconv.ParseUint(id, 16, 64)\n     if err != nil {\n          return \"\"\n     }\n     return strconv.FormatUint(intValue, 10)\n}\n```", "```\nfunc SuccessfullyFinishedRequestEvent(span trace.Span, opts ...trace.EventOption) {\n     opts = append(opts, trace.WithAttributes(attribute.String(\"someKey\", \"someValue\")))\n     span.AddEvent(\"successfully finished request operation\", opts...)\n}\n```", "```\nmeter := global.Meter(\"demo-client-meter\")\nrequestLatency := metric.Must(meter).NewFloat64Histogram(\n\t\"demo_client/request_latency\",\n\tmetric.WithDescription(\n\t\t\"The latency of requests processed\"\n\t),\n)\nrequestCount := metric.Must(meter).NewInt64Counter(\n\t\"demo_client/request_counts\",\n\tmetric.WithDescription(\"The number of requests processed\"),\n)\n```", "```\nmeter.RecordBatch(\n    ctx,\n    commonLabels,\n    requestLatency.Measurement(latencyMs),\n    requestCount.Measurement(1),\n)\n```", "```\n.\n├── readme.md\n├── client\n│   ├── Dockerfile\n│   ├── go.mod\n│   ├── go.sum\n│   └── main.go\n├── .env\n├── docker-compose.yaml\n├── otel-collector-config.yaml\n├── prometheus.yaml\n└── server\n    ├── Dockerfile\n    ├── go.mod\n    ├── go.sum\n    └── main.go\n```", "```\nscrape_configs:\n  - job_name: 'otel-collector'\n    scrape_interval: 10s\n    static_configs:\n      - targets: ['otel-collector:8889']\n      - targets: ['otel-collector:8888']\n```", "```\nversion: \"2\"\nservices:\n  # omitted Jaeger config\n  # Collector\n  otel-collector:\n    image: ${OTELCOL_IMG}\n    command: [\"--config=/etc/otel-collector-config.yaml\", \"${OTELCOL_ARGS}\"]\n    volumes:\n      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml\n    ports:\n      - \"8888:8888\"   # Prometheus metrics exposed by the collector\n      - \"8889:8889\"   # Prometheus exporter metrics\n      - \"4317\"        # OTLP gRPC receiver\n    depends_on:\n      - jaeger-all-in-one\n  # omitted demo-client and demo-server\n  prometheus:\n    container_name: prometheus\n    image: prom/prometheus:latest\n    volumes:\n      - ./prometheus.yaml:/etc/prometheus/prometheus.yml\n    ports:\n      - \"9090:9090\"\n```", "```\nreceivers:\n  otlp:\n    protocols:\n      grpc:\nexporters:\n  prometheus:\n    endpoint: \"0.0.0.0:8889\"\n    const_labels:\n      label1: value1\n  logging:\n  # omitted jaeger exporter\nprocessors:\n  batch:\nservice:\n  pipelines:\n    # omitted tracing pipeline\n    metrics:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [logging, prometheus]\n```", "```\nfunc main() {\n     shutdown := initTraceAndMetricsProvider()\n     defer shutdown()\n     continuouslySendRequests()\n}\n```", "```\nfunc initTraceAndMetricsProvider() func() {\n\tctx := context.Background()\n\tvar cancel context.CancelFunc\n\ttimeout := 1 * time.Second\n\tendpoint := \"OTEL_EXPORTER_OTLP_ ENDPOINT\"\n\totelAgentAddr, ok := os.LookupEnv(endpoint)\n\tif !ok {\n\t\totelAgentAddr = \"0.0.0.0:4317\"\n\t}\n\tcloseMetrics := initMetrics(ctx, otelAgentAddr)\n\tcloseTraces := initTracer(ctx, otelAgentAddr)\n\treturn func() {\n\t\tctx, cancel = context.WithTimeout(ctx, timeout)\n\t\tdefer cancel()\n\t\tcloseTraces(doneCtx)\n\t\tcloseMetrics(doneCtx)\n\t}\n}\n```", "```\nfunc initMetrics(ctx context.Context, otelAgentAddr string) func(context.Context) {\n     metricClient := otlpmetricgrpc.NewClient(\n          otlpmetricgrpc.WithInsecure(),\n          otlpmetricgrpc.WithEndpoint(otelAgentAddr))\n     metricExp, err := otlpmetric.New(ctx, metricClient)\n     handleErr(err, \"Failed to create the collector metric exporter\")\n     pusher := controller.New(\n          processor.NewFactory(\n               simple.NewWithHistogramDistribution(),\n               metricExp,\n          ),\n          controller.WithExporter(metricExp),\n          controller.WithCollectPeriod(2*time.Second),\n     )\n     global.SetMeterProvider(pusher)\n     err = pusher.Start(ctx)\n     handleErr(err, \"Failed to start metric pusher\")\n     return func(doneCtx context.Context) {\n          // pushes any last exports to the receiver\n          if err := pusher.Stop(doneCtx); err != nil {\n               otel.Handle(err)\n          }\n     }\n}\n```", "```\nfunc continuouslySendRequests() {\n     var (\n          meter        = global.Meter(\"demo-client-meter\")\n          instruments  = NewClientInstruments(meter)\n          commonLabels = []attribute.KeyValue{\n               attribute.String(\"method\", \"repl\"),\n               attribute.String(\"client\", \"cli\"),\n          }\n          rng = rand.New(rand.NewSource(time.Now().UnixNano()))\n     )\n     for {\n          startTime := time.Now()\n          ctx, span := tracer.Start(context.Background(), \"ExecuteRequest\")\n          makeRequest(ctx)\n          span.End()\n          latencyMs := float64(time.Since(startTime)) / 1e6\n          nr := int(rng.Int31n(7))\n          for i := 0; i < nr; i++ {\n               randLineLength := rng.Int63n(999)\n               meter.RecordBatch(\n                    ctx,\n                    commonLabels,\n                    instruments.LineCounts.Measurement(1),\n                    instruments.LineLengths.Measurement(\n  randLineLength\n),\n               )\n               fmt.Printf(\"#%d: LineLength: %dBy\\n\", i, randLineLength)\n          }\n          meter.RecordBatch(\n               ctx,\n               commonLabels,\n               instruments.RequestLatency.Measurement(\n  latencyMs\n),\n               instruments.RequestCount.Measurement(1),\n          )\n          fmt.Printf(\"Latency: %.3fms\\n\", latencyMs)\n          time.Sleep(time.Duration(1) * time.Second)\n     }\n}\n```", "```\nfunc NewClientInstruments(meter metric.Meter) \nClientInstruments {\n     return ClientInstruments{\n          RequestLatency: metric.Must(meter).\n               NewFloat64Histogram(\n                    \"demo_client/request_latency\",\n                    metric.WithDescription(\"The latency of requests processed\"),\n               ),\n          RequestCount: metric.Must(meter).\n               NewInt64Counter(\n                    \"demo_client/request_counts\",\n                    metric.WithDescription(\"The number of requests processed\"),\n               ),\n          LineLengths: metric.Must(meter).\n               NewInt64Histogram(\n                    \"demo_client/line_lengths\",\n                    metric.WithDescription(\"The lengths of the various lines in\"),\n               ),\n          LineCounts: metric.Must(meter).\n               NewInt64Counter(\n                    \"demo_client/line_counts\",\n                    metric.WithDescription(\"The counts of the lines in\"),\n               ),\n     }\n}\n```", "```\nfunc main() {\n     shutdown := initProvider()\n     defer shutdown()\n     // create a handler wrapped in OpenTelemetry instrumentation\n     handler := handleRequestWithRandomSleep()\n     wrappedHandler := otelhttp.NewHandler(handler, \"/hello\")\n     http.Handle(\"/hello\", wrappedHandler)\n     http.ListenAndServe(\":7080\", nil)\n}\n```", "```\nfunc handleRequestWithRandomSleep() http.HandlerFunc {\n     var (\n          meter        = global.Meter(\"demo-server-meter\")\n          instruments  = NewServerInstruments(meter)\n          commonLabels = []attribute.KeyValue{\n               attribute.String(\"server-attribute\", \"foo\"),\n          }\n     )\n     return func(w http.ResponseWriter, req *http.Request) {\n          var sleep int64\n          switch modulus := time.Now().Unix() % 5; modulus {\n          case 0:\n               sleep = rng.Int63n(2000)\n          case 1:\n               sleep = rng.Int63n(15)\n          case 2:\n               sleep = rng.Int63n(917)\n          case 3:\n               sleep = rng.Int63n(87)\n          case 4:\n               sleep = rng.Int63n(1173)\n          }\n          time.Sleep(time.Duration(sleep) * time.Millisecond)\n          ctx := req.Context()\n          meter.RecordBatch(\n               ctx,\n               commonLabels,\n               instruments.RequestCount.Measurement(1),\n          )\n          span := trace.SpanFromContext(ctx)\n          span.SetAttributes(commonLabels...)\n          w.Write([]byte(\"Hello World\"))\n     }\n}\n```", "```\nversion: \"2\"\nservices:\n  # omitted previous configurations\n  prometheus:\n    container_name: prometheus\n    image: prom/prometheus:latest\n    volumes:\n      - ./prometheus.yaml:/etc/prometheus/prometheus.yml\n      - ./rules:/etc/prometheus/rules\n    ports:\n      - \"9090:9090\"\n  alertmanager:\n    container_name: alertmanager\n    image: prom/alertmanager:latest\n    restart: unless-stopped\n    ports:\n      - \"9093:9093\"\n    volumes:\n      - ./alertmanager.yml:/config/alertmanager.yaml\n      - alertmanager-data:/data\n    command: --config.file=/config/alertmanager.yaml -- log.level=debug\nvolumes:\n  alertmanager-data:\n```", "```\nroute:\n  receiver: default\n  group_by: [ alertname ]\n  routes:\n    - match:\n        exported_job: demo-server\n      receiver: demo-server\nreceivers:\n  - name: default\n    pagerduty_configs:\n      - service_key: \"**Primary-Integration-Key**\"\n  - name: demo-server\n    pagerduty_configs:\n      - service_key: \"**Server-Team-Integration-Key**\"\n```", "```\nscrape_configs:\n  - job_name: 'otel-collector'\n    scrape_interval: 10s\n    static_configs:\n      - targets: ['otel-collector:8889']\n      - targets: ['otel-collector:8888']\nalerting:\n  alertmanagers:\n    - scheme: http\n      static_configs:\n        - targets: [ 'alertmanager:9093' ]\nrule_files:\n  - /etc/prometheus/rules/*\n```", "```\ngroups:\n  - name: demo-server\n    rules:\n      - alert: HighRequestLatency\n        expr: |\n          histogram_quantile(0.5, rate(http_server_duration_bucket{exported_job=\"demo-server\"}[5m])) > 200000\n        labels:\n          severity: page\n        annotations:\n          summary: High request latency\n```", "```\n$ docker-compose up -d\n```"]