<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer065">
			<h1 id="_idParaDest-301"><a id="_idTextAnchor669"/>Chapter <a id="_idTextAnchor670"/>14: Deploying and Building Applications in Kubernetes</h1>
			<p>It's difficult to overstate the impact Kubernetes has had on the world of DevOps. Over the years since it was open sourced by Google in 2014, Kubernetes has experienced a meteoric rise in popularity. In that period, Kubernetes has become the preeminent solution for orchestrating cloud-native container workloads, differentiating itself from a field of orchestrators such as Apache Mesos and Docker Swarm. By providing a common API over heterogeneous environments, Kubernetes has become the common tool for deploying applications across cloud and hybrid environments.</p>
			<p>So, what is Kubernetes? According to its documentation, <em class="italic">"Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation"</em> (<a href="https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/">https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/</a>). That is a lot to unpack. I'll sum up that statement a little differently. Kubernetes is a set of APIs and abstractions that makes running containerize applications easier. It provides services such as service discovery, load balancing, storage abstraction and orchestration, automated rollouts and rollbacks, self-healing, and secret, certificate, and configuration management. Furthermore, if Kubernetes doesn't offer a specific bit of functionality you need directly, there is likely a solution available in the vibrant open source ecosystem built around the core of Kubernetes. The Kubernetes ecosystem is a vast set of tools for you to achieve your operational objectives without needing to reinvent the wheel. </p>
			<p>All of the aforementioned functionality is exposed through the Kubernetes API and is infinitely programmable.</p>
			<p>This chapter will not be a deep dive into all aspects of Kubernetes. To properly explore Kubernetes in depth would require multiple books. The good news is there are many great books on the topic: <a href="https://www.packtpub.com/catalogsearch/result?q=kubernetes">https://www.packtpub.com/catalogsearch/result?q=kubernetes</a>. Also, the fantastic community-driven documentation (<a href="https://kubernetes.io/docs/home/">https://kubernetes.io/docs/home/</a>) for Kubernetes is an invaluable resource for getting a deeper understanding of it.</p>
			<p>The goal of this chapter is to provide a starting point for your journey in programming Kubernetes using Go. We will start by creating a simple Go program to deploy a Kubernetes resource to a local Kubernetes cluster to run a load-balanced HTTP service. We will then learn how to extend the Kubernetes API with custom resources to show how Kubernetes can be used to orchestrate and manage any external resource. We will build custom pet resources that will be stored in our pet store service running within the cluster to illustrate the concept of managing external resources. By the end of this chapter, you will be equipped with the knowledge to work effectively with the Kubernetes API and understand some of the core design principles of Kubernetes.</p>
			<p>We will cover the following topics in this chapter:</p>
			<ul>
				<li>Interacting with the Kubernetes API</li>
				<li>Deploying a load-balanced HTTP application using Go</li>
				<li>Extending Kubernetes with custom resources and operators</li>
				<li>Building a pet store operator</li>
			</ul>
			<h1 id="_idParaDest-302"><a id="_idTextAnchor671"/>Technical requirements</h1>
			<p>This chapter will require the following tools:</p>
			<ul>
				<li>Docker: <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a></li>
				<li>KinD: <a href="https://kind.sigs.k8s.io/#installation-and-usage">https://kind.sigs.k8s.io/#installation-and-usage</a></li>
				<li>operator-sdk: <a href="https://sdk.operatorframework.io/docs/installation/">https://sdk.operatorframework.io/docs/installation/</a></li>
				<li>Tilt.dev: <a href="https://docs.tilt.dev/install.html">https://docs.tilt.dev/install.html</a></li>
				<li>ctlptl: <a href="https://github.com/tilt-dev/ctlptl#how-do-i-install-it">https://github.com/tilt-dev/ctlptl#how-do-i-install-it</a></li>
			</ul>
			<p><a id="_idTextAnchor672"/>The code files for this chapter can be downloaded from <a href="https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/14">https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/14</a></p>
			<h1 id="_idParaDest-303"><a id="_idTextAnchor673"/>Interacting with the Kubernetes API</h1>
			<p>In the introduction, we talked about the Kubernetes API as if it is just one thing, although <a id="_idIndexMarker1349"/>in a sense it can be thought of in that way. However, the Kubernetes API we have been talking about is an aggregation of multiple APIs served by the core of Kubernetes, the control plane API server. The API server exposes an HTTP API that exposes the aggregated API and allows for the query and manipulation of API objects such as Pods, Deployments, Services, and Namespaces.</p>
			<p>In this section, we will learn how to use KinD to create a local cluster. We will use the local cluster to manipulate a namespace resource using <strong class="source-inline">kubectl</strong>. We will examine the basic structure <a id="_idIndexMarker1350"/>of a Kubernetes resource and see how we can address individual resources by their Group, Version, Kind, Name, and usually, Namespace. Lastly, we'll discuss authentication and the <strong class="source-inline">kubeconfig</strong> file. This section will prepare us for interacting with the Kubernetes API at a lower level using Go<a id="_idTextAnchor674"/>.</p>
			<h2 id="_idParaDest-304"><a id="_idTextAnchor675"/>Creating a KinD cluster</h2>
			<p>Prior to getting started interacting with the Kubernetes API, let's build a local Kubernetes <a id="_idIndexMarker1351"/>cluster using <strong class="bold">KinD</strong>. This is a tool that enables us to <a id="_idIndexMarker1352"/>create a Kubernetes cluster locally using Docker rather than running as services on the host. To create the cluster, run the following:</p>
			<p class="source-code">$ kind create cluster</p>
			<p>The preceding command will create a cluster named <strong class="source-inline">kind</strong>. It will build a Kubernetes control plane and set the current context of <strong class="source-inline">kubectl</strong> to point to the newly created cluster. </p>
			<p>You can list the clusters created by <strong class="source-inline">kind</strong> by running the following:</p>
			<p class="source-code">$ kind get clusters</p>
			<p class="source-code">kind</p>
			<p>You can see from the output of <strong class="source-inline">get clusters</strong> that there is a new cluster named <strong class="source-inline">kind</strong> creat<a id="_idTextAnchor676"/>ed.</p>
			<h2 id="_idParaDest-305"><a id="_idTextAnchor677"/>Using kubectl to interact with the API</h2>
			<p>Kubernetes <a id="_idIndexMarker1353"/>offers a command-line tool for interacting with the API, <strong class="source-inline">kubectl</strong>. There are some nice developer experience features in <strong class="source-inline">kubectl</strong>, but its <a id="_idIndexMarker1354"/>main use is to perform <strong class="bold">Create, Read, Update, Delete</strong> (<strong class="bold">CRUD</strong>) operations targeting the API server. For example, let's look at two ways to create a namespace using <strong class="source-inline">kubectl</strong>:</p>
			<p class="source-code">$ kubectl create namespace petstore</p>
			<p>The preceding command creates a namespace named <strong class="source-inline">petstore</strong>:</p>
			<p class="source-code">$ cat &lt;&lt;EOF | kubectl create -f -</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">kind: Namespace</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: petstore</p>
			<p class="source-code">EOF</p>
			<p>The preceding <a id="_idIndexMarker1355"/>command creates the same namespace with an inline YAML document. Next, let's use <strong class="source-inline">kubectl</strong> to fetch the namespace as YAML:</p>
			<p class="source-code">$ kubectl get namespace petstore -o yaml</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">kind: Namespace</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  creationTimestamp: "2022-03-06T15:55:09Z"</p>
			<p class="source-code">  labels:</p>
			<p class="source-code">    kubernetes.io/metadata.name: petstore</p>
			<p class="source-code">  name: petstore</p>
			<p class="source-code">  resourceVersion: "2162"</p>
			<p class="source-code">  uid: cddb2eb8-9c46-4089-9c99-e31259dfcd1c</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  finalizers:</p>
			<p class="source-code">  - kubernetes</p>
			<p class="source-code">status:</p>
			<p class="source-code">  phase: Active</p>
			<p>The preceding command fetched the <strong class="source-inline">petstore</strong> namespace and output the entire resource <a id="_idIndexMarker1356"/>in the <strong class="source-inline">.yaml</strong> format. Pay special attention to the top-level keys, <strong class="source-inline">apiVersion</strong>, <strong class="source-inline">kind</strong>, <strong class="source-inline">metadata</strong>, <strong class="source-inline">spec</strong>, and <strong class="source-inline">status</strong>. The values and structures in these keys will be common to all resources in Kube<a id="_idTextAnchor678"/>rnetes.</p>
			<h3>The Group Version Kind (GVK) namespace name</h3>
			<p>In the Kubernetes API, you can identify any resource by the combination of its group, kind, version, name, and usually, namespace. I say usually namespace since not all resources <a id="_idIndexMarker1357"/>belong to a namespace. A namespace is an example of a resource that exists outside a namespace (as well as other low-level resources such as Nodes and PersistentVolumes). However, most other resources such as Pods, Services, and Deployments exist within a namespace. For the namespace example from the previous section, the group is omitted, since it is in the Kubernetes core API and is assumed by the API server. Effectively, the identifier for the <strong class="source-inline">petstore</strong> namespace is <strong class="source-inline">apiVersion: v1</strong>, <strong class="source-inline">kind: Namespace</strong>, and <strong class="source-inline">metadata.name: petstore</strong>. </p>
			<p>Internalize the idea of a group, version, kind, namespace, and name. It will be critical to understand how to interact with the Kuberne<a id="_idTextAnchor679"/>tes API.</p>
			<h3>The spec and status sections</h3>
			<p>Each resource in Kubernetes has a <strong class="source-inline">spec</strong> and a <strong class="source-inline">status</strong> section. The <strong class="source-inline">spec</strong> section of the resource <a id="_idIndexMarker1358"/>is a structure that describes the desired state of the resource. It is Kubernetes' job to reconcile the state of the system to achieve <a id="_idIndexMarker1359"/>that desired state. In some cases, <strong class="source-inline">spec</strong> will describe the desired state of an external system. For example, <strong class="source-inline">spec</strong> can be a description of <a id="_idIndexMarker1360"/>a load balancer, including the desired external IP. The reconciler for that resource would be responsible for creating a network interface and setting up routing to ensure that the IP routes to that specific network interface.</p>
			<p>The <strong class="source-inline">status</strong> section <a id="_idIndexMarker1361"/>of the resource is a structure that describes the current state of the resource. It is intended to be mutated by Kubernetes, not the user. For example, <strong class="source-inline">status</strong> for a Deployment includes the number of ready replicas of a given Deployment. <strong class="source-inline">spec</strong> for the Deployment will contain the desired number of replicas. It is Kubernetes' job to drive toward that desired state and update the <strong class="source-inline">status</strong> with the current state of the resource.</p>
			<p>We will learn more about <strong class="source-inline">spec</strong> and <strong class="source-inline">status</strong> as we progress in t<a id="_idTextAnchor680"/>his chapter.</p>
			<h3>Authentication</h3>
			<p>So far, we have <a id="_idIndexMarker1362"/>just assumed access to the Kubernetes cluster, but that was actually handled for us by <strong class="source-inline">kind</strong> and its ability to set the default context for <strong class="source-inline">kubectl</strong>. The default context for <strong class="source-inline">kubectl</strong> is stored in your home directory. You can see what was set by running the following command:</p>
			<p class="source-code">$ cat ~/.kube/config</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">clusters:</p>
			<p class="source-code">- cluster:</p>
			<p class="source-code">    certificate-authority-data:</p>
			<p class="source-code">    server: https://127.0.0.1:55451</p>
			<p class="source-code">  name: kind-kind</p>
			<p class="source-code">contexts:</p>
			<p class="source-code">- context:</p>
			<p class="source-code">    cluster: kind-kind</p>
			<p class="source-code">    user: kind-kind</p>
			<p class="source-code">  name: kind-kind</p>
			<p class="source-code">current-context: kind-kind</p>
			<p class="source-code">kind: Config</p>
			<p class="source-code">preferences: {}</p>
			<p class="source-code">users:</p>
			<p class="source-code">- name: kind-kind</p>
			<p class="source-code">  user:</p>
			<p class="source-code">    client-certificate-data:</p>
			<p class="source-code">    client-key-data:</p>
			<p>In the preceding output, I've omitted the certificate data to provide a more concise view of the config. It contains all the information we need to create a secure connection to the local cluster instance. Note the address of the service and the names of the cluster and the user.</p>
			<p>By running the following command, we can get the <strong class="source-inline">kubeconfig</strong> for the <strong class="source-inline">kind</strong> cluster:</p>
			<p class="source-code">$ kind get kubeconfig --name kind &gt; .tmp-kubeconfig</p>
			<p>If you <strong class="source-inline">cat</strong> the contents of the file, you will see a very similar structure in <strong class="source-inline">~/.kube/config</strong>. The <strong class="source-inline">kubeconfig</strong> file is a convenient way to encapsulate the information needed to authenticate to the API server and is used with many of the tools in the Kubernetes ecosystem. For example, you can override the context of <strong class="source-inline">kubectl</strong> to use a different <strong class="source-inline">kubeconfig</strong> with the following command:</p>
			<p class="source-code">$ KUBECONFIG=./.tmp-kubeconfig kubectl get namespaces</p>
			<p>The preceding <a id="_idIndexMarker1363"/>command will list all the namespaces in the <strong class="source-inline">kind</strong> cluster, but it will use the local <strong class="source-inline">kubeconfig</strong> file we just created.</p>
			<p>There are <a id="_idIndexMarker1364"/>a variety of tools for managing whichever cluster you are using. One great example is <strong class="source-inline">kubectx</strong> (<a href="https://ahmet.im/blog/kubectx/">https://ahmet.im/blog/kubectx/</a>) from Ahmet Alp Balkan, which can be used to work fluently with multiple clusters. As I mentioned previously, the vibrant open source ecosystem provides a wide variety of tools to make your experience using Kubernetes delightful.</p>
			<p>Finally, let's clean up the <strong class="source-inline">petstore</strong> namespace and delete our <strong class="source-inline">kind</strong> cluster:</p>
			<p class="source-code">$ kubectl delete namespace petstore</p>
			<p class="source-code">$ kind delete cluster --name kind</p>
			<p>In this section, we learned the basics of interacting with the Kubernetes API and the basic structure of Kubernetes resources. We are able to create a local Kubernetes experience, and we are ready to approach building an application to interact with Kubernetes using Go.</p>
			<p>In the next section, we are going to leverage what we have learned about the Kubernetes API and use that to build a Go application to deploy a load-balanced HT<a id="_idTextAnchor681"/>TP application.</p>
			<h1 id="_idParaDest-306"><a id="_idTextAnchor682"/>Deploying a load-balanced HTTP application using Go</h1>
			<p>Now that <a id="_idIndexMarker1365"/>we understand a bit more about the Kubernetes API and the resources exposed by the API, we can <a id="_idIndexMarker1366"/>move away from <strong class="source-inline">kubectl</strong> toward using Go.</p>
			<p>In this section, we will use Go to do many of the same things we did in the previous section using <strong class="source-inline">kubectl</strong>. We will authenticate using our default context and create a namespace. However, we will not stop there. We will deploy a load-balanced HTTP application to our cluster and watch the logs stream to STDOUT as we make reques<a id="_idTextAnchor683"/>ts to the service.</p>
			<p>The code for this section can be found at <a href="https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/14/workloads">https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/14/workloads</a>. The demo we are about to walk through can be executed with the following commands:</p>
			<p class="source-code">$ kind create cluster --name workloads --config kind-config.yaml</p>
			<p class="source-code">$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml</p>
			<p class="source-code">$ kubectl wait --namespace ingress-nginx \</p>
			<p class="source-code">  --for=condition=ready pod \</p>
			<p class="source-code">  --selector=app.kubernetes.io/component=controller \</p>
			<p class="source-code">  --timeout=90s</p>
			<p class="source-code">$ go run .</p>
			<p>The preceding command will create a KinD cluster named <strong class="source-inline">workloads</strong> and use a config file that will enable host network  ingress for the cluster. We will use ingress to expose the service running in the cluster on <strong class="source-inline">localhost:port</strong>. The command then deploys the NGINX ingress controller and waits for it to be ready. Finally, we run our Go program to deploy our application. After the service has been deployed and is running, open a browser at <strong class="source-inline">http://localhost:8080/hello</strong>. You should see the following when you browse there:</p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="Images/B17626_14_001.jpg" alt="Figure 14.1 – The deployed NGINX hello world&#13;&#10;" width="1459" height="809"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.1 – The deployed NGINX hello world</p>
			<p>You <a id="_idIndexMarker1367"/>should see the request <a id="_idIndexMarker1368"/>logs stream to STDOUT. They should look like the following:</p>
			<p class="source-code">10.244.0.7 - - [07/Mar/2022:02:34:59 +0000] "GET /hello HTTP/1.1" 200 7252 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.3 Safari/605.1.15" "172.22.0.1"</p>
			<p>If you refresh the page, you should see the server name change, indicating that the requests are load balancing across the two pod replicas in the deployment. Press <em class="italic">Ctrl</em> + <em class="italic">C</em> to terminate the Go program.</p>
			<p>To tear down the cluster, run the following command:</p>
			<p class="source-code">$ kind delete cluster --name workloads</p>
			<p>The preceding command will delete the <strong class="source-inline">kind</strong> cluster named <strong class="source-inline">workloads</strong>. Next, let's explore this Go application to understand w<a id="_idTextAnchor684"/>hat just happened.</p>
			<h2 id="_idParaDest-307"><a id="_idTextAnchor685"/>It all starts with main</h2>
			<p>Let's dive <a id="_idIndexMarker1369"/>right into the code and see what is happening in this Go program:</p>
			<p class="source-code">func main() {</p>
			<p class="source-code">     ctx, cancel := context.WithCancel(context.Background())</p>
			<p class="source-code">     defer cancel()</p>
			<p class="source-code">     clientSet := getClientSet()</p>
			<p class="source-code">     nsFoo := createNamespace(ctx, clientSet, "foo")</p>
			<p class="source-code">     defer func() {</p>
			<p class="source-code">          deleteNamespace(ctx, clientSet, nsFoo)</p>
			<p class="source-code">     }()</p>
			<p class="source-code">     deployNginx(ctx, clientSet, nsFoo, "hello-world")</p>
			<p class="source-code">     fmt.Printf("You can now see your running service: http://localhost:8080/hello\n\n")</p>
			<p class="source-code">     listenToPodLogs(ctx, clientSet, nsFoo, "hello-world")</p>
			<p class="source-code">     // wait for ctrl-c to exit the program</p>
			<p class="source-code">     waitForExitSignal()</p>
			<p class="source-code">}</p>
			<p>In the preceding code, we establish a context derived from the background context. This is largely ineffectual in this scenario but would be a powerful tool in the future if you needed to <a id="_idIndexMarker1370"/>cancel a request that is taking too long. Next, we create <strong class="source-inline">clientSet</strong>, which is a strongly typed client for interacting with the Kubernetes API. We then use <strong class="source-inline">clientSet</strong> in <strong class="source-inline">createNamespace</strong>, <strong class="source-inline">deployNginx</strong>, and <strong class="source-inline">listenToPodLogs</strong>. Finally, we wait for a signal to terminate the program. That's it! </p>
			<p>Next, let's delve into each function, startin<a id="_idTextAnchor686"/>g with <strong class="source-inline">getClientSet</strong>.</p>
			<h2 id="_idParaDest-308"><a id="_idTextAnchor687"/>Creating a ClientSet</h2>
			<p>Let's <a id="_idIndexMarker1371"/>take a look at <strong class="source-inline">getClientSet</strong>:</p>
			<p class="source-code">func getClientSet() *kubernetes.Clientset {</p>
			<p class="source-code">	var kubeconfig *string</p>
			<p class="source-code">	if home := homedir.HomeDir(); home != "" {</p>
			<p class="source-code">		kubeconfig = flag.String(</p>
			<p class="source-code">			"kubeconfig",</p>
			<p class="source-code">			filepath.Join(home, ".kube", "config"),</p>
			<p class="source-code">			"(optional) absolute path to the kubeconfig file",</p>
			<p class="source-code">		)</p>
			<p class="source-code">	} else {</p>
			<p class="source-code">		kubeconfig = flag.String(</p>
			<p class="source-code">			"kubeconfig",</p>
			<p class="source-code">			"",</p>
			<p class="source-code">			"absolute path to the kubeconfig file",</p>
			<p class="source-code">		)</p>
			<p class="source-code">	}</p>
			<p class="source-code">	flag.Parse()</p>
			<p class="source-code">	// use the current context in kubeconfig</p>
			<p class="source-code">	config, err := clientcmd.BuildConfigFromFlags(</p>
			<p class="source-code">		"",</p>
			<p class="source-code">		*kubeconfig,</p>
			<p class="source-code">	)</p>
			<p class="source-code">	panicIfError(err)</p>
			<p class="source-code">	</p>
			<p class="source-code">	// create the clientSet</p>
			<p class="source-code">	cs, err := kubernetes.NewForConfig(config)</p>
			<p class="source-code">	panicIfError(err)</p>
			<p class="source-code">	return cs</p>
			<p class="source-code">}</p>
			<p>In the <a id="_idIndexMarker1372"/>preceding code, you can see that we build flag bindings to either use the existing <strong class="source-inline">~/.kube/config</strong> context or accept a <strong class="source-inline">kubeconfig</strong> file via an absolute file path. We then build a config using this flag or default. The config is then used to create <strong class="source-inline">*kubernetes.ClientSet</strong>. As we learned in the <strong class="source-inline">kubectl</strong> section, <strong class="source-inline">kubeconfig</strong> contains all the information we need to connect and authenticate to the server. We now have a client ready to interact with the Kubernetes cluster.</p>
			<p>Next, let's see th<a id="_idTextAnchor688"/>e <strong class="source-inline">ClientSet</strong> in action.</p>
			<h2 id="_idParaDest-309"><a id="_idTextAnchor689"/>Creating a namespace</h2>
			<p>Now that <a id="_idIndexMarker1373"/>we have a <strong class="source-inline">ClientSet</strong>, we can use it to create the resource we need to deploy our load-balanced HTTP application. Let's take a look at <strong class="source-inline">createNamespace</strong>:</p>
			<p class="source-code">func createNamespace(</p>
			<p class="source-code">	ctx context.Context,</p>
			<p class="source-code">	clientSet *kubernetes.Clientset,</p>
			<p class="source-code">	name string,</p>
			<p class="source-code">) *corev1.Namespace {</p>
			<p class="source-code">	fmt.Printf("Creating namespace %q.\n\n", name)</p>
			<p class="source-code">	ns := &amp;corev1.Namespace{</p>
			<p class="source-code">		ObjectMeta: metav1.ObjectMeta{</p>
			<p class="source-code">			Name: name,</p>
			<p class="source-code">		},</p>
			<p class="source-code">	}</p>
			<p class="source-code">	ns, err := clientSet.CoreV1().</p>
			<p class="source-code">		Namespaces().</p>
			<p class="source-code">		Create(ctx, ns, metav1.CreateOptions{})</p>
			<p class="source-code">	panicIfError(err)</p>
			<p class="source-code">	return ns</p>
			<p class="source-code">}</p>
			<p>In the preceding code, we build a <strong class="source-inline">corev1.Namespace</strong> structure, supplying the name in the <strong class="source-inline">ObjectMeta</strong> field. If you recall from our YAML example that created a namespace using <strong class="source-inline">kubectl</strong>, this field maps to <strong class="source-inline">metadata.name</strong>. The Go structures of the Kubernetes <a id="_idIndexMarker1374"/>resource map closely to their YAML representations. Finally, we use <strong class="source-inline">clientSet</strong> to create the namespace via the Kubernetes API server and return the namespace. The <strong class="source-inline">metav1.CreateOptions</strong> contains some options for changing the behavior of the <strong class="source-inline">create</strong> operation, but we will not explore this structure in this book.</p>
			<p>We have now created the namespace where we will deploy our application. Let's see how we will<a id="_idTextAnchor690"/> deploy the application.</p>
			<h2 id="_idParaDest-310"><a id="_idTextAnchor691"/>Deploying the application into the namespace</h2>
			<p>Now that <a id="_idIndexMarker1375"/>we have <strong class="source-inline">clientSet</strong> and namespace created, we are ready to deploy the resources that will represent our application. Let's have a look at the <strong class="source-inline">deployNginx</strong> func:</p>
			<p class="source-code">func deployNginx(</p>
			<p class="source-code">	ctx context.Context,</p>
			<p class="source-code">	clientSet *kubernetes.Clientset,</p>
			<p class="source-code">	ns *corev1.Namespace,</p>
			<p class="source-code">	name string,</p>
			<p class="source-code">) {</p>
			<p class="source-code">	deployment := createNginxDeployment(</p>
			<p class="source-code">		ctx,</p>
			<p class="source-code">		clientSet,</p>
			<p class="source-code">		ns,</p>
			<p class="source-code">		name,</p>
			<p class="source-code">	)</p>
			<p class="source-code">	waitForReadyReplicas(ctx, clientSet, deployment)</p>
			<p class="source-code">	createNginxService(ctx, clientSet, ns, name)</p>
			<p class="source-code">	createNginxIngress(ctx, clientSet, ns, name)</p>
			<p class="source-code">}</p>
			<p>In the preceding code, we create the NGINX deployment resource and wait for the replicas of the deployment to be ready. After the deployment is ready, the code creates the service resource to load-balance across the pods in the deployment. Finally, we create the ingress resource to expose the service on a local host port.</p>
			<p>Next, let's review each of these functions to unders<a id="_idTextAnchor692"/>tand what they are doing.</p>
			<h2 id="_idParaDest-311"><a id="_idTextAnchor693"/>Creating the NGINX deployment</h2>
			<p>The first <a id="_idIndexMarker1376"/>function in deploying our application is <strong class="source-inline">createNginxDeployment</strong>:</p>
			<p class="source-code">func createNginxDeployment(</p>
			<p class="source-code">	ctx context.Context,</p>
			<p class="source-code">	clientSet *kubernetes.Clientset,</p>
			<p class="source-code">	ns *corev1.Namespace,</p>
			<p class="source-code">	name string,</p>
			<p class="source-code">) *appv1.Deployment {</p>
			<p class="source-code">	var (</p>
			<p class="source-code">		matchLabel = map[string]string{"app": "nginx"}</p>
			<p class="source-code">		objMeta    = metav1.ObjectMeta{</p>
			<p class="source-code">			Name:      name,</p>
			<p class="source-code">			Namespace: ns.Name,</p>
			<p class="source-code">			Labels:    matchLabel,</p>
			<p class="source-code">		}</p>
			<p class="source-code">            [...]</p>
			<p class="source-code">	)</p>
			<p class="source-code">	deployment := &amp;appv1.Deployment{</p>
			<p class="source-code">		ObjectMeta: objMeta,</p>
			<p class="source-code">		Spec: appv1.DeploymentSpec{</p>
			<p class="source-code">			Replicas: to.Int32Ptr(2),</p>
			<p class="source-code">			Selector: &amp;metav1.LabelSelector{</p>
			<p class="source-code">				MatchLabels: matchLabel,</p>
			<p class="source-code">			},</p>
			<p class="source-code">			Template: template,</p>
			<p class="source-code">		},</p>
			<p class="source-code">	}</p>
			<p class="source-code">	deployment, err := clientSet.</p>
			<p class="source-code">		AppsV1().</p>
			<p class="source-code">		Deployments(ns.Name).</p>
			<p class="source-code">		Create(ctx, deployment, metav1.CreateOptions{})</p>
			<p class="source-code">	panicIfError(err)</p>
			<p class="source-code">	return deployment</p>
			<p class="source-code">}</p>
			<p>The preceding <a id="_idIndexMarker1377"/>code initializes <strong class="source-inline">matchLabel</strong> with a key/value pair that will be used to connect the Deployment with the Service. We also initialize <strong class="source-inline">ObjectMeta</strong> for the Deployment resource using the namespace and <strong class="source-inline">matchLabel</strong>. Next, we build a Deployment structure containing a spec with two desired replicas, a <strong class="source-inline">LabelSelector</strong> using the <strong class="source-inline">matchLabel</strong> we built earlier, and a pod template that will run a single container with the <strong class="source-inline">nginxdemos/hello:latest</strong> image exposing port <strong class="source-inline">80</strong> on the container. Finally, we create the <a id="_idIndexMarker1378"/>deployment specifying the namespace and the Deployment structure we've built.</p>
			<p>Now that we have created our Deployment, let's see how we wait for the pods in the<a id="_idTextAnchor694"/> Deployment to become ready.</p>
			<h2 id="_idParaDest-312"><a id="_idTextAnchor695"/>Waiting for ready replicas to match desired replicas</h2>
			<p>When <a id="_idIndexMarker1379"/>a Deployment is created, pods for each replica need to be created and start running before they will be able to service requests. There is nothing about Kubernetes or the API requests we are authoring that requires us to wait for these pods. This is here just to provide some user feedback and illustrate a use for the status portion of the resource. Let's take a look at how we wait for the Deployment state to match the desired state:</p>
			<p class="source-code">func waitForReadyReplicas(</p>
			<p class="source-code">	ctx context.Context,</p>
			<p class="source-code">	clientSet *kubernetes.Clientset,</p>
			<p class="source-code">	deployment *appv1.Deployment,</p>
			<p class="source-code">) {</p>
			<p class="source-code">	fmt.Printf("Waiting for ready replicas in: %q\n", deployment.Name)</p>
			<p class="source-code">	for {</p>
			<p class="source-code">		expectedReplicas := *deployment.Spec.Replicas</p>
			<p class="source-code">		readyReplicas := getReadyReplicasForDeployment(</p>
			<p class="source-code">			ctx,</p>
			<p class="source-code">			clientSet,</p>
			<p class="source-code">			deployment,</p>
			<p class="source-code">		)</p>
			<p class="source-code">		if readyReplicas == expectedReplicas {</p>
			<p class="source-code">			fmt.Printf("replicas are ready!\n\n")</p>
			<p class="source-code">			return</p>
			<p class="source-code">		}</p>
			<p class="source-code">		fmt.Printf("replicas are not ready yet. %d/%d\n",</p>
			<p class="source-code">			readyReplicas, expectedReplicas)</p>
			<p class="source-code">		time.Sleep(1 * time.Second)</p>
			<p class="source-code">	}</p>
			<p class="source-code">}</p>
			<p class="source-code">func getReadyReplicasForDeployment(</p>
			<p class="source-code">	ctx context.Context,</p>
			<p class="source-code">	clientSet *kubernetes.Clientset,</p>
			<p class="source-code">	deployment *appv1.Deployment,</p>
			<p class="source-code">) int32 {</p>
			<p class="source-code">	dep, err := clientSet.</p>
			<p class="source-code">		AppsV1().</p>
			<p class="source-code">		Deployments(deployment.Namespace).</p>
			<p class="source-code">		Get(ctx, deployment.Name, metav1.GetOptions{})</p>
			<p class="source-code">	panicIfError(err)</p>
			<p class="source-code">	return dep.Status.ReadyReplicas</p>
			<p class="source-code">}</p>
			<p>In the <a id="_idIndexMarker1380"/>preceding code, we loop to check for the desired number of replicas to match the number of ready replicas and return if they do. If they do not match, then we sleep for a second and try again. This code is not very resilient, but it illustrates the goal-seeking nature of Kubernetes operations.</p>
			<p>Now that we have a running deployment, we can build the Service to load-balance acro<a id="_idTextAnchor696"/>ss the pods in the deployment.</p>
			<h2 id="_idParaDest-313"><a id="_idTextAnchor697"/>Creating a Service to load-balance</h2>
			<p>The two pod replicas in the deployment are now running the NGINX demo on port <strong class="source-inline">80</strong>, but each <a id="_idIndexMarker1381"/>has its own interface. We can address traffic to each one individually, but it would be more convenient to address a single address and load-balance the requests. Let's create a Service resource to do that:</p>
			<p class="source-code">func createNginxService(</p>
			<p class="source-code">	ctx context.Context,</p>
			<p class="source-code">	clientSet *kubernetes.Clientset,</p>
			<p class="source-code">	ns *corev1.Namespace,</p>
			<p class="source-code">	name string,</p>
			<p class="source-code">) {</p>
			<p class="source-code">	var (</p>
			<p class="source-code">		matchLabel = map[string]string{"app": "nginx"}</p>
			<p class="source-code">		objMeta    = metav1.ObjectMeta{</p>
			<p class="source-code">			Name:      name,</p>
			<p class="source-code">			Namespace: ns.Name,</p>
			<p class="source-code">			Labels:    matchLabel,</p>
			<p class="source-code">		}</p>
			<p class="source-code">	)</p>
			<p class="source-code">	service := &amp;corev1.Service{</p>
			<p class="source-code">		ObjectMeta: objMeta,</p>
			<p class="source-code">		Spec: corev1.ServiceSpec{</p>
			<p class="source-code">			Selector: matchLabel,</p>
			<p class="source-code">			Ports: []corev1.ServicePort{</p>
			<p class="source-code">				{</p>
			<p class="source-code">					Port:     80,</p>
			<p class="source-code">					Protocol: corev1.ProtocolTCP,</p>
			<p class="source-code">					Name:     "http",</p>
			<p class="source-code">				},</p>
			<p class="source-code">			},</p>
			<p class="source-code">		},</p>
			<p class="source-code">	}</p>
			<p class="source-code">	service, err := clientSet.</p>
			<p class="source-code">		CoreV1().</p>
			<p class="source-code">		Services(ns.Name).</p>
			<p class="source-code">		Create(ctx, service, metav1.CreateOptions{})</p>
			<p class="source-code">	panicIfError(err)</p>
			<p class="source-code">}</p>
			<p>In the preceding code, we initialize the same <strong class="source-inline">matchLabel</strong> and <strong class="source-inline">ObjectMeta</strong> as we did in the deployment. However, instead of creating a Deployment resource, we create a Service resource structure, specifying the Selector to match on and the port to expose over <strong class="bold">Transmission Control Protocol</strong> (<strong class="bold">TCP</strong>). The Selector label is the key to ensuring that <a id="_idIndexMarker1382"/>the correct pods are in the backend <a id="_idIndexMarker1383"/>pool for the load balancer. Finally, we create the Service as we have with the other Kubernetes resources.</p>
			<p>We only have one step left. We need to expose our service via an ingress so that we can send traffic into the cluster<a id="_idTextAnchor698"/> via a port on the local machine.</p>
			<h2 id="_idParaDest-314"><a id="_idTextAnchor699"/>Creating an ingress to expose our application on a local host port</h2>
			<p>At this point, we are unable to reach our service via <strong class="source-inline">localhost:port</strong>. We can forward <a id="_idIndexMarker1384"/>traffic into the cluster via <strong class="source-inline">kubectl</strong>, but I'll leave that for you to explore. We are going to create an ingress and open a port on our local host network. Let's see how we create the ingress resource:</p>
			<p class="source-code">func createNginxIngress(</p>
			<p class="source-code">	ctx context.Context,</p>
			<p class="source-code">	clientSet *kubernetes.Clientset,</p>
			<p class="source-code">	ns *corev1.Namespace,</p>
			<p class="source-code">	name string,</p>
			<p class="source-code">) {</p>
			<p class="source-code">	var (</p>
			<p class="source-code">		prefix  = netv1.PathTypePrefix</p>
			<p class="source-code">		objMeta = metav1.ObjectMeta{</p>
			<p class="source-code">			Name:      name,</p>
			<p class="source-code">			Namespace: ns.Name,</p>
			<p class="source-code">		}</p>
			<p class="source-code">		ingressPath = netv1.HTTPIngressPath{</p>
			<p class="source-code">			PathType: &amp;prefix,</p>
			<p class="source-code">			Path:     "/hello",</p>
			<p class="source-code">			Backend: netv1.IngressBackend{</p>
			<p class="source-code">				Service: &amp;netv1.IngressServiceBackend{</p>
			<p class="source-code">					Name: name,</p>
			<p class="source-code">					Port: netv1.ServiceBackendPort{</p>
			<p class="source-code">						Name: "http",</p>
			<p class="source-code">					},</p>
			<p class="source-code">				},</p>
			<p class="source-code">			},</p>
			<p class="source-code">		}</p>
			<p class="source-code">	ingress := &amp;netv1.Ingress{</p>
			<p class="source-code">		ObjectMeta: objMeta,</p>
			<p class="source-code">		Spec: netv1.IngressSpec{</p>
			<p class="source-code">			Rules: rules,</p>
			<p class="source-code">		},</p>
			<p class="source-code">	}</p>
			<p class="source-code">	ingress, err := clientSet.</p>
			<p class="source-code">		NetworkingV1().</p>
			<p class="source-code">		Ingresses(ns.Name).</p>
			<p class="source-code">		Create(ctx, ingress, metav1.CreateOptions{})</p>
			<p class="source-code">	panicIfError(err)</p>
			<p class="source-code">}</p>
			<p>In the preceding code, we initialize a prefix, the same <strong class="source-inline">objMeta</strong> as previously, and <strong class="source-inline">ingressPath</strong>, which will map the path prefix of <strong class="source-inline">/hello</strong> to the service name and port name we created. Yes, Kubernetes does the magic of tying the networking together for us! Next, we <a id="_idIndexMarker1385"/>build the Ingress structure as we saw with the previous structures and create the ingress using <strong class="source-inline">clientSet</strong>. With this last bit, we deploy our entire application stack using Go and the Kubernetes API.</p>
			<p>Next, let's return to <strong class="source-inline">main.go</strong> and look at how we can use Kubernetes to stream the logs of the pods to show the incoming HTTP req<a id="_idTextAnchor700"/>uests while the program is running.</p>
			<h2 id="_idParaDest-315"><a id="_idTextAnchor701"/>Streaming pod logs for the NGINX application</h2>
			<p>The Kubernetes API exposes a bunch of great features for running workloads. One of the most <a id="_idIndexMarker1386"/>basic and useful is the ability to access logs for running pods. Let's see how we can stream logs from multiple running pods to STDOUT:</p>
			<p class="source-code">func listenToPodLogs(</p>
			<p class="source-code">	ctx context.Context,</p>
			<p class="source-code">	clientSet *kubernetes.Clientset,</p>
			<p class="source-code">	ns *corev1.Namespace,</p>
			<p class="source-code">	containerName string,</p>
			<p class="source-code">) {</p>
			<p class="source-code">	// list all the pods in namespace foo</p>
			<p class="source-code">	podList := listPods(ctx, clientSet, ns)</p>
			<p class="source-code">	for _, pod := range podList.Items {</p>
			<p class="source-code">		podName := pod.Name</p>
			<p class="source-code">		go func() {</p>
			<p class="source-code">			opts := &amp;corev1.PodLogOptions{</p>
			<p class="source-code">				Container: containerName,</p>
			<p class="source-code">				Follow:    true,</p>
			<p class="source-code">			}</p>
			<p class="source-code">			podLogs, err := clientSet.</p>
			<p class="source-code">				CoreV1().</p>
			<p class="source-code">				Pods(ns.Name).</p>
			<p class="source-code">				GetLogs(podName, opts).</p>
			<p class="source-code">				Stream(ctx)</p>
			<p class="source-code">			panicIfError(err)</p>
			<p class="source-code">			_, _ = os.Stdout.ReadFrom(podLogs)</p>
			<p class="source-code">		}()</p>
			<p class="source-code">	}</p>
			<p class="source-code">}</p>
			<p class="source-code">func listPods(</p>
			<p class="source-code">	ctx context.Context,</p>
			<p class="source-code">	clientSet *kubernetes.Clientset,</p>
			<p class="source-code">	ns *corev1.Namespace,</p>
			<p class="source-code">) *corev1.PodList {</p>
			<p class="source-code">	podList, err := clientSet.</p>
			<p class="source-code">		CoreV1().</p>
			<p class="source-code">		Pods(ns.Name).</p>
			<p class="source-code">		List(ctx, metav1.ListOptions{})</p>
			<p class="source-code">	panicIfError(err)</p>
			<p class="source-code">	/* omitted some logging for brevity */</p>
			<p class="source-code">	return podList</p>
			<p class="source-code">}</p>
			<p>In the <a id="_idIndexMarker1387"/>preceding code, <strong class="source-inline">listenToPodLogs</strong> lists the pods in the given namespace and then starts <strong class="source-inline">go func</strong> for each one. In <strong class="source-inline">go func</strong>, we use the Kubernetes API to request a stream of <strong class="source-inline">podLogs</strong>, which returns <strong class="source-inline">io.ReadCloser</strong> to deliver logs from the pod as they are created. We then tell STDOUT to read from that pipe, and the logs land in our STDOUT. </p>
			<p>If you thought that getting logs from your running workloads was going to be a lot tougher than this, I don't think you would be alone. Kubernetes is quite complex, but the concept that everything is exposed as an API makes the platform incredibly flexible and programmable.</p>
			<p>We have explored every function except <strong class="source-inline">waitForExitSignal</strong>, which is relatively trivial and doesn't add anything to the Kubernetes story told here. If you'd like to take a look at it, refer to the source repository.</p>
			<p>Having <a id="_idIndexMarker1388"/>explored this example of using the Kubernetes API to programmatically deploy an application using Go, I hope you will take away from the experience a feeling of empowerment to go and learn, build, and feel relatively comfortable interacting with the Kubernetes API. There is so much more to the Kubernetes API, and it's ever-growing. In fact, in the next section, we are going to start talking about how we can extend the Kuberne<a id="_idTextAnchor702"/>tes API with our own custom resources.</p>
			<h1 id="_idParaDest-316"><a id="_idTextAnchor703"/>Extending Kubernetes with custom resources and operators</h1>
			<p>In the <a id="_idIndexMarker1389"/>previous sections, we've learned that the <a id="_idIndexMarker1390"/>Kubernetes API is not just a single API but also an aggregation of APIs backed by cooperative services called <strong class="bold">operators</strong> and <strong class="bold">controllers</strong>. Operators <a id="_idIndexMarker1391"/>are extensions to Kubernetes that make use of custom resources to manage systems and applications via controllers. Controllers are <a id="_idIndexMarker1392"/>components of operators that execute control loops for a kind of resource. A control loop for a custom resource is an iterative process that observes a desired <a id="_idIndexMarker1393"/>state of the resource and works, possibly over several loops, to drive the state of a system to that desired state.</p>
			<p>Those <a id="_idIndexMarker1394"/>previous sentences are rather abstract. I like to sum it up differently. Kubernetes is a platform for automation. An automation is a series of steps and decision trees that drives to reach an end goal. I like to think of operators in a similar way. I think of writing operators as taking a runbook, the human steps for completing an operational activity, and making the computer execute the automation. Operators and controllers are like crystallizing operational knowledge into code to be run in Kubernetes.</p>
			<p>Custom resources can represent anything. They can be things related to Kubernetes resources, or they can be something completely external to Kubernetes. For an example of a custom <a id="_idIndexMarker1395"/>resource related to cluster workloads, in <a href="B17626_09.xhtml#_idTextAnchor461"><em class="italic">Chapter 9</em></a>, <em class="italic">Observability with OpenTelemetry</em>, we discussed <a id="_idIndexMarker1396"/>the OTel collector and deployed it via its <a id="_idIndexMarker1397"/>container image in <strong class="source-inline">docker-compose</strong>, but we <a id="_idIndexMarker1398"/>could have used the Kubernetes operator for OTel to do the same thing in a Kubernetes cluster. The OTel operator exposes a custom resource, like the following:</p>
			<p class="source-code">apiVersion: opentelemetry.io/v1alpha1</p>
			<p class="source-code">kind: OpenTelemetryCollector</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: simplest</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  config: |</p>
			<p class="source-code">    receivers:</p>
			<p class="source-code">      otlp:</p>
			<p class="source-code">        protocols:</p>
			<p class="source-code">          grpc:</p>
			<p class="source-code">          http:</p>
			<p class="source-code">    processors:</p>
			<p class="source-code">    exporters:</p>
			<p class="source-code">      logging:</p>
			<p class="source-code">    service:</p>
			<p class="source-code">      pipelines:</p>
			<p class="source-code">        traces:</p>
			<p class="source-code">          receivers: [otlp]</p>
			<p class="source-code">          processors: []</p>
			<p class="source-code">          exporters: [logging]</p>
			<p>In the preceding code block, we see a custom resource describing the OTel collector from <a href="https://github.com/open-telemetry/opentelemetry-operator">https://github.com/open-telemetry/opentelemetry-operator</a>. This custom resource describes in a domain-specific language how the OpenTelemetry operator should configure and run an OpenTelemetry collector. However, a custom resource can as easily <a id="_idIndexMarker1399"/>be a custom <strong class="source-inline">Pet</strong> resource that represents a pet in a pet store, as we will see in the next section.</p>
			<p>Do you <a id="_idIndexMarker1400"/>remember how to identify the group, version, kind, namespace, and name for the preceding resource? The answer is <strong class="source-inline">group: opentelemetry.io</strong>, <strong class="source-inline">version: v1alpha1</strong>, <strong class="source-inline">kind: OpenTelemetryCollector</strong>, <strong class="source-inline">namespace: default</strong>, and <strong class="source-inline">name: simplest</strong>.</p>
			<p>In this <a id="_idIndexMarker1401"/>section, I want to impress upon you that <a id="_idIndexMarker1402"/>if someone were to strip away pods, nodes, storage, networks, and much of the rest of the Kubernetes container workload scheduling and all that was left was the Kubernetes API server, it would still be an incredibly useful piece of software. In this section, we are going to cover a bit of background about operators, <strong class="bold">custom resource definitions</strong> (<strong class="bold">CRDs</strong>), controllers, and powerful features of the Kubernetes API server. We will not be able to cover all of it in depth, but this survey will help to implement our first operator and hopefully encourage y<a id="_idTextAnchor704"/>ou to learn more about extending the Kubernetes API.</p>
			<h2 id="_idParaDest-317"><a id="_idTextAnchor705"/>Custom Resource Definitions</h2>
			<p>CRDs are resources that can be applied to a Kubernetes <a id="_idIndexMarker1403"/>cluster to create a new RESTful resource <a id="_idIndexMarker1404"/>path for a custom resource. Let's take a look at the example of a CronJob from the Kubernetes docs: <a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#create-a-customresourcedefinition">https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#create-a-customresourcedefinition</a>.</p>
			<p class="source-code">apiVersion: apiextensions.k8s.io/v1</p>
			<p class="source-code">kind: CustomResourceDefinition</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  # name must be in the form: &lt;plural&gt;.&lt;group&gt;</p>
			<p class="source-code">  name: crontabs.stable.example.com</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  # group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt;</p>
			<p class="source-code">  group: stable.example.com</p>
			<p class="source-code">  # list of versions supported by this CustomResourceDefinition</p>
			<p class="source-code">  versions:</p>
			<p class="source-code">    - name: v1</p>
			<p class="source-code">      # Each version can be enabled/disabled by Served flag.</p>
			<p class="source-code">      served: true</p>
			<p class="source-code">      # only one version must be marked as the storage version.</p>
			<p class="source-code">      storage: true</p>
			<p class="source-code">      schema:</p>
			<p class="source-code">        openAPIV3Schema:</p>
			<p class="source-code">          type: object</p>
			<p class="source-code">          properties:</p>
			<p class="source-code">            spec:</p>
			<p class="source-code">              type: object</p>
			<p class="source-code">              properties:</p>
			<p class="source-code">                cronSpec:</p>
			<p class="source-code">                  type: string</p>
			<p class="source-code">                image:</p>
			<p class="source-code">                  type: string</p>
			<p class="source-code">                replicas:</p>
			<p class="source-code">                  type: integer</p>
			<p class="source-code">  # either Namespaced or Cluster</p>
			<p class="source-code">  scope: Namespaced</p>
			<p class="source-code">  names:</p>
			<p class="source-code">    plural: crontabs</p>
			<p class="source-code">    singular: crontab</p>
			<p class="source-code">    kind: CronTab</p>
			<p class="source-code">    shortNames:</p>
			<p class="source-code">    - ct</p>
			<p>As you can <a id="_idIndexMarker1405"/>see from the preceding YAML, a CRD is specified as any other resource in Kubernetes. The CRD resource has <strong class="source-inline">group</strong>, <strong class="source-inline">version</strong>, <strong class="source-inline">kind</strong>, and <strong class="source-inline">name</strong>, but within the <strong class="source-inline">spec</strong>, you can see metadata describing a new resource type with a strongly typed schema, using OpenAPI V3 to describe the schema. Also, note that the spec contains the group, version, and kind of the custom resource. As implied by the YAML structure, there can be multiple versions of the custom resource served at any given time, but only one version can be marked as the storage version.</p>
			<p>In the next section, we'll discuss how Kubernetes is able<a id="_idTextAnchor706"/> to store only one version but serve multiple versions.</p>
			<h3>Custom resource versioning and conversion</h3>
			<p>As mentioned in the previous section, Kubernetes will store only one version of a resource. A new version of a resource is usually introduced when there is a change to the schema <a id="_idIndexMarker1406"/>of that resource – for example, a new field was added or some other mutation of the schema. In this case, Kubernetes would need some way  <a id="_idIndexMarker1407"/>to translate between resource versions. The Kubernetes approach to this is to use conversion Webhooks. That means that you can register a Webhook to convert from the storage version of a resource to the requested version. This forms a hub and spoke model for versioning where the hub is the storage version and the spokes are the other supported versions. You can see an example of this in the Kubernetes docs here: <a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#configure-customresourcedefinition-to-use-conversion-webhooks">https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#configure-customresourcedefinition-to-use-conversion-webhooks</a>.</p>
			<p>Take that in for a moment. This is a powerful feature for any API platform to offer. Having a standardized way of translating one API version to another allows for a more grace<a id="_idTextAnchor707"/>ful adoption of components in a microservice environment.</p>
			<h3>Structured schema, validation, and defaulting</h3>
			<p>As we saw in the previous example of the CronJob CRD spec, we are able to use OpenAPI <a id="_idIndexMarker1408"/>to describe a strongly typed schema for resources. This is highly beneficial for generating API clients for programming languages that may need to interact with the API. Furthermore, we have the <a id="_idIndexMarker1409"/>ability to describe a variety of validations to ensure different aspects of structure and values for resources. For example, we are able to describe what fields are required, valid ranges of values, valid patterns <a id="_idIndexMarker1410"/>of strings, and many other aspects of the structures and values. Additionally, we can provide default values for fields and specify them in the schema.</p>
			<p>Beyond just the schema, the API server exposes validating and mutating Webhooks that can fill the void where the schema fails – for example, if you want to validate or mutate a resource based on some logic that is beyond the scope of schema. These Webhooks can be employed to make the developer experience when using your customer resources much better than accepting a possibly invalid resource, or defaulting some difficult-to-<a id="_idTextAnchor708"/>calculate value so that the user doesn't need to provide it.</p>
			<h2 id="_idParaDest-318"><a id="_idTextAnchor709"/>Controllers</h2>
			<p>The heart of reconciliation is a controller, which executes a control loop for a specific resource kind. The controller <a id="_idIndexMarker1411"/>watches a resource kind in the Kubernetes API and observes that there has been a change. The controller receives the new version of the resource, observes the desired state, observes the state of the system it controls, and attempts to make progress toward changing the state of the system into the desired state expressed in the resource. A controller does not act on the difference between the version of a resource but rather on the current desired state. I've noticed there is an initial drive for people who are new to controller development to try to think about acting only on things that have changed between two resource versions, but that is not recommended.</p>
			<p>Usually, a controller has the ability to reconcile many resources concurrently but will never reconcile the same resource concurrently. This simplifies the model for reconciliation quite a bit. </p>
			<p>Furthermore, most controllers will run with only one leader at a time. For example, if there are two instances of your operator running, only one will be a leader at a time. The other will be idl<a id="_idTextAnchor710"/>e, waiting to become the leader if the other process crashes.</p>
			<h2 id="_idParaDest-319"><a id="_idTextAnchor711"/>Standing on the shoulders of giants</h2>
			<p>I'm sure this sounds quite complex, and it truly is. However, we can thankfully rely on some projects <a id="_idIndexMarker1412"/>that have paved the way to make building operators, controllers, and CRDs so much easier. There is a vibrant, growing ecosystem for Kubernetes operators.</p>
			<p>The projects that most come to mind and which we will depend on in the next section are <strong class="source-inline">controller-runtime</strong> (<a href="https://github.com/kubernetes-sigs/controller-runtime">https://github.com/kubernetes-sigs/controller-runtime</a>), <strong class="source-inline">kubebuilder</strong> (<a href="https://github.com/kubernetes-sigs/kubebuilder">https://github.com/kubernetes-sigs/kubebuilder</a>), and <strong class="source-inline">operator-sdk</strong> (<a href="https://github.com/operator-framework/operator-sdk">https://github.com/operator-framework/operator-sdk</a>). <strong class="source-inline">controller-runtime</strong> provides a set of Go libraries that makes it easier to build controllers and is used in both <strong class="source-inline">kubebuilder</strong> and <strong class="source-inline">operator-sdk</strong>. <strong class="source-inline">kubebuilder</strong> is a framework for building Kubernetes APIs and offers a set of tools that makes it easy to generate API structure, controllers, and related manifests for Kubernetes APIs. <strong class="source-inline">operator-sdk</strong> is a component in the Operator Framework (<a href="https://github.com/operator-framework">https://github.com/operator-framework</a>), which extends from <strong class="source-inline">kubebuilder</strong> and attempts to solve life cycle, publication, and other higher-level problems faced by operator developers.</p>
			<p>If you are interested in a highly ambitious project that extends the Kubernetes API to create declarative <a id="_idIndexMarker1413"/>cluster infrastructure and enables Kubernetes to build new Kubernetes clusters, I encourage you to check out the Cluster API (<a href="https://github.com/kubernetes-sigs/cluster-api">https://github.com/kubernetes-sigs/cluster-api</a>).</p>
			<p>I hope this section has left you in awe of how powerful the Kubernetes API is and spurred you on to want to learn more. I believe we have covered enough of the basics of extending the Kubernetes API that we can approach building our own reconciler without too much trouble. In the upcoming section, we will use <strong class="source-inline">operator-sdk</strong> to build a <strong class="source-inline">Pet</strong> <a id="_idTextAnchor712"/>resource and operator to reconcile pets in a pet store service.</p>
			<h1 id="_idParaDest-320"><a id="_idTextAnchor713"/>Building a pet store operator</h1>
			<p>In this section, we will build on the background information we learned in the previous section about <a id="_idIndexMarker1414"/>CRDs, operators, and controllers to implement our own operator. This operator will have only one CRD, <strong class="source-inline">Pet</strong>, and only one controller to reconcile those <strong class="source-inline">Pet</strong> resources. The desired state of <strong class="source-inline">Pet</strong> will be reconciled to our pet store service, which we used in previous chapters. </p>
			<p>As we discussed in the previous section, this will be an example of using Kubernetes control loops to reconcile the state of a resource that has no dependency on other resources within Kubernetes. Remember, you can model anything in CRDs and use Kubernetes as a tool for building robust APIs for any type of resource.</p>
			<p>In this section, you will learn to build an operator from scratch. You will define a new CRD and controller. You will examine the build tools and the different code generation tools used to eliminate the majority of boilerplate code. You will deploy your controller and the pet store service to a local <strong class="source-inline">kind</strong> cluster and learn how to use <strong class="source-inline">Tilt.dev</strong> for faster inner-loop development cycles. The code for this repository is located at <a href="https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/14/petstore-operator">https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/14/petstore-operator</a>.</p>
			<h2 id="_idParaDest-321"><a id="_idTextAnchor714"/>Initializing the new operator</h2>
			<p>In this <a id="_idIndexMarker1415"/>section, we will initialize the new operator using the <strong class="source-inline">operator-sdk</strong> command-line tool. This will be used to scaffold out a project structure for our operator:</p>
			<p class="source-code">$ operator-sdk init --domain example.com --repo github.com/Go-for-DevOps/chapter/14/petstore-operator</p>
			<p class="source-code">Writing kustomize manifests for you to edit...</p>
			<p class="source-code">Writing scaffold for you to edit...</p>
			<p class="source-code">Get controller runtime:</p>
			<p class="source-code">$ go get sigs.k8s.io/controller-runtime@v0.11.0</p>
			<p class="source-code">Update dependencies:</p>
			<p class="source-code">$ go mod tidy</p>
			<p class="source-code">Next: define a resource with:</p>
			<p class="source-code">$ operator-sdk create api</p>
			<p>By executing the preceding command, <strong class="source-inline">operator-sdk</strong> will scaffold a new operator project using an example domain, which will form the suffix of the group name for our future CRDs. The <strong class="source-inline">–repo</strong> flag is based on the repo for the book's code, but you would want that to reflect the repo path for your project or omit it and allow it to default. Let's see what is in the repo after scaffolding:</p>
			<p class="source-code">$ ls -al</p>
			<p class="source-code">total 368</p>
			<p class="source-code">-rw-------  1 david  staff    776 Feb 27 10:15 Dockerfile</p>
			<p class="source-code">-rw-------  1 david  staff   9884 Feb 27 10:16 Makefile</p>
			<p class="source-code">-rw-------  1 david  staff    261 Feb 27 10:16 PROJECT</p>
			<p class="source-code">drwx------  8 david  staff    256 Feb 27 10:16 config/</p>
			<p class="source-code">-rw-------  1 david  staff   3258 Feb 27 10:16 go.mod</p>
			<p class="source-code">-rw-r--r--  1 david  staff  94793 Feb 27 10:16 go.sum</p>
			<p class="source-code">drwx------  3 david  staff     96 Feb 27 10:15 hack/</p>
			<p class="source-code">-rw-------  1 david  staff   2791 Feb 27 10:15 main.go</p>
			<p>The preceding listing shows the top-level structure of the project. The Dockerfile contains commands to build the controller image. The Makefile contains a variety of helpful tasks; however, we will not use it much in this walk-through. The <strong class="source-inline">PROJECT</strong> file contains metadata about the operator. The <strong class="source-inline">config</strong> directory contains the manifests needed to describe and deploy the operator and CRDs to Kubernetes. The <strong class="source-inline">hack</strong> directory contains <a id="_idIndexMarker1416"/>a boilerplate license header that will be added to generated files and is a good place to put helpful development or build scripts. The rest of the files are just regular Go application code.</p>
			<p>Now that we have a general idea of what was scaffolded for us, we can move on to generating our <strong class="source-inline">Pet</strong> resources and controller:</p>
			<p class="source-code">$ operator-sdk create api --group petstore --version v1alpha1 --kind Pet --resource --controller</p>
			<p class="source-code">Writing kustomize manifests for you to edit...</p>
			<p class="source-code">Writing scaffold for you to edit...</p>
			<p class="source-code">api/v1alpha1/pet_types.go</p>
			<p class="source-code">controllers/pet_controller.go</p>
			<p class="source-code">Update dependencies:</p>
			<p class="source-code">$ go mod tidy</p>
			<p class="source-code">Running make:</p>
			<p class="source-code">$ make generate</p>
			<p class="source-code">go: creating new go.mod: module tmp</p>
			<p class="source-code"># ... lots of go mod output ...</p>
			<p class="source-code">Next: implement your new API and generate the manifests (e.g. CRDs,CRs) with:</p>
			<p class="source-code">$ make manifests</p>
			<p>By executing the preceding commands, I've instructed <strong class="source-inline">operator-sdk</strong> to create a new API in the <strong class="source-inline">petstore</strong> group with the <strong class="source-inline">v1alpha1</strong> version of the <strong class="source-inline">Pet</strong> kind and generate both the CRD and the controller for the type. Note that the command created <strong class="source-inline">api/v1alpha1/pet_types.go</strong> and <strong class="source-inline">controllers/pet_controller.go</strong>, and then ran <strong class="source-inline">make generate</strong> and <strong class="source-inline">make manifests</strong>. Shortly, we will see that <strong class="source-inline">code</strong> comments in <a id="_idIndexMarker1417"/>both of the Go files cause <strong class="source-inline">make generate</strong> and <strong class="source-inline">make manifests</strong> to generate CRD manifests as well as Kubernetes' <strong class="bold">Role-Based Authorization Controls</strong> (<strong class="bold">RBAC</strong>) for the controller. The RBAC <a id="_idIndexMarker1418"/>entries for the operator will give rights to the controller to perform CRUD operations on the newly generated resource. The CRD manifest will contain the schema for our newly created resource.</p>
			<p>Next, let's take a quick look at the files that have changed:</p>
			<p class="source-code">$ git status</p>
			<p class="source-code">M  PROJECT</p>
			<p class="source-code">A  api/v1alpha1/groupversion_info.go</p>
			<p class="source-code">A  api/v1alpha1/pet_types.go</p>
			<p class="source-code">A  api/v1alpha1/zz_generated.deepcopy.go</p>
			<p class="source-code">A  config/crd/bases/petstore.example.com_pets.yaml</p>
			<p class="source-code">A  config/crd/kustomization.yaml</p>
			<p class="source-code">A  config/crd/kustomizeconfig.yaml</p>
			<p class="source-code">A  config/crd/patches/cainjection_in_pets.yaml</p>
			<p class="source-code">A  config/crd/patches/webhook_in_pets.yaml</p>
			<p class="source-code">A  config/rbac/pet_editor_role.yaml</p>
			<p class="source-code">A  config/rbac/pet_viewer_role.yaml</p>
			<p class="source-code">A  config/samples/kustomization.yaml</p>
			<p class="source-code">A  config/samples/petstore_v1alpha1_pet.yaml</p>
			<p class="source-code">A  controllers/pet_controller.go</p>
			<p class="source-code">A  controllers/suite_test.go</p>
			<p class="source-code">M  go.mod</p>
			<p class="source-code">M  main.go</p>
			<p>As we can see, there are quite a few changes to files. I will not go into depth on each of the changes. The most notable is the generation of <strong class="source-inline">config/crd/bases/petstore.example.com_pets.yaml</strong>, which contains the CRD for our <strong class="source-inline">Pet</strong> resource. In operator projects, it is common to describe the resources in <a id="_idIndexMarker1419"/>the API in the <strong class="source-inline">api/</strong> directory, the Kubernetes manifests under <strong class="source-inline">config/</strong>, and the controllers under <strong class="source-inline">controllers/</strong>.</p>
			<p>Next, let's see what has been generated in <strong class="source-inline">api/v1alpha1/pet_types.go</strong>:</p>
			<p class="source-code">// EDIT THIS FILE!  THIS IS SCAFFOLDING FOR YOU TO OWN!</p>
			<p class="source-code">// NOTE: json tags are required.  Any new fields you add must have json tags for the fields to be serialized.</p>
			<p class="source-code">// PetSpec defines the desired state of Pet</p>
			<p class="source-code">type PetSpec struct {</p>
			<p class="source-code">     // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster</p>
			<p class="source-code">     // Important: Run "make" to regenerate code after modifying this file</p>
			<p class="source-code">     // Foo is an example field of Pet. Edit pet_types.go to remove/update</p>
			<p class="source-code">     Foo string `json:"foo,omitempty"`</p>
			<p class="source-code">}</p>
			<p class="source-code">// PetStatus defines the observed state of Pet</p>
			<p class="source-code">type PetStatus struct {</p>
			<p class="source-code">     // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster</p>
			<p class="source-code">     // Important: Run "make" to regenerate code after modifying this file</p>
			<p class="source-code">}</p>
			<p>The preceding code shows a snippet from the <strong class="source-inline">pet_types.go</strong> file. The <strong class="source-inline">create api</strong> command has generated a <strong class="source-inline">Pet</strong> resource with <strong class="source-inline">spec</strong> and <strong class="source-inline">status</strong>. The <strong class="source-inline">PetSpec</strong> contains one field named <strong class="source-inline">Foo</strong>, which will serialize with the key <strong class="source-inline">foo</strong> and is optional to provide when creating or updating the resource. <strong class="source-inline">status</strong> contains nothing. </p>
			<p>Note the <a id="_idIndexMarker1420"/>comments in the file. They instruct us that this is the place to add new fields to the type and to run <strong class="source-inline">make</strong> after we do to ensure that the CRD manifests are updated in the <strong class="source-inline">config/</strong> directory.</p>
			<p>Now, let's look at the rest of the file:</p>
			<p class="source-code">//+kubebuilder:object:root=true</p>
			<p class="source-code">//+kubebuilder:subresource:status</p>
			<p class="source-code">// Pet is the Schema for the pets API</p>
			<p class="source-code">type Pet struct {</p>
			<p class="source-code">     metav1.TypeMeta   `json:",inline"`</p>
			<p class="source-code">     metav1.ObjectMeta `json:"metadata,omitempty"`</p>
			<p class="source-code">     Spec   PetSpec   `json:"spec,omitempty"`</p>
			<p class="source-code">     Status PetStatus `json:"status,omitempty"`</p>
			<p class="source-code">}</p>
			<p class="source-code">//+kubebuilder:object:root=true</p>
			<p class="source-code">// PetList contains a list of Pet</p>
			<p class="source-code">type PetList struct {</p>
			<p class="source-code">     metav1.TypeMeta `json:",inline"`</p>
			<p class="source-code">     metav1.ListMeta `json:"metadata,omitempty"`</p>
			<p class="source-code">     Items           []Pet `json:"items"`</p>
			<p class="source-code">}</p>
			<p class="source-code">func init() {</p>
			<p class="source-code">     SchemeBuilder.Register(&amp;Pet{}, &amp;PetList{})</p>
			<p class="source-code">}</p>
			<p>Here, we can see the definition of <strong class="source-inline">Pet</strong> and <strong class="source-inline">PetList</strong>, which both get registered in the following <a id="_idIndexMarker1421"/>schema builder. Note the <strong class="source-inline">//+kubebuilder build</strong> comments. These build comments instruct <strong class="source-inline">kubebuilder</strong> on how to generate the CRD manifests.</p>
			<p>Note that <strong class="source-inline">Pet</strong> has the spec and status defined with the <strong class="source-inline">json</strong> tags that we have seen in the other Kubernetes resources we have worked with. <strong class="source-inline">Pet</strong> also includes both <strong class="source-inline">TypeMeta</strong>, which informs Kubernetes of the group version kind information, and <strong class="source-inline">ObjectMeta</strong>, which contains the name, namespace, and other metadata about the resource.</p>
			<p>With these structures, we already have a fully functional custom resource. However, the resource doesn't represent the fields we want to represent our pet resource and will need to be updated to better represent our pet structure.</p>
			<p>Next, let's look at what was generated for <strong class="source-inline">PetReconciler</strong> in <strong class="source-inline">controllers/pet_controller.go</strong>, the controller that will run the control loop for reconciling pets:</p>
			<p class="source-code">type PetReconciler struct {</p>
			<p class="source-code">     client.Client</p>
			<p class="source-code">     Scheme *runtime.Scheme</p>
			<p class="source-code">}</p>
			<p class="source-code">//+kubebuilder:rbac:groups=petstore.example.com,resources=pets,verbs=get;list;watch;create;update;patch;delete</p>
			<p class="source-code">//+kubebuilder:rbac:groups=petstore.example.com,resources=pets/status,verbs=get;update;patch</p>
			<p class="source-code">//+kubebuilder:rbac:groups=petstore.example.com,resources=pets/finalizers,verbs=update</p>
			<p class="source-code">func (r *PetReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {</p>
			<p class="source-code">     _ = log.FromContext(ctx)</p>
			<p class="source-code">     return ctrl.Result{}, nil</p>
			<p class="source-code">}</p>
			<p class="source-code">func (r *PetReconciler) SetupWithManager(mgr ctrl.Manager) error {</p>
			<p class="source-code">     return ctrl.NewControllerManagedBy(mgr).</p>
			<p class="source-code">          For(&amp;petstorev1alpha1.Pet{}).</p>
			<p class="source-code">          Complete(r)</p>
			<p class="source-code">}</p>
			<p>In the <a id="_idIndexMarker1422"/>preceding code, we can see a <strong class="source-inline">PetReconciler</strong> type that embeds a <strong class="source-inline">client.Client</strong>, which is a generic Kubernetes API client, and <strong class="source-inline">*runtime.Scheme</strong>, which contains the known types and the schemas registered. If we continue downward, we can see a collection of <strong class="source-inline">//+kubebuilder:rbac build</strong> comments that instruct the code generator to create RBAC rights for the controller to be able to manipulate the <strong class="source-inline">Pet</strong> resource. Next, we can see the <strong class="source-inline">Reconcile func</strong>, which will be called each time a resource has been changed and needs to be reconciled with the pet store. Finally, we can see the <strong class="source-inline">SetupWithManager</strong> function, which is called from <strong class="source-inline">main.go</strong> to start the controller and inform it and the manager what kind of resource the controller will reconcile.</p>
			<p>We have <a id="_idIndexMarker1423"/>covered the impactful changes from the scaffolding process. We can proceed to implement our <strong class="source-inline">Pet</strong> resource to reflect the domain model we have in the pet store. The <strong class="source-inline">pet</strong> entity in our pet store has three mutable, required properties, <strong class="source-inline">Name</strong>, <strong class="source-inline">Type</strong>, and <strong class="source-inline">Birthday</strong>, and one read-only property, <strong class="source-inline">ID</strong>. We need to add these to our <strong class="source-inline">Pet</strong> resource to expose them to the API:</p>
			<p class="source-code">// PetType is the type of the pet. For example, a dog.</p>
			<p class="source-code">// +kubebuilder:validation:Enum=dog;cat;bird;reptile</p>
			<p class="source-code">type PetType string</p>
			<p class="source-code">const (</p>
			<p class="source-code">    DogPetType     PetType = "dog"</p>
			<p class="source-code">    CatPetType     PetType = "cat"</p>
			<p class="source-code">    BirdPetType    PetType = "bird"</p>
			<p class="source-code">    ReptilePetType PetType = "reptile"</p>
			<p class="source-code">)</p>
			<p class="source-code">// PetSpec defines the desired state of Pet</p>
			<p class="source-code">type PetSpec struct {</p>
			<p class="source-code">     // Name is the name of the pet</p>
			<p class="source-code">     Name string `json:"name"`</p>
			<p class="source-code">     // Type is the type of pet Type PetType `json:"type"`</p>
			<p class="source-code">     // Birthday is the date the pet was born</p>
			<p class="source-code">     Birthday metav1.Time `json:"birthday"`</p>
			<p class="source-code">}</p>
			<p class="source-code">// PetStatus defines the observed state of Pet</p>
			<p class="source-code">type PetStatus struct {</p>
			<p class="source-code">    // ID is the unique ID for the pet</p>
			<p class="source-code">    ID string `json:"id,omitempty"`</p>
			<p class="source-code">}</p>
			<p>The preceding <a id="_idIndexMarker1424"/>are the code changes I've made to <strong class="source-inline">Pet</strong> to reflect the domain model of the pet store service. Note <strong class="source-inline">// +kubebuilder:validation:Enum</strong> preceding the <strong class="source-inline">PetType</strong> type. That indicates to the CRD manifest generator that the schema should add validation to ensure only those strings can be supplied for the <strong class="source-inline">Type</strong> field of <strong class="source-inline">PetSpec</strong>. Also, note that each of the fields in <strong class="source-inline">spec</strong> does not have the <strong class="source-inline">omitempty</strong> JSON tag. That will inform the CRD manifest generator that those fields are required.</p>
			<p>The status of <strong class="source-inline">Pet</strong> has only an <strong class="source-inline">ID</strong> field, which is allowed to be empty. This will store the unique identifier returned from the pet store service.</p>
			<p>Now that we have defined our <strong class="source-inline">Pet</strong>, let's reconcile <strong class="source-inline">pet</strong> with the pet store in the controller loop:</p>
			<p class="source-code">// Reconcile moves the current state of the pet to be the desired state described in the pet.spec.</p>
			<p class="source-code">func (r *PetReconciler) Reconcile(ctx context.Context, req ctrl.Request) (result ctrl.Result, errResult error) {</p>
			<p class="source-code">     logger := log.FromContext(ctx)</p>
			<p class="source-code">     pet := &amp;petstorev1.Pet{}</p>
			<p class="source-code">     if err := r.Get(ctx, req.NamespacedName, pet); err != nil {</p>
			<p class="source-code">          if apierrors.IsNotFound(err) {</p>
			<p class="source-code">               logger.Info("object was not found")</p>
			<p class="source-code">               return reconcile.Result{}, nil</p>
			<p class="source-code">          }</p>
			<p class="source-code">          logger.Error(err, "failed to fetch pet from API server")</p>
			<p class="source-code">          // this will cause this pet resource to be requeued</p>
			<p class="source-code">          return ctrl.Result{}, err</p>
			<p class="source-code">     }</p>
			<p class="source-code">     helper, err := patch.NewHelper(pet, r.Client)</p>
			<p class="source-code">     if err != nil {</p>
			<p class="source-code">          return ctrl.Result{}, errors.Wrap(err, "failed to create patch helper")</p>
			<p class="source-code">     }</p>
			<p class="source-code">     defer func() {</p>
			<p class="source-code">          // patch the resource</p>
			<p class="source-code">          if err := helper.Patch(ctx, pet); err != nil {</p>
			<p class="source-code">               errResult = err</p>
			<p class="source-code">          }</p>
			<p class="source-code">     }()</p>
			<p class="source-code">     if pet.DeletionTimestamp.IsZero() {</p>
			<p class="source-code">          // the pet is not marked for delete</p>
			<p class="source-code">          return r.ReconcileNormal(ctx, pet)</p>
			<p class="source-code">     }</p>
			<p class="source-code">     // pet has been marked for delete</p>
			<p class="source-code">     return r.ReconcileDelete(ctx, pet)</p>
			<p class="source-code">}</p>
			<p>The preceding <a id="_idIndexMarker1425"/>code has been added to reconcile the <strong class="source-inline">pet</strong> resource. When we receive a change from the API server, we are not given much information. We are only provided with <strong class="source-inline">NamespacedName</strong> of the pet. <strong class="source-inline">NamespacedName</strong> contains both the namespace and the name of the pet that has changed. Remember that <strong class="source-inline">PetReconciler</strong> has a <strong class="source-inline">client.Client</strong> embedded on it. It provides us with access to the Kubernetes API server. We use the <strong class="source-inline">Get</strong> method to request the pet we need to reconcile. If the pet is not found, we return an empty reconcile result and a nil error. This informs the controller to wait for another change to occur. If there is an error making the request, we return an empty reconcile result and an error. If the error is not nil, the reconciler will try again and back off exponentially.</p>
			<p>If we are able to fetch the pet, we then create a patch helper, which will allow us to track changes to the <strong class="source-inline">Pet</strong> resource during the reconciliation loop and patch the resource change back to the Kubernetes API server at the end of the reconcile loop. The defer ensures that we patch at the end of the <strong class="source-inline">Reconcile</strong> func.</p>
			<p>If the pet has no deletion timestamp set, then we know that Kubernetes has not marked the resource for deletion, so we call <strong class="source-inline">ReconcileNormal</strong>, where we will attempt to persist the desired state to the pet store. Otherwise, we call <strong class="source-inline">ReconcileDelete</strong> to delete the pet from the pet store.</p>
			<p>Let's next look at <strong class="source-inline">ReconcileNormal</strong> and understand what we do when we have a state change to a non-deleted pet resource:</p>
			<p class="source-code">func (r *PetReconciler) ReconcileNormal(ctx context.Context, pet *petstorev1.Pet) (ctrl.Result, error) {</p>
			<p class="source-code">     controllerutil.AddFinalizer(pet, PetFinalizer)</p>
			<p class="source-code">     psc, err := getPetstoreClient()</p>
			<p class="source-code">     if err != nil {</p>
			<p class="source-code">          return ctrl.Result{}, errors.Wrap(err, "unable to construct petstore client")</p>
			<p class="source-code">     }</p>
			<p class="source-code">     </p>
			<p class="source-code">     psPet, err := findPetInStore(ctx, psc, pet)</p>
			<p class="source-code">     if err != nil {</p>
			<p class="source-code">          return ctrl.Result{}, errors.Wrap(err, "failed trying to find pet in pet store")</p>
			<p class="source-code">     }</p>
			<p class="source-code">     if psPet == nil {</p>
			<p class="source-code">          // no pet was found, create a pet in the store</p>
			<p class="source-code">          err := createPetInStore(ctx, pet, psc)</p>
			<p class="source-code">          return ctrl.Result{}, err</p>
			<p class="source-code">     }</p>
			<p class="source-code">     // pet was found, update the pet in the store</p>
			<p class="source-code">     if err := updatePetInStore(ctx, psc, pet, psPet.Pet); err != nil {</p>
			<p class="source-code">          return ctrl.Result{}, err</p>
			<p class="source-code">     }</p>
			<p class="source-code">     return ctrl.Result{}, nil</p>
			<p class="source-code">}</p>
			<p>In <strong class="source-inline">ReconcileNormal</strong>, we always make sure that <strong class="source-inline">PetFinalizer</strong> has been added to the resource. Finalizers are the way that Kubernetes knows when it can garbage-collect a resource. If a resource still has a finalizer on it, then Kubernetes will not delete the resource. Finalizers are useful in controllers when a resource has some external resource that needs <a id="_idIndexMarker1426"/>to be cleaned up prior to deletion. In this case, we need to remove <strong class="source-inline">Pet</strong> from the pet store prior to the Kubernetes <strong class="source-inline">Pet</strong> resource being deleted. If we didn't, we may have pets in the pet store that don't ever get deleted.</p>
			<p>After we set the finalizer, we build a pet store client. We won't go into more detail here, but suffice it to say that it builds a gRPC client for the pet store service. With the pet store client, we query for the pet in the store. If we can't find the pet, then we create one in the store; otherwise, we update the pet in the store to reflect the desired state specified in the Kubernetes <strong class="source-inline">Pet</strong> resource. </p>
			<p>Let's take a quick look at the <strong class="source-inline">createPetInStore</strong> func:</p>
			<p class="source-code">func createPetInStore(ctx context.Context, pet *petstorev1.Pet, psc *psclient.Client) error {</p>
			<p class="source-code">     pbPet := &amp;pb.Pet{</p>
			<p class="source-code">          Name:     pet.Spec.Name,</p>
			<p class="source-code">          Type:     petTypeToProtoPetType(pet.Spec.Type),</p>
			<p class="source-code">          Birthday: timeToPbDate(pet.Spec.Birthday),</p>
			<p class="source-code">     }</p>
			<p class="source-code">     ids, err := psc.AddPets(ctx, []*pb.Pet{pbPet})</p>
			<p class="source-code">     if err != nil {</p>
			<p class="source-code">          return errors.Wrap(err, "failed to create new pet")</p>
			<p class="source-code">     }</p>
			<p class="source-code">     pet.Status.ID = ids[0]</p>
			<p class="source-code">     return nil</p>
			<p class="source-code">}</p>
			<p>When we create <a id="_idIndexMarker1427"/>the pet in the pet store, we call <strong class="source-inline">AddPets</strong> on the gRPC client with the Kubernetes <strong class="source-inline">Pet</strong> resource desired state and record <strong class="source-inline">ID</strong> in the Kubernetes <strong class="source-inline">Pet</strong> resource status.</p>
			<p>Let's move on to the <strong class="source-inline">updatePetInStore</strong> func:</p>
			<p class="source-code">func updatePetInStore(ctx context.Context, psc *psclient.Client, pet *petstorev1.Pet, pbPet *pb.Pet) error {</p>
			<p class="source-code">     pbPet.Name = pet.Spec.Name</p>
			<p class="source-code">     pbPet.Type = petTypeToProtoPetType(pet.Spec.Type)</p>
			<p class="source-code">     pbPet.Birthday = timeToPbDate(pet.Spec.Birthday)</p>
			<p class="source-code">     if err := psc.UpdatePets(ctx, []*pb.Pet{pbPet}); err != nil {</p>
			<p class="source-code">          return errors.Wrap(err, "failed to update the pet in the store")</p>
			<p class="source-code">     }</p>
			<p class="source-code">     return nil</p>
			<p class="source-code">}</p>
			<p>When we update the pet in store, we use the fetched store pet and update the fields with the desired state from the Kubernetes <strong class="source-inline">Pet</strong> resource.</p>
			<p>If at any point in the flow we run into an error, we bubble up the error to <strong class="source-inline">Reconcile</strong>, where it will trigger a re-queue of the reconciliation loop, backing off exponentially. The actions in <strong class="source-inline">ReconcileNormal</strong> are idempotent. They can run repeatedly to achieve the same state and in the face of errors will retry. Reconciliation loops can be pretty resilient to failures.</p>
			<p>That's about it for <strong class="source-inline">ReconcileNormal</strong>. Let's look at what happens in <strong class="source-inline">ReconcileDelete</strong>:</p>
			<p class="source-code">// ReconcileDelete deletes the pet from the petstore and removes the finalizer.</p>
			<p class="source-code">func (r *PetReconciler) ReconcileDelete(ctx context.Context, pet *petstorev1.Pet) (ctrl.Result, error) {</p>
			<p class="source-code">     psc, err := getPetstoreClient()</p>
			<p class="source-code">     if err != nil {</p>
			<p class="source-code">          return ctrl.Result{}, errors.Wrap(err, "unable to construct petstore client")</p>
			<p class="source-code">     }</p>
			<p class="source-code">     if pet.Status.ID != "" {</p>
			<p class="source-code">          if err := psc.DeletePets(ctx, []string{pet.Status.ID}); err != nil {</p>
			<p class="source-code">               return ctrl.Result{}, errors.Wrap(err, "failed to delete pet")</p>
			<p class="source-code">          }</p>
			<p class="source-code">     }</p>
			<p class="source-code">     // remove finalizer, so K8s can garbage collect the resource.</p>
			<p class="source-code">     controllerutil.RemoveFinalizer(pet, PetFinalizer)</p>
			<p class="source-code">     return ctrl.Result{}, nil</p>
			<p class="source-code">}</p>
			<p>In <strong class="source-inline">ReconcileDelete</strong> in the preceding code block, we get a pet store client to interact with <a id="_idIndexMarker1428"/>the pet store. If <strong class="source-inline">pet.Status.ID</strong> is not empty, we attempt to delete the pet from the pet store. If that operation is successful, we will remove the finalizer, informing Kubernetes that it can then delete the resource.</p>
			<p>You have extended Kubernetes and created your first CRD and controller! Let's give it a run.</p>
			<p>To start the project and see your Kubernetes operator in action, run the following:</p>
			<p class="source-code">$ ctlptl create cluster kind --name kind-petstore --registry=ctlptl-registry</p>
			<p class="source-code">$ tilt up</p>
			<p>The preceding <a id="_idIndexMarker1429"/>commands will create a <strong class="source-inline">kind</strong> cluster and a local <strong class="bold">Open Container Initiative</strong> (<strong class="bold">OCI</strong>) image registry, enabling you to publish images <a id="_idIndexMarker1430"/>locally rather than to an external registry. Tilt will start at the command line. Press the <em class="italic">spacebar</em> to open the web view of <strong class="source-inline">Tilt.dev</strong>. Once you do, you should see something like the following:</p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="Images/B17626_14_002.jpg" alt="Figure 14.2 – Tilt's All Resources web view&#13;&#10;" width="1511" height="1095"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.2 – Tilt's All Resources web view</p>
			<p>Wait for each of the services on the left panel to turn green. Once they are, it means that the pet store operator and Service have deployed successfully. If you click on one of the Services listed on the left, it will show you the log output for that component. <strong class="source-inline">petstore-operator-controller-manager</strong> is your Kubernetes controller. Next, we are going to apply some pets to our Kubernetes cluster and see what happens.</p>
			<p>Let's first look <a id="_idIndexMarker1431"/>at the pet samples we are going to apply. The samples are in <strong class="source-inline">config/samples/petstore_v1alpha1_pet.yaml</strong>:</p>
			<p class="source-code">---</p>
			<p class="source-code">apiVersion: petstore.example.com/v1alpha1</p>
			<p class="source-code">kind: Pet</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: pet-sample1</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  name: Thor</p>
			<p class="source-code">  type: dog</p>
			<p class="source-code">  birthday: 2021-04-01T00:00:00Z</p>
			<p class="source-code">---</p>
			<p class="source-code">apiVersion: petstore.example.com/v1alpha1</p>
			<p class="source-code">kind: Pet</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: pet-sample2</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  name: Tron</p>
			<p class="source-code">  type: cat</p>
			<p class="source-code">  birthday: 2020-06-25T00:00:00Z</p>
			<p>We have two pets, <strong class="source-inline">Thor</strong> and <strong class="source-inline">Tron</strong>. We can apply them with the following command:</p>
			<p class="source-code">$ kubectl apply -f config/samples/petstore_v1alpha1_pet.yaml</p>
			<p>That should have replied that they were created, and you should then be able to fetch them by running the following command:</p>
			<p class="source-code">$ kubectl get pets</p>
			<p class="source-code">NAME          AGE</p>
			<p class="source-code">pet-sample1   2m17s</p>
			<p class="source-code">pet-sample2   2m17s</p>
			<p>We can see <a id="_idIndexMarker1432"/>that we have two pets defined. Let's make sure they have IDs. Run the following command:</p>
			<p class="source-code">$ kubectl get pets -o yaml</p>
			<p class="source-code">apiVersion: petstore.example.com/v1alpha1</p>
			<p class="source-code">kind: Pet</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  finalizers:</p>
			<p class="source-code">  - pet.petstore.example.com</p>
			<p class="source-code">  name: pet-sample2</p>
			<p class="source-code">  namespace: default</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  birthday: "2020-06-25T00:00:00Z"</p>
			<p class="source-code">  name: Tron</p>
			<p class="source-code">  type: cat</p>
			<p class="source-code">status:</p>
			<p class="source-code">  id: 23743da5-34fe-46f6-bed8-1f5bdbaabbe6</p>
			<p>I've omitted some noisy content from the preceding code, but this is roughly what you should see. Tron has an ID generated from the pet store service; it was applied to the Kubernetes <strong class="source-inline">Pet</strong> resource status.</p>
			<p>Now, let's test our reconciliation loop by changing the name of <strong class="source-inline">Thor</strong> to <strong class="source-inline">Thorbert</strong>:</p>
			<p class="source-code">$ kubectl edit pets pet-sample1</p>
			<p>This will open your default editor. You can go and change the value of <strong class="source-inline">Thor</strong> to <strong class="source-inline">Thorbert</strong> to cause a new reconcile loop.</p>
			<p>You should see something similar to this output in your browser, with Tilt in the pet store operator logs:</p>
			<p class="source-code">[manager] 1.6466368389433222e+09     INFO     controller.pet     finding pets in store     {"reconciler group": "petstore.example.com", "reconciler kind": "Pet", "name": "pet-sample1", "namespace": "default", "pet": "Thorbert", "id": "cef9499f-6214-4227-b217-265fd8f196e6"}</p>
			<p>As you can <a id="_idIndexMarker1433"/>see from the preceding code, <strong class="source-inline">Thor</strong> is now changed to <strong class="source-inline">Thorbert</strong>.</p>
			<p>Finally, let's delete these pets by running the following command:</p>
			<p class="source-code">$ kubectl delete pets --all</p>
			<p class="source-code">pet.petstore.example.com "pet-sample1" deleted</p>
			<p class="source-code">pet.petstore.example.com "pet-sample2" deleted</p>
			<p>After deleting the resources, you should be able to check back in Tilt and see the log output reflecting that the <strong class="source-inline">delete</strong> operations succeeded.</p>
			<p>In this section, you learned to build an operator from scratch, extended the Kubernetes API with a custom resource that reconciled state to an external Service, and used some really useful tools along the way.</p>
			<h1 id="_idParaDest-322"><a id="_idTextAnchor715"/>Summary</h1>
			<p>In this chapter, we learned how to use Go to deploy and manipulate resources in Kubernetes. We built upon that knowledge to extend Kubernetes with our custom <strong class="source-inline">Pet</strong> resources and learned how to continuously reconcile the desired state of our pets with the state of the pet store. We learned that we can extend Kubernetes to represent any external resources and that it provides a robust platform to describe nearly any domain.</p>
			<p>You should be able to take what you learned in this chapter and apply it to automate interactions with Kubernetes resources and extend Kubernetes to natively expose your own resources through the Kubernetes API. I bet you can think of some services and resources at your company that you would like to be able to manage by simply applying some YAML to your Kubernetes cluster. You are now empowered with the knowledge to solve those problems.</p>
			<p>In the next chapter, we will learn about using Go to program the cloud. We'll learn how to mutate cloud resources using Go client libraries to interact with cloud service provider APIs, and how to use those cloud services and infrastructure after we've provisioned them.</p>
		</div>
	</div></body></html>