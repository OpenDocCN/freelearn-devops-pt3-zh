<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer069">&#13;
			<h1 id="_idParaDest-107" class="chapter-number"><a id="_idTextAnchor108"/>5</h1>&#13;
			<h1 id="_idParaDest-108"><a id="_idTextAnchor109"/>Deploying Kubernetes Apps Like a True Cloud Native</h1>&#13;
			<p>When engineers start hearing about Kubernetes or want to start implementing it, the typical reason is from a Dev perspective of managing and deploying applications. The whole premise around Kubernetes making engineering teams’ lives easier, regardless of whether it’s Dev or Ops, is based on <span class="No-Break">application deployment.</span></p>&#13;
			<p>Deploying applications is at the forefront of every business’s mind, whether it’s a website, some mobile application, or an internal app in any company, from a software company to an auto-parts company to a beer manufacturer. Regardless of the industry, almost every company deploys some type of application and some type of software. As all engineers know, deploying and maintaining an application successfully isn’t an easy task. Whether you’re running an app on bare metal, on a VM, in the cloud, or even in a container, that app could be (and most likely is) the make or break between a successful business and a <span class="No-Break">bankrupt company.</span></p>&#13;
			<p>Throughout this chapter, you’re going to notice that a lot of topics covered will remind you of how other application deployments work. From the actual deployment to scaling and upgrading, the overall concepts are the same. For example, scaling an application is scaling an application. There’s no magical new methodology with Kubernetes. However, what Kubernetes does give you is the ease of scalability. With that being said, the major thing you’ll notice throughout this chapter is that Kubernetes isn’t reinventing the wheel. It’s making what we’ve been doing for 30+ <span class="No-Break">years easier.</span></p>&#13;
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break">Cloud-native apps</span></li>&#13;
				<li>Controllers, controller deployments, <span class="No-Break">and Pods</span></li>&#13;
				<li>Segregation <span class="No-Break">and namespaces</span></li>&#13;
				<li>Stateless and <span class="No-Break">stateful apps</span></li>&#13;
				<li><span class="No-Break">Upgrading deployments</span></li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-109"><a id="_idTextAnchor110"/>Technical requirements</h1>&#13;
			<p>To follow along with this chapter, you should have already deployed a Kubernetes app via a Kubernetes manifest. This chapter is going to break down the process of things such as deploying apps and what a Kubernetes manifest is, but to fully grasp the chapter, you should be familiar with the deployment process. Think of it like this – you should be at a beginner/mid level with the Kubernetes deployment process, and this chapter will get you to the <span class="No-Break">production level.</span></p>&#13;
			<p>The code for this chapter can be found at the following GitHub <span class="No-Break">URL: </span><a href="https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch5"><span class="No-Break">https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch5</span></a><a href="https://github.com/AdminTurnedDevOps/Packt/tree/main/50-Kubernetes-concepts-every-DevOps-Engineer-should-know/Ch5%0D"/></p>&#13;
			<h1 id="_idParaDest-110"><a id="_idTextAnchor111"/>Understanding cloud-native apps</h1>&#13;
			<p>Although the whole <em class="italic">cloud-native</em> thing can feel a bit buzzword-ish in today’s world, there is some merit behind the idea of building cloud-native apps. The way an application is architected matters as<a id="_idIndexMarker251"/> it relates to how it can be deployed, managed, and maintained later. The way a platform is built matters because that’s the starting point for how an application can be deployed and how it can <span class="No-Break">be run.</span></p>&#13;
			<p>Throughout the years of technology’s existence, there have been multiple different methodologies around how applications are architected and built. The original methods were formed around on-premises systems, such as mainframes and servers. After that, applications started to be architected for virtualized hardware platforms, such as ESXi, and other virtualization products, with the idea in mind of utilizing more of the server, but for different workloads instead of just one workload running like in the bare-metal days. After virtualization, there was architecture and planning for apps around cloud workloads, which started to introduce the idea of cloud native and how applications would work if they only ran in the cloud. Considerations such as bandwidth, size of servers, and overall cost consumed a lot of conversations around cloud workloads, and <span class="No-Break">still do.</span></p>&#13;
			<p>Now, we’re faced with the <em class="italic">fourth phase</em>, which is containerized workloads. Containerized workloads really kicked off the focus around cloud-native applications and deployments, and for good reason, especially with the idea of microservices starting to become a real thing for many organizations that would’ve thought it wasn’t possible just 5 <span class="No-Break">years ago.</span></p>&#13;
			<p>In this section, you’re going to learn what cloud-native applications are and a brief history of application architecture, cloud deployments, <span class="No-Break">and microservices.</span></p>&#13;
			<h2 id="_idParaDest-111"><a id="_idTextAnchor112"/>What’s a cloud-native app?</h2>&#13;
			<p>Before deploying applications in a cloud-native way, let’s take a step back and think about a core computer <a id="_idIndexMarker252"/>science concept – <span class="No-Break">distributing computing.</span></p>&#13;
			<p>Distributed computing is a field that studies distributed systems, and distributed systems are systems that have components located on different network-connected computers. Those different network-connected computers then communicate with each other to send data, or packets, back <span class="No-Break">and forth.</span></p>&#13;
			<p>The important part here is this – distributed systems equal multiple software components that are on multiple systems but run as a single system. Distributed computing sounds like microservices, right? (More on <span class="No-Break">microservices later.)</span></p>&#13;
			<p>Cloud native takes the concept of distributed computing and expands it to a whole other level. Think about it from an AWS or Azure perspective. AWS and Azure are by definition distributed systems. When you log in to the AWS portal, there are a ton of services at your fingertips – EC2, databases, storage, and a lot more. All of those services that you interact with are from a <em class="italic">single system</em>, but the network components that make up the <em class="italic">single system</em> span hundreds of thousands of servers, across multiple data centers across the entire world. Cloud native doesn’t just mean the public cloud, however. Remember, the cloud is a distribution of services. Something that’s <em class="italic">cloud native</em> can also be, for example, an entire OpenStack <span class="No-Break">server farm.</span></p>&#13;
			<p>Combining the whole idea of cloud native and distributed computing, you have a major concept – cloud-native applications. Cloud-native apps aim to give you the ability to design and build apps that are <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break">Easily scalable</span></li>&#13;
				<li><span class="No-Break">Resilient</span></li>&#13;
				<li><span class="No-Break">Elastic</span></li>&#13;
			</ul>&#13;
			<p>The important thing to remember is that these concepts aren’t any different than what we’ve already had in the engineering world. We’ve had distributed computing for a long time. We’ve had distributed applications for a while. What we didn’t always have is the ability to easily implement distributed computing. Thinking about the AWS or Azure example from previously, how long would it take us to build the same infrastructure as Azure or AWS? Then, think about how many people it would take to manage and maintain it. With distributed computing at the cloud level, all of the <em class="italic">day-one</em> configurations are abstracted away from you, leaving you with only worrying about the <em class="italic">day-two</em> complexities of <a id="_idIndexMarker253"/>building a cloud-native/distrusted <span class="No-Break">computing application.</span></p>&#13;
			<p>In the cloud, if you want to scale your application, you click a few buttons, write a few lines of code, and boom, you have autoscaling groups. If you want to build resilient applications, you point and click on what data centers you want your apps to run in instead of having to physically build out those data centers. Again, the concept of distributed computing is the same thing as cloud native and cloud-native apps. The difference is that you don’t have to worry about building out the data center. You just have to worry about scaling <span class="No-Break">the app.</span></p>&#13;
			<h2 id="_idParaDest-112"><a id="_idTextAnchor113"/>Cloud-specific cloud native</h2>&#13;
			<p>One major point to keep in<a id="_idIndexMarker254"/> mind, whether it’s with a standard app deployment in the cloud or an app deployment in Kubernetes, is <em class="italic">cloud native</em> doesn’t just mean <em class="italic">the cloud</em>. It’s more or less the overall concept, but again, the whole idea of cloud native is distributed computing without the need to focus on the day-one implementation <span class="No-Break">and configuration.</span></p>&#13;
			<p>For example, let’s take OpenStack. OpenStack is a private cloud. You can deploy OpenStack in your data center and interact with it just like you would with any other cloud service. However, here’s the catch – some teams may see it as cloud native and others may see it as general distributed computing. The infrastructure teams that are building out OpenStack will see the <em class="italic">behind-the-scenes</em> configurations, such as building out the hardware and scaling it across multiple data centers. To them, it’s no different than a standard distributed computing environment. Same for the infrastructure engineers that are building, managing, and maintaining the infrastructure for AWS or Azure. Then, there are the teams that interact with OpenStack after it’s already built. They’re logging into the UI and communicating with OpenStack via the CLI and other API methods, so they’re getting the full satisfaction of a true cloud-native environment just like many engineers are getting the full satisfaction of interacting with AWS and Azure without needing to worry about the infrastructure and <span class="No-Break">services on-premises.</span></p>&#13;
			<p>Another example is the hybrid cloud. If you’re running Azure Stack HCI on-premises, that means you’re utilizing some server that runs the Azure Stack HCI operating system, which interacts with the Azure cloud. The engineers that are managing Azure Stack HCI see what’s happening behind the scenes. Other engineers that are simply interacting with <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>) don’t see<a id="_idIndexMarker255"/> the underlying infrastructure. They just know that they go to a specific location to create a new <span class="No-Break">Kubernetes cluster.</span></p>&#13;
			<p>Regardless of <a id="_idIndexMarker256"/>where a platform or app is deployed, it could be considered cloud native to some and standard distributed computing to others. You could be an engineer that’s building a cloud-native platform so others can interact with it in a <span class="No-Break">cloud-native fashion.</span></p>&#13;
			<h2 id="_idParaDest-113"><a id="_idTextAnchor114"/>What are microservices?</h2>&#13;
			<p>Taking the idea of distributed <a id="_idIndexMarker257"/>computing and cloud native to the next level gives us a microservice. By definition, a microservice is a loosely coupled architecture that has components that have no dependencies on <span class="No-Break">each other.</span></p>&#13;
			<p>Say you have five pieces that make up your application: three backend APIs, some middleware to connect the backend and frontend, and a frontend that consists of a website with multiple paths. In a monolithic-style environment, you would take that entire application, package it up, deploy it to a server, and run the binary. Then, if you had to update or upgrade any part of that application, such as the one backend API, you would have to take down the rest of the application. This not only brings down production but also would slow down the ability to get new updates and features out because you’d have to specify a specific window to bring down the <span class="No-Break">entire platform.</span></p>&#13;
			<p>Microservices allow you to take those five pieces of the application and split them out into their own individual pieces. Then, you can manage those <em class="italic">pieces</em> separately instead of having to worry about combining them to deploy and have a <span class="No-Break">working platform.</span></p>&#13;
			<p>In a Kubernetes environment, you would have <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li>One container image for backend <span class="No-Break">API 1</span></li>&#13;
				<li>One container image for backend <span class="No-Break">API 2</span></li>&#13;
				<li>One container image for backend <span class="No-Break">API 3</span></li>&#13;
				<li>One container image for <span class="No-Break">the middleware</span></li>&#13;
				<li>One container image for <span class="No-Break">the frontend</span></li>&#13;
			</ul>&#13;
			<p>Then, each of those container images can be updated, upgraded, deployed, and <span class="No-Break">managed separately.</span></p>&#13;
			<p>It’s important to note that microservices aren’t just for containers and Kubernetes. The same concepts talked about previously can work just as well on five different Ubuntu VMs. It’s just easier to manage a container from a microservice perspective than it is to manage it from a VM <a id="_idIndexMarker258"/>perspective. Way less automation and repeatable practices are needed to do the same thing you would have to do on a VM inside of Kubernetes. It’s possible and 100% doable, but it takes <span class="No-Break">more effort.</span></p>&#13;
			<p>In the next section, you’re going to take what you learned in this section and start applying it to <span class="No-Break">Kubernetes-based scenarios.</span></p>&#13;
			<h1 id="_idParaDest-114"><a id="_idTextAnchor115"/>Learning about Kubernetes app deployments</h1>&#13;
			<p>When engineers are first getting started with deploying an application to a Kubernetes cluster, it looks<a id="_idIndexMarker259"/> something <span class="No-Break">like this:</span></p>&#13;
			<ol>&#13;
				<li>Create a <span class="No-Break">Kubernetes manifest.</span></li>&#13;
				<li>Run a command such as <strong class="source-inline">kubectl apply -f</strong> or <strong class="source-inline">kubectl create -f</strong> against <span class="No-Break">the manifest.</span></li>&#13;
				<li>Ensure that the application has <span class="No-Break">Pods running.</span></li>&#13;
				<li>Access your app to ensure it’s running the way you were expecting it <span class="No-Break">to run.</span></li>&#13;
			</ol>&#13;
			<p>Although this is a great approach to getting started with deploying applications to Kubernetes, we must dive a little bit deeper to fully understand how the deployment process of an app occurs, why it works the way that it does, how manifests interact with Kubernetes to ensure an application is deployed, and how Kubernetes keeps the desired state of <span class="No-Break">Pods running.</span></p>&#13;
			<p>It seems like how Kubernetes deploys apps is simply magic that occurs on the platform because that’s how it’s built and that’s the way it’s supposed to be, but there’s a ridiculous number of pieces built that allow Kubernetes to appear to be the magical deployment platform that it makes itself out <span class="No-Break">to be.</span></p>&#13;
			<p>In this section, you’re going to learn from start to finish how app deployments work inside of Kubernetes and the internals of everything that’s needed to make a <span class="No-Break">successful deployment.</span></p>&#13;
			<h2 id="_idParaDest-115"><a id="_idTextAnchor116"/>Kubernetes manifests</h2>&#13;
			<p>Before actually deploying an application, you’ll need to learn the ins and outs of how most applications are deployed to<a id="_idIndexMarker260"/> Kubernetes – a <strong class="bold">Kubernetes manifest</strong>. The idea is that you already know what a Kubernetes manifest is, but perhaps you don’t know the breakdown of the internals of a <span class="No-Break">Kubernetes manifest.</span></p>&#13;
			<p>A Kubernetes manifest is a YAML- or JSON-based configuration that interacts differently with the Kubernetes API. The Kubernetes manifest is where you specify what API you want to work with and what resource you want to work with from that API. There are two groups <span class="No-Break">of APIs:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Core group</strong>: Consists of the original <a id="_idIndexMarker261"/>APIs that Kubernetes came with <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">/api/v1</strong></span></li>&#13;
				<li><strong class="bold">Named group</strong>: Consists of all new APIs that<a id="_idIndexMarker262"/> are being built and are <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">/apis/$GROUP_NAME/$VERSION</strong></span></li>&#13;
			</ul>&#13;
			<p>For example, the following is a code snippet showcasing that the Deployment resource is in the <strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">api/v1</strong></span><span class="No-Break"> group:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: apps/v1&#13;
kind: Deployment</pre>&#13;
			<p>The following is another example of an Ingress controller, which you can see is <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">/apis/networking.k8s.io/v1</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: networking.k8s.io/v1&#13;
kind: Ingress</pre>&#13;
			<p><strong class="source-inline">apiVersion</strong> is the Kubernetes API you’re utilizing, and <strong class="source-inline">kind</strong> is what Kubernetes resource you’re creating, updating, <span class="No-Break">or deleting.</span></p>&#13;
			<p>A Kubernetes manifest consists of four <span class="No-Break">key parts:</span></p>&#13;
			<ul>&#13;
				<li><strong class="source-inline">apiVersion</strong>: Which version of the Kubernetes API you’re using to create <span class="No-Break">the object/resource</span></li>&#13;
				<li><strong class="source-inline">Kind</strong>: What kind of object you want to create, update, <span class="No-Break">or delete</span></li>&#13;
				<li><strong class="source-inline">Metadata</strong>: Data that helps uniquely identify <span class="No-Break">the resource/object</span></li>&#13;
				<li><strong class="source-inline">Spec</strong>: What you want the resource to <span class="No-Break">look like</span></li>&#13;
			</ul>&#13;
			<p>The following is a <span class="No-Break">Kubernetes </span><span class="No-Break"><a id="_idIndexMarker263"/></span><span class="No-Break">manifest:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
  name: nginx-deployment&#13;
spec:&#13;
  selector:&#13;
    matchLabels:&#13;
      app: nginxdeployment&#13;
  replicas: 2&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: nginxdeployment&#13;
    spec:&#13;
      containers:&#13;
      - name: nginxdeployment&#13;
        image: nginx:latest&#13;
        ports:&#13;
        - containerPort: 80</pre>&#13;
			<p>Let’s break <span class="No-Break">that down.</span></p>&#13;
			<p>First, you have the API version. You can see that the API version indicates that it’s utilizing a resource in the core group. Next, there’s <strong class="source-inline">kind</strong>, which specifies what resource you’re creating/updating/deleting. Then there’s <strong class="source-inline">metadata</strong>, which is specifying a name for the deployment to uniquely identify it via metadata. Finally, there’s <strong class="source-inline">spec</strong>, which indicates how you want your containerized app to look. For example, the spec shown earlier indicates that the<a id="_idIndexMarker264"/> manifest is using the latest version of the Nginx container image and utilizing <span class="No-Break">port </span><span class="No-Break"><strong class="source-inline">80</strong></span><span class="No-Break">.</span></p>&#13;
			<p>To wrap up this section, something you should know about Kubernetes manifests, and the way Kubernetes works in general, is that it’s declarative. Declarative means “<em class="italic">tell me what to do, not how to do it</em>.” Imperative means “<em class="italic">tell me what to do and how to </em><span class="No-Break"><em class="italic">do it</em></span><span class="No-Break">.”</span></p>&#13;
			<p>For example, let’s say you were teaching someone how to bake a cake. If it was imperative, you would be telling the person what ingredients to use, the size for each ingredient, and how to do it step by step, ultimately leading them to the finished product. Declarative would mean you tell them what ingredients they need and they figure out how to do it on <span class="No-Break">their own.</span></p>&#13;
			<p>Kubernetes manifests are declarative because you tell Kubernetes what resource you want to create, including the name of the resource, ports, volumes, and so on, but you don’t tell Kubernetes how to make that resource. You simply define what you want, but not how to <span class="No-Break">do it.</span></p>&#13;
			<h3>The common way but not the only way</h3>&#13;
			<p>Nine times out of ten, when<a id="_idIndexMarker265"/> you’re deploying a resource to Kubernetes, you’ll most likely be using a Kubernetes manifest. However, as you’ve learned throughout this book so far, the core of Kubernetes is an API. Because it’s an API, you can utilize any programmatic approach to interact <span class="No-Break">with it.</span></p>&#13;
			<p>For example, the following is a code snippet using Pulumi, a popular IaaS platform to create an Nginx deployment inside of Kubernetes. This code requires more to run it, so don’t try to run it. This is just a <span class="No-Break">pseudo example.</span></p>&#13;
			<p>There’s no YAML and no configuration language. It’s raw Go (Golang) code interacting with the <span class="No-Break">Kubernetes API:</span></p>&#13;
			<pre class="source-code">&#13;
        deployment, err := appsv1.NewDeployment(ctx, conf.Require("deployment"), &amp;appsv1.DeploymentArgs{&#13;
            Spec: appsv1.DeploymentSpecArgs{&#13;
                Selector: &amp;metav1.LabelSelectorArgs{&#13;
                    MatchLabels: appLabels,&#13;
                },&#13;
                Replicas: pulumi.Int(2),&#13;
                Template: &amp;corev1.PodTemplateSpecArgs{&#13;
                    Metadata: &amp;metav1.ObjectMetaArgs{&#13;
                        Labels: appLabels,&#13;
                    },&#13;
                    Spec: &amp;corev1.PodSpecArgs{&#13;
                        Containers: corev1.ContainerArray{&#13;
                            corev1.ContainerArgs{&#13;
                                Name:  pulumi.String(conf.Require("containerName")),&#13;
                                Image: pulumi.String(conf.Require("imageName")),&#13;
                            }},&#13;
                    },&#13;
                },&#13;
            },&#13;
        })&#13;
        if err != nil {&#13;
            return err&#13;
        }</pre>&#13;
			<p>Although you won’t see this too often, you should know that this type of method exists and you can create <a id="_idIndexMarker266"/>any resource you want in Kubernetes, in any programmatic fashion, as long as you do it via the<a id="_idIndexMarker267"/> <span class="No-Break">Kubernetes API.</span></p>&#13;
			<h2 id="_idParaDest-116"><a id="_idTextAnchor117"/>Controllers and operators</h2>&#13;
			<p>Kubernetes comes out of the box with a way to ensure that the current state of a deployed application is the desired state. For example, let’s say you deploy a Kubernetes deployment that is supposed to<a id="_idIndexMarker268"/> have two replicas. Then, for whatever reason, one of the Pods goes away. The deployment controller would see that and do whatever it needs to do to ensure a second replica/Pod gets created. All resources <a id="_idIndexMarker269"/>that can be created in Kubernetes (Pods, Services, Ingress, Secrets, and so on) have <span class="No-Break">a </span><span class="No-Break"><strong class="bold">controller</strong></span><span class="No-Break">.</span></p>&#13;
			<p>An <strong class="bold">operator</strong> is a special form <a id="_idIndexMarker270"/>of a controller. Operators implement the controller pattern, and their primary job is to move the resources inside of the Kubernetes cluster to the <span class="No-Break">desired state.</span></p>&#13;
			<p>Operators also add the Kubernetes API extendibility to use <strong class="bold">CustomResourceDefinitions </strong>(<strong class="bold">CRDs</strong>), which are a way that engineers can utilize the existing Kubernetes API without <a id="_idIndexMarker271"/>having to build an entire controller and, instead, just use the CRD controller. You’ll see a lot of products/platforms that tie into Kubernetes to create <span class="No-Break">a CRD.</span></p>&#13;
			<p>A popular way of building your own<a id="_idIndexMarker272"/> operator and controller is with <span class="No-Break">Kubebuilder: </span><a href="https://book.kubebuilder.io/"><span class="No-Break">https://book.kubebuilder.io/</span></a><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer066" class="IMG---Figure">&#13;
					<img src="Images/B19116_05_01.jpg" alt="Figure 5.1 – Controllers&#13;&#10;" width="1407" height="798"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Controllers</p>&#13;
			<p>You’ll hear the terms <em class="italic">operator</em> and <em class="italic">controller</em> used interchangeably. To give a frame of reference, just remember that the operator is like the big boss working at a high level, ensuring that things<a id="_idIndexMarker273"/> are going well for the organization, and the operator is like the engineer doing the hands-on work to make sure that the organization gets what <span class="No-Break">it needs.</span></p>&#13;
			<p>Another form of this that’s gaining increased popularity is GitOps. GitOps looks at the desired state of a Kubernetes manifest that’s in source control as opposed to what controllers do, which is look at what’s actively deployed on a <span class="No-Break">Kubernetes cluster.</span></p>&#13;
			<h2 id="_idParaDest-117"><a id="_idTextAnchor118"/>Different ways to deploy with higher-level controllers</h2>&#13;
			<p>When you deploy a Pod by<a id="_idIndexMarker274"/> itself, the manifest can look like <span class="No-Break">the following:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: static-web&#13;
  labels:&#13;
    role: myrole&#13;
spec:&#13;
  containers:&#13;
    - name: web&#13;
      image: nginx&#13;
      ports:&#13;
        - name: web&#13;
          containerPort: 80&#13;
          protocol: TCP</pre>&#13;
			<p>The problem with this method is you have no high-level controller that manages the Pod(s) for you. Without the higher-level controller, like a Deployment or DaemonSet, if the Pod fails, the kubelet watches the static Pod and restarts it if it fails. There’s also no management of the <a id="_idIndexMarker275"/>current state and desired state. From a production perspective, you never want to deploy a Pod resource by itself. It’s fine if you want to test a container image, but that’s about it. It should be used for testing/development <span class="No-Break">purposes only.</span></p>&#13;
			<p>Not always, but most of the time, you’ll see the following production-level controllers that <span class="No-Break">manage Pods:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Deployments</strong>: A deployment is one of the highest-level controllers in the Core API group. It gives you the<a id="_idIndexMarker276"/> ability to control one Pod or multiple replicas and scale out across the cluster. Deployments also give you the ability to self-heal and confirm that the current state of a deployed containerized application is the desired state. The following code is an example <span class="No-Break">deployment resource:</span><pre class="console">&#13;
<strong class="bold">apiVersion: apps/v1</strong></pre><pre class="console">&#13;
<strong class="bold">kind: Deployment</strong></pre><pre class="console">&#13;
<strong class="bold">metadata:</strong></pre><pre class="console">&#13;
<strong class="bold">  name: nginx-deployment</strong></pre><pre class="console">&#13;
<strong class="bold">spec:</strong></pre><pre class="console">&#13;
<strong class="bold">  selector:</strong></pre><pre class="console">&#13;
<strong class="bold">    matchLabels:</strong></pre><pre class="console">&#13;
<strong class="bold">      app: nginxdeployment</strong></pre><pre class="console">&#13;
<strong class="bold">  replicas: 2</strong></pre><pre class="console">&#13;
<strong class="bold">  template:</strong></pre><pre class="console">&#13;
<strong class="bold">    metadata:</strong></pre><pre class="console">&#13;
<strong class="bold">      labels:</strong></pre><pre class="console">&#13;
<strong class="bold">        app: nginxdeployment</strong></pre><pre class="console">&#13;
<strong class="bold">    spec:</strong></pre><pre class="console">&#13;
<strong class="bold">      containers:</strong></pre><pre class="console">&#13;
<strong class="bold">      - name: nginxdeployment</strong></pre><pre class="console">&#13;
<strong class="bold">        image: nginx:latest</strong></pre><pre class="console">&#13;
<strong class="bold">        ports:</strong></pre><pre class="console">&#13;
<strong class="bold">        - containerPort: 80</strong></pre></li>&#13;
				<li><strong class="bold">DaemonSets</strong>: This is like a deployment resource but is cluster wide. It ensures that either all nodes or<a id="_idIndexMarker277"/> the nodes you <a id="_idIndexMarker278"/>choose run a copy/replica of the Pod. A key difference you’ll see in a DaemonSet is that there’s no field for replicas. That’s because the Pod can’t run more replicas than the number of worker nodes, meaning you can’t have five Pod replicas if you only have three worker nodes. In that case, you’d only be able to have three Pods. The following code is an <span class="No-Break">example DaemonSet:</span><pre class="console">&#13;
<strong class="bold">apiVersion: apps/v1</strong></pre><pre class="console">&#13;
<strong class="bold">kind: DaemonSet</strong></pre><pre class="console">&#13;
<strong class="bold">metadata:</strong></pre><pre class="console">&#13;
<strong class="bold">  name: nginx-deployment</strong></pre><pre class="console">&#13;
<strong class="bold">spec:</strong></pre><pre class="console">&#13;
<strong class="bold">  # nodeSelecter: Field that you can specify what worker nodes you want the Pod to deploy to</strong></pre><pre class="console">&#13;
<strong class="bold">  selector:</strong></pre><pre class="console">&#13;
<strong class="bold">    matchLabels:</strong></pre><pre class="console">&#13;
<strong class="bold">      app: nginxdeployment</strong></pre><pre class="console">&#13;
<strong class="bold">  template:</strong></pre><pre class="console">&#13;
<strong class="bold">    metadata:</strong></pre><pre class="console">&#13;
<strong class="bold">      labels:</strong></pre><pre class="console">&#13;
<strong class="bold">        app: nginxdeployment</strong></pre><pre class="console">&#13;
<strong class="bold">    spec:</strong></pre><pre class="console">&#13;
<strong class="bold">      containers:</strong></pre><pre class="console">&#13;
<strong class="bold">      - name: nginxdeployment</strong></pre><pre class="console">&#13;
<strong class="bold">        image: nginx:latest</strong></pre><pre class="console">&#13;
<strong class="bold">        ports:</strong></pre><pre class="console">&#13;
<strong class="bold">        - containerPort: 80</strong></pre></li>&#13;
				<li><strong class="bold">StatefulSets</strong>: This is like a Deployment<a id="_idIndexMarker279"/> but for applications that aren’t stateless. The StatefulSet maintains a sticky ID for each Pod. For example, let’s say that you have an app that needs to communicate with a Pod via a specific network ID or unique ID. With a Deployment, Pods are ephemeral, so they lose their unique ID. With a <a id="_idIndexMarker280"/>StatefulSet, a new Pod can get created after the old one failed, but it’ll have the same network ID as the old Pod. For a StatefulSet to work, it requires a service to control the network domain. Because its only job is to control the network domain, a headless service makes the most sense. The following is how <a id="_idIndexMarker281"/>you can create a <span class="No-Break">standard </span><span class="No-Break"><strong class="source-inline">StatefulSet</strong></span><span class="No-Break">:</span><pre class="console">&#13;
<strong class="bold">apiVersion: apps/v1</strong></pre><pre class="console">&#13;
<strong class="bold">kind: StatefulSet</strong></pre><pre class="console">&#13;
<strong class="bold">metadata:</strong></pre><pre class="console">&#13;
<strong class="bold">  name: nginx-deployment</strong></pre><pre class="console">&#13;
<strong class="bold">spec:</strong></pre><pre class="console">&#13;
<strong class="bold">  selector:</strong></pre><pre class="console">&#13;
<strong class="bold">    matchLabels:</strong></pre><pre class="console">&#13;
<strong class="bold">      app: nginxdeployment</strong></pre><pre class="console">&#13;
<strong class="bold">  serviceName: nginxservice</strong></pre><pre class="console">&#13;
<strong class="bold">  replicas: 2</strong></pre><pre class="console">&#13;
<strong class="bold">  template:</strong></pre><pre class="console">&#13;
<strong class="bold">    metadata:</strong></pre><pre class="console">&#13;
<strong class="bold">      labels:</strong></pre><pre class="console">&#13;
<strong class="bold">        app: nginxdeployment</strong></pre><pre class="console">&#13;
<strong class="bold">    spec:</strong></pre><pre class="console">&#13;
<strong class="bold">      containers:</strong></pre><pre class="console">&#13;
<strong class="bold">      - name: nginxdeployment</strong></pre><pre class="console">&#13;
<strong class="bold">        image: nginx:latest</strong></pre><pre class="console">&#13;
<strong class="bold">        ports:</strong></pre><pre class="console">&#13;
<strong class="bold">        - containerPort: 80</strong></pre><pre class="console">&#13;
<strong class="bold">---</strong></pre><pre class="console">&#13;
<strong class="bold">apiVersion: v1</strong></pre><pre class="console">&#13;
<strong class="bold">kind: Service</strong></pre><pre class="console">&#13;
<strong class="bold">metadata:</strong></pre><pre class="console">&#13;
<strong class="bold">  name: nginxservice</strong></pre><pre class="console">&#13;
<strong class="bold">spec:</strong></pre><pre class="console">&#13;
<strong class="bold">  selector:</strong></pre><pre class="console">&#13;
<strong class="bold">    app: nginxdeployment</strong></pre><pre class="console">&#13;
<strong class="bold">  ports:</strong></pre><pre class="console">&#13;
<strong class="bold">    - protocol: TCP</strong></pre><pre class="console">&#13;
<strong class="bold">      port: 80</strong></pre><pre class="console">&#13;
<strong class="bold">  clusterIP: None</strong></pre></li>&#13;
			</ul>&#13;
			<p>In terms of which <a id="_idIndexMarker282"/>controller to use, it’s going to depend on your use case. There’s no right or wrong answer here unless it’s something straightforward, for example, if you have a containerized <a id="_idIndexMarker283"/>app that needs to hold on to its network ID, in which case you’d use <span class="No-Break">a StatefulSet.</span></p>&#13;
			<h2 id="_idParaDest-118"><a id="_idTextAnchor119"/>Scaling</h2>&#13;
			<p>One of the key <a id="_idIndexMarker284"/>components out of the box with Kubernetes is its ability to easily scale both horizontally and vertically. From a production perspective, this takes a ton of load off of your back. In a standard VM environment, you would have to worry about deploying a new server, installing the operating system, getting packages up to date, and deploying the application binary, and finally, the app would <span class="No-Break">be running.</span></p>&#13;
			<p>Horizontal Pod <a id="_idIndexMarker285"/>autoscaling is the most common, which means more Pods get created to handle load. Vertical autoscaling means the CPU/memory of a Pod gets raised. Vertical Pod autoscaling is not all that common, <span class="No-Break">but possible.</span></p>&#13;
			<p>When you’re scaling, you can have standard ReplicaSets, but in production, the number may not be so cut and dry. For example, if you have three replicas, but you may need four or ten, you need a way to account for that. The best thing that you can do is start with at least three to four replicas, and if needed, work your way up. If you have to scale up to five or ten, you can update the Kubernetes manifest and redeploy it with a GitOps solution or in another repeatable fashion using the <strong class="source-inline">kubectl apply -f </strong><span class="No-Break"><strong class="source-inline">name_of_manifest.yaml</strong></span><span class="No-Break"> command.</span></p>&#13;
			<p>When you’re scaling a Pod, or for that matter, when you’re doing anything for a Pod deployment, never <a id="_idIndexMarker286"/>use commands such as <strong class="source-inline">kubectl patch</strong> or any of the other quick fixes on the command line. If you do, any time the Pod gets redeployed, your configurations won’t exist because you did them ad hoc/manually on the command line. Always make changes in a Kubernetes manifest and deploy them properly (remember, current state versus <span class="No-Break">desired state).</span></p>&#13;
			<h3>How to horizontally scale Pods</h3>&#13;
			<p>When you’re scaling Pods <a id="_idIndexMarker287"/>horizontally, it’s all about replica count. For example, let’s say you have a Kubernetes manifest like the following, which contains <span class="No-Break">two replicas:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
  name: nginx-deployment&#13;
spec:&#13;
  selector:&#13;
    matchLabels:&#13;
      app: nginxdeployment&#13;
  replicas: 2&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: nginxdeployment&#13;
    spec:&#13;
      containers:&#13;
      - name: nginxdeployment&#13;
        image: nginx:latest&#13;
        ports:&#13;
        - containerPort: 80</pre>&#13;
			<p>You run <strong class="source-inline">kubectl apply -f nginx.yaml</strong> on the preceding manifest, and then you come to realize that <a id="_idIndexMarker288"/>due to user load on the Nginx frontend, you need to bump the replicas from two to four. At that point, you can update the Kubernetes manifest to go from two replicas <span class="No-Break">to four:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
  name: nginx-deployment&#13;
spec:&#13;
  selector:&#13;
    matchLabels:&#13;
      app: nginxdeployment&#13;
  replicas: 4&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: nginxdeployment&#13;
    spec:&#13;
      containers:&#13;
      - name: nginxdeployment&#13;
        image: nginx:latest&#13;
        ports:&#13;
        - containerPort: 80</pre>&#13;
			<p>This method won’t recreate anything as you’re using <strong class="source-inline">kubectl apply -f</strong> instead of <strong class="source-inline">kubectl create -f</strong>. <strong class="source-inline">create</strong> is for creating net-new resources and <strong class="source-inline">apply</strong> is for updating/patching <span class="No-Break">a resource.</span></p>&#13;
			<h3>How to vertically scale Pods</h3>&#13;
			<p>Vertically scaling Pods, as<a id="_idIndexMarker289"/> discussed, is not a common practice. However, it is doable. The typical method is to use the <strong class="source-inline">VerticalPodAutoscaler</strong> resource from the <strong class="source-inline">autoscaling.k8s.io</strong> API. It gives you the ability to point to an existing deployment so that deployment is managed by the autoscaler. For example, the following Kubernetes manifest shows a target reference of a deployment <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">nginxdeployment</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: autoscaling.k8s.io/v1&#13;
kind: VerticalPodAutoscaler&#13;
metadata:&#13;
  name: nginx-verticalscale&#13;
spec:&#13;
  targetRef:&#13;
    apiVersion: "apps/v1"&#13;
    kind:       Deployment&#13;
    name:       nginxdeployment&#13;
  updatePolicy:&#13;
    updateMode: "Auto"</pre>&#13;
			<p>Please note that with the vertical Pod autoscaler turned to <strong class="source-inline">Auto</strong> for the update mode, it’ll have the ability to do <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break">Delete Pods</span></li>&#13;
				<li>Adjust <span class="No-Break">the CPU</span></li>&#13;
				<li>Adjust <span class="No-Break">the memory</span></li>&#13;
				<li>Create a <span class="No-Break">new Pod</span></li>&#13;
			</ul>&#13;
			<p>It requires a restart <a id="_idIndexMarker290"/>of the application running inside <span class="No-Break">the Pod.</span></p>&#13;
			<h2 id="_idParaDest-119"><a id="_idTextAnchor120"/>Multi-container Pods</h2>&#13;
			<p><strong class="bold">Sidecars</strong>, sometimes called <strong class="bold">multi-container Pods</strong>, are a way to tightly couple containers into <a id="_idIndexMarker291"/>one Pod. Typically, and <a id="_idIndexMarker292"/>especially from an application perspective, one Pod runs one container. However, there<a id="_idIndexMarker293"/> may be use cases where you want to run sidecars. The biggest use case is when you’re running some type of log collector/aggregator for your Pods. A lot of engineers will put the container running the log collector into the same Pod where the application is running. That way, it’s straightforward to communicate with the application and pull the logs from it as containers inside of a Pod share the same IP address but are reachable on <span class="No-Break">different ports.</span></p>&#13;
			<p>One thing you should absolutely never do is run multiple applications in a Pod. For example, you never want to put the frontend app and the backend app inside of the same Pod. That defeats the whole purpose of containers and microservices. Sidecars are only for very specific use cases and if it’s absolutely necessary. A personal belief of mine is that you should never use sidecars unless you absolutely have to. Other engineers will disagree with me, but I believe a Pod should run one workload. That’s the purpose of a microservice architecture. The only time that I see it absolutely necessary is when you’re running a service mesh and you need the service mesh proxy inside of <span class="No-Break">the Pod.</span></p>&#13;
			<p>The following is what a Kubernetes manifest would look like if you have multiple containers inside of a Pod. Notice how under <strong class="source-inline">spec.containers</strong>, there’s <strong class="source-inline">container1</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">container2</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: testsidecar&#13;
spec:&#13;
  containers:&#13;
  - name: container1&#13;
    image: nginx&#13;
  - name: container2&#13;
    image: debian&#13;
    command: ["/bin/sh", "-c"]</pre>&#13;
			<h2 id="_idParaDest-120"><a id="_idTextAnchor121"/>Liveness and readiness probes</h2>&#13;
			<p>Whenever you’re deploying any type of application, whether it’s containerized or not, you want to ensure that the<a id="_idIndexMarker294"/> application is running as expected. Within a containerized environment, it’s no different. Let’s say you have a Pod that’s running an Nginx frontend. The Pod could be up and running, have all of the appropriate resources, and so on. However, that doesn’t mean that the binary running inside the Pod is working as expected. To ensure that the actual application is running as expected, you can use <strong class="bold">liveness probes</strong> and <span class="No-Break"><strong class="bold">readiness probes</strong></span><span class="No-Break">.</span></p>&#13;
			<p>A liveness <a id="_idIndexMarker295"/>probe indicates whether a container is running. It helps Kubernetes understand the overall health of the Pod. The kubelet continuously sends a <em class="italic">ping</em> of sorts to the container to ensure that it’s running as expected. If the liveness probe deems a container unhealthy, the kubelet restarts <span class="No-Break">the Pod.</span></p>&#13;
			<p>A readiness probe indicates<a id="_idIndexMarker296"/> whether the container is ready to receive requests. Readiness probes are a bit more important from an application perspective because they tell Kubernetes whether or not to route service traffic to Pods. If a service is trying to route traffic to a Pod and that Pod is down or unhealthy, the application won’t be reachable. The readiness probe tells the service which Pods are ready to receive requests and <span class="No-Break">which aren’t.</span></p>&#13;
			<p>The following is an example of a <span class="No-Break">readiness probe:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
  name: nginx-deployment&#13;
spec:&#13;
  selector:&#13;
    matchLabels:&#13;
      app: nginxdeployment&#13;
  replicas: 2&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: nginxdeployment&#13;
    spec:&#13;
      containers:&#13;
      - name: nginxdeployment&#13;
        image: nginx:latest&#13;
        imagePullPolicy: Never&#13;
        ports:&#13;
        - containerPort: 80&#13;
        readinessProbe:&#13;
          tcpSocket:&#13;
            port: 8080&#13;
          initialDelaySeconds: 5&#13;
          periodSeconds: 10</pre>&#13;
			<p>The following is an <a id="_idIndexMarker297"/>example of a <span class="No-Break">liveness probe:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
  name: nginx-deployment&#13;
spec:&#13;
  selector:&#13;
    matchLabels:&#13;
      app: nginxdeployment&#13;
  replicas: 2&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: nginxdeployment&#13;
    spec:&#13;
      containers:&#13;
      - name: nginxdeployment&#13;
        image: nginx:latest&#13;
        imagePullPolicy: Never&#13;
        ports:&#13;
        - containerPort: 80&#13;
        livenessProbe:&#13;
          httpGet:&#13;
            scheme: HTTP&#13;
            path: /index.html&#13;
            port: 80&#13;
          initialDelaySeconds: 5&#13;
          periodSeconds: 5</pre>&#13;
			<p>All production-level Kubernetes<a id="_idIndexMarker298"/> deployments should use <span class="No-Break">readiness probes.</span></p>&#13;
			<p>In the next section, you’re going to dive into an important topic, which is segregating your containerized apps, and a few different ways of <span class="No-Break">doing it.</span></p>&#13;
			<h1 id="_idParaDest-121"><a id="_idTextAnchor122"/>Exploring segregation and namespaces</h1>&#13;
			<p>Once applications are deployed, engineers wipe the sweat off their foreheads, give high fives to their team, and rejoice in their victory. However, what comes after the deployment? Better yet, what if you have to deploy the applications again? Or other types of applications? Or to <a id="_idIndexMarker299"/>a different location or segregation point? (Segregation will be discussed later in this chapter.) Getting an application up and running is a mental workout in itself, but the <em class="italic">what-comes-next</em> questions you ask yourself are typically the most important. These are things such as <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li>Will the next deployment be automated <span class="No-Break">and repeatable?</span></li>&#13;
				<li>If you have to deploy the application again, will it be an <span class="No-Break">effective deployment?</span></li>&#13;
				<li>Can (or should) the apps run right next to <span class="No-Break">each other?</span></li>&#13;
				<li>Which engineers should have access to what apps <span class="No-Break">and why?</span></li>&#13;
			</ul>&#13;
			<p>Deploying an application is a great victory but designing how and where an application should run is the difference between a successful and a broken-down production environment. Questions around application segregation and multi-tenancy keep engineers up at night because it’s less <em class="italic">engineering</em> work and more planning/architecture work. It’s less hands-on-keyboard and more critical thinking at a higher level compared to being down in the trenches in <span class="No-Break">the code.</span></p>&#13;
			<p>In this section, you’re going to learn about a few of the most popular segregation techniques. Let’s <span class="No-Break">get started!</span></p>&#13;
			<h2 id="_idParaDest-122"><a id="_idTextAnchor123"/>Namespaces</h2>&#13;
			<p>The first level of <a id="_idIndexMarker300"/>segregation, typically, is a <strong class="bold">namespace</strong>. When you’re deploying <a id="_idIndexMarker301"/>Pods, the last thing that you want to do is deploy everything and anything to the default namespace. Instead, you want to ensure that applications have their own namespaces. At a network level, Pods within one namespace can communicate with another namespace. However, if you have a service account that’s used for Pod deployments in one namespace and a service account that’s used to deploy Pods in another namespace, that means the same service account cannot be used to <a id="_idIndexMarker302"/>manage all of the Pods. That gives you a bit more segregation from a Pod management perspective and ensures that there’s proper authentication and authorization. But from a<a id="_idIndexMarker303"/> network perspective, Pods can still communicate with other Pods in <span class="No-Break">separate namespaces.</span></p>&#13;
			<p>Notice in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.2</em> that there are <span class="No-Break">three namespaces:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break"><strong class="source-inline">argoCD</strong></span></li>&#13;
				<li><span class="No-Break"><strong class="source-inline">kube-system</strong></span></li>&#13;
				<li><span class="No-Break"><strong class="source-inline">monitoring</strong></span></li>&#13;
			</ul>&#13;
			<div>&#13;
				<div id="_idContainer067" class="IMG---Figure">&#13;
					<img src="Images/B19116_05_02.jpg" alt="Figure 5.2 – Kubernetes resources&#13;&#10;" width="949" height="644"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – Kubernetes resources</p>&#13;
			<p>The preceding screenshot shows that everything in the <strong class="source-inline">argocd</strong> namespace is segregated/isolated from everything in the <strong class="source-inline">kube-system</strong> namespace. If an engineer were to run <strong class="source-inline">kubectl get pods</strong>, they would only see the Pods in the namespace that they have <span class="No-Break">access to.</span></p>&#13;
			<h2 id="_idParaDest-123"><a id="_idTextAnchor124"/>Single tenancy</h2>&#13;
			<p>Taking segregation and isolation a step further, there are <strong class="bold">tenancy models</strong>. First, let’s start with <strong class="bold">single tenancy</strong>, but<a id="_idIndexMarker304"/> before diving in, let’s talk about what tenancy models mean <span class="No-Break">in Kubernetes.</span></p>&#13;
			<p>Isolating via tenancy could be<a id="_idIndexMarker305"/> anything from users to engineers to applications and all different resources. For example, single tenancy could mean running one containerized application across a cluster, or it could mean ensuring that one engineer has access to a cluster that no one else has access to it, but they can run as many applications as <span class="No-Break">they want.</span></p>&#13;
			<p>A typical scenario of single tenancy is isolating development environments. Let’s say you’re a developer and you need a Kubernetes cluster to test an application stack. The scenario would be that the platform engineering team, or whichever team manages Kubernetes clusters, gives you your own Kubernetes cluster to test the application stack. This is a great way to perform single tenancy as it allows all engineers working on different tech stacks to test their code without it compromising or interfering with other <span class="No-Break">application stacks.</span></p>&#13;
			<h2 id="_idParaDest-124"><a id="_idTextAnchor125"/>Multi-tenancy</h2>&#13;
			<p>On the flip side is <strong class="bold">multi-tenancy</strong>. Multi-tenancy is where you have multiple engineers, users, or applications running on the same<a id="_idIndexMarker306"/> Kubernetes cluster. If you take<a id="_idIndexMarker307"/> a look again at <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.2</em> showing the namespaces, you’ll see that Prometheus, ArgoCD, and Nginx are running on the same cluster. That would be considered a multi-tenancy cluster. Single tenancy would be if ArgoCD, Nginx, and Prometheus were all running on <span class="No-Break">separate clusters.</span></p>&#13;
			<p>In the real world, rarely do you see applications running on different clusters, or rather, an application per cluster. Instead, you usually see the multi-tenancy model for applications and the single-tenancy model for developers testing application stacks, and once the application stack is tested, it moves into the Kubernetes cluster with the rest of <span class="No-Break">the applications.</span></p>&#13;
			<p>In the next section, you’re going to learn how to think about stateless apps, stateful apps, <span class="No-Break">and volumes.</span></p>&#13;
			<h1 id="_idParaDest-125"><a id="_idTextAnchor126"/>Investigating stateless and stateful apps</h1>&#13;
			<p>At a high level, applications come in two<a id="_idIndexMarker308"/> forms – apps that need data stored and apps that don’t care whether the state of the data is stored. Let’s think about two <span class="No-Break">different scenarios.</span></p>&#13;
			<p>When you log in to your Gmail account, or another email service provider, everything stays where it’s supposed to be. You can see the emails in your inbox, the sent messages, the emails in your trash bin, and so on. The application/platform stays how it’s supposed to be because the data is stateful. Now, on the opposite side of the spectrum, let’s take <strong class="source-inline">www.google.com</strong> into consideration. When you go to <strong class="source-inline">www.google.com</strong> in a web browser, you always have a fresh start. The entry box to type in your question is there, but the results to the previous question that you asked Google isn’t there. It’s always a fresh, clean slate. That’s because <strong class="source-inline">www.google.com</strong> is stateless, as in, it doesn’t just hold on to your data (well, it does… but that’s a separate discussion) and keep it in the web browser after <span class="No-Break">every search.</span></p>&#13;
			<p>Of course, stateless versus stateful is a much deeper discussion, but that’s a high-level definition of how you can think about the two different types <span class="No-Break">of applications.</span></p>&#13;
			<p>In the next section, you’re going to learn about the different deployment methods for stateless and stateful applications inside of Kubernetes, along with resource considerations including limits, quotas, and requests for Pods to ensure that the production-level environment you’re running <span class="No-Break">is sustainable.</span></p>&#13;
			<h2 id="_idParaDest-126"><a id="_idTextAnchor127"/>Stateful versus stateless</h2>&#13;
			<p>In the opening of this section, I shared the Gmail example, which essentially shows what a stateful app is and what a stateless app is. From a Kubernetes perspective, the key difference is that a stateless application doesn’t need to store data. Stateful applications require backend storage, such as volumes. Another key difference is that stateful applications require keeping unique IDs, so if a Pod goes down, the Pod that comes up and replaces it must have the same unique ID. A stateless app doesn’t need to keep <span class="No-Break">unique IDs.</span></p>&#13;
			<p>A common misconception is that stateless apps never use volumes, and that’s not the case. You can have a stateless application that, for example, requires a backend database or a volume/hard drive to <span class="No-Break">store values.</span></p>&#13;
			<p>Volumes and hard drives aren’t what make a stateful application. The unique ID is what makes a <span class="No-Break">stateful application.</span></p>&#13;
			<h2 id="_idParaDest-127"><a id="_idTextAnchor128"/>Container Storage Interface</h2>&#13;
			<p>For Kubernetes to interact with outside components that aren’t native, there must be a plugin of sorts. In the previous chapters, you learned about <strong class="bold">Container Network Interface</strong> (<strong class="bold">CNI</strong>), which is a plugin to use different network frameworks in Kubernetes. <strong class="bold">Container Storage Interface</strong> (<strong class="bold">CSI</strong>) is the same thing, but for storage devices. For example, you <a id="_idIndexMarker309"/>can have a CSI for NetApp, AWS S3, Azure Storage, and a ton of other <span class="No-Break">storage providers.</span></p>&#13;
			<p>Before these interfaces, organizations had to put the code to connect the resources that aren’t native in the core Kubernetes code. Just as an example, let’s say that Azure wanted to allow Kubernetes engineers to utilize Azure Storage inside of Kubernetes for storing the output of a Pod. Before CSI, Azure would’ve had to put the code to make it all possible inside of the core Kubernetes code. That was a major hassle because not only did Azure have to wait for a new release of the Kubernetes API to push the feature out, but if there was a bug or a new update that they wanted to push out, they would’ve had to wait for the next Kubernetes <span class="No-Break">API release.</span></p>&#13;
			<p>CSI, and interfaces/plugins across Kubernetes in general, ensures that organizations can create plugins for Kubernetes separately from the core <span class="No-Break">Kubernetes code.</span></p>&#13;
			<p>If you want to see an example of CSI, you can check it out on <span class="No-Break">GitHub: </span><a href="https://github.com/kubernetes-sigs/azuredisk-csi-driver%0D"><span class="No-Break">https://github.com/kubernetes-sigs/azuredisk-csi-driver</span><span class="No-Break">.</span></a></p>&#13;
			<h2 id="_idParaDest-128"><a id="_idTextAnchor129"/>Volumes</h2>&#13;
			<p><strong class="bold">Volumes</strong> are hard drives, plain <span class="No-Break">and simple.</span></p>&#13;
			<p>With a volume, you give a Pod, or <a id="_idIndexMarker310"/>multiple Pods, the<a id="_idIndexMarker311"/> ability to store data in a location. That location could be Azure, AWS, NetApp, some other storage <a id="_idIndexMarker312"/>provider, or even the worker node that the Pod is running on (definitely not recommended. Just <span class="No-Break">an example).</span></p>&#13;
			<p>When you’re creating a volume for a Pod, there are typically <span class="No-Break">three steps:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">StorageClass</strong>: A storage class is a way to ask some storage vendor (dynamically) for a hard drive. For example, you can create a storage class that connects to EBS. Then, you can<a id="_idIndexMarker313"/> call upon that storage class later with a volume (which you’ll learn about in a minute) and utilize the connection to the storage. You can do the same thing in Azure, GCP, and all of the other cloud providers, including most of the <span class="No-Break">storage providers:</span><pre class="console">&#13;
<strong class="bold">kind: StorageClass</strong></pre><pre class="console">&#13;
<strong class="bold">apiVersion: storage.k8s.io/v1</strong></pre><pre class="console">&#13;
<strong class="bold">metadata:</strong></pre><pre class="console">&#13;
<strong class="bold">  name: azurefile-csi</strong></pre><pre class="console">&#13;
<strong class="bold">provisioner: file.csi.azure.com</strong></pre><pre class="console">&#13;
<strong class="bold">allowVolumeExpansion: true</strong></pre><pre class="console">&#13;
<strong class="bold">mountOptions:</strong></pre><pre class="console">&#13;
<strong class="bold">  - dir_mode=0777</strong></pre><pre class="console">&#13;
<strong class="bold">  - file_mode=0777</strong></pre><pre class="console">&#13;
<strong class="bold">  - uid=0</strong></pre><pre class="console">&#13;
<strong class="bold">  - gid=0</strong></pre><pre class="console">&#13;
<strong class="bold">  - mfsymlinks</strong></pre><pre class="console">&#13;
<strong class="bold">  - cache=strict</strong></pre><pre class="console">&#13;
<strong class="bold">  - actimeo=30</strong></pre><pre class="console">&#13;
<strong class="bold">parameters:</strong></pre><pre class="console">&#13;
<strong class="bold">  skuName: Premium_LRS</strong></pre></li>&#13;
				<li><strong class="bold">PersistentVolume</strong>: A persistent volume is created manually by an engineer that uses the storage class to utilize storage from an available source. For example, the Persistent Volume would connect to the EBS storage class from the <span class="No-Break">previous example:</span><pre class="console">&#13;
<strong class="bold">apiVersion: v1</strong></pre><pre class="console">&#13;
<strong class="bold">kind: PersistentVolume</strong></pre><pre class="console">&#13;
<strong class="bold">metadata:</strong></pre><pre class="console">&#13;
<strong class="bold">  name: azure-pv</strong></pre><pre class="console">&#13;
<strong class="bold">spec:</strong></pre><pre class="console">&#13;
<strong class="bold">  storageClassName: " azurefile-csi "</strong></pre><pre class="console">&#13;
<strong class="bold">  claimRef:</strong></pre><pre class="console">&#13;
<strong class="bold">    name: azurefile</strong></pre><pre class="console">&#13;
<strong class="bold">    namespace: default</strong></pre></li>&#13;
				<li><strong class="bold">PersistentVolumeClaim</strong>: The last piece is the persistent volume claim, which is a request made by a user, usually in a Kubernetes manifest that’s creating a Pod, to use<a id="_idIndexMarker314"/> some of the storage that’s available in the storage class. The engineer can say “hey, I want 10 GB of storage from this storage class for <span class="No-Break">my Pod”:</span><pre class="console">&#13;
<strong class="bold">apiVersion: v1</strong></pre><pre class="console">&#13;
<strong class="bold">kind: PersistentVolumeClaim</strong></pre><pre class="console">&#13;
<strong class="bold">metadata:</strong></pre><pre class="console">&#13;
<strong class="bold">  name: azurefile</strong></pre><pre class="console">&#13;
<strong class="bold">spec:</strong></pre><pre class="console">&#13;
<strong class="bold">  accessModes:</strong></pre><pre class="console">&#13;
<strong class="bold">    - ReadWriteMany</strong></pre><pre class="console">&#13;
<strong class="bold">  storageClassName: azurefile-csi</strong></pre><pre class="console">&#13;
<strong class="bold">  resources:</strong></pre><pre class="console">&#13;
<strong class="bold">    requests:</strong></pre><pre class="console">&#13;
<strong class="bold">      storage: 10Gi</strong></pre></li>&#13;
			</ul>&#13;
			<p>At this point, you may be wondering “well, why do I need a persistent volume if I can just automatically request some storage with a claim?”, and that’s a good question. The answer is going to depend on your environment. If you’re using NetApp storage, and you have 1,000 GB of storage, you want an engineer to create a persistent volume and manage<a id="_idIndexMarker315"/> those volumes because you only have 1,000 GB of storage. If you attempt to go over that 1,000 GB, failures will start to occur, so having someone manage it makes sense. On the flip side, if you’re using cloud storage, such as in Azure or AWS, that storage is <em class="italic">unlimited</em> to a user (you of course have to pay for it), so going straight to a persistent volume claim instead of having an engineer create a persistent volume would <span class="No-Break">make sense.</span></p>&#13;
			<h2 id="_idParaDest-129"><a id="_idTextAnchor130"/>Resource requests and limits</h2>&#13;
			<p>In any production<a id="_idIndexMarker316"/> environment, you have Kubernetes clusters that are running on servers, regardless of whether it’s on-premises or a managed Kubernetes service. Because they are running on servers, those servers have hardware resources, and all servers have a limit. There’s no <em class="italic">unlimited CPU</em> on a server or <em class="italic">unlimited memory</em>. There are limits to a server’s resources and servers can reach <span class="No-Break">100% capacity.</span></p>&#13;
			<p>Because of that, when you’re creating Pods, you should specify limits and requests. You never want to give anything, whether it’s a virtualized VM or a containerized app, open reign in an environment to take as much CPU and memory as it wants. If you don’t control resources, such as memory and CPU, every application could take whatever resources it wanted <span class="No-Break">to take.</span></p>&#13;
			<p>Let’s think about a basic example. Say <a id="_idIndexMarker317"/>you have an application that has a memory leak. If you containerize it, the Pod that it’s running in will continue to take more and more memory until the worker node eventually fails and/or the application crashes, and you’ll only know when it’s <span class="No-Break">too late.</span></p>&#13;
			<p>Before <a id="_idIndexMarker318"/>diving in, let’s define the<a id="_idIndexMarker319"/> difference between a <strong class="bold">limit</strong> and <span class="No-Break">a </span><span class="No-Break"><strong class="bold">request</strong></span><span class="No-Break">.</span></p>&#13;
			<p>A limit is telling a Pod “you cannot go above this.” For example, if you specify <em class="italic">X</em> amount of CPU or memory on a Pod, that Pod cannot go above that limit. It’s <span class="No-Break">completely blocked.</span></p>&#13;
			<p>The following is an example of a limit. As you can see, the Nginx app is limited to 128 Mi of memory. Anything above that and Kubernetes will say “nope, you can’t <span class="No-Break">have it”:</span></p>&#13;
			<pre class="source-code">&#13;
    spec:&#13;
      containers:&#13;
      - name: nginxdeployment&#13;
        image: nginx:latest&#13;
        resources:&#13;
          limits:&#13;
            memory: "128Mi"&#13;
        ports:&#13;
        - containerPort: 80</pre>&#13;
			<p>A request is what the Pod is <a id="_idIndexMarker320"/>guaranteed to get. If a Pod requests a resource, Kubernetes will only schedule it on<a id="_idIndexMarker321"/> a worker node that can <a id="_idIndexMarker322"/>give it <span class="No-Break">that resource.</span></p>&#13;
			<p>The following is an example of a request. In this example, Kubernetes will say “alright, you want 64 Mi of <a id="_idIndexMarker323"/>memory and 250m of CPU. Let me schedule you onto a worker node that can <span class="No-Break">handle this”:</span></p>&#13;
			<pre class="source-code">&#13;
    spec:&#13;
      containers:&#13;
      - name: nginxdeployment&#13;
        image: nginx:latest&#13;
        resources:&#13;
          requests:&#13;
            memory: "64Mi"&#13;
            cpu: "250m"&#13;
        ports:&#13;
        - containerPort: 80</pre>&#13;
			<p>The <a id="_idIndexMarker324"/>following is an entire<a id="_idIndexMarker325"/> <span class="No-Break">manifest </span><span class="No-Break"><a id="_idIndexMarker326"/></span><span class="No-Break">example:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
  name: nginx-deployment&#13;
spec:&#13;
  selector:&#13;
    matchLabels:&#13;
      app: nginxdeployment&#13;
  replicas: 2&#13;
  template:&#13;
    metadata:&#13;
      namespace: webapp&#13;
      labels:&#13;
        app: nginxdeployment&#13;
    spec:&#13;
      containers:&#13;
      - name: nginxdeployment&#13;
        image: nginx:latest&#13;
        resources:&#13;
          requests:&#13;
            memory: "64Mi"&#13;
            cpu: "250m"&#13;
          limits:&#13;
            memory: "128Mi"&#13;
        ports:&#13;
        - containerPort: 80</pre>&#13;
			<h3>Which should you choose?</h3>&#13;
			<p>There’s some confusion around how requests and <span class="No-Break">limits work.</span></p>&#13;
			<p>When Pods are done using memory, they give that memory back to the worker node and it goes back into the pool for other Pods to use. With the CPU, it does not. The Pod will hold on to that CPU. Because of that, it’s not a best practice to let the Pod just hold on to the CPU <a id="_idIndexMarker327"/>until it gets deleted because it may not always need that amount of CPU. It’s essentially wasting <span class="No-Break">CPU resources.</span></p>&#13;
			<p>So, which should <span class="No-Break">you choose?</span></p>&#13;
			<p>In every production environment, you should always set up requests, but you should only <span class="No-Break">limit CPU.</span></p>&#13;
			<h3>Namespace quotas</h3>&#13;
			<p>When it comes to limits and requests, one of the really awesome things that you can do is set them up for namespaces. For <a id="_idIndexMarker328"/>example, you can have a namespace that has a limit of 1,000 Mi and a request of 512 Mi. That way, all nodes running in that namespace automatically get limited to the required resources, which means you don’t have to put limits and requests into every single Kubernetes Pod manifest. The following code block showcases the <span class="No-Break">resource quota:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: v1&#13;
kind: namespace&#13;
metadata:&#13;
  name: test&#13;
---&#13;
apiVersion: v1&#13;
kind: ResourceQuota&#13;
metadata:&#13;
  name: memorylimit&#13;
  namespace: test&#13;
spec:&#13;
  hard:&#13;
    requests.memory: 512Mi&#13;
    limits.memory: 1000Mi</pre>&#13;
			<p>In the next and final section, you’re going to learn how to upgrade apps and different types of <span class="No-Break">update </span><span class="No-Break"><a id="_idIndexMarker329"/></span><span class="No-Break">methods.</span></p>&#13;
			<h1 id="_idParaDest-130"><a id="_idTextAnchor131"/>Upgrading Kubernetes apps</h1>&#13;
			<p>Throughout this chapter, you<a id="_idIndexMarker330"/> learned some very <span class="No-Break">important lessons:</span></p>&#13;
			<ul>&#13;
				<li>How to deploy <span class="No-Break">an app</span></li>&#13;
				<li>How to deploy different types of apps <span class="No-Break">on Kubernetes</span></li>&#13;
				<li>How to ensure apps are <span class="No-Break">properly scaled</span></li>&#13;
				<li>How to ensure apps are running as <span class="No-Break">you expected</span></li>&#13;
			</ul>&#13;
			<p>Once you get an application to where you’d like it to be, it’s a great accomplishment. Then, before you know it, it’s time to upgrade or update the application and you have to start on the journey all over again. You must test out the new version of the app, get it deployed without taking down the entire production environment, and retest all the components to ensure it’s running <span class="No-Break">as expected.</span></p>&#13;
			<p>There may also be times, which is extremely common, when you must roll back an update or upgrade to a previous application version. Perhaps it wasn’t properly tested in the staging environment, or something popped up that the QA/regression testing didn’t catch. In any case, you need a solid plan and methodology on how to do <span class="No-Break">a rollback.</span></p>&#13;
			<p>In this section, you’re going to learn a few different ways to test out application updates and upgrades, how you can upgrade and update applications running in Kubernetes, and how you can roll back updates and upgrades <span class="No-Break">when necessary.</span></p>&#13;
			<h2 id="_idParaDest-131"><a id="_idTextAnchor132"/>Types of upgrades</h2>&#13;
			<p>First, let’s break down the typical types of upgrades <span class="No-Break">in Kubernetes.</span></p>&#13;
			<p>A/B testing is a way to<a id="_idIndexMarker331"/> have a set of users on one version of the application and a set of users on another version of the application. For example, let’s say you’re testing out two versions of an app, v1.1 and v1.2. A set of users would get v1.1 and another set of users would get v1.2. At that point, you can test things such as performance, how the users are interacting with the new version of the app, bugs, and issues. This type of test is a <span class="No-Break">controlled experiment.</span></p>&#13;
			<p>Canary deployments are pretty much identical to A/B testing except they’re done with real users. Taking the previous example, let’s say you had v1.1 and v1.2 of an app. You would roll out v1.2 in production and put a set of users on v1.2 but keep a set of users on v1.1. That way, you can see how users interact with the new version <span class="No-Break">in production.</span></p>&#13;
			<p>Blue/green testing is when you have two production environments, one on v1.1 and one on v1.2. All the users are still on v1.1, but you slowly start to migrate all of the users to v1.2. All users are moved over to v1.2 once it’s confirmed to <span class="No-Break">be working.</span></p>&#13;
			<p>In Kubernetes, the most <a id="_idIndexMarker332"/>popular upgrade method is a <strong class="bold">rolling update</strong>, which, based on the preceding explanations, is a <span class="No-Break">blue/green deployment.</span></p>&#13;
			<h2 id="_idParaDest-132"><a id="_idTextAnchor133"/>What happens to an app being upgraded?</h2>&#13;
			<p>When you’re upgrading a<a id="_idIndexMarker333"/> container image in a Pod, what happens is the new Pod comes up and is tested and the old Pod then <span class="No-Break">gets deleted.</span></p>&#13;
			<p>Let’s take the example from the previous section regarding v1.1 and v.1.2 with the help of the <span class="No-Break">following diagram:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer068" class="IMG---Figure">&#13;
					<img src="Images/B19116_05_03.jpg" alt="Figure 5.3 – Rolling update&#13;&#10;" width="900" height="667"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Rolling update</p>&#13;
			<p>In the preceding architecture diagram, what’s happening is v1.1 is running on a Pod with an IP address of 10.0.0.5. Then, the new Pod running v1.2 comes up and is running at the same time as the<a id="_idIndexMarker334"/> old Pod. Once the deployment confirms that v1.2 of the Pod is working properly and as expected, the users will begin to move over to the new Pod. Once all users are on the new Pod running v1.2, the old Pod running v1.1 <span class="No-Break">gets deleted.</span></p>&#13;
			<h2 id="_idParaDest-133"><a id="_idTextAnchor134"/>Rolling updates</h2>&#13;
			<p>What was explained in the <a id="_idIndexMarker335"/>previous section was a rolling update. Let’s take a look at it from a code perspective. The following is a Kubernetes manifest that’s running a deployment spec with a containerized Nginx image <span class="No-Break">using v1.1:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
  name: nginx-deployment&#13;
spec:&#13;
  selector:&#13;
    matchLabels:&#13;
      app: nginxdeployment&#13;
  replicas: 2&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: nginxdeployment&#13;
    spec:&#13;
      containers:&#13;
      - name: nginxdeployment&#13;
        image: nginx:1.1&#13;
        ports:&#13;
        - containerPort: 80</pre>&#13;
			<p>Then, the time comes to upgrade the containerized app. To upgrade the app with <strong class="source-inline">RollingUpdate</strong> (blue/green deployment), you would swap out the <strong class="source-inline">nginx:1.1</strong> container image version with <strong class="source-inline">nginx:1.2</strong>. The <strong class="source-inline">RollingUpdate</strong> configuration contains a <strong class="source-inline">progressDeadlineSeconds</strong> and <strong class="source-inline">minReadySeconds</strong> configuration to confirm that the new version of the containerized app comes up appropriately. Within the strategy map, you <a id="_idIndexMarker336"/>specify a <strong class="source-inline">RollingUpdate</strong> type and ensure that one replica is always running the old containerized app version as the update occurs. That way, users aren’t kicked off the app. The following code will perform the proper rolling <span class="No-Break">update action:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
  name: nginx-deployment&#13;
spec:&#13;
  selector:&#13;
    matchLabels:&#13;
      app: nginxdeployment&#13;
  revisionHistoryLimit: 3&#13;
  progressDeadlineSeconds: 300&#13;
  minReadySeconds: 10&#13;
  strategy:&#13;
    type: RollingUpdate&#13;
    rollingUpdate:&#13;
      maxUnavailable: 1&#13;
      maxSurge: 1&#13;
  replicas: 4&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: nginxdeployment&#13;
    spec:&#13;
      containers:&#13;
      - name: nginxdeployment&#13;
        image: nginx:1.2&#13;
        ports:&#13;
        - containerPort: 80</pre>&#13;
			<p>You would <a id="_idIndexMarker337"/>then run <strong class="source-inline">kubectl apply -f</strong> against the Kubernetes manifest, and the rolling update <span class="No-Break">would begin.</span></p>&#13;
			<h2 id="_idParaDest-134"><a id="_idTextAnchor135"/>Rollbacks</h2>&#13;
			<p>If you’d like to roll back <strong class="source-inline">RollingUpdate</strong>, you’ll need <span class="No-Break">two commands.</span></p>&#13;
			<p>First, get the revision<a id="_idIndexMarker338"/> number that you want to roll back to from the <span class="No-Break">following command:</span></p>&#13;
			<pre class="console">&#13;
kubectl rollout history deployment nginxdeployment</pre>&#13;
			<p>Next, <span class="No-Break">undo </span><span class="No-Break"><strong class="source-inline">RollingUpdate</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="console">&#13;
kubectl rollout undo deployment/nginxdeployment --to-revision=whichever_revision_number_youd_like</pre>&#13;
			<p>Not only are updates and rollbacks important to understand from an educational perspective, but you’ll most likely see this a fair amount as your organization moves to a more <span class="No-Break">microservice-driven approach.</span></p>&#13;
			<h1 id="_idParaDest-135"><a id="_idTextAnchor136"/>Summary</h1>&#13;
			<p>There are many types of resource deployments when it comes to Kubernetes and often, there’s no right or wrong answer to which you choose. The only time that there’s a true right or wrong answer is depending on the deployment. If you have a stateful application, you want to use a StatefulSet. There’s no mystery as to which controller you should be using and there’s no <em class="italic">good or bad</em>. It simply depends on the type of application and workload you need to deploy <span class="No-Break">and manage.</span></p>&#13;
			<p>In the next chapter, we’ll be diving a little bit deeper into different types of deployments from a more <span class="No-Break">advanced perspective.</span></p>&#13;
			<h1 id="_idParaDest-136"><a id="_idTextAnchor137"/>Further reading</h1>&#13;
			<ul>&#13;
				<li><em class="italic">Kubernetes – An Enterprise Guide</em> by Marc Boorshtein and Scott <span class="No-Break">Surovich: </span><a href="https://www.packtpub.com/product/kubernetes-an-enterprise-guide-second-edition/9781803230030"><span class="No-Break">https://www.packtpub.com/product/kubernetes-an-enterprise-guide-second-edition/9781803230030</span></a></li>&#13;
			</ul>&#13;
		</div>&#13;
	</div></body></html>