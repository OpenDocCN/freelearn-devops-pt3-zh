- en: '[*Chapter 9*](B17124_09_Final_SK_ePub.xhtml#_idTextAnchor223): High Availability
    and Elasticity on AWS'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most applications follow a design pattern that comprises several layers—such
    as the network layer, the compute layer, and the storage and database layers.
    We call this a multi-tier application. So, for example, you can have a three-tier
    application stack comprising a web services layer that offers frontend web interface
    access, an application layer where perhaps all data processing happens, and a
    backend database layer to store and manage data.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we start to bring together the various core **Amazon Web Services**
    (**AWS**) services we have learned about so far to design and architect a complete
    **end-to-end** (**E2E**) solution. Furthermore, in previous chapters, we have
    only deployed single resource instances of various AWS services—for example, a
    single **Elastic Compute Cloud** (**EC2**) instance to offer compute capability,
    or as in the previous chapter, where we deployed a single Amazon **Relational
    Database Service** (**RDS**) database instance in the public subnet of our **virtual
    private cloud** (**VPC**).
  prefs: []
  type: TYPE_NORMAL
- en: In real-world scenarios, you generally need to incorporate components that will
    help you achieve **high availability** (**HA**) and **scalability**. We can increase
    our application's availability by having more than one EC2 instance serving the
    same application or website. This way, if one of the EC2 instances fails, users
    can continue to access the services offered by being directed to other healthy
    EC2 instances in the fleet, and away from those that are unhealthy or in a failed
    state. We can also ensure that we place our EC2 instances across multiple **Availability
    Zones** (**AZs**), ensuring that if one AZ fails or just simply goes offline,
    users can be redirected to healthy EC2 instances in another AZ.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we need to offer solutions that are scalable. AWS offers services
    that are capable of automatically scaling out when required; for example, when
    we notice an increase in traffic, we can add more EC2 instances to cope with the
    load. On the flip side, the same AWS service can automatically scale in when demand
    drops, allowing us to save on the unnecessary expense of running underutilized
    servers in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we also need to consider the global availability of our services. Many
    companies have global customers, and while many AWS services are designed for
    regional availability and scalability options, other AWS services can help us
    deliver global availability and even offer resilience against regional outages.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we discuss the following key concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to vertical and horizontal scaling concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of the **Open Systems Interconnection** (**OSI**) model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributing web traffic with Amazon **Elastic Load Balancing** (**ELB**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing elasticity with AWS Auto Scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing multi-Region HA solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To complete this chapter and the exercises within, you need to have access to
    your AWS account and be logged in as **Alice**, our **Identity and Access Management**
    (**IAM**) user (administrator) that we created in [*Chapter 4*](B17124_04_Final_SK_ePub.xhtml#_idTextAnchor068),
    *Identity and Access Management*.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to vertical and horizontal scaling concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you deploy a given EC2 instance in your VPC, you need to choose an instance
    type and one or more associated **Elastic Block Store** (**EBS**) (or instance
    store) volumes of specific sizes. Your EC2 instance will always need one root
    volume and one or more data volumes based on your application requirements.
  prefs: []
  type: TYPE_NORMAL
- en: However, from time to time, you may need to upgrade your original configuration—
    perhaps you need more memory or more **central processing units** (**CPUs**) to
    cope with the load on your server. You may be running out of storage space and
    therefore need to increase the amount of storage on your EBS volumes. When upgrading
    to an instance of a higher specification, we call this **vertical scaling**. To
    perform most upgrades this way, you generally need to stop processing application
    requests, and most of the time, you may first need to shut the EC2 instance down.
  prefs: []
  type: TYPE_NORMAL
- en: "The actual upgrade can take anything from a few minutes to a few hours, depending\
    \ on what you are upgrading. For example, upgrading the instance type usually\
    \ involves shutting down the server, modifying the instance type, and restarting\
    \ it again, as shown \Lin the following screenshot:"
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Changing EC2 instance type: vertical scaling'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17124_09_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.1 – Changing EC2 instance type: vertical scaling'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, you will select the drop-down arrow next to **Instance
    type** to select a higher-specification EC2 instance type. Once selected, simply
    click on **Apply** and start up the EC2 instance again. The EC2 instance is started
    with the upgraded specifications.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, you can also upgrade the storage volume attached to the server or
    attach additional volumes. You can modify existing volumes to increase the storage
    size or change the type of storage from **General Purpose SSD (gp2)** to **Provisioned
    IOPS (io1)**. When upgrading your storage, AWS needs to perform some optimization
    tasks, and this will take some time, depending on the size of the volume.
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertical scaling** does have its limitations, however. It cannot offer HA;
    so, if there is a problem with the EC2 instance and it fails, you will need to
    provision a new EC2 instance as a replacement.'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than having a single EC2 instance host your application, you could consider
    hosting multiple EC2 instances with the same application offering. This way, if
    one EC2 instance fails, customers can be redirected to another EC2 instance that
    is in a healthy state.
  prefs: []
  type: TYPE_NORMAL
- en: Often, you need more than one EC2 instance participating as a fleet to cope
    with demand and offer HA in case of failures of any instance. AWS offers a service
    called Auto Scaling (which we look at in detail later in this chapter), which
    can automatically launch (or terminate) an EC2 instance to cope with load based
    on performance parameters such as average CPU utilization across a fleet of servers.
    This ability to then launch additional EC2 instances serving the same application
    is known as **horizontal scaling**.
  prefs: []
  type: TYPE_NORMAL
- en: With horizontal scaling, you can add more EC2 instances to your fleet when demand
    increases and terminate unnecessary instances when demand drops.
  prefs: []
  type: TYPE_NORMAL
- en: This requires careful architectural design, as the application needs to be aware
    that it is being run from multiple EC2 instances. For example, if you have two
    copies of your WordPress blog running from two EC2 instances, content data is
    usually stored on the local storage attached to a single EC2 instance—in this
    case, the EBS volumes. If you remember from [*Chapter 7*](B17124_07_Final_SK_ePub.xhtml#_idTextAnchor157),
    *AWS Compute Services – EC2, Lightsail,* EBS volumes can only be attached to one
    EC2 instance at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Amazon EBS Multi-Attach is a new feature that enables you to attach a single
    Provisioned IOPS SSD (io1 or io2) volume to multiple instances located in the
    same AZ. However, it has several limitations and does not necessarily replace
    the use case for **Elastic File System** (**EFS**) volumes. For additional information,
    refer to [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that any blog articles you write will be stored on one EC2 instance,
    and the other EC2 instance running WordPress will not be aware of this content,
    resulting in inconsistencies between the two servers. One way to handle this is
    to offer some ability to share data between the EC2 instances, such as having
    the application data hosted on an EFS volume that can act as a file share for
    multiple EC2 instances. The WordPress application will also have to be configured
    to store all blog content and related media on this central EFS volume instead,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Enabling horizontal scaling at the application layer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17124_09_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.2 – Enabling horizontal scaling at the application layer
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we compared vertical scaling and horizontal scaling scalability
    options on AWS. Vertical scaling refers to an in-place upgrade to add more CPUs,
    memory, or storage. Vertical scaling does not offer any HA because if the EC2
    instance fails, you cannot fail over to another instance. Horizontal scaling is
    designed to add more nodes (compute or otherwise) to your fleet and can help reduce
    the overall load on an individual instance. With horizontal scaling, you can also
    offer HA so that if one node fails, traffic can be redirected to other healthy
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we introduce the **OSI model**, which is a reference model for how applications
    communicate over a network and how network traffic flows across from the physical
    layer of network cabling and Wi-Fi through to the application itself. Having a
    broad understanding of this reference model will help you appreciate and assist
    with troubleshooting communication issues between your applications across networks.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the OSI model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The flow and distribution of network traffic across various devices and applications
    are defined by a concept known as the **OSI model**. Published in 1984, this model
    provides a visual description of how network traffic flows over a particular network
    system
  prefs: []
  type: TYPE_NORMAL
- en: There are **seven** layers to the OSI model that flow from top to bottom, with
    layer 7 being at the top and layer 1 at the bottom. The OSI model is used as a
    reference point for various vendors to express which layer of network communication
    the product they are offering works on. This is because different hardware and
    software products operate at different layers.
  prefs: []
  type: TYPE_NORMAL
- en: The OSI model also assists in identifying network problems. When analyzing the
    source of any network issue, identifying whether only a single user is affected
    or whether a network segment or the entire network is down can help identify potential
    equipment or mediums that are experiencing the fault.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the seven layers of the OSI model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – OSI model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17124_09_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.3 – OSI model
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we look at ELBs, which form a crucial part of the overall
    design architecture for HA and horizontal scalability.
  prefs: []
  type: TYPE_NORMAL
- en: Distributing web traffic with Amazon ELB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you have more than one EC2 instance that works as part of a fleet hosting
    a given application, you need a mechanism in place to distribute traffic to those
    instances in a manner that spreads the load across the fleet. At a very basic
    level, this is what Amazon ELBs are designed to do. Amazon ELBs distribute traffic
    across multiple targets, which can be EC2 instances, containers, **Internet Protocol**
    (**IP**) addresses, and even Lambda functions. They can handle varying traffic
    for your application, evenly distributing the load across those registered targets
    either in a single AZ or across multiple AZs within a given Region. This also
    means that ELBs can assist in designing architecture that offers HA and fault
    tolerance, as well as working with services such as Auto Scaling to deliver automatic
    scalability features to your applications. Note, however, that ELBs are regional-based
    only, so you cannot use an ELB to distribute traffic across Regions.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancers and VPCs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon load balancers are designed to work with your VPC. AWS recommends you
    enable more than one AZ for your load balancer. Amazon **Elastic Load Balancing**
    (**ELB**) then creates **load balancer nodes** in the AZs you specify. The load
    balancer then distributes traffic to its nodes across the AZs. Its nodes then
    connect to the targets in the relevant AZs.
  prefs: []
  type: TYPE_NORMAL
- en: 'An important point to note here is that your internet-based clients need to
    only connect to the load balancer, which then distributes traffic to targets in
    your VPC. This means you no longer need to place any targets such as EC2 instances
    in the public subnet unless you have a specific reason to do so. You can place
    your web servers, for instance, in a private subnet, and because they are registered
    to the load balancer, traffic will be routed to them. Furthermore, this also means
    that your web servers can function with just private IP addresses, reducing the
    overall attack surface by not having any public IP addresses, as depicted in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Amazon ELB VPC configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17124_09_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.4 – Amazon ELB VPC configuration
  prefs: []
  type: TYPE_NORMAL
- en: As seen in the preceding diagram, the web servers will receive traffic from
    the **Application Load Balancer** (**ALB**) (we look at the types of load balancers
    next). The ALB will distribute this traffic using the default round-robin method
    based on the number of AZs enabled for the load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, when you create a load balancer, you need to specify whether you
    are creating an *internet-facing* load balancer or an *internal* load balancer.
    These are outlined in more detail here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Internet-facing load balancer**—This has a publicly resolvable **Domain Name
    System** (**DNS**) name, enabling it to route requests from internet-based clients.
    The DNS name resolves client requests to public IP addresses of the load balancer
    nodes for your load balancer. In the preceding diagram, we can see an example
    of an internet-facing load balancer that accepts traffic from clients on the internet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Internal load balancer**—The nodes of an internal load balancer only have
    private IP addresses, and its DNS name is resolvable to the private IPs of the
    nodes. This means that internal load balancers can only route requests from clients
    that already have access to the VPC.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Internal load balancers are particularly useful when you are designing a multi-tier
    application stack—for example, when you have multiple web servers that receive
    traffic from your internet-based clients and then need to send on that traffic
    for processing to application or database servers distributed across multiple
    AZs in private subnets, via another load balancer. In this case, the web servers
    would be registered against the internet-facing load balancers and the application/database
    servers would be registered against the internal load balancers, as depicted in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Internal versus internet-facing ELBs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17124_09_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.5 – Internal versus internet-facing ELBs
  prefs: []
  type: TYPE_NORMAL
- en: Another important point to note is that for traffic to be accepted by an ELB,
    you need to configure security groups, specifying inbound rules that the port,
    protocol, and source of the traffic for that load balancer can accept. In addition,
    the security groups associated with your targets must also be configured to allow
    inbound traffic from the load balancers. You can usually do this by specifying
    the source of the traffic as being the security group associated with the load
    balancer itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon offers four types of ELBs, as listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: ALB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network Load Balancer** (**NLB**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gateway Load Balancer** (**GWLB**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classic Load Balancer** (**CLB**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at these individually in some detail next.
  prefs: []
  type: TYPE_NORMAL
- en: ALB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will start with the Amazon **ALB**, which is the most common type of load
    balancer to use for most applications. ALBs are designed to act as a single entry
    point for clients to connect to your applications running on targets such as a
    fleet of EC2 instances. It is recommended that your EC2 instances are based across
    multiple AZs, increasing the overall availability of your application in the case
    of a single AZ failure.
  prefs: []
  type: TYPE_NORMAL
- en: ALBs are designed to distribute traffic at the application layer (using **HyperText
    Transfer Protocol** (**HTTP**) and **HTTP Secure** (**HTTPS**)). The application
    aayer is also known as the *seventh layer of the OSI model*. ALBs are therefore
    ideal for ensuring an even distribution of traffic to your web applications across
    the internet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key benefits of ALBs include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Support for path-based routing, allowing you to forward requests based on the
    **Uniform Resource Locator** (**URL**) in the request
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for host-based routing, allowing you to forward requests based on the
    URL in the request
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for routing requests to multiple applications on a single EC2 instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for registering Lambda functions as targets, as well as containerized
    applications, and much more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ALB has a configuration component called a **listener**. This listener service
    allows you to define rules on how the load balancer will route requests from clients
    to registered targets. These rules consist of a priority, actions, and any conditions
    that once met will enable the action to be performed. Your listener should have
    at least one default rule, and you can have additional rules. These rules also
    define which protocol and port to use to connect to your targets.
  prefs: []
  type: TYPE_NORMAL
- en: When configuring an ALB, you also need to configure one or more `www.mycompany.com`)
    and another target group contains targets that host the actual e-commerce portion
    of your website (for example, `shop.mycompany.com`). Another feature of ALBs is
    that you can also register a target with multiple target groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach allows you to better manage traffic to your website and use different
    targets to ensure better performance and management. As an example, if your e-commerce
    portion is having a major upgrade that will take that portion of the site down
    for a few hours, users can still visit the corporate portion of your website to
    access the latest information or services offered, as illustrated in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – AWS ALB with multiple listener rules'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17124_09_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.6 – AWS ALB with multiple listener rules
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, you will note that we have three target groups, and
    one target is part of two groups. **Target Group 1** could be our primary corporate
    web pages, whereas **Target Group 3** could be our e-commerce site. Each listener
    will also contain a default rule, and the listener on the right contains an additional
    rule that routes requests to another group.
  prefs: []
  type: TYPE_NORMAL
- en: Health checks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to defining listener rules to route traffic to appropriate targets
    within target groups, ALBs also perform health checks against your targets to
    determine whether they are in a healthy state. If a target such as an EC2 instance
    is not responding within a predefined set of requests based on the health check
    settings, it is marked as *unhealthy* and the ALB stops sending traffic to it,
    redirecting traffic to only those targets that are in a *healthy* state.
  prefs: []
  type: TYPE_NORMAL
- en: Using this approach, end users are always directed to EC2 instances (or any
    other targets) that are functioning and responding to the ALB, reducing their
    chances of experiencing outages.
  prefs: []
  type: TYPE_NORMAL
- en: Traffic routing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When it comes to routing traffic to individual targets in a target group, ALBs
    use **round robin** as the default method, but you can also configure routing
    based on the **least outstanding requests** (**LOR**) routing algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Previously, we mentioned that you use the ALB to split traffic across two target
    groups in our example of an e-commerce store. The first target group will host
    EC2 instances that offer access to the corporate website (`www.mycompany.com`),
    and the second target group could host the e-commerce portion (`shop.mycompany.com`).
    This is also known as `www` versus `shop`). In addition to host-based routing,
    ALBs can be used to route traffic for the following use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mycompany.com/store` for users looking to purchase products, and another portion
    to `mycompany.com/blog` for users looking to read articles about the latest market
    trends.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Host-header conditions**—This allows you to route traffic based on fields
    in the request URL—for example, query patterns or by source IP address.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple applications on a single EC2 instance**—This allows you to register
    more than one application on an EC2 instance using different port numbers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support for registering targets by IP address**—This allows you to also redirect
    traffic from your ALB to your on-premises servers using private IP addressing
    over **virtual private network** (**VPN**) tunnels or Direct Connect connections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support for registering Lambda functions as targets**—This allows you to
    configure Lambda functions as targets. Any traffic forwarded will invoke the Lambda
    function, passing any content in **JavaScript Object Notation** (**JSON**) format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support for containerized applications**—This includes **Elastic Container
    Service** (**ECS**), where you can schedule and register tasks with a target group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we discussed how traffic can be routed with ALBs and saw different
    use cases. Next, we take a look at some security features of ALBs.
  prefs: []
  type: TYPE_NORMAL
- en: ALB and WAF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon also offers several security tools that we will examine in detail in
    a later chapter. One such tool is called the **Web Application Firewall** (**WAF**),
    which helps protect against common web exploits such as SQL injections and **cross-site
    scripting** (**XSS**). Amazon ALBs offer WAF integration to help you protect your
    applications from such common web attacks.
  prefs: []
  type: TYPE_NORMAL
- en: NLB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**NLBs** are designed to operate at the fourth layer of the OSI model and be
    able to handle millions of requests per second. NLBs are designed for load balancing
    of both **Transmission Control Protocol** (**TCP**) and **User Datagram Protocol**
    (**UDP**) traffic and maintain ultra-low latencies. With NLBs, you can preserve
    the client''s source IP, allowing your backend services to see the IP address
    of the client, which may be a requirement for the application to function.'
  prefs: []
  type: TYPE_NORMAL
- en: NLBs also offer support for static IP addresses and elastic IP addresses, the
    latter allowing you to configure one fixed IP per AZ. Again, this may be a requirement
    for your application, and hence you would need NLBs.
  prefs: []
  type: TYPE_NORMAL
- en: NLBs do not inspect the application layer and so are unaware of content types,
    cookie data, or any custom header information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key benefits of NLBs include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Ability to handle volatile workloads and handle millions of requests per second.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for static IP addresses for your load balancer and one elastic IP address
    per subnet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can register targets by IP address—this allows you to register targets outside
    the VPC such as in your on-premises environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for routing requests to multiple applications on a single instance using
    multiple ports.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for containerized applications such as those running on Amazon ECS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we discussed NLBs and learned about their various use cases,
    particularly when you need to support millions of requests per second and operate
    at the fourth layer of the OSI model over TCP and UDP protocols. In the next section,
    we look at GWLBs, designed to enable you to distribute traffic across various
    software appliances offered on the AWS Marketplace.
  prefs: []
  type: TYPE_NORMAL
- en: GWLB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before allowing traffic to enter your VPC, you may wish to perform security
    inspections and analysis of that traffic to block any kind of suspicious activity.
    Often, you could deploy your own security tools on EC2 instances to inspect that
    traffic to deploy third-party tools procured from the AWS Marketplace such as
    firewalls, **intrusion detection systems/intrusion prevention systems** (**IDSes/IPSes**),
    and so on. Managing traffic being routed via these third-party tools is made easier
    with the help of Amazon **GWLBs**.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon GLWBs can manage the availability of these third-party virtual appliances
    and act as a single entry and exit point for all traffic destined for these services.
    This enables you to scale the availability and load-balance traffic across a fleet
    of your virtual appliances. GWLB operates at the third layer of the OSI model
    (the network layer) and exchanges application traffic with your virtual appliances
    using the `6081`. Traffic is sent in both directions to the appliance, allowing
    it to perform stateful traffic processing.
  prefs: []
  type: TYPE_NORMAL
- en: CLB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon CLB is a previous-generation ELB designed to operate at both layer 4
    and 7 of the OSI model but without the extended features offered by the ALB or
    the level of throughput you can expect from an NLB. CLBs enable you to distribute
    traffic across EC2 instances that are in a single AZ or across multiple AZs. They
    are ideal for testing and for non-production environments, or if your existing
    application is running in the **EC2-Classic network mode**.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we looked at Amazon ELBs, which enable you to centrally distribute
    incoming traffic across multiple targets such as a fleet of EC2 instances offering
    access to a web application. Amazon ELBs can also perform health checks against
    your targets and redirect traffic away from unhealthy targets to ones that are
    responding and in a healthy state. This reduces the chances of your end users
    experiencing any kind of outage by inadvertently being connected to an instance
    that is not healthy.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon ELBs ultimately help you design your architecture for HA and support
    scalability features in conjunction with Amazon Auto Scaling, which we will discuss
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing elasticity with Amazon Auto Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most amazing services on AWS is the ability to automatically scale
    your workloads when demand increases and then scale back in when demand drops.
    This service is offered as part of various core technologies—for example, computing
    services such as EC2 and database services such as DynamoDB.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic scaling in response to a particular condition such as an increase
    in demand (for example, when average CPU utilization across your fleet of EC2
    instances goes above a threshold such as 70%) can help provision additional capacity
    when it is required most. However, you are not stuck with the new size of your
    fleet. You can configure Auto Scaling so that if demand drops below a specific
    threshold value, it will terminate EC2 instances and therefore *save on costs*.
    Let's look at Auto Scaling for EC2 instances in detail next.
  prefs: []
  type: TYPE_NORMAL
- en: Auto Scaling is a regional service, and you can scale across AZs within a given
    Region, allowing you to launch EC2 instances across AZs for **HA** and **resilience**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we can see that two additional EC2 instances were
    added to a fleet across AZs **2A** and **2B**. This was due to the average CPU
    utilization rising above 80%. Once the new instances are part of the fleet, the
    average CPU utilization should start to fall as the load on the application is
    spread across six instances now instead of the original four:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Auto Scaling service example'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17124_09_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.7 – Auto Scaling service example
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Auto Scaling helps you provision necessary EC2 instances on-demand and
    terminate them when the demand for your resources drops to or below a certain
    threshold. With Amazon Auto Scaling, you do not need to carry out complex capacity
    planning exercises. Next, we look at some core components of the Amazon Auto Scaling
    service.
  prefs: []
  type: TYPE_NORMAL
- en: Auto Scaling groups
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you configure Auto Scaling, you define a collection called an Auto Scaling
    group. This Auto Scaling group will monitor and manage your fleet of EC2 instances.
    As part of the configuration, you need to define the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Minimum number of EC2 instances**—This is the minimum size of the group,
    and Auto Scaling will ensure that the number of EC2 instances in your fleet never
    drops below this level. If an instance fails, taking the total count below this
    value, then the Auto Scaling service will launch additional EC2 instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Desired number of EC2 instances**—If you specify the desired capacity (usually
    because you know that at this value, your users have optimal experience), then
    the Auto Scaling service will always try to ensure that you have the number of
    EC2 instances equal to the desired capacity. Note that your desired number can
    be the same as the minimum number, which would mean that the Auto Scaling service
    ensures that you always have this minimum number of EC2 instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maximum number of EC2 instances**—This is the maximum size of the fleet.
    You need to specify the maximum size that you would want to scale out to. This
    also has the effect of ensuring that Auto Scaling does not deploy more than the
    maximum number of instances if, say, a bug in the application running on those
    instances causes unnecessary scale-outs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following diagram, we can see how Amazon Auto Scaling will provision
    your desired capacity of EC2 instances and can then scale out to the maximum number
    of EC2 instances as per your Auto Scaling group configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Auto Scaling groups'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17124_09_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.8 – Auto Scaling groups
  prefs: []
  type: TYPE_NORMAL
- en: The Auto Scaling service will launch and/or terminate EC2 instances as part
    of the group based on the parameters you define and then scale out or scale back
    in based on the scaling policies you set. You also define how health checks are
    made against the EC2 instances in the group—this can be either with the Auto Scaling
    service performing health checks itself or using the health check services of
    the ELB that the fleet of EC2 instances is registered to. Depending on the health
    check results, your scaling policies will be triggered accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration templates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To set up AWS Auto Scaling, you need to configure either a **Launch Template**
    or a **Launch Configuration**. Configuration templates enable you to define specifications
    of the EC2 instances to launch within the group. So, for example, the template
    will define the **Amazon Machine Image** (**AMI**) **identifier** (**ID**), instance
    type, key pairs, security groups, and block device mappings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the configuration template, you can also define scripts to be run at
    launch time, known as **bootstrapping**, which will allow you to automatically
    configure the EC2 instance to participate in the fleet of existing instances,
    where possible. For example, at the launch of an EC2 instance, you can configure
    it with Apache Web Services so that it can function as a web server. These scripts
    can be defined in the **user data** section of the template and can be written
    in **Bash** for Linux **operating systems** (**OSes**) or **PowerShell** for Windows
    OSes. They are described in more detail here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Launch Configuration**—A Launch Configuration is a basic template where you
    specify information for instances such as AMI IDs, instance types, key pairs,
    and security groups. You can associate your Launch Configuration with multiple
    Auto Scaling groups, but you can only launch one specific Launch Configuration
    for an Auto Scaling group at a time. Furthermore, once you have defined the parameters
    of a Launch Configuration, you cannot change it and you will have to re-create
    it if you need to modify it in any way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Launch Configurations are the original way of defining configuration templates
    for your Auto Scaling groups and while still available, are now no longer recommended
    by Amazon. Instead, you are advised to use Launch Templates, which offer more
    features and flexibility and we'll look at next.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`gp3` and `io2`), **EBS volume tagging**, **elastic inference**, and **Dedicated
    Hosts**. You cannot use Launch Configurations to set up Dedicated Hosts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Launch Templates are the preferred option when configuring your templates for
    EC2 instances to be launched as they provide a lot more flexibility. Next, we
    look at different scaling options to suit different use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling options
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final configuration component of your Auto Scaling service is to determine
    your scaling policy. Scaling refers to the automatic addition or termination of
    your compute capacity to meet the demands of your end users and the load on your
    application. Scaling actions are triggered by an event—for example, the average
    CPU utilization across your fleet of servers has gone above 80% for the last 20
    minutes and users will start to experience poor performance. Based on this event,
    Auto Scaling can be configured to launch one or more EC2 instances to bring the
    average CPU utilization down to below 60%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on your business use case, you have several scaling options to choose
    from, as outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Always maintain current instance levels**—Your scaling options can be configured
    to maintain the number of EC2 instances in the fleet, which is where you do not
    scale in or out; instead, AWS Auto Scaling simply replaces any failed or unhealthy
    EC2 instances to maintain the fleet size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scale manually**—You can change the minimum, maximum, and desired number
    of instances and Auto Scaling will make the necessary modifications to reflect
    your change.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scale based on schedule**—You can configure Auto Scaling to automatically
    launch new EC2 instances or terminate existing ones at predefined schedules, whereby
    you specify a date and time for the scaling action to take place. For example,
    a large payroll company could scale out the number of EC2 instances running a
    payroll application in the third week of the month when clients need to submit
    all their payroll data before a specific deadline. During other weeks, the payroll
    company can operate on a small number of instances and still offer the optimal
    client experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic scaling (scale on-demand)**—This is ideal when you are not able to
    predict demand. Dynamic scaling will be triggered when an event occurs, such as
    CPU utilization rising above a predefined threshold value for a period of time.
    Likewise, if demand drops, you can then automatically scale back in. There are
    three different forms of dynamic scaling on offer, outlined as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Target tracking scaling policy**—Auto Scaling will launch or terminate EC2
    instances in the fleet based on a target value of a specific metric. So, for example,
    if you know that average CPU utilization of 45% is ideal for the end user experience
    and anything above this threshold affects performance, then you can set your target
    tracking scaling policy for CPU utilization to 45%. If demand on your application
    increases, causing this metric to rise, then additional EC2 instances are launched.
    Likewise, if demand drops, causing the metric to fall much below 45%, Auto Scaling
    can terminate EC2 instances. You can think of a target-tracking scaling policy
    as a home thermostat where you try to maintain an ideal room temperature at home.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step scaling**—Here, an increase or decrease in capacity is based on a series
    of *step adjustments*, where the size of the breach of threshold specified determines
    the amount of scaling action.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simple scaling**—This is where capacity is increased or decreased based on
    a single scaling metric.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predictive scaling**—A more advanced form of scaling that uses **load forecasting**,
    **scheduled scaling actions**, and **maximum capacity behavior**. Maximum capacity
    behavior enables you to override the maximum number of instances in the fleet
    if the forecast capacity is higher than this maximum capacity value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we looked at the AWS Auto Scaling service, which enables you
    to automatically scale your compute resources (and other resources such as databases)
    across multiple AZs in a given Region. You can scale out as well as scale back
    in to cope with demand and ensure that you always have the right number of resources
    to offer the best end user experience. By automatically scaling back in when demand
    is low, you can also ensure effective cost management.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we move to examine how we can offer global HA and fault
    tolerance of the AWS resources that power your application.
  prefs: []
  type: TYPE_NORMAL
- en: Designing multi-Region HA solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 6*](B17124_06_Final_SK_ePub.xhtml#_idTextAnchor122), *AWS Networking
    Services – VPCs, Route53, and CloudFront*, we looked at Amazon Route 53, which
    offers DNS and traffic routing policies to help design highly available and resilient
    architectures incorporating configurations that increase performance and security
    best practices. We also looked at how Amazon CloudFront can help cache content
    locally closer to end users, which reduces latency and improves overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: While Amazon Auto Scaling and ELB services help you offer HA and scalable services
    within a given Region on their own, there is no provision for global availability
    of services. If you were to host your application in a single Region alone and
    if that Region were to fail, your end users would not be able to access your applications
    until the Region came back online and resources made available.
  prefs: []
  type: TYPE_NORMAL
- en: Services such as Route 53 and CloudFront, however, enable you to extend your
    application's availability to be even more resilient on a global scale. In this
    section, we look at one such option to offer global availability if your primary
    Region experiences a major outage and where you perhaps have a global customer
    base.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, Amazon Route 53 offers several routing policies, one of which
    is known as a failover routing policy, which helps you design an active/passive
    configuration for your application availability.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we deploy two copies of the application across two
    AZs. We then configure Amazon Route 53 with a **failover** routing policy, where
    the primary version of your site is based in the London Region and the secondary
    site is in the Sydney Region. The following diagram and associated key points
    highlight the proposed architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Route 53 configured with failover routing policy, enabling an
    active/passive solution for application architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17124_09_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.9 – Route 53 configured with failover routing policy, enabling an active/passive
    solution for application architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, we have a **primary site based in the London Region**
    with the following deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: EC2 instances are deployed as part of an Auto Scaling group. In the London Region
    (our primary site), we have a desired/minimum capacity of four EC2 instances that
    can expand to a maximum of six EC2 instances to support demand as required.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traffic is distributed via the ALB to the EC2 instances across two AZs in the
    London Region. The source of that traffic is routed via Route 53 from end users
    on the internet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Route 53 performs health checks against the primary site. Health checks are
    run against each EC2 instance via the ALB, and if the primary site is reachable,
    Route 53 continues to direct traffic to the primary site only.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there is a regional outage or if the Auto Scaling group fails to maintain
    healthy EC2 instances behind the load balancer, Route 53 marks the site as unhealthy
    and performs a failover.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During failover, Route 53 redirects all traffic to the secondary site in Sydney,
    shown in *Figure 9.9* as the dotted yellow traffic lines on the right.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While traffic is being redirected to the secondary site, which may start off
    with a minimum number of instances, Auto Scaling can scale out the number of nodes
    in the fleet to cope with demand up to the maximum specified number of instances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding example and diagram illustrate how we can combine regional services
    such as Auto Scaling and ALBs along with global services such as Route 53 to design
    a highly available and resilient application architecture.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about designing application solutions that offer
    multi-regional HA options using both ELBs and Route 53 services specifically.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we examine a series of hands-on exercises that will help
    you configure the various services you have learned about so far, incorporating
    IAM, **Simple Storage Service** (**S3**), VPCs, EC2, RDS, ELBs, and Auto Scaling,
    to build a two-tier application solution. This two-tier application solution will
    comprise a web/application tier and a backend database tier.
  prefs: []
  type: TYPE_NORMAL
- en: 'To complete the upcoming exercises, it is vital that you have completed all
    previous exercises in all the previous chapters. Furthermore, to complete the
    exercise, you will need to access the source code files of the application, which
    are available at the *Packt Publishing* GitHub repository: [https://github.com/PacktPublishing/AWS-Certified-Cloud-Practitioner-Exam-Guide](https://github.com/PacktPublishing/AWS-Certified-Cloud-Practitioner-Exam-Guide)'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, as you carry out each exercise, you will be provided with some background
    details to help you appreciate the architecture and reasons behind the deployment.
    All exercises need to be done in the `us-east-1` Region, which is where you have
    already built your **production VPC** and host your **RDS database**.
  prefs: []
  type: TYPE_NORMAL
- en: For all exercises, ensure that you are logged in to your AWS account as our
    IAM user **Alice**.
  prefs: []
  type: TYPE_NORMAL
- en: Extended exercises – setting the scene
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The upcoming exercises are based on the following scenario. You work for a fictitious
    company called **The Vegan Studio**. The company is in the hospitality industry.
    Specifically, the company runs a chain of cafes and restaurants across the **United
    States** (**US**), serving only vegan dishes for those looking to indulge in meat-free
    cuisine. The company employs over 4,000 employees across its business, and keeping
    everyone engaged and feeling part of a large family is something the business
    takes great pride in.
  prefs: []
  type: TYPE_NORMAL
- en: Every year, they run several contests for their employees to participate in.
    This year, they are running a *My Good Deed for the Month* contest. A web application
    has been designed by one of the developers, which you need to now deploy in a
    highly available and scalable manner on AWS in the `us-east-1` Region. The contest
    will run for a month and all employees are encouraged to submit a statement of
    any good deeds they carried out. Five winners will be chosen from a list of entries
    and awarded a special hamper prize. Participants must back up their good deeds
    with evidence if requested (just to be sure no one is fooling around)!
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will walk you through a series of exercises to deploy the application
    designed by your developer with HA and scalability features.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Some AWS services, such as ELBs and **Network Address Translation** (**NAT**)
    gateways, are chargeable. We suggest you complete all the exercises in reasonably
    quick succession and then perform the cleanup exercise at the end. Overall, the
    cost should not be more than $5\. To ensure costs are kept to a minimum, we will
    not be configuring the RDS database you deployed in the previous chapter with
    Multi-AZ.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following exercises will make use of a multi-tier application design that
    will be deployed in the **production VPC** that you already built in the previous
    chapters. Recall that the VPC comprises both public and private subnets, spanning
    across two AZs. From the previous chapters, you have already built a foundation
    architecture as per the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Production VPC architecture prior to deploying the "Good Deed
    of the Month" contest application'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17124_09_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.10 – Production VPC architecture prior to deploying the "Good Deed
    of the Month" contest application
  prefs: []
  type: TYPE_NORMAL
- en: 'As per the previous diagram, your current architecture is comprised of the
    following key AWS services and resources:'
  prefs: []
  type: TYPE_NORMAL
- en: A VPC created in the `us-east-1` Region with public and private subnets across
    two AZs. The private subnets have been designed to support a two-tier application
    solution comprising a web/application tier and a database tier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The public subnets will normally be used to deploy bastion hosts for remote
    administration and NAT gateways. For the upcoming series of exercises, we will
    not be deploying any bastion hosts as this is not required for the labs in these
    exercises. However, we will amend the bastion host security group to allow inbound
    `22`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application tier private subnets do not currently have any EC2 instances
    deployed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The database tier private subnets currently host a single instance MySQL RDS
    database in the `us-east-1a` AZ.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through the upcoming exercises in this chapter, we will build on the architecture
    to design a fully functional application solution with HA and scalability features.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 9.1 – setting up an Amazon S3 bucket to host source files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will first create an Amazon S3 bucket that will be used
    to host your source files for your application. You need to first download the
    source file, which is available in a ZIP folder format, and extract its contents
    into a new folder or onto the desktop of your computer for easy access.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to prepare your source code files. Your source code files
    contain a database connection file that will need to be amended to the specific
    RDS database you configured in the previous chapter. Follow these next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Once you unzip the downloaded folder, you can see the contents of the main `vegan-php-files`
    folder, as per the following screenshot:![Figure 9.11 – vegan-php-files source
    code
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17124_09_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.11 – vegan-php-files source code
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You will note that in the `v5` directory, there is a file called `db` that
    is a **PHP: Hypertext Preprocessor** (**PHP**) file. This file contains default
    database connection string details that you will first need to amend before you
    upload the source code to your S3 bucket. Specifically, you will need to provide
    the RDS database connection details, which include the RDS endpoint DNS name,
    master username, password, and database name. *Recall that you made a note of
    these values in the last chapter*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In a notepad or text editor tool, open the `db.php` file from the `v5` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Within the PHP file, you will need to edit the values of the placeholders with
    the appropriate database connection values. In the following screenshot, you will
    see the placeholders:![Figure 9.12 – db.php file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17124_09_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.12 – db.php file
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You will need to replace the placeholders with the connection details to your
    database, making sure to place all the values within single quotation marks. Do
    not make any other changes to the code. Here is a screenshot of where you can
    obtain these values after you create your database. The database endpoint is visible
    on the main **Connectivity & security** tab, and you will find the username and
    database name in the **Configuration** tab. Note that the password is not visible,
    as you should have made a note of it when launching the database instance:![Figure
    9.13 – Amazon RDS database settings
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17124_09_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.13 – Amazon RDS database settings
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Save the file in its original location.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in to your AWS Management Console and navigate to the Amazon S3 dashboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the left-hand menu, click **Buckets**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Create bucket** in the right-hand pane of the dashboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the `vegan-good-deed`. Ensure that the Region selected is the `us-east-1`
    Region.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll to the bottom of the page, leaving all settings at their default values,
    and click the **Create bucket** button. Your Amazon S3 bucket will be created,
    and you will be redirected back to the list of available buckets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the bucket you just created, and you will be redirected to the **Objects**
    listing page, where you will note that there are currently no objects, as per
    the following screenshot:![Figure 9.14 – New bucket creation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17124_09_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.14 – New bucket creation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click the **Upload** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, you want to try to resize your browser page with the S3 bucket `vegan-php-files`
    folder so that you can easily drag and drop all the folders and files into the
    S3 bucket's **Object** area, as per the following screenshot. You need to ensure
    that the folder hierarchy is maintained for the application to work:![Figure 9.15
    – Copying files and folders to the S3 bucket
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17124_09_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.15 – Copying files and folders to the S3 bucket
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Your Amazon S3 **Upload** page will provide a summary of files and folders to
    be uploaded. You will need to then click on the **Upload** button at the bottom
    of the page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once all the files and folders have been uploaded, you receive an **Upload succeeded**
    message.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that your source code and files for your application have been uploaded,
    we will move on to the next exercise. As part of this series of exercises, you
    will need to configure your EC2 instances to download the source code files for
    the application. Using Bash scripts at the time of launching your EC2 instances,
    you will download the source code from the Amazon S3 bucket and place it in the
    appropriate folders within the EC2 instances to serve the application.
  prefs: []
  type: TYPE_NORMAL
- en: Because your EC2 instance would need to have permissions to access the previous
    S3 bucket we created and download the source code, we need to configure an IAM
    role that your EC2 instance will use to authenticate to Amazon S3.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 9.2 – creating an IAM role
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, you will create an IAM role that your EC2 instances will
    use to authenticate and access the source code files in your Amazon S3 bucket.
    Proceed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that you are logged in to your AWS account and navigate to the IAM dashboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Roles** from the left-hand menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Create role**, as per the following screenshot:![Figure 9.16 – Creating
    an IAM role
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17124_09_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.16 – Creating an IAM role
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the **Select type of trusted entity** field, click the **AWS services** option,
    and under **Choose a use case**, select **EC2** under **Common use cases**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click the **Next: Permissions** button at the bottom of the page.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the `S3`. Next, select the `AmazonS3ReadOnlyAccess` policy and click the
    **Next: Tags** button, as per the following screenshot:![Figure 9.17 – Creating
    an IAM role (continued)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17124_09_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.17 – Creating an IAM role (continued)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Set a key-value pair to tag your role with a key of `EC2-to-S3-Read-Access`.
    This allows us to easily identify the role. Click the **Next: Review** button
    on the bottom right-hand corner of the page.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `EC2-to-S3-Read-Access`, and a description.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, click the **Create role** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AWS will now create your IAM role, and you will need to reference this role
    when we launch our EC2 instances in a later chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this stage, you have the following AWS services configured:'
  prefs: []
  type: TYPE_NORMAL
- en: An Amazon VPC with public and private subnets across two AZs. You have a public
    subnet to host bastion hosts and NAT gateways, and four private subnets—two for
    your web/application servers located in the **web/application tier** and another
    two for your **database tier**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An RDS database that will store all data such as the *good deeds of the month*
    that the employees of The Vegan Studio will submit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Amazon S3 bucket with the source code files configured to point to the RDS
    database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An IAM role to allow your EC2 instances to download the source code files from
    the S3 bucket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you deploy your application, you will install Apache Web Services and host
    your application files on EC2 instances. Specifically, you will be deploying two
    EC2 instances that will be placed across two AZs. To distribute traffic across
    those EC2 instances, you will need to configure an ALB that will be configured
    to accept inbound HTTP (port `80`) traffic from the internet and distribute them
    to your EC2 instances. In the next exercise, you will need to configure your ALB.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 9.3 – configuring an ALB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will be configuring an ALB that will be used to accept
    inbound traffic from your users on the internet and distribute them across the
    EC2 instances you deploy later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: ALBs, as discussed earlier in this chapter, can be used to distribute web and
    application traffic using HTTP and HTTPS protocols. You will configure an internet-facing
    load balancer so that you can accept inbound requests from the internet.
  prefs: []
  type: TYPE_NORMAL
- en: 'CLBs and ALBs require you to also configure a security group within which you
    define which traffic would be permitted inbound to those load balancers. Therefore,
    the first step is to revisit the VPC dashboard and create a new security group
    for your ALB, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the VPC dashboard and ensure you are still in the `us-east-1` Region.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the left-hand menu, click on **Security Groups**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Create security group** button in the top right-hand corner of the
    screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide the security group with a name such as `ALB-SG` and a description such
    as `Allow inbound HTTP traffic from Internet`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure that you select **ProductionVPC** from the VPC drop-down list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Add rule** button under **Inbound rules**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For `0.0.0.0/0`. This source denotes the public internet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide an optional description and then click on the **Create security group**
    button on the bottom right-hand corner of the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AWS will now create your security group, which we will use to configure our
    ALB.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: ELBs do not fall under the Free Tier offering from AWS, and you must ensure
    you delete them once you have completed all the labs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have configured a security group, we can move on to configuring
    our load balancing service. However, your ALB requires a **target group** to send
    traffic to. The target group will be used to register the EC2 instances that will
    accept traffic from the ALB. So, the first step is to create your target group,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the EC2 dashboard and ensure that you are in the `us-east-1` Region.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the left-hand menu, click on **Target Groups**, under the **Load Balancing**
    menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the right-hand pane, click on **Create target group**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, you are presented with a two-step wizard. In *Step 1*, select `Production-TG`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under `80`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, under `Production-VPC`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll further down till you reach the **Health checks** section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, set the **Health check protocol** to **HTTP**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the `/health.html` as per the following screenshot:![Figure 9.18 – Load
    balancer target group health checks
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17124_09_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.18 – Load balancer target group health checks
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, expand the **Advanced health check settings** field.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the **Port** value to **Traffic Port**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the `3`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the `2`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, set the `2`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, set the `10` seconds.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Next** button at the bottom of the page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This will take you to *Step 2*, where you would normally register any EC2 instances.
    However, as we have not launched any EC2 instances yet, you can ignore this step
    and simply click on the **Create target group** button at the bottom of the page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, now that we have the **target group** configured, we can launch our ALB.
    ELBs are configured in the EC2 management console or via the **command-line interface**
    (**CLI**).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the left-hand menu, click on **Load Balancers** under the **Load Balancing**
    category.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, click on the **Create Load Balancer** button at the top of the screen
    in the right-hand pane.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Create** button in the **Application Load Balancer** section
    of the page, as per the following screenshot:![Figure 9.19 – Selecting Application
    Load Balancer as load balancer type
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17124_09_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.19 – Selecting Application Load Balancer as load balancer type
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In `Production-ALB`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure that the `Production-VPC` under the `us-east-1a` and `us-east-1b` AZs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `us-east-1a` AZ, select the `us-east-1b` AZ, select the **Public Subnet
    Two** subnet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'AWS will then deploy the ALB *nodes* in these public subnets, routing incoming
    traffic from the internet to the EC2 instances in the private subnets that we
    registered as targets for the load balancer. Internet-facing load balancers should
    be created in subnets that have been configured with an internet gateway such
    as in this case: the public subnets of your VPC.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, under `ALB-SG` security group from the drop-down list. You can also delete
    the `ALB-SG` security group.*   In the `80`. Next, under `Production-TG` target
    group you created earlier from the drop-down list.*   Finally, scroll further
    down and click on the **Create load balancer** button.*   You will receive a confirmation
    message stating that the load balancer has been created successfully. Click on
    `Production-ALB` load balancer in the list. After a few moments, the status of
    the load balancer should change from **Provisioning** to **Active**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At this point, we have now configured our ALB. Let''s go ahead and look at
    our architectural diagram to see how our configuration is coming along:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.20 – Production VPC architecture after configuring S3 bucket, IAM
    role, and ALB'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17124_09_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.20 – Production VPC architecture after configuring S3 bucket, IAM role,
    and ALB
  prefs: []
  type: TYPE_NORMAL
- en: For traffic to be allowed inbound to the application servers, we need to ensure
    that the security groups associated with those servers have been correctly configured.
    Specifically, the `AppServers-SG` security group must allow traffic on the HTTP
    protocol (port `80`) from the ALB we deployed in the previous exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, in [*Chapter 7*](B17124_07_Final_SK_ePub.xhtml#_idTextAnchor157),
    *AWS Compute Services*, you configured the `AppServers-SG` security group to accept
    traffic from the `BastionHost-SG` security group. This was to enable inbound traffic
    on `3389`), which enables the Windows Remote Desktop client to perform remote
    access operations. Although we will not be deploying any bastion hosts in the
    remaining exercises in this chapter, we will amend the inbound rule on the `AppServers-SG`
    security group such that the protocol and port used to accept traffic from the
    `BastionHost-SG` security group will be set to the SSH protocol on port `22`.
    This is because we will be deploying Linux EC2 instances to host our application,
    and any remote management of Linux servers requires you to configure SSH access.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 9.4 – amending the Production-VPC security group
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will amend the RDP inbound rule in the `AppServers-SG`
    security group such that it is configured to accept traffic on the SSH protocol
    (port `22`) from the `BastionHost-SG` security group. Next, we will add a new
    rule to accept traffic on the HTTP protocol (port `80`) from the ALB's security
    group, `ALB-SG`. Finally, we will amend the `BastionHost-SG` security group such
    that it is configured to accept traffic on the SSH protocol (port `22`) from the
    internet. This is useful if you later wish to perform any remote administration
    of your Linux servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Amend the `BastionHost-SG` security group, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the VPC dashboard and ensure that you are in the `us-east-1` Region.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the left-hand menu, click on **Security Groups**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the middle pane, select the checkbox next to the `BastionHost-SG` security
    group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the pane below, click on **Inbound rules** and then click on the **Edit inbound
    rules** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, delete the existing **RDP** rule by clicking on the **Delete** button
    on the far right of the page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Add rule** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the type, select `0.0.0.0/0`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, click on the `AppServers-SG` security group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Security Groups** link from the left-hand menu again to see all
    your security groups in the VPC.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the middle pane, select the checkbox next to the `AppServers-SG` security
    group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the pane below, click on **Inbound rules** and then click on the **Edit inbound
    rules** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, delete the existing **RDP** rule by clicking on the **Delete** button
    on the far right of the page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Add rule** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the type, select `sg-`. You will notice that a list of your security groups
    will become visible. Select the `BastionHost-SG` security group from this list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, click the **Add rule** button again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the type, select `sg-`. You will notice that a list of your security groups
    will become visible. This time, select the `ALB-SG` security group from the list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, click on the **Save rules** button in the bottom right-hand corner
    of the page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will not need to amend the `Database-SG` security group because this has
    already been configured to only accept traffic from the `AppServers-SG` security
    group using the MySQL port `3306`.
  prefs: []
  type: TYPE_NORMAL
- en: Recall from the architectural diagram in *Figure 9.20* that the web/application
    EC2 instances are going to be placed in a private subnet. Our The Vegan Studio
    employees will be able to access the *Good Deed of the Month* contest application
    on those EC2 instances via the ALB. However, the EC2 instances will need access
    to the internet to download updates as well as the source code files stored on
    the Amazon S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that, unlike the public subnet, the private subnet does not grant direct
    access to the internet. Any EC2 instance in the private subnet would need to direct
    internet-bound traffic via an AWS NAT gateway, as discussed in [*Chapter 6*](B17124_06_Final_SK_ePub.xhtml#_idTextAnchor122),
    *AWS Networking Services – VPCs, Route53, and CloudFront*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next exercise, we will deploy a NAT gateway.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 9.5 – deploying a NAT gateway
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will deploy a NAT gateway in the **Public Subnet One**
    subnet of our production VPC. Ideally, you want to deploy multiple NAT gateways
    in each public subnet across the AZs you have resources in to avoid a **single
    point of failure** (**SPOF**). However, for the purposes of this lab, we will
    use a single NAT gateway.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, you will need to configure your **main route table** with a new
    route that will allow outbound traffic to the internet via this NAT gateway.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start this exercise by first allocating an elastic IP address for our
    AWS account, which is a requirement to configure a NAT gateway. To do this, follow
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to your VPC dashboard and ensure that you are in the `us-east-1` Region.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: NAT gateways require an elastic IP address, and so you will need to allocate
    one first to your AWS account. From the left-hand menu, click on **Elastic IPs**.
    In the right-hand pane, click the **Allocate Elastic IP address** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will be presented with the **Allocate Elastic IP address** page. Ensure
    that **Amazon's pool of IPv4 addresses** is selected and then click the **Allocate**
    button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'AWS will allocate an elastic IP address from its pool of available addresses
    for your AWS account. Next, you will need to set up your NAT gateway, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: From the left-hand menu, click on **NAT Gateways**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the right-hand pane, click the **Create NAT gateway** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the `Production-NAT`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, from the **Subnet** drop-down list, select the **Public Subnet One** subnet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, from the drop-down list under **Elastic IP allocation ID**, select the
    elastic IP you allocated to your account moments ago.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, click the **Create NAT gateway** button at the bottom of the page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The NAT gateway will take a couple of minutes to be provisioned. Once ready,
    the NAT gateway state will be set to **Available**, as per the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.21 – NAT gateway'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17124_09_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.21 – NAT gateway
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have deployed your NAT gateway, you will need to configure your
    main route table with a route to the internet that uses the NAT gateway, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: From the left-hand menu in the VPC dashboard, select **Route Tables**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the checkbox next to **Main Route Table**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the bottom pane, click on the **Routes** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, click the **Edit routes** button on the far right-hand side of the page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will be presented with the **Edit routes** page. Click the **Add route**
    button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under the `0.0.0.0/0`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, click on the `Production-NAT` NAT gateway in the list. Go ahead and select
    this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, click on the **Save changes** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Your main route table has now been configured with a route to the internet that
    will use the NAT gateway.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have configured your NAT gateway and the main route table correctly,
    we can proceed with deploying our EC2 instances that will host the *Good Deed
    of the Month* application in the next exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 9.6 – deploying your application servers with Amazon Auto Scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will configure the Amazon Auto Scaling service to define
    a **Launch Configuration** for our deployment, which will include a script to
    configure our EC2 instances with the Apache web service and download the application
    source files from the Amazon S3 bucket. As part of the exercise, you will also
    create an EC2 instance profile that will be used to contain the IAM role you created
    earlier and allow the EC2 instance to assume that role.
  prefs: []
  type: TYPE_NORMAL
- en: The EC2 instances will also be provisioned as targets in the `Production-TG`
    target group we created earlier in the ALB exercise. The `Production-ALB` ALB
    will then be able to distribute inbound traffic from our The Vegan Studio employees
    on the internet to those EC2 instances, enabling them to submit any good deeds
    they carried out for review by our panel.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we will configure Auto Scaling policies to always ensure that we
    always have two running EC2 instances, one in each private subnet, across the
    two AZs, `us-east-1a` and `us-east-1b`. In terms of health checks, these will
    be performed both at the EC2 level and via the ALBs using the health check parameters
    you defined in *Exercise 9.3* earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Auto Scaling Launch Configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As part of this exercise, you will need access to a Bash script that we have
    included in the GitHub repository [https://github.com/PacktPublishing/AWS-Certified-Cloud-Practitioner-Exam-Guide](https://github.com/PacktPublishing/AWS-Certified-Cloud-Practitioner-Exam-Guide).
    In the `vegan-php-files.zip` file you downloaded earlier, which you unzipped,
    you will find a file called `userdata-script` in the top-level folder. You will
    need to amend this script to match your configuration. Open the script file in
    a notepad or text editor application and change the last line of the script, replacing
    `[Source Bucket]` with the actual name of your bucket. So, for example, if your
    bucket name is `vegan-good-deed`, then the last line should be changed from `aws
    s3 cp s3://[Source Bucket] /var/www/html –recursive` to `aws s3 cp s3://vegan-good-deed
    /var/www/html –recursive`. Make sure to save the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we look at the steps required to set up our AWS Auto Scaling Launch Configuration,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the EC2 dashboard and ensure that you are in the `us-east-1` Region.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the left-hand menu, select **Launch Configurations** from the **Auto Scaling**
    category.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the right-hand pane, click on the **Create Launch Configuration** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will be presented with the **Create Launch Configuration** page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide a name for your Launch Configuration—for example, `Production-LC`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, you need to search for the Amazon Linux 2 AMI. It might be difficult to
    find the AMI in the new `64-bit x86` architecture, which I have highlighted as
    per the following screenshot:![Figure 9.22 – AMI ID for Amazon Linux 2 instance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17124_09_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.22 – AMI ID for Amazon Linux 2 instance
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Back in the previous browser window where you are configuring your Auto Scaling
    Launch Configuration, click on the drop-down arrow under **AMI** and paste in
    the AMI ID you copied previously in the search box. You should then be able to
    find the relevant AMI to use. Make sure you select this AMI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, under `t2.micro`. You can then select the `t2.micro` instance type from
    the filtered list. Go ahead and click the **Choose** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, under the `EC2-to-S3-Read-Access` instance profile that contains the IAM
    role you created earlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, expand the **Advanced details** section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under **User Data**, ensure that **As Text** is the selected option and, in
    the textbox provided, go ahead and paste in a copy of the Bash script file you
    amended a few moments ago.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, under **IP address type**, ensure that you select **Do not assign a public
    IP address to any instances**. This is because the EC2 instances are going to
    be launched in the private subnets and will not require a public IP address.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Leave the settings in the **Storage (volumes)** field at their default values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, under `AppServers-SG` security group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Key pair (login)** section, select **Choose an existing key pair**
    from the **Key pair options** drop-down list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the drop-down list under `USEC2Keys` key pair.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, tick the box to acknowledge that you have access to the private key file
    that you downloaded earlier in [*Chapter 7*](B17124_07_Final_SK_ePub.xhtml#_idTextAnchor157),
    *AWS Compute Services*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, click on the **Create Launch Configuration** button at the bottom of
    the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At this point, you have successfully created your first Auto Scaling Launch
    Configuration, as per the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.23 – Amazon Auto Scaling Launch Configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17124_09_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.23 – Amazon Auto Scaling Launch Configuration
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have created your Launch Configuration, you can proceed to configure
    your Auto Scaling groups.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Auto Scaling groups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As part of creating your Auto Scaling groups, you can define Auto Scaling policies.
    Because we will not be performing any real load testing on our application servers,
    we will simply configure our Auto Scaling policy to ensure that we always have
    a minimum of two EC2 instances across the two AZs. Proceed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: From the left-hand menu of the EC2 dashboard, click on the **Auto Scaling Groups**
    link under **Auto Scaling**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Create an Auto Scaling group** button in the right-hand pane of
    the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In `Production-ASG`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next section of the screen, you will have an option to select a Launch
    Template from a drop-down list. However, instead of a Launch Template, we have
    configured a Launch Configuration. To access your Launch Configuration, click
    on the **Switch to Launch Configuration** link on the far right-hand side of the
    screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, under `Production-LC` Launch Configuration you created earlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Next** button to move on to *Step 2*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In `Production-VPC` from the **VPC** drop-down list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the drop-down list under **Subnets**, ensure that you select both the **Private
    Subnet One - App** and **Private Subnet Two - App** subnets, as per the following
    screenshot:![Figure 9.24 – Auto Scaling group subnet selection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17124_09_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.24 – Auto Scaling group subnet selection
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click the **Next** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In **Step 3, Load balancing – optional**, we will be using the ALB you created
    earlier. Select the **Attach to an existing load balancer** option.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, ensure that **Choose from your load balancer target groups** is selected
    under the **Attach to an existing load balancer** section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the drop-down list under `Production-TG` target group that is associated
    with the `Production-ALB` ALB.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, under the **Health checks – optional** section, select the **ELB** checkbox.
    This is to enable ELB health checks in addition to the EC2 health checks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Next** button at the bottom of the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In `2` each. We want to always maintain two EC2 instances in our fleet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under **Scaling policies – optional**, ensure that **None** is selected, and
    then click the **Next** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In **Step 5, Add notifications**, do not add any notifications and click on
    the **Next** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In `Name` and the `Production-Servers`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Next** button to continue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You are then presented with a **Review** page. Review the configuration settings
    that you have defined to make sure you followed the preceding series of steps
    correctly. When you are satisfied, go ahead and click the **Create Auto Scaling
    group** button at the bottom of the page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AWS will then start configuring your Auto Scaling group and proceed to launch
    two EC2 instances based on the parameters of the groups. The EC2 instances will
    be configured as per the configuration you defined in the Launch Configuration
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once Auto Scaling has completed the deployment of your EC2 instances, you will
    be able to see the details of your deployment, as per the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.25 – Auto Scaling group deployment completed'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17124_09_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.25 – Auto Scaling group deployment completed
  prefs: []
  type: TYPE_NORMAL
- en: At this point, your application has now been deployed across two EC2 instances.
    If you click on the **Activity** tab, you will see the AWS Auto Scaling service
    launched two EC2 instances in response to the fact that the minimum and desired
    capacity were not met before the launch of those EC2 instances. The Auto Scaling
    service will always try to ensure you have the desired number of EC2 instances
    in your fleet. Next, we will review the deployment and access the application.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing your deployment and accessing your application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can check whether the Auto Scaling service has correctly deployed your application.
    Specifically, you can check whether two EC2 instances have been deployed and registered
    with your ALB. Furthermore, you can also check whether the ALB has marked those
    EC2 instances as healthy, indicating that the health checks have passed as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps to perform these checks and then access the application:'
  prefs: []
  type: TYPE_NORMAL
- en: In the EC2 dashboard, click on the **Target Groups** link from the left-hand
    menu under **Load Balancing**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the right-hand pane, click on the `Production-TG` target group that you created
    earlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the details page of the `Production-TG` target group, you will note that
    two EC2 instances have been launched and both are in a **healthy** state, as per
    the following screenshot:![Figure 9.26 – Healthy EC2 instances registered to load
    balancer target group
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17124_09_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.26 – Healthy EC2 instances registered to load balancer target group
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, click on the **Instances** link from the left-hand menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will notice that two instances with the name `Production-Servers` have been
    launched, with one EC2 instance in the `us-east-1a` AZ and the other in the `us-east-1b`
    AZ, as per the following screenshot:![Figure 9.27 – Auto Scaling group successfully
    launched two EC2 instances
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17124_09_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.27 – Auto Scaling group successfully launched two EC2 instances
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we can access our application. From the left-hand menu, click on the **Load
    Balancers** link under **Load Balancing**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the right-hand pane, you will find your ALB details, as per the following
    screenshot:![Figure 9.28 – ALB details for Production-ALB
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17124_09_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.28 – ALB details for Production-ALB
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the bottom pane, you will find a **DNS name** link for your ALB. Copy this
    URL and paste it into a new browser window. If you have successfully completed
    all of the previous exercises, you will be able to access the **Good Deed of the
    Month Contest** web application, as per the following screenshot:![Figure 9.29
    – Good Deed of the Month Contest web application
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17124_09_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.29 – Good Deed of the Month Contest web application
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can test your application by entering details of some potential good deeds
    you have done yourself. Once you have filled in the form in the middle of the
    web page, click the **SUBMIT** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will note that when the web page reloads after you click on the **Submit**
    button, your *good deed of the month* is read back from the MySQL RDS database
    and presented on the page. If you submit more entries, these are also reported
    back. This demonstrates how the application can write to and read from the backend
    RDS database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s take another look at the application architectural diagram to see how
    you have built this multi-tier solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.30 – Multi-tier application architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17124_09_30.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.30 – Multi-tier application architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'The key components of your architecture include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A VPC in the `us-east-1` Region that consists of public and private subnets
    across the `us-east-1a` and `us-east-1b` AZs. The VPC consists of six subnets
    in total: two public subnets to host your ALB nodes and NAT gateway, two private
    subnets to host your application tier, and another two private subnets to host
    the database tier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You also have an Amazon S3 bucket to host all the application source code and
    files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An IAM role that will allow your EC2 instances to gain authorization to read
    and download the application source code from the S3 bucket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An RDS database deployed in the `us-east-1a` AZ as a single database instance.
    Ideally, you would want to configure your database with Multi-AZ for HA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two EC2 instances deployed using an Auto Scaling group and Launch Configuration.
    The Launch Configuration contains the necessary **bootstrapping** script to set
    up and configure the EC2 instances as web servers and automatically serve the
    application to your users. Furthermore, the Auto Scaling group automatically registers
    any EC2 instances deployed to the ALB target group. The target group runs health
    checks against the EC2 instances, marking them as healthy or unhealthy based on
    the health checks defined.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the application code has the necessary database connection to access
    the backend RDS database and store any application data. Note that storing database
    connection details within the application is not considered best practice, and
    AWS offers several options such as the **AWS Systems Manager Parameter Store**
    or **AWS Secrets Manager** to manage such pieces of sensitive data. To keep this
    lab simple, we stored the database connection details within the application code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we look at how to test the AWS Auto Scaling service by simulating a failure
    of an EC2 instance.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the Auto Scaling service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this part of the exercise, you will stop an EC2 instance to simulate failure.
    When the EC2 instance is in a stopped state, it will not respond to the load balancer
    health checks. The load balancer will then mark the EC2 instances as unavailable.
    This will send a notification to the Auto Scaling group, confirming that there
    are fewer than two EC2 instances in the group, which is less than the desired
    capacity. Auto Scaling should then replace the instance and new server. Let''s
    proceed with simulating this failure of an EC2 instance, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In the EC2 dashboard, click on the **Instances** link from the left-hand menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, in the right-hand pane of the screen, you will note that you have two
    EC2 instances running. Select the instance that is in the `us-east-1b` AZ, as
    per the following screenshot:![Figure 9.31 – EC2 instances in Running state
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17124_09_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.31 – EC2 instances in Running state
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the `us-east-1b` AZ is selected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will be prompted with a dialog box to confirm whether you want to stop the
    selected instance. Go ahead and click the **Stop** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AWS will then perform a shutdown of your EC2 instance, which will take a couple
    of minutes. Wait until the EC2 instance is in a **Stopped** state, and then proceed
    to click on the **Auto Scaling Group** link under the **Auto Scaling** category
    from the left-hand menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, in the right-hand pane, click the `Production-ASG` Auto Scaling group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, click on the **Activity** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will find additional activities that clearly show that the Auto Scaling
    service terminated the stopped EC2 instance. This is because, in a stopped state,
    it cannot respond to health checks. This is then followed by the launch of a new
    EC2 instance to replace the one that got terminated (see the following screenshot),
    in order to maintain our desired capacity at two instances as per the Auto Scaling
    group configuration. You will note that the Auto Scaling service will not try
    to restart the stopped instance. The Auto Scaling group will use the same Launch
    Configuration to configure the server with the application and register it to
    the ALB's target group, as illustrated here:![Figure 9.32 – Auto Scaling activity
    history
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17124_09_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.32 – Auto Scaling activity history
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that the Auto Scaling service has replaced your EC2 instance, you can visit
    your application via the ALB URL to confirm that your application is still functioning
    as expected. Note that when one of the EC2 instances was stopped, the application
    was still accessible via the ALB URL because traffic would have been forwarded
    onto the other EC2 instance that was still running in the `us-east-1a` AZ.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Congratulations! Well done on completing the series of exercises to get to this
    stage. You have now learned how to design and architect a multi-tier application
    solution using a combination of AWS services to help you build an HA and scalable
    application.
  prefs: []
  type: TYPE_NORMAL
- en: In the next exercise, you will perform a cleanup operation to terminate unwanted
    resources so that you do not incur any further charges.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 9.7 – cleanup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, you will terminate the various resources you deployed in
    the previous exercises. The first step is to delete the Auto Scaling group, which
    will terminate your EC2 instances. If you try terminating the EC2 instances manually,
    then the Auto Scaling group will simply launch new ones. Proceed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: From the EC2 dashboard, click on **Auto Scaling Group** from the left-hand menu
    under **Auto Scaling**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the right-hand pane, select the `Production-ASG` Auto Scaling group. Click
    the `delete` in the textbox and clicking the **Delete** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, click on **Launch Configuration** from the left-hand menu under the **Auto
    Scaling** service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, select the `Production-LC` Launch Configuration, and from the **Actions**
    menu, click **Delete Launch Configuration**. Confirm the delete request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, click **Load Balancers** under the **Load Balancing** menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the right-hand pane, select the `Production-ALB` load balancer, and from
    the **Actions** drop-down list, click **Delete**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, click on `Production-TG` target group, and from the **Actions** drop-down
    list, click **Delete** and delete the target group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Your load balancer and the Auto Scaling group have been removed from your account.
    Next, navigate to the Amazon RDS console, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: From the left-hand menu, click on **Databases**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the right-hand pane, select the database that you created earlier in [*Chapter
    8*](B17124_08_Final_SK_ePub.xhtml#_idTextAnchor189), *AWS Databases Services*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the **Actions** drop-down list, click **Delete**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Uncheck the `delete me` in the confirmation textbox and click the **Delete**
    button. Your Amazon RDS database will now be deleted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we should also remove the database subnet group created previously. From
    the left-hand menu, click on **Subnet groups**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the right-hand pane, select the database subnet group you created previously
    and click the **Delete** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that your database has also been deleted, we can delete the VPC. Navigate
    to the VPC console.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Before we can delete the VPC, you need to delete the NAT gateway. From the left-hand
    menu, click on `Production-NAT` NAT gateway, and from the `delete` in the confirmation
    box and click the **Delete** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, you need to check whether there are any network interfaces still attached
    to your VPC. Usually, you will find that the Amazon RDS network interface (`RDSNetworkInterface`)
    may still be attached to the `Database-SG` security group. If that is the case,
    you will first need to delete this interface before you can delete the VPC, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to the EC2 console and select `Production-VPC` security group ID.
    Select the network interface and then, from the **Actions** menu, click the **Delete**
    button. The following screenshot shows the attached network interface:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.33 – Network interfaces attached to your VPC'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17124_09_33.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.33 – Network interfaces attached to your VPC
  prefs: []
  type: TYPE_NORMAL
- en: Navigate back to the VPC console. Next, from the left-hand menu, click on **Your
    VPCs**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the right-hand pane, select the `Production-VPC` security group, and then,
    from the **Actions** drop-down list, select **Delete VPC**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will be presented with a list of all components of your VPC that will be
    deleted. Confirm your delete request by typing `delete` into the confirmation
    textbox and then clicking the **Delete** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Your VPC should now get deleted. Within the VPC console, there is still one
    more component you need to delete, and that is the elastic IP address you allocated
    to your AWS account. This is because elastic IP addresses are only free if they
    are associated with running instances (or in our case, the NAT gateway). Proceed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: From the left-hand menu, click on **Elastic IPs**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the right-hand pane, select the IP address you allocated to your AWS account,
    and from the **Actions** drop-down list, click the **Release Elastic IP addresses**
    link. Next, in the **Release Elastic IP addresses** dialog box, click the **Release**
    button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, your elastic IP address has been released back to AWS. You will
    not incur any charges on unused elastic IP addresses in your account.
  prefs: []
  type: TYPE_NORMAL
- en: This completes your cleanup exercise for this chapter, and you can now rest
    assured that you will not incur any further costs associated with this lab.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: You still have an Amazon S3 bucket that hosts all the source code for the application
    you deployed in this chapter. While you could delete that resource, we advise
    you to keep the bucket as we will be using it for the exercises in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we provide a summary of this chapter and the key concepts to remember
    for the exam.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the differences between vertical scaling
    and horizontal scaling. We discussed options to increase an EC2 instance's specification
    and capacity. We then examined the AWS ELB service and how it can be used to evenly
    distribute incoming application traffic across a fleet of EC2 instances. You learned
    about the different types of ELBs and their use cases—ALBs, NLBs, GWLBs, and CLBs.
    We discussed how, using ELB, you can distribute the placement of EC2 instances
    that power your application across multiple AZs, thereby offering HA of services
    in case of AZ failures or outages.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we examined how we can automatically scale out (add more EC2 instances
    to our fleet of servers that support an application) using the Amazon Auto Scaling
    service. Auto Scaling can help us scale out when demand increases and equally
    scale back in when demand drops, ensuring that you always have the right number
    of EC2 instances to provide the best **user experience** (**UX**) for your application.
  prefs: []
  type: TYPE_NORMAL
- en: Both ELB and Auto Scaling, however, are Regional services only. This means using
    these two services alone cannot offer global resilience. To offer HA across Regions,
    we discussed how we can use Route 53 and other global services such as CloudFront.
    Route 53 offers several routing policies to make it possible to distribute traffic
    to application servers spread across the globe and offers options to build complete
    **disaster recovery** (**DR**) and business continuity solutions for your business.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we look at a number of AWS services designed to help you
    build applications on AWS that move away from traditional monolith architectures
    in favor of modern decoupled architecture and microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You are planning on developing a website in multiple languages such that you
    have one fleet of EC2 instances that serves the English version of your site and
    another fleet that serves the Spanish version of your site. For each language
    version, you will be configuring URLs with different paths such that the English
    version of your site will contain `/en/` in the path and the Spanish version will
    contain `/es/`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which type of load balancer would you use to route traffic to ensure users connect
    to the site in their desired language?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: CLB
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: NLB
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: ALB
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Path-based load balancer
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: You are building a multi-tier architecture with web servers placed in the public
    subnet and application servers placed in the private subnet of your VPC. You need
    to deploy ELBs to distribute traffic to both the web server farm and the application
    server farm. Which type of load balancer would you choose to distribute traffic
    to your application servers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Internet-facing
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Internal load balancer
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Dynamic load balancer
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Static load balancer
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which ELB is ideal for handling volatile workloads and can scale to millions
    of requests per second?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ALB
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: NLB
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: CLB
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Premium load balancer
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which configuration feature of the AWS Auto Scaling service enables you to define
    a maximum number of EC2 instances that can be launched in your fleet?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Auto Scaling group
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Auto Scaling Launch Configuration
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Auto Scaling max fleet size
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Auto Scaling policy
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: When an ELB detects an unhealthy EC2 instance, which action does it perform
    regarding distributing incoming traffic?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It continues to send traffic to the failed instance.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It terminates the failed instance so that it is not part of the ELB target group.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It only sends traffic to the remaining healthy instances.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It restarts the unhealthy EC2 instance.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which service does an AWS ALB integrate with to protect your applications from
    common web attacks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: WAF
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Shield
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Inspector
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Key Management Service** (**KMS**)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
