<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer247">
			<h1 id="_idParaDest-271"><a id="_idTextAnchor276"/><a href="B17124_11_Final_SK_ePub.xhtml#_idTextAnchor276"><em class="italic">Chapter 11</em></a>: Analytics on AWS</h1>
			<p>In this age of information, understanding your data has become extremely important. With current cutting-edge technologies, extensive amounts of data are generated every second – data that needs to be stored and analyzed. Companies perform data analytics to explain, predict, and ultimately gain a competitive advantage in business. Traditional analytics would include retail analytics, supply chain analytics, or stock rotation analytics. With <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) and <strong class="bold">artificial intelligence</strong> taking a firm hold on the economy, new evolutions of analytics have come into play, such as cognitive analytics, fraud analytics, and speech analytics. The list is almost endless but suffice it to say that understanding your raw data has required considerable effort and a whole business unit dedicated to <strong class="bold">data analytics</strong> alone. </p>
			<p>AWS offers a vast array of analytics tools that you can use to ingest, store, and effectively understand the data that's generated by your business. In this chapter, we will we look at some of those services in detail. </p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Learning about data streaming with Amazon Kinesis</li>
				<li>Learning how to query data stored in Amazon S3 with Amazon Athena</li>
				<li>Introduction to Amazon Elasticsearch</li>
				<li>Overview of Amazon Glue and QuickSight</li>
				<li>Additional analytics services</li>
			</ul>
			<h1 id="_idParaDest-272"><a id="_idTextAnchor277"/>Technical requirements</h1>
			<p>To complete the exercises in this chapter, you will need access to your AWS account and be logged in as the IAM user <strong class="bold">Alice</strong>. </p>
			<h1 id="_idParaDest-273"><a id="_idTextAnchor278"/>Learning about data streaming with Amazon Kinesis</h1>
			<p>To analyze your <a id="_idIndexMarker1190"/>business data, you need to ingest that data into a service that can perform the required analysis on it. Businesses generate tons <a id="_idIndexMarker1191"/>of data from a wide range of sources, including logs generated by applications, content such as videos, images, and documents, clickstream data from websites, IoT data, and more. Ingesting this data is the first step toward understanding it. </p>
			<p>However, rather than ingesting all the data first and then figuring out how you would go about understanding that data, <strong class="bold">Amazon Kinesis</strong> lets<a id="_idIndexMarker1192"/> you process and analyze data as it arrives and respond to it instantly. Amazon Kinesis is a fully managed service that enables you to process streaming data at any scale in a cost-effective manner. Furthermore, it is <strong class="bold">serverless</strong>, meaning<a id="_idIndexMarker1193"/> that you do not need to set up and manage expensive infrastructure to process your data. Amazon Kinesis is comprised of the following four key services:</p>
			<ul>
				<li>Amazon Kinesis Data Firehose</li>
				<li>Amazon Kinesis Data Streams</li>
				<li>Amazon Kinesis Data Analytics</li>
				<li>Amazon Kinesis Video Streams</li>
			</ul>
			<p>Let's look at each of these services in detail.</p>
			<h2 id="_idParaDest-274"><a id="_idTextAnchor279"/>Amazon Kinesis Data Firehose</h2>
			<p>Modern <a id="_idIndexMarker1194"/>business approaches and strategies to keep customers loyal and engaged have resulted in an insurmountable amount of data to collect, process, and make sense of. Whether you are trying to analyze what products your customers click on your website, make recommendations based on their product searches, or alert your security team about potentially fraudulent transactions, you need to collect and process data as it is being generated. Traditionally, you would have had to build the infrastructure to provide this kind of backend ingestion and processing of data, which can be cost-prohibitive for many businesses – not to mention the management overhead associated with maintaining hundreds of servers, storage, and network components.</p>
			<p><strong class="bold">Amazon Kinesis Firehose</strong> is a fully managed service that can ingest and deliver streaming data to <a id="_idIndexMarker1195"/>AWS data stores such as <em class="italic">Amazon S3</em>, <em class="italic">Redshift</em>, and <em class="italic">Amazon Elasticsearch</em> for near real-time analytics with existing <strong class="bold">business intelligence</strong> (<strong class="bold">BI</strong>) tools. The <a id="_idIndexMarker1196"/>service workflow can be illustrated with the following diagram:</p>
			<div>
				<div id="_idContainer235" class="IMG---Figure">
					<img src="Images/B17124_11_01.jpg" alt="Figure 11.1 – Kinesis Firehose&#13;&#10;" width="1404" height="555"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1 – Kinesis Firehose</p>
			<p>Amazon Kinesis Firehose can also deliver data to third-party services such as <em class="italic">Datadog</em>, <em class="italic">New Relic</em>, <em class="italic">MongoDB</em>, and <em class="italic">Splunk</em>. Amazon Kinesis Firehose will also allow you to batch process, compress, transform, and even encrypt data before loading it into a service, which means you reduce overall storage costs and enhance security.</p>
			<p>In addition, incoming data streams can be automatically converted into open standard formats such<a id="_idIndexMarker1197"/> as <strong class="bold">Apache Parquet</strong> and <strong class="bold">Apache ORC</strong>. Finally, with Amazon Kinesis Firehose, there are no infrastructure setup costs to worry about. You simply pay <a id="_idIndexMarker1198"/>for the data you transfer through the service, any data conversion costs, delivery to Amazon VPCs, and data transfers.</p>
			<p>Next, we will look at the Kinesis Data Streams service.</p>
			<h2 id="_idParaDest-275"><a id="_idTextAnchor280"/>Amazon Kinesis Data Streams</h2>
			<p>Whereas <a id="_idIndexMarker1199"/>Kinesis Firehose is designed to load massive amounts of data into <strong class="bold">data stores</strong> such<a id="_idIndexMarker1200"/> as Amazon S3 or Redshift for <em class="italic">near real-time</em>, <strong class="bold">Amazon Kinesis Data Streams</strong> is a fully managed <em class="italic">real-time</em> continuous data streaming service that allows you to capture gigabytes of data per second and stream it into custom applications for processing and analysis.</p>
			<p>Amazon Kinesis Data Streams <a id="_idIndexMarker1201"/>can make streaming data available to several analytical applications, such as Amazon S3 and AWS Lambda, within 70 milliseconds of the data being collected. It offers high levels of durability by replicating your streaming data across three Availability Zones.</p>
			<p>Kinesis Firehose does not offer any data storage capabilities. However, Amazon Kinesis Data Streams will store and make your data accessible for up to 24 hours by default, but this can be raised to 7 days by enabling the extended data retention feature, or even up to 365 days by enabling the long-term data retention feature.</p>
			<p>You ingest and store streaming data for processing to build real-time applications services such as <strong class="bold">real-time dashboards</strong>, <strong class="bold">real-time anomaly detection</strong>, <strong class="bold">dynamic pricing</strong>, and so<a id="_idIndexMarker1202"/> on. Like Kinesis Firehose, you are charged on a pay-as-you-go basis with no upfront cost nor minimum fees. However, there is a fundamental <a id="_idIndexMarker1203"/>difference in that Kinesis Data Streams uses the concept of <em class="italic">shards</em>, which uniquely identify the data records in a stream. A stream can be comprised of multiple shards that determine the overall capacity. Specifically, each <a id="_idIndexMarker1204"/>shard represents up to five transactions per second for reads with a maximum data read rate of 2 MB per second. For writes, you can have up to 1,000 records per second and a total data write rate of 1 MB per second (including partition keys). The total capacity of the stream is the sum of the capacities of its shards. The important concept to appreciate here is that you are charged for each shard that's provisioned per hour, regardless of whether you use it or not.</p>
			<p>In <a href="B17124_10_Final_SK_ePub.xhtml#_idTextAnchor249"><em class="italic">Chapter 10</em></a>, <em class="italic">Application Integration Services</em>, we discussed a service called <strong class="bold">Amazon SQS</strong>. Now, it may seem that Kinesis and SQS do the same thing, but they are very different. Amazon SQS is a<a id="_idIndexMarker1205"/> message queueing service that helps store messages while they travel between the different components of your application. Amazon SQS helps you decouple your application stack so that individual messages can be tracked and managed independently, and so that the different components of your application can work independently. </p>
			<p>Next, we will look at <a id="_idIndexMarker1206"/>the Kinesis Data Analytics service.</p>
			<h2 id="_idParaDest-276"><a id="_idTextAnchor281"/>Amazon Kinesis Data Analytics</h2>
			<p><strong class="bold">Kinesis Data Analytics</strong> lets you <a id="_idIndexMarker1207"/>query and analyze stream data in real time. Data can be streamed into the Kinesis Data Analytics application from various sources, including Amazon <strong class="bold">Managed Streaming for Kafka</strong> (<strong class="bold">MSK</strong>) and <strong class="bold">Amazon Kinesis Data Streams</strong> (discussed earlier). With Kinesis Data Analytics, you do not need to build complex streaming integrated applications <a id="_idIndexMarker1208"/>with other AWS services. Instead, you can use standard programming and database query languages such as Java, Python, and SQL to query streaming data or build streaming applications. The following diagram illustrates these key features:</p>
			<div>
				<div id="_idContainer236" class="IMG---Figure">
					<img src="Images/B17124_11_02.jpg" alt="Figure 11.2 – Amazon Kinesis Data Analytics&#13;&#10;" width="936" height="400"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2 – Amazon Kinesis Data Analytics</p>
			<p>Amazon Kinesis Data Analytics also enables you to analyze streaming data in real time and build streaming applications using open source libraries and connectors for <strong class="bold">Apache Flink</strong>. Apache Flink<a id="_idIndexMarker1209"/> is a fully open source, unified stream processing and batch processing framework <a id="_idIndexMarker1210"/>developed by the <strong class="bold">Apache Software Foundation</strong>.</p>
			<p>Finally, you have access to <a id="_idIndexMarker1211"/>the <strong class="bold">Kinesis Data Analytics Studio</strong>, which allows you to build sophisticated stream processing applications using SQL, Java, Python, and Scala.</p>
			<p>In terms of pricing, you are only charged for the resources that you use to run your streaming applications and there are no upfront commitments.</p>
			<p>Next, we will look at the Amazon Kinesis Video Streams service.</p>
			<h2 id="_idParaDest-277"><a id="_idTextAnchor282"/>Amazon Kinesis Video Streams</h2>
			<p>If you are<a id="_idIndexMarker1212"/> looking to stream video devices to AWS for analytics, ML, playback, and other processing services, then the Amazon Kinesis Video Streams service is going to be the tool you use. Amazon Kinesis Video Streams can also ingest data from edge devices, smartphones, security cameras, and more. </p>
			<p>Amazon Kinesis Video Streams makes use of Amazon S3 as the underlying storage repository from your streaming videos, which, as you already know, offers high levels of <strong class="bold">data durability</strong>. In <a id="_idIndexMarker1213"/>addition, you can search for and retrieve video fragments based on devices and timestamps. Your videos can be encrypted and indexed as well. </p>
			<p>With Amazon Kinesis Video Streams, you can play back videos for live or on-demand viewing. In addition, you can use Kinesis Video Streams to help you build applications that make use of computer vision video analytics technologies on AWS such as <strong class="bold">Amazon Rekognition</strong>. Incidentally, Amazon Rekognition<a id="_idIndexMarker1214"/> is a fully managed image and video analysis service that can be used to identify objects, people, text, scenes, and activities in images and videos. Amazon Rekognition can also be used to detect any inappropriate content. The following diagram illustrates the Amazon Kinesis Video Streams service:</p>
			<div>
				<div id="_idContainer237" class="IMG---Figure">
					<img src="Images/B17124_11_03.jpg" alt="Figure 11.3 – Amazon Kinesis Video Streams&#13;&#10;" width="898" height="492"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3 – Amazon Kinesis Video Streams</p>
			<p>Amazon Kinesis Video Streams <a id="_idIndexMarker1215"/>enables you to design applications for a wide range of use cases. One such use case includes the ability to stream video and audio for smart home devices such as doorbells. Amazon Kinesis Video Streams will ingest, index, and store the media streams and your application can use HTTP live streaming to play the stream to a smartphone app, allowing you to monitor and communicate with the person <em class="italic">knocking</em> on the door. </p>
			<p>In this section, we looked at Amazon Kinesis and discussed its four key offerings. In the next section, we will introduce you to another AWS analytics service known as Amazon Athena, which is a fully managed, serverless interactive query service that enables you to analyze data in Amazon S3 using standard SQL.</p>
			<h1 id="_idParaDest-278"><a id="_idTextAnchor283"/>Learning how to query data stored in Amazon S3 with Amazon Athena</h1>
			<p>Businesses store<a id="_idIndexMarker1216"/> vast amounts of data in repositories such as Amazon S3. A lot of this data is not necessarily being hosted on regular Amazon RDS or NoSQL databases. In many cases, this is because the dataset is not being regularly updated and queried. Previously, even if you wanted to perform ad hoc queries or analysis against some of that data, you would need to ingest it into a database and then run your queries against the database. </p>
			<p><strong class="bold">Amazon Athena</strong> is<a id="_idIndexMarker1217"/> a fully managed serverless solution that allows you to interactively query and analyze data directly in <strong class="bold">Amazon S3</strong> using standard <a id="_idIndexMarker1218"/>SQL. There is no infrastructure to provision, and you only pay for the queries you run. </p>
			<p>Amazon Athena uses <strong class="bold">Presto</strong>, which<a id="_idIndexMarker1219"/> is an open source SQL query engine that's designed to allow you to perform ad hoc analysis. You can use standard ANSI SQL, which provides full support for large joins, window functions, and arrays.</p>
			<p>Data can be presented to Amazon Athena in a variety of formats, such as CSV, JSON, ORC, Avro, or Parquet. Furthermore, you can use Athena's JDBC driver to connect to a wide range of BI tools.</p>
			<p>Amazon Athena allows you to present unstructured, semi-structured, and structured data to it, which you can then use to run queries to analyze that data. This process involves creating a database within the Athena service and one or more tables for each specific dataset that you want to query and analyze. These tables allow you to define metadata that tells Athena where the data is held in S3 and the structure of that data; for example, the column names and data types. </p>
			<p>Tables need to be registered in Athena to perform queries and have the results returned. These tables can be created automatically or manually. Once your tables have been registered, you can use <strong class="source-inline">SQL SELECT</strong> statements to query them. Your query results can also be stored in Amazon S3 in a location you specify.</p>
			<p>In an upcoming exercise, <em class="italic">Analyzing your sales report with Amazon Athena and AWS Glue</em>, we will upload some data to Amazon S3 and use Amazon Athena to query it. In this section, we introduced you to the Amazon Athena service and provided an overview of how you can use Athena to query data stored directly in Amazon S3.</p>
			<p>In the next section, we will look at another AWS analytics tool known as Amazon Elasticsearch.</p>
			<h1 id="_idParaDest-279"><a id="_idTextAnchor284"/>Introduction to Amazon Elasticsearch</h1>
			<p><strong class="bold">Elasticsearch</strong> is an<a id="_idIndexMarker1220"/> open source text search and analytics engine that's capable of storing, analyzing, and performing search functions against big volumes of data in near real time. You can use Elasticsearch to analyze all types of data such as textual, numerical, geospatial, structured, and unstructured data. </p>
			<p>Amazon's offering <a id="_idIndexMarker1221"/>of Elasticsearch as a service comes as a fully managed service with no need to set up and manage any infrastructure, allowing you to focus on your applications and their functionalities. Following the same pay-as-you-consume model, there are also no upfront costs, although you can reserve instances for a 1- or 3-year term for a significant discount over the on-demand pricing model.</p>
			<p>Amazon Elasticsearch is designed to be highly scalable and can index all types of content to help you deliver applications for use cases such as the following:</p>
			<ul>
				<li>Website search</li>
				<li>Application search</li>
				<li>Logging and log analytics</li>
				<li>Infrastructure metrics and monitoring</li>
				<li>Security analytics</li>
			</ul>
			<p>Raw data such as log files, messages, metrics, documents, and lists are ingested, normalized, and then indexed in Elasticsearch. You can then run complex queries against this data and use aggregations to review data summaries.</p>
			<p>Amazon Elasticsearch also offers integration<a id="_idIndexMarker1222"/> with <strong class="bold">Kibana</strong>, a data visualization tool that's used to analyze large datasets to help you produce visual representations of that data in the form of graphs, pie charts, heat maps, and much more. </p>
			<p>Another service that Amazon Elasticsearch integrates <a id="_idIndexMarker1223"/>with is <strong class="bold">Logstash</strong>, which is an open source, server-side data processing pipeline that allows you to ingest data from a wide range of sources and transform it and send it to a <strong class="bold">stash</strong> such as <a id="_idIndexMarker1224"/>Elasticsearch. Elasticsearch, Kibana, and Logstash are often referred to by the <a id="_idIndexMarker1225"/>acronym <strong class="bold">ELK</strong>.</p>
			<p>Finally, Amazon Elasticsearch <a id="_idIndexMarker1226"/>supports querying your cluster using standard SQL, making it easy for your developers to start using the service. You can also connect to your existing SQL-based BI and ETL tools using a JDBC driver.</p>
			<p>In this section, we introduced you to the Amazon Elasticsearch service, which allows you to create highly scalable, secure, and available Elasticsearch clusters and offers full integration with Kibana and Logstash for a complete managed ELK solution.</p>
			<p>In the next section, we will look at Amazon Glue and QuickSight.</p>
			<h1 id="_idParaDest-280"><a id="_idTextAnchor285"/>Overview of Amazon Glue and QuickSight</h1>
			<p>Business data can often be stored in a wide range of services – databases, storage buckets, spreadsheets, and more. Being able to bring all the relevant data together for analysis can sometimes <a id="_idIndexMarker1227"/>be a big project. Later, you may wish to extract and present that data<a id="_idIndexMarker1228"/> in a manner that is easy to digest and understand using BI tools or seamlessly integrate insights from that data into your applications, dashboards, and reporting. Two services offered by AWS that can help with these types of requirements are <strong class="bold">Amazon Glue</strong> and <strong class="bold">QuickSight</strong>. We'll take a quick look at each of these services next.</p>
			<h2 id="_idParaDest-281"><a id="_idTextAnchor286"/>Overview of Amazon Glue</h2>
			<p>Amazon Glue is<a id="_idIndexMarker1229"/> a serverless <strong class="bold">Extract, Transform, and Load</strong> (<strong class="bold">ETL</strong>) service. With Amazon Glue, you can discover, prepare, enrich, clean, and transform your data from various sources. You can then load the data into databases, data warehouses, and data lakes. Data from streaming sources can also be loaded for regular reporting and analysis. This data can then be used for analytics, as per your business requirements, and help with decision-making.</p>
			<p>Amazon Glue comes <a id="_idIndexMarker1230"/>with a <strong class="bold">Data Catalog</strong>, which is a central metadata repository that stores information about your data, such as table definitions. You use a <strong class="bold">crawler</strong> service<a id="_idIndexMarker1231"/> to scan various repositories, classify data, and <em class="italic">infer</em> schema information such as its format and data types. The metadata is then stored as tables in the Data Catalog and used to generate ETL scripts to transform, flatten, and enrich your data. The data is then populated into your chosen data warehousing solution or data lakes, for example.</p>
			<p>Amazon Glue also <a id="_idIndexMarker1232"/>comes with the <strong class="bold">AWS Glue console</strong> service to help you define and orchestrate your ETL workflow. It lets you do the following:</p>
			<ul>
				<li>Define Glue objects such as jobs, crawlers, tables, and so on.</li>
				<li>Schedule <strong class="bold">crawler</strong> run operations.</li>
				<li>Define events or schedules for job triggers.</li>
				<li>Search for and filter lists of objects in Glue.</li>
				<li>Edit transformation scripts directly or by using the visual tools provided.</li>
			</ul>
			<p>Amazon Glue is a fully managed service and scales resources as needed to run your jobs. It handles errors and retries automatically. With Amazon Glue, you are charged an hourly rate, which is billing by the second. This pricing is based on running crawlers (for discovering data) and performing ETL jobs (for processing and loading data). In addition, you pay a monthly fee to store and access the metadata in the AWS Glue Catalog.</p>
			<p>Next, we will look at the Amazon QuickSight service.</p>
			<h2 id="_idParaDest-282"><a id="_idTextAnchor287"/>Overview of Amazon QuickSight</h2>
			<p>Amazon QuickSight is <a id="_idIndexMarker1233"/>a serverless and fully managed BI service in the cloud that can be used to create and publish interactive BI dashboards for your business data. This provides you with access to meaningful information for your business so that you can make decisions.</p>
			<p>Amazon QuickSight can connect to your data wherever it is stored – whether it's stored in <strong class="bold">AWS services</strong>, <strong class="bold">on-premises databases</strong>, <strong class="bold">spreadsheets</strong>, <strong class="bold">SaaS data</strong>, or <strong class="bold">B2B data</strong>. This data can then be transformed into rich dashboards and reporting tools that can help your business understand operations, sales figures, profits, successes, and where there may be room for improvement. Amazon QuickSight can publish dashboards securely to enable collaborative efforts from your organization's workforce via mobile phones, email, or web applications. </p>
			<p>Amazon QuickSight also integrates with ML services, which allows it to build and deliver deeper insights from your data. With <strong class="bold">ML Insights</strong>, you <a id="_idIndexMarker1234"/>can discover hidden insights across your datasets, such as any anomalies and variations, enabling you to quickly act to changes that occur. You can also schedule automatic anomaly detection jobs. With ML Insights, you can perform better forecasting, which can be used to perform accurate <em class="italic">what-if</em> analysis. Finally, you can summarize your data into easy-to-consume natural language narratives, which can help you deliver better contextual information in your dashboards and reporting services.</p>
			<p>Amazon QuickSight's <a id="_idIndexMarker1235"/>pricing model is offered as a pay-as-you-use service and is determined by who is using the service; for example, admins, authors, and readers. Therefore, the pricing is based on the number of users, similar to a user-based license. Additional charges are incurred for services such as alerting and anomaly detection too. You <a id="_idIndexMarker1236"/>can view the pricing overview at <a href="https://aws.amazon.com/quicksight/pricing/">https://aws.amazon.com/quicksight/pricing/</a>.</p>
			<p>In this section, we reviewed two additional AWS services that fall within the analytics category. We introduced you to the Amazon Glue service, which is a fully managed and serverless ETL solution. We also provided an overview of the cloud-native BI tool, which uses ML to offer greater business insights from your data.</p>
			<p>In the next section, we will cover a couple of additional tools as part of the overall analytics offering.</p>
			<h1 id="_idParaDest-283"><a id="_idTextAnchor288"/>Additional analytics services</h1>
			<p>In this section, we <a id="_idIndexMarker1237"/>will take a very quick look at some other AWS analytics services that you need to be aware of. Specifically, we <a id="_idIndexMarker1238"/>will <a id="_idIndexMarker1239"/>look at<a id="_idIndexMarker1240"/> the <strong class="bold">Elastic Map Reduce</strong> (<strong class="bold">EMR</strong>) service, <strong class="bold">CloudSearch</strong>, and <strong class="bold">Data Pipeline</strong>:</p>
			<ul>
				<li><strong class="bold">AWS EMR</strong>: This<a id="_idIndexMarker1241"/> provides a <a id="_idIndexMarker1242"/>managed <strong class="bold">Hadoop framework</strong> to enable you to process vast amounts of big data. You can use open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR <a id="_idIndexMarker1243"/>comes <a id="_idIndexMarker1244"/>with an <strong class="bold">integrated development environment</strong> (<strong class="bold">IDE</strong>) called <strong class="bold">EMR Studio</strong> to help you develop, visualize, and debug data engineering and data science applications written in R, Python, Scala, and PySpark. You can run your EMR workloads on EC2 Instances, Amazon <strong class="bold">Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>) clusters, and on-premises using the <a id="_idIndexMarker1245"/>AWS Outpost service. In terms of pricing, you are charged at a per-instance rate for every second used, with a 1-minute minimum charge.</li>
				<li><strong class="bold">AWS Data Pipeline</strong>: This<a id="_idIndexMarker1246"/> is a web service that allows you to schedule and automate how your data is moved and transformed from various sources, including <strong class="bold">on-premises servers</strong>, into <a id="_idIndexMarker1247"/>services such as Amazon S3, RDS, DynamoDB, and EMR. With AWS Data Pipeline, you can create workflows that transfer and transform data at scheduled intervals to ensure alignment with the application processes. For example, you can archive your web server logs to an Amazon S3 bucket daily, and then run weekly Amazon EMR jobs to analyze those logs and generate traffic reports that can be consumed by your application. </li>
				<li><strong class="bold">AWS CloudSearch</strong>: This is <a id="_idIndexMarker1248"/>a fully managed service that enables you to deploy, manage, and scale a search solution for your web applications. Amazon CloudSearch supports 34 languages and adds rich search capabilities to your website, including free text, Boolean, and faceted search. It also offers features such as automated suggestions, highlighting, and more.</li>
			</ul>
			<p>In this section, we looked at some additional services offered by AWS that fall within the analytics category. In the next section, we will move on to this chapter's exercises.</p>
			<h1 id="_idParaDest-284"><a id="_idTextAnchor289"/>Exercise 11.1 – analyzing your sales report with Amazon Athena and AWS Glue</h1>
			<p>In this exercise, you<a id="_idIndexMarker1249"/> will need to download a sample CSV file, which is available in the Packt GitHub repository for this chapter <a href="https://github.com/PacktPublishing/AWS-Certified-Cloud-Practitioner-Exam-Guide">https://github.com/PacktPublishing/AWS-Certified-Cloud-Practitioner-Exam-Guide</a>. This is a simple CSV file that contains some sales data for the Vegan Studio, the fictitious company that you have been carrying out a series of exercises for in the previous chapters. </p>
			<p>You will need to <a id="_idIndexMarker1250"/>store this CSV file in an Amazon S3 bucket and then use Amazon Athena to run queries against the data. Ensure that you have downloaded the CSV file and stored it on your computer before you start this exercise.</p>
			<h2 id="_idParaDest-285"><a id="_idTextAnchor290"/>Step 1 – Amazon S3</h2>
			<ol>
				<li>Log into your <a id="_idIndexMarker1251"/>AWS account using the IAM user ID of our senior administrator, <strong class="bold">Alice</strong>. </li>
				<li>Navigate to the Amazon S3 dashboard.</li>
				<li>Create two new buckets with appropriate names. For example, I have named my buckets <strong class="source-inline">vegan-sales-report</strong> (to store the CSV file) and <strong class="source-inline">vegan-query-results</strong> (to store the Athena query results). Since I have taken these names, you will not be able to use them since bucket names must be unique in the AWS ecosystem. Ensure that the buckets are created in the <strong class="source-inline">us-east-1 (N.Virginia)</strong> Region.</li>
				<li>Next, upload your CSV file to the bucket that will be used to host the data. Recall the steps required from the previous chapters to complete the upload. (Tip: try to do this from memory as this will help you build your confidence.)<h2 id="_idParaDest-286"><a id="_idTextAnchor291"/>Step 2 – Amazon Athena and Amazon Glue</h2></li>
				<li>Navigate to <a id="_idIndexMarker1252"/>the Amazon Athena dashboard. You can search for the Athena service from the top search bar in your AWS Management Console. </li>
				<li>If this is the first time you are accessing Amazon Athena, you should see a splash screen. Click <strong class="bold">Get Started</strong>. If you do not see the <strong class="bold">Get Started</strong> option, this is because you are using the new user interface. AWS is notorious for making changes to the UI. If you do see the new console, then you will need to click on the <strong class="bold">Explore the query editor</strong> button. For this lab, we suggest that you use the old console for now. To access the old console, click on the ellipsis (three dashes) in <a id="_idIndexMarker1253"/>the top far left of the console and switch the toggle to disable the <strong class="bold">New Athena experience</strong> option. This will take you back to the old console interface:<p class="figure-caption"><img src="Images/B17124_11_04.png" alt="Figure 11.4 – Disabling the New Athena experience toggle switch&#13;&#10;" width="598" height="460"/></p><p class="figure-caption">Figure 11.4 – Disabling the New Athena experience toggle switch</p></li>
				<li>From the top right-hand corner, click on the <strong class="bold">Settings</strong> link. You will be presented with the following dialog box. You will need to provide the S3 bucket details to store your query results. The format should be <strong class="source-inline">s3://bucket-name</strong>. You can also store your queries in a sub-folder and choose to encrypt your query results:<div id="_idContainer239" class="IMG---Figure"><img src="Images/B17124_11_05.jpg" alt="Figure 11.5 – Amazon Athena – Settings&#13;&#10;" width="858" height="688"/></div><p class="figure-caption">Figure 11.5 – Amazon Athena – Settings</p></li>
				<li>Next, click <a id="_idIndexMarker1254"/>the <strong class="bold">Save</strong> button.</li>
				<li>Next, from the left-hand menu, click on <strong class="bold">Connect data source</strong>, as per the following screenshot:<div id="_idContainer240" class="IMG---Figure"><img src="Images/B17124_11_06.jpg" alt="Figure 11.6 – Amazon Athena – Connect data source&#13;&#10;" width="438" height="223"/></div><p class="figure-caption">Figure 11.6 – Amazon Athena – Connect data source</p></li>
				<li>Under <strong class="bold">Choose where your data is located</strong>, ensure that <strong class="bold">Query data in Amazon S3</strong> is selected.</li>
				<li>Under <strong class="bold">Choose a metadata catalog</strong>, ensure that <strong class="bold">AWS Glue Data Catalog</strong> is selected. For <a id="_idIndexMarker1255"/>this exercise, you will use AWS Glue to crawl your data and create a schema. There is a slight charge to this, but it is very minimum, and you will only need to do this once. </li>
				<li>Click the <strong class="bold">Next</strong> button on the right-hand pane of the page.</li>
				<li>Under <strong class="bold">Connection details</strong>, ensure that <strong class="bold">AWS Glue Data Catalog in this account</strong> is selected. Then, under <strong class="bold">Choose a way to create a table</strong>, select <strong class="bold">Create a crawler in AWS Glue</strong>. </li>
				<li>Click the <strong class="bold">Connect to AWS Glue</strong> button. This will launch <strong class="bold">AWS Glue</strong> in a new browser tab. Switch over to this tab to configure Amazon Glue. Do not close the Amazon Athena browser tab as we will return to this later.</li>
				<li>If you see the splash screen, click the <strong class="bold">Get started</strong> button.</li>
				<li>From the left-hand menu, click <strong class="bold">Crawlers</strong>.</li>
				<li>From the right-hand pane, click the <strong class="bold">Add crawler</strong> button. You will be presented with the <strong class="bold">Add crawler</strong> wizard:<ol><li>Provide <a id="_idIndexMarker1256"/>a name for the crawler. Click <strong class="bold">Next</strong>.</li><li>On the <strong class="bold">Specify crawler source type</strong> page, ensure that the <strong class="bold">Data stores</strong> and <strong class="bold">Craw all folders</strong> options are selected.</li><li>Click <strong class="bold">Next</strong>.</li><li>On the <strong class="bold">Choose a data store</strong> page, ensure that <strong class="bold">S3</strong> is selected as your chosen data store.</li><li>Next, under <strong class="bold">Craw data in</strong>, select the <strong class="bold">Specified path in my account</strong> option. For <strong class="bold">Include path</strong>, provide the S3 bucket URL of your S3 bucket that hosts the sales report. In this case, the URL is in the format of <strong class="source-inline">s3://bucket-name. </strong><span class="hidden">Note that at the end of the path defined for your S3 bucket, ensure that you add another slash</span> (<strong class="source-inline">/</strong>). <strong class="bold">For example, my bucket path reads</strong> <strong class="source-inline">s3://vegan-sales-report/</strong>.</li><li>Click <strong class="bold">Next</strong>.</li><li>On the <strong class="bold">Add another data store</strong> page, click <strong class="bold">No</strong> and then <strong class="bold">Next</strong>.</li><li>Next, on the <strong class="bold">Choose an IAM Role</strong> page, select the <strong class="bold">Create an IAM role</strong> option and type a name in the text box next to <strong class="bold">AWSGlueServiceRole-</strong>. This will form the path of your IAM <strong class="bold">role</strong> name. For example, I have typed in <strong class="source-inline">VeganSalesRole</strong> so that it is self-explanatory. Click <strong class="bold">Next</strong>.</li><li>On the <strong class="bold">Create a schedule for this crawler</strong> page, set <strong class="bold">Frequency</strong> to <strong class="bold">Run on demand</strong>. Click <strong class="bold">Next</strong>.</li><li>On the <strong class="bold">Configure the crawler's output</strong> page, click the <strong class="bold">Add database</strong> button.</li><li>In the <strong class="bold">Add database</strong> dialog box that appears, provide a name for your database, such as <strong class="source-inline">vegansalesdb</strong>. Click <strong class="bold">Create</strong>.</li><li>You will be taken back to the <strong class="bold">Configure the crawler's output</strong> page, with your newly created database name shown in the <strong class="bold">Database</strong> text box. Click <strong class="bold">Next</strong>.</li><li>On the <strong class="bold">Review all steps</strong> page, click <strong class="bold">Finish</strong>.</li></ol></li>
				<li>You will be <a id="_idIndexMarker1257"/>redirected to the <strong class="bold">Crawlers</strong> page. Here, you will be able to see that your crawler has been created. Click on the checkbox next to your crawler and click the <strong class="bold">Run crawler</strong> button, as per the following screenshot:<div id="_idContainer241" class="IMG---Figure"><img src="Images/B17124_11_07.jpg" alt="Figure 11.7 – Amazon Glue – Run crawler&#13;&#10;" width="765" height="222"/></div><p class="figure-caption">Figure 11.7 – Amazon Glue – Run crawler</p></li>
				<li> After a minute or two, you should find that its <strong class="bold">Status</strong> is set to <strong class="bold">Ready</strong> and that the crawler has successfully run. You will see that a table has been added, as per the following screenshot:<div id="_idContainer242" class="IMG---Figure"><img src="Images/B17124_11_08.jpg" alt="Figure 11.8 – Amazon Glue – Crawl complete&#13;&#10;" width="993" height="199"/></div><p class="figure-caption">Figure 11.8 – Amazon Glue – Crawl complete</p></li>
				<li>Now that the <a id="_idIndexMarker1258"/>crawl is complete, we can return to the Amazon Athena browser tab.<h2 id="_idParaDest-287"><a id="_idTextAnchor292"/>Step 3 – Amazon Athena</h2></li>
				<li>Back in <a id="_idIndexMarker1259"/>the Athena browser tab, go ahead and click on the <strong class="bold">cancel</strong> button at the bottom right-hand corner of the page. This will take you back to the main <strong class="bold">Amazon Athena Data sources</strong> page, where you will see that your recently created Glue Catalog is listed under <strong class="bold">Data sources</strong>, as per the following screenshot:<div id="_idContainer243" class="IMG---Figure"><img src="Images/B17124_11_09.jpg" alt="Figure 11.9 – Amazon Athena – Data sources&#13;&#10;" width="840" height="401"/></div><p class="figure-caption">Figure 11.9 – Amazon Athena – Data sources</p></li>
				<li>Next, click on the <strong class="bold">Query editor</strong> tab.</li>
				<li>From<a id="_idIndexMarker1260"/> the left-hand menu, select the database you created earlier in <em class="italic">Step 2</em>, which will also reveal the table that you created within the database, as per the following screenshot:<div id="_idContainer244" class="IMG---Figure"><img src="Images/B17124_11_10.jpg" alt="Figure 11.10 – Amazon Athena – Query editor&#13;&#10;" width="472" height="474"/></div><p class="figure-caption">Figure 11.10 – Amazon Athena – Query editor</p></li>
				<li>Now, you can easily preview the data held in Amazon S3 by clicking on the ellipsis (the three dots next to your table name) and then clicking on <strong class="bold">Preview table</strong> from the context menu that appears. This will run a SQL query and retrieve the <a id="_idIndexMarker1261"/>sample data from your table, as per the following screenshot:<div id="_idContainer245" class="IMG---Figure"><img src="Images/B17124_11_11.jpg" alt="Figure 11.11 – Amazon Athena – Sample query &#13;&#10;" width="1270" height="628"/></div><p class="figure-caption">Figure 11.11 – Amazon Athena – Sample query </p></li>
				<li>You can run additional queries. For example, you can replace the SQL statement in the top half of the pane with the following: <p class="source-code"><strong class="bold">SELECT * FROM "vegansalesdb"."vegan_sales_report" WHERE total &gt;=100000; </strong></p><p>The preceding statement will showcase all the cities where the sales that were achieved were equal to or above $100,000, as per the following screenshot:</p></li>
			</ol>
			<div>
				<div id="_idContainer246" class="IMG---Figure">
					<img src="Images/B17124_11_12.jpg" alt="Figure 11.12 – Amazon Athena – Query to identify those cities where &#13;&#10;sales were greater than or equal to $100,000&#13;&#10;" width="982" height="615"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.12 – Amazon Athena – Query to identify those cities where sales were greater than or equal to $100,000</p>
			<p>As you can see, Amazon Athena is<a id="_idIndexMarker1262"/> extremely powerful in being able to access and query your raw data in Amazon S3. You do not need to set up and deploy servers or run expensive databases for such ad hoc analysis of your data.</p>
			<p>Now, we will perform a cleanup exercise to remove unwanted resources from our AWS account.</p>
			<h1 id="_idParaDest-288"><a id="_idTextAnchor293"/>Exercise 11.2 – cleaning up</h1>
			<p>In this exercise, you <a id="_idIndexMarker1263"/>will delete the resources you created in the previous exercise to ensure that there are no unwanted costs:</p>
			<ol>
				<li value="1">Navigate to the Amazon Glue console.</li>
				<li>From the left-hand menu, click the <strong class="bold">Crawlers</strong> link. In the right-hand pane, select <strong class="bold">vegan-sales-crawler</strong>. From the <strong class="bold">Actions</strong> drop-down list, click the <strong class="bold">Delete Crawler</strong> option and then confirm the delete operation.</li>
				<li>Next, from the left-hand menu, click <strong class="bold">Databases</strong>. In the right-hand pane, select the <strong class="bold">vegansalesdb</strong> database. Then, from the <strong class="bold">Actions</strong> drop-down list, click the <strong class="bold">Delete database</strong> option.</li>
				<li>Click the <strong class="bold">Delete</strong> button in the <strong class="bold">Delete Database</strong> confirmation dialog box that appears.</li>
			</ol>
			<p>Next, you will <a id="_idIndexMarker1264"/>need to delete the Amazon S3 buckets as they are no longer required:</p>
			<ol>
				<li value="1">Navigate to the Amazon S3 console. From the left-hand menu, click on <strong class="bold">Buckets</strong>.</li>
				<li>In the right-hand pane, select the <strong class="bold">vegan-query-results</strong> bucket and then click the <strong class="bold">Empty</strong> button. Confirm that you want to empty the bucket by typing <strong class="source-inline">permanently delete</strong> in the confirmation text box. Next, click the <strong class="bold">Empty</strong> button. You will get a confirmation message, stating that the bucket has been successfully emptied. Click the <strong class="bold">Exit</strong> button.</li>
				<li>Next, with the <strong class="bold">vegan-query-results</strong> bucket still highlighted, click the <strong class="bold">Delete</strong> button.</li>
				<li>Confirm the delete operation by typing the bucket's name in the confirmation text box and then clicking on the <strong class="bold">Delete bucket</strong> button.</li>
				<li>Repeat <em class="italic">Steps 1</em> to <em class="italic">4</em> for the <strong class="bold">vegan-sales-report</strong> bucket.</li>
			</ol>
			<p>Now that you have completed the cleanup exercise, we will summarize this chapter.</p>
			<h1 id="_idParaDest-289"><a id="_idTextAnchor294"/>Summary</h1>
			<p>In this chapter, we discussed several services from AWS that fall within the analytics category. Businesses today possess a vast array of data and being able to analyze and make sense of that data is extremely important. Information that's obtained from this data can help businesses respond to their customers' needs and demands, address potential issues, and even predict future growth. Ultimately, businesses can gain an advantage over competitors.</p>
			<p>In this chapter, you learned about services such as Amazon Kinesis, which allows customers to stream and respond in real time and near real time to data. You also learned about services that can be used to quickly query your data, such as Amazon Athena, as well as services to help you present that data using BI tools. Most of these analytical services are also offered as fully managed services on a pay-as-you-consume pricing model, making them very affordable for almost any business. </p>
			<p>In the next chapter, you will learn about various deployment and orchestration tools on AWS that can help you provision and deploy your applications in the cloud without extensive manual configuration. We will look at <strong class="bold">Infrastructure as Code</strong> (<strong class="bold">IaC</strong>), which has taken the IT world by storm as you can design and deploy end-to-end infrastructure solutions in a matter of minutes using predefined templates. We will also look at how to automate common IT tasks using serverless compute services such as AWS Lambda. </p>
			<h1 id="_idParaDest-290"><a id="_idTextAnchor295"/>Questions</h1>
			<p>Answer the following questions to test your knowledge of this chapter:</p>
			<ol>
				<li value="1">Which AWS service can help you ingest and deliver massive amounts of streaming data into Amazon Redshift for near real-time analytics?<ol><li>Amazon Athena</li><li>Amazon Kinesis Firehose</li><li>Amazon Kinesis Video Streams</li><li>Amazon RDS</li></ol></li>
				<li>Which AWS service can help you query streaming data using standard SQL queries in real time?<ol><li>Amazon Kinesis Data Streams</li><li>Amazon Kinesis Data Analytics</li><li>Amazon Glue</li><li>Amazon QuickSight</li></ol></li>
			</ol>
			<ol>
				<li value="3">You are planning on building an application that will capture video streams from speed cameras on country roads for analysis. You need to be able to capture all the vehicles that break the speed limit and identify the offending drivers via the vehicles' license plates. Which two services on AWS can help you achieve these requirements? (Choose 2 answers.)<ol><li>Amazon Athena</li><li>Amazon Kinesis Data Analytics</li><li>Amazon Kinesis Video Streams</li><li>Amazon Elasticsearch</li><li>Amazon Rekognition</li></ol></li>
				<li>Which AWS service enables you to index all types of content, offers integration with <strong class="bold">Kibana</strong>, and helps you build data visualization tools to analyze large datasets?<ol><li>Amazon Elasticsearch</li><li>Amazon Glue</li><li>Amazon Athena</li><li>Amazon Kinesis Firehose</li></ol></li>
				<li>You store several network log files (in CSV format) in an Amazon S3 bucket. You have been asked to analyze the contents of a specific file for possible malicious attacks. Which AWS service can help you analyze raw data in Amazon S3 and perform the necessary ad hoc analysis?<ol><li>Amazon Glue</li><li>Amazon QuickSight</li><li>Amazon Athena</li><li>Amazon Data Pipeline</li></ol></li>
			</ol>
			<ol>
				<li value="6">Which AWS service can be used to perform serverless ETL functions to discover, prepare, enrich, clean, and transform your data from various sources for analysis?<ol><li>AWS Glue</li><li>AWS Athena</li><li>AWS QuickSight</li><li>AWS Rekognition</li></ol></li>
				<li>Which AWS service enables you to create and publish interactive BI dashboards for your business data to provide access to meaningful information for your business to make decisions?<ol><li>AWS Kinesis Data Analytics</li><li>AWS Glue</li><li>AWS QuickSight</li><li>AWS Kinesis Firehose</li></ol></li>
			</ol>
		</div>
	</div></body></html>