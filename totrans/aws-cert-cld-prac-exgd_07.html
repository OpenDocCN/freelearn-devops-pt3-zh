<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer102">
			<h1 id="_idParaDest-94"><a id="_idTextAnchor094"/><a href="B17124_05_Final_SK_ePub.xhtml#_idTextAnchor094"><em class="italic">Chapter 5</em></a>: Amazon Simple Storage Service (S3)</h1>
			<p>In this chapter, we look at one of the available storage services on <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>). Many clients who are just starting on their cloud journey often consider storage services in the cloud as a stepping stone to going cloud-native in the long run. While storage options have become cheaper over the years, the fact remains that we continue to consume more and more storage with the passage of time. That said, it is vital that organizations also have a smart life cycle policy for their storage needs. Companies may be required to keep data for many years, and for as long as 7 to 10 years for compliance and regulatory purposes. However, at some point, a substantial amount of data is no longer required, and purging this data from the network not only makes management easier but also saves on cost.</p>
			<p>Access to AWS storage services is extremely easy, and rather than procuring new storage hardware to host on-premises, it is much easier and more cost-effective to use the services offered by a cloud vendor such as AWS. Understandably, there will some types of data that need to be stored on-premises primarily because of latency issues, but from a security standpoint, AWS offers numerous options to ensure that your data is accessible only to you.</p>
			<p>AWS offers different storage options, and in this chapter, we look at one of its flagship products: <strong class="bold">Amazon Simple Storage Service</strong> (<strong class="bold">Amazon S3</strong>). Amazon S3 is an object storage solution and offers very high levels of availability, durability, and scalability. AWS also offers other types of storage options, which we look at in subsequent chapters.</p>
			<p>By the end of this chapter, you will understand the fundamentals of object storage on AWS and how Amazon S3 can help fulfill core storage requirements for your business in the cloud. You will also learn how about various features that can be used to manage your cloud storage, address regulatory and compliance concerns, and design cost-effective solutions. Finally, you will learn how to access your cloud storage from on-premises locations and how to migrate large datasets to the cloud.</p>
			<p>The topics in this chapter include the following:</p>
			<ul>
				<li>Introduction to storage options on AWS</li>
				<li>Introduction to Amazon S3</li>
				<li>Learning about archiving solutions with Amazon S3 Glacier</li>
				<li>Connecting your on-premises storage to AWS with Amazon Storage Gateway</li>
				<li>Migrating large datasets to AWS with the Amazon Snow Family</li>
				<li>Exercise 5.1—Setting up an Amazon S3 bucket</li>
				<li>Exercise 5.2—Configuring public access to S3 buckets</li>
				<li>Exercise 5.3—Enabling versioning on your bucket</li>
				<li>Exercise 5.4—Setting up static website hosting</li>
			</ul>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor095"/>Technical requirements</h1>
			<p>To complete the exercises in this chapter, you will need to have an AWS account via the AWS Management Console. You will need to be logged in as the IAM user, <strong class="bold">Alice</strong>, that you created in the last chapter.  </p>
			<h1 id="_idParaDest-96"><a id="_idTextAnchor096"/>Introduction to storage options on AWS</h1>
			<p>A storage service <a id="_idIndexMarker323"/>provides the necessary infrastructure to enable you to store and access data. However, different use cases require varied storage architectures to ensure performance, reliability, durability, and the right type of<a id="_idIndexMarker324"/> access to the data. There are three primary storage options available, and AWS offers services to cater to each of these.  </p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor097"/>Block storage</h2>
			<p><strong class="bold">Block storage</strong> is an architectural <a id="_idIndexMarker325"/>design that enables the storage of data onto media such as a hard disk, in fixed-sized chunks. Data is broken up into small blocks and placed on the media in these chunks, with a unique address<a id="_idIndexMarker326"/> assigned that forms part of its metadata. Block storage makes use of a management software (which can be part of the operating system) to organize the blocks of data. When a user tries to retrieve a file, the management software identifies the blocks to retrieve, reassembles the data, and presents the whole file to the user.</p>
			<p>On AWS, block <a id="_idIndexMarker327"/>storage options are available as <strong class="bold">Elastic Block Store</strong> (<strong class="bold">EBS</strong>). These can be configured as volumes attached to your <strong class="bold">Elastic Compute Cloud</strong> (<strong class="bold">EC2</strong>) instances and <a id="_idIndexMarker328"/>offer ultra-low latency required for high-performance workloads. One advantage of EBS volumes is that they are not directly attached to the EC2 instance you deploy, but instead are connected <a id="_idIndexMarker329"/>via high-speed network links. This allows you to detach an EBS volume from one EC2 instance and attach it to another if, for example, the first EC2 instance experiences<a id="_idIndexMarker330"/> some sort of failure.</p>
			<p>Typical use cases include <a id="_idIndexMarker331"/>running and managing system files such as those used by your operating system, large databases, or for applications such as <strong class="bold">enterprise resource planning</strong> (<strong class="bold">ERP</strong>) solutions. These types of applications <a id="_idIndexMarker332"/>require very low-latency access to the data and <a id="_idIndexMarker333"/>generally make use of <strong class="bold">direct-attached storage</strong> (<strong class="bold">DAS</strong>) or <strong class="bold">storage area networks</strong> (<strong class="bold">SANs</strong>).  </p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor098"/>File storage</h2>
			<p>Another<a id="_idIndexMarker334"/> storage architectural design is <strong class="bold">file storage</strong>. The architecture offers a centralized location for your corporate data, and files are stored in folders and sub-folders. File storage<a id="_idIndexMarker335"/> offers a hierarchical structure to store your data, and this means you can imitate your real-life counterpart—the filing cabinet—to organize your data.</p>
			<p>Retrieval of the data requires you to know the file and folder structure and provide this information. For example, if I need last August's balance sheet Excel document, I will need to look in the <strong class="source-inline">2020</strong> folder and the <strong class="source-inline">August</strong> sub-folder, and this would enable me to retrieve that specific data.</p>
			<p>Due to the nature of file storage, metadata information can be limited, and a key limitation to be aware of is that you cannot have unlimited folders and sub-folders due to your operating system restrictions. Your hierarchical structure can also be the cause of some performance issues, and therefore you need to decide on this structure carefully. </p>
			<p>File storage lends itself well to being used for file sharing within a corporate organization. Because of the folder/sub-folder architecture, it becomes very easy to organize your data to fit in<a id="_idIndexMarker336"/> well with your organizational structure.</p>
			<p>Amazon offers three different file storage systems, outlined as follows:</p>
			<ul>
				<li><strong class="bold">Elastic File System</strong> (<strong class="bold">EFS</strong>)—This is a managed elastic filesystem designed to let you share file data without provisioning or managing storage as you would with EBS. Your filesystem will <a id="_idIndexMarker337"/>grow and shrink as you add and remove data, and mount points can only be created on Linux EC2 instances.</li>
				<li><strong class="bold">FSx for Lustre</strong>—A high-performance filesystem designed for applications that require fast storage and can <a id="_idIndexMarker338"/>scale to hundreds of <strong class="bold">gigabytes</strong> (<strong class="bold">GB</strong>) of throughput and millions of <strong class="bold">input/output operations per second</strong> (<strong class="bold">IOPS</strong>). FSx for Lustre is also designed for a wide range of Linux-based EC2 instances.</li>
				<li><strong class="bold">FSx for Windows File Server</strong>—Designed for Microsoft Windows EC2 instances and offers a fully<a id="_idIndexMarker339"/> managed file-share solution, natively supporting the Windows file system such as the industry-standard <strong class="bold">Server Message Block</strong> (<strong class="bold">SMB</strong>) protocol. Typical use<a id="_idIndexMarker340"/> cases include file-sharing services, local archiving, application data sharing, and data protection.</li>
			</ul>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor099"/>Object storage</h2>
			<p>By contrast, <strong class="bold">object storage</strong> involves storing<a id="_idIndexMarker341"/> complete files as individual objects. Object storage presents a flat file structure—you create some form of container and place your objects<a id="_idIndexMarker342"/> within this container without using any folder or file-level hierarchy. This is also known as unstructured data. Object <a id="_idIndexMarker343"/>storage metadata (information about the object—such as its name, and so on), along with other attributes, is then used to create a unique identifier to easily locate that data in your storage pool. Due to the nature of object storage, the metadata can contain a vast array of information, enabling you to use object storage for data analytics far more easily than a file-based storage solution.</p>
			<p>An additional benefit is that object storage lends itself well to offering higher levels of performance, durability, and scalability. In a file-level storage solution, the depth of the folder and file structure will often have a limit based on the operating system you are using. With object storage, however, you can potentially scale to having limitless amounts of data.</p>
			<p>The flat file structure is another advantage point as this enables you to retrieve data much faster due to the extended categorization feature, as opposed to retrieving data from a file storage service.</p>
			<p>On AWS, object storage options are available with the Amazon S3 service. Amazon S3 lets you create <a id="_idIndexMarker344"/>containers called <strong class="bold">buckets</strong>, within which you place your data (objects) in a flat file structure (unstructured manner). You can store and retrieve any amount of data—anytime, anywhere.  </p>
			<p>Typical use cases for object storage include storing digital assets for your websites and applications (documents, images, video), the ability to perform analytics on your objects, and offering<a id="_idIndexMarker345"/> storage solutions to cutting-edge technologies such as <strong class="bold">Internet of Things</strong> (<strong class="bold">IoT</strong>).</p>
			<p>In this section, we<a id="_idIndexMarker346"/> looked at the three main types of storage options available, their key features, and typical use cases. We also highlighted some<a id="_idIndexMarker347"/> examples of AWS services that offer storage solutions.</p>
			<p>In the next section, we will introduce you to the Amazon S3 service, which is an object storage solution for the cloud.</p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor100"/>Introduction to Amazon S3</h1>
			<p>Amazon S3 is one of Amazon's flagship products, and offers a robust, scalable, durable, and cost-effective <strong class="bold">object storage</strong> solution in the cloud. Customers can use Amazon S3 to store any<a id="_idIndexMarker348"/> amount of data for a wide range of use cases, including digital media content for websites, data lakes, mobile applications, IoT device data, and big data analytics.</p>
			<p>Amazon S3 can offer up to 99.999999999% durability and fulfills the storage requirements for a majority of clients and their individual business needs.  </p>
			<p>What does eleven 9s of durability mean? According to AWS, if you store 10,000,000 objects on Amazon<a id="_idIndexMarker349"/> S3, then on average you can expect to incur a loss of a single object once every 10,000 years. You can review all <strong class="bold">frequently asked questions</strong> (<strong class="bold">FAQs</strong>) for Amazon <a id="_idIndexMarker350"/>S3 here: <a href="https://aws.amazon.com/s3/faqs/">https://aws.amazon.com/s3/faqs/</a>.</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor101"/>Buckets and objects</h2>
			<p>Before you can upload <a id="_idIndexMarker351"/>any data to Amazon S3, you need to create a container called a bucket. Buckets need to have a unique global namespace as their contents can be made accessible over the public internet. This means that your bucket names must be unique across the <a id="_idIndexMarker352"/>AWS ecosystem. For example, a document named <strong class="source-inline">blueberry-muffin.txt</strong> stored in a bucket named <strong class="source-inline">just-desserts</strong> can be accessible via the internet in two primary ways, outlined as follows:</p>
			<ul>
				<li><strong class="bold">Virtual hosted-style endpoints</strong>—Here, the<a id="_idIndexMarker353"/> bucket name forms part of the <strong class="bold">Domain Name System</strong> (<strong class="bold">DNS</strong>) subdomain name so that the preceding <strong class="source-inline">blueberry-muffin.txt</strong> object is accessible via the S3 <strong class="bold">Uniform Resource Locator</strong> (<strong class="bold">URL</strong>), such as <a href="https://just-desserts.s3.amazonaws.com/blueberry-muffin.txt">https://just-desserts.s3.amazonaws.com/blueberry-muffin.txt</a>.</li>
				<li><strong class="bold">Website endpoint</strong>—Here, the<a id="_idIndexMarker354"/> bucket is configured with a static website hosting service and the website is available at the AWS Region-specific website endpoint in one of the following two formats:<p>a) <strong class="source-inline">s3-website</strong> dash (<strong class="source-inline">-</strong>) Region—<strong class="source-inline">http://bucket-name.s3-website-Region.amazonaws.com</strong>. For example, our recipe will be available at <a href="http://just-desserts.s3-website-us-east-1.amazonaws.com/blueberry-muffin.txt">http://just-desserts.s3-website-us-east-1.amazonaws.com/blueberry-muffin.txt</a>.</p><p>b) <strong class="source-inline">s3-website</strong> dot (<strong class="source-inline">.</strong>) Region—<strong class="source-inline">http://bucket-name.s3-website.Region.amazonaws.com</strong>. For example, our recipe will be available at <a href="http://just-desserts.s3-website.us-east-1.amazonaws.com/blueberry-muffic.txt">http://just-desserts.s3-website.us-east-1.amazonaws.com/blueberry-muffic.txt</a>. We look at static website hosting on Amazon S3 later in this chapter. </p></li>
			</ul>
			<p>When creating buckets, you may therefore find that common names are not available. For example, if you wanted to create a bucket name called <strong class="source-inline">marketing</strong>, then you will most likely not be able to use this name as it may have been used already. You could instead create a marketing bucket with a unique prefix or suffix to get an appropriate name. Most companies will try to associate their bucket names with their organization name or project name—for example, your marketing bucket name could be <strong class="source-inline">my-company-marketing</strong>. That said, you still cannot solely depend on any specific naming convention you <a id="_idIndexMarker355"/>define for your buckets, because another customer may have chosen the exact same name that you might have wanted, and bucket names are<a id="_idIndexMarker356"/> defined on a first-come first-serve basis.</p>
			<p>Some key attributes of Amazon S3 buckets are provided here:</p>
			<ul>
				<li>Bucket names must be between 3 and 63 characters long.</li>
				<li>Bucket names must always be in lowercase letters. They can, however, contain numbers, hyphens (<strong class="source-inline">-</strong>), and dots (<strong class="source-inline">.</strong>) only.</li>
				<li>Bucket names must also begin and end with either a letter or number and should not include spaces between the names.</li>
				<li>Bucket names must not be formatted as an <strong class="bold">Internet Protocol</strong> (<strong class="bold">IP</strong>) address (<strong class="source-inline">192.168.1.1</strong>).<strong class="source-inline">  </strong></li>
				<li>Buckets used with <strong class="bold">Amazon S3 Transfer Acceleration</strong> (<strong class="bold">S3TA</strong>) cannot have dots (<strong class="source-inline">.</strong>) in their names. We discuss<a id="_idIndexMarker357"/> Amazon S3TA later in this chapter.</li>
				<li>You cannot have nested buckets—for example, a bucket within another bucket.<p class="callout-heading">Important note </p><p class="callout">Except for website-style endpoints, you should avoid using dots (<strong class="source-inline">.</strong>) as part of your bucket names as they cannot <a id="_idIndexMarker358"/>be used with virtual hosted-style endpoints using <strong class="bold">Secure Sockets Layer</strong> (<strong class="bold">SSL</strong>) and the <strong class="bold">HyperText Transfer Protocol Secure</strong> (<strong class="bold">HTTPS</strong>) protocol. Note that<a id="_idIndexMarker359"/> the only reason they work with website-style endpoints is because static website hosting is only available over HTTP. You can get around this problem if you need to serve your content over HTTPS by incorporating a Amazon CloudFront distribution point. We discuss CloudFront in <a href="B17124_06_Final_SK_ePub.xhtml#_idTextAnchor122"><em class="italic">Chapter 6</em></a>, <em class="italic">AWS Networking Services – VPCs, Route53, and CloudFront</em>.</p></li>
			</ul>
			<p>Any data stored in an<a id="_idIndexMarker360"/> Amazon S3 bucket is represented as an <strong class="bold">object</strong>. An object is stored in its entirety within a given bucket rather than with block storage, where a file may be broken up into chunks and stored on a given media such as a hard disk. As discussed <a id="_idIndexMarker361"/>previously, objects are stored in an unstructured manner as a <strong class="bold">flat filesystem</strong>. This means that accessing an<a id="_idIndexMarker362"/> object requires you to know its unique ID, which is generally part <a id="_idIndexMarker363"/>of the object's metadata. You can store an unlimited number of objects within a given bucket, and the<a id="_idIndexMarker364"/> maximum size of an object on Amazon S3 is 5 <strong class="bold">terabytes</strong> (<strong class="bold">TB</strong>).</p>
			<p>The filename of an object is called a <strong class="bold">key</strong>, and the<a id="_idIndexMarker365"/> data contained as part of that <em class="italic">object</em> is known as its <strong class="bold">value</strong>. Keys can <a id="_idIndexMarker366"/>be up to 1,024 bytes long and your objects can consist of letters and numbers, as well as characters such as, <strong class="source-inline">!</strong>, <strong class="source-inline">-</strong>, <strong class="source-inline">_</strong>, <strong class="source-inline">.</strong>, <strong class="source-inline">*</strong>, and <strong class="source-inline">(  )</strong>.</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor102"/>Managing your objects in a bucket</h2>
			<p>As mentioned <a id="_idIndexMarker367"/>previously, objects are stored in a <strong class="bold">flat filesystem</strong>, and this can sometimes make it difficult to <a id="_idIndexMarker368"/>manage your objects. As humans, we find it easier to categorize objects into folders and sub-folders, and this helps us organize our data. While object storage does not provide a folder structure, it offers the <a id="_idIndexMarker369"/>ability to use <strong class="bold">prefix</strong> and <strong class="bold">delimiter</strong> (<strong class="source-inline">/</strong>) parameters to help you manage and browse your objects in a<a id="_idIndexMarker370"/> hierarchical fashion. </p>
			<p>At first glance, the usage of prefixes and delimiters (<strong class="source-inline">/</strong>) appears as a typical folder hierarchy, but the prefixes and delimiters themselves also form part of the object's <strong class="bold">key</strong>, as we can see in the following screenshot:</p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="Images/B17124_05_01.jpg" alt="Figure 5.1 – Amazon S3 prefixes and delimiter example&#13;&#10;" width="1349" height="639"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – Amazon S3 prefixes and delimiter example</p>
			<p>As you can see in the preceding screenshot, a file (object) called <a href="B17124_05_Final_SK_ePub.xhtml#_idTextAnchor094"><em class="italic">Chapter 5</em></a><strong class="source-inline"> – Amazon Simple Storage Service S3.docx</strong> appears to be stored in a <strong class="source-inline">cloud-practitioner</strong> sub-folder, under another sub-folder named <strong class="source-inline">campaign</strong>, in a bucket named <strong class="source-inline">packt-marketing</strong>.</p>
			<p>This architecture allows you to better manage your objects, but the fact remains that the keys of the object itself comprise those prefixes and delimiters. This means that the <strong class="source-inline">development/sourcecodes.php</strong> and <strong class="source-inline">production/sourcecodes.php</strong> keys can reside in the same bucket and are considered unique objects because of the different prefixes.</p>
			<p>Prefixes are also used to<a id="_idIndexMarker371"/> help you limit search results to only those keys that begin with a specific prefix name. In addition, delimiters enable you to perform list operations such that all the keys that share a common prefix can be retrieved as a single summary list result.</p>
			<p>Prefixes further enhance performance when it comes to accessing your objects in Amazon S3. For example, you can achieve 3,500 <strong class="source-inline">PUT</strong>/<strong class="source-inline">COPY</strong>/<strong class="source-inline">POST</strong>/<strong class="source-inline">DELETE</strong> or 5,500<em class="italic"> </em><strong class="source-inline">GET</strong>/<strong class="source-inline">HEAD</strong> operations per second per <strong class="bold">prefix</strong> in a bucket. This means that if you have five prefixes to categorize five different collections of objects, you can achieve your read performance of 27,500 read requests per second for your <strong class="source-inline">GET</strong> operations. </p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor103"/>Regional hosting – global availability</h2>
			<p>Your buckets and the <a id="_idIndexMarker372"/>objects contained within them are globally accessible if you define the necessary permissions. However, it is important to realize that a given bucket and any objects it holds are stored in one specific Region alone. When you <a id="_idIndexMarker373"/>create an Amazon S3 bucket in your AWS account using either the web console, <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>), or via <strong class="bold">application programming interface</strong> (<strong class="bold">API</strong>) access, you must specify the <strong class="bold">Region</strong> in which you wish to <a id="_idIndexMarker374"/>create it. </p>
			<p>Your choice of the Region to create a given bucket is going to depend on several factors, including the following ones:</p>
			<ul>
				<li>Optimizing latency by creating buckets closer to end users who need access to them</li>
				<li>Minimizing costs</li>
				<li>Addressing any regulatory requirements such as data-sovereignty laws</li>
			</ul>
			<p>Amazon will <em class="italic">never</em> replicate your data outside of the Region in which you create it. This offers the assurance that you can meet and adhere to any data-residency laws you may be required to follow as a business.</p>
			<p>You can, however, replicate the contents of one bucket in a given Region to another bucket in a different Region for several use cases, including <strong class="bold">disaster recovery</strong> (<strong class="bold">DR</strong>) or sharing content with your colleagues, such that the data is closer to them to reduce overall latency.  </p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor104"/>Access permissions</h2>
			<p>To create and manage <a id="_idIndexMarker375"/>your Amazon S3 buckets and upload and download objects to the bucket, you need to define the necessary permissions. By default, only the resource owner, which is the AWS account that creates the resource, can access a bucket.</p>
			<p>However, you can also grant access to other users in your account, as well as define permissions for specific <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) roles to have access to those resources. In addition, you<a id="_idIndexMarker376"/> can grant access to users in other AWS accounts, and even grant access to members of the public by configuring anonymous access. Anonymous access is ideal when you wish to host publicly accessible digital assets such as documents, images, and videos for your websites.</p>
			<p>Amazon S3 offers two primary methods of granting access to resources such as buckets and objects. You can attach a resource-based policy <a id="_idIndexMarker377"/>known as a bucket policy, or<a id="_idIndexMarker378"/> you can attach <strong class="bold">access-control lists</strong> (<strong class="bold">ACLs</strong>). Let's examine both <a id="_idIndexMarker379"/>of these methods for granting access next.</p>
			<h3>Bucket policies</h3>
			<p>A bucket policy is applied directly to<a id="_idIndexMarker380"/> an entire bucket and can be used to grant access to both the bucket itself and the objects stored within it. Bucket policies can be used to specify different levels of access for different types of objects within the same policy <a id="_idIndexMarker381"/>document. A bucket policy document is also written in <strong class="bold">JavaScript Object Notation Format</strong> (<strong class="bold">JSON</strong>) format, just like IAM policies are. </p>
			<p>With bucket policies, you can also grant anonymous access to object in your buckets, such as a web page, image, or video, which means that anyone with the S3 object URL can access it.</p>
			<p>Bucket policies are very<a id="_idIndexMarker382"/> flexible and allow you to grant cross-account access too. This means that if users in other AWS accounts are permitted, you can grant them access to your buckets by specifying their account ID and their IAM user <strong class="bold">Amazon Resource Name</strong> (<strong class="bold">ARN</strong>).</p>
			<p>Here is an example of a typical bucket policy granting anonymous access to the objects it contains:</p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="Images/B17124_05_02.jpg" alt="Figure 5.2 – Bucket policy example granting anonymous access to the &#13;&#10;contents of the 'packt-marketing' S3 bucket&#13;&#10;" width="568" height="358"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – Bucket policy example granting anonymous access to the contents of the 'packt-marketing' S3 bucket</p>
			<p>As seen in the preceding screenshot, the policy allows anonymous access by specifying the <strong class="source-inline">Principal</strong> attribute as a wildcard (<strong class="source-inline">*</strong>), which indicates everyone. The action allowed on the <strong class="source-inline">packt-marketing</strong> bucket is <strong class="source-inline">s3:GetObject</strong>, which restricts access to only being able to read/download the object(s).</p>
			<p>Another element that<a id="_idIndexMarker383"/> can be added to a bucket policy is a <strong class="bold">condition</strong>. This allows you to restrict the application of a policy based on a predefined condition. You can even set the <strong class="source-inline">Effect</strong> attribute of a policy to <strong class="source-inline">Deny</strong> unless the condition is met. For <a id="_idIndexMarker384"/>example, you can restrict the ability for users to access and download contents from an S3 bucket, unless the request originates from a range of IP addresses specified in the condition, as illustrated in the following screenshot:</p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="Images/B17124_05_03.jpg" alt="Figure 5.3 – Bucket policy defined with a conditional statement&#13;&#10;" width="899" height="684"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – Bucket policy defined with a conditional statement</p>
			<p>In the preceding<a id="_idIndexMarker385"/> screenshot, you will note that the <strong class="source-inline">Effect</strong> attribute of the policy is to deny access unless the <strong class="source-inline">NotIPAddress</strong> condition is met. You will need to replace the dummy IP address range shown (<strong class="source-inline">w.x.y.z/c</strong>) with a real IP address range for your use case.</p>
			<h3>Bucket and object ACLs</h3>
			<p>ACLs are now considered legacy <a id="_idIndexMarker386"/>control systems because bucket policies tend to be flexible and offer more granular levels of access. You can mostly use ACLs to grant anonymous access to objects and buckets, or to other AWS accounts. Since ACLs do not allow you to specify an individual IAM user as the grantee of those permissions, their use case is limited and preference is given to bucket policies instead.  </p>
			<p>However, certain features require you to configure ACLs instead of bucket policies. <strong class="bold">Server access logging</strong>, for example, is a<a id="_idIndexMarker387"/> feature you can enable to provide detailed <a id="_idIndexMarker388"/>records for the requests that are made to an Amazon S3 bucket.</p>
			<h3>IAM policies</h3>
			<p>As previously discussed in <a href="B17124_04_Final_SK_ePub.xhtml#_idTextAnchor068"><em class="italic">Chapter 4</em></a>, <em class="italic">Identity and Access Management</em>, you can create IAM policies that are<a id="_idIndexMarker389"/> assigned to IAM identities (such as IAM users, groups of users, or IAM roles) and define what they can or cannot do with any specific service and/or resource in your AWS account.</p>
			<p>You can, therefore, grant your IAM principals access to S3 buckets and objects contained within those buckets. However, IAM policies cannot be attached directly to the resource. Furthermore, you cannot attach an IAM policy directly to an IAM user in another AWS account. You would first need to create an IAM role with the necessary permissions and then enable a trust policy for the IAM user in the other AWS account to be able to assume the role. </p>
			<p>Finally, IAM policies cannot be used to grant anonymous access because of the simple principle that IAM policies can only be attached to an IAM identity.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can use a combination of bucket policies, ACLs, and IAM policies to grant access. You must remember, however, that any conflicting <strong class="source-inline">Deny</strong> permission will always override an <strong class="source-inline">Allow</strong> permission. However, these policy options are not mutually exclusive. </p>
			<h3>Choosing the right S3 storage class</h3>
			<p>Amazon S3 allows you store <a id="_idIndexMarker390"/>an unlimited amount of data in the cloud. However, not all data needs to be treated the same. For example, you may have some data that you require instant access to, but also other types of data that may be rarely accessed as it represents old archives stored for compliance purposes.</p>
			<p>You may also have some data that you can afford to lose because recreating it would be easy, whereas other types of data may be simply irreplaceable. Depending on the data, its importance, and access patterns, AWS offers different storage classes for different use cases. So, for example, if you need to store old archives, you have the Amazon Glacier storage class, and if you need instant rapid access to your data, you have the Amazon S3 Standard storage class. All storage classes offer 99.999999999% <strong class="bold">durability</strong> for your data. Durability refers to long-term data protection, and AWS offers the necessary infrastructure and processes to manage your data, replicate copies, ensure data redundancy, and safeguard against degradation or other corruption. </p>
			<p>Depending on the storage class that you choose to store your data in, AWS also offers different levels of <strong class="bold">availability</strong>, which determines the percentage of time an object is available for retrieval, based on the underlying storage system being operational (or the Region and <strong class="bold">Availability Zones</strong> (<strong class="bold">AZs</strong>) being<a id="_idIndexMarker391"/> online and accessible). Critical data such as digital assets, medical records, or financial statements would be ideal candidates for those storage classes that offer higher levels of availability. However, secondary copies of data may not need to be as available as the primary copies of the same data.  </p>
			<p>Amazon S3 charges are based around six cost components. These comprise the storage itself (comprising the amount of storage, duration, and storage class your objects are placed in), requests and data retrievals, data transfers, use of transfer acceleration, data management, and analytics, and the use of an Amazon S3 Object Lambda (which is the ability to modify and <a id="_idIndexMarker392"/>process data as it is returned to an application using Lambda functions). Note—data transferred in from the internet to an S3 bucket is free. One way of minimizing costs is to identify data that may not require instance access or that may be replaceable and store it in classes that are cheaper.  </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">A key point to note here is that you can host different objects under different storage classes within the same bucket. You do not need to create separate buckets for each storage class.  </p>
			<p>Let's look at the different storage classes next.</p>
			<h3>Frequent access </h3>
			<p><strong class="bold">Amazon S3 Standard</strong>—This is the <a id="_idIndexMarker393"/>default storage class when you upload objects to a bucket, unless you specify otherwise. Amazon <a id="_idIndexMarker394"/>S3 Standard offers the full eleven 9s (99.9999999%) of durability and four 9s (99.99%) of availability. With Amazon S3 Standard, your objects are always replicated across a minimum of three AZs in the Region you place them in.</p>
			<h3>Infrequent access</h3>
			<p>Amazon S3 offers two<a id="_idIndexMarker395"/> types of infrequent-access storage classes. These can be used to store objects that you are not going to frequently access, but at the same time, you have instant access to the data when you need it.  </p>
			<p>AWS offers these classes at lower costs on the condition that you do not access your data frequently, as you would with the Standard storage class. To enforce the conditions, AWS will charge additional retrieval fees. Furthermore, there is a minimum object size of 128 <strong class="bold">kilobytes</strong> (<strong class="bold">KB</strong>). You can still store objects under this minimum size, but those objects will be billed as though it is a minimum of 128 KB in size. </p>
			<p><strong class="bold">Amazon S3 Standard-Infrequent Access</strong> (<strong class="bold">S3 Standard-IA</strong>)—S3 Standard-IA is designed for data that is just as critical as <a id="_idIndexMarker396"/>with the Standard storage class but is infrequently accessed and is therefore ideal for long-term storage, such as for backups, and to act as a data store for DR purposes.  </p>
			<p><strong class="bold">Amazon S3 One Zone-Infrequent Access</strong> (<strong class="bold">S3 One Zone-IA</strong>)—Data stored in this storage class is restricted to one AZ only within the Region you upload it to. This reduces your overall availability of the<a id="_idIndexMarker397"/> data to 99.5% but is also much cheaper than the Standard or the Standard-IA storage classes. This also means that if there is an outage of the AZ in which your data is stored, you would have to wait for the AZ to come back online before you can access any data. In the unlikely event of the destruction of an AZ, you may also lose that data.</p>
			<p>Amazon recommends this class for data that can act as a secondary backup or that can be recreated.</p>
			<h3>Archive storage</h3>
			<p>Often, data <a id="_idIndexMarker398"/>needs to be retained for archival purposes so that it is available when needed for auditing or reference. More often, regulatory and compliance requirements state that certain types of data should retained for <em class="italic">n</em> number of years. These could be financial information or past medical records, for example.  </p>
			<p>Amazon offers very low-cost storage for such requirements through its archival solution, <strong class="bold">Amazon Glacier</strong>.</p>
			<p><strong class="bold">Amazon Glacier</strong>—This storage <a id="_idIndexMarker399"/>class is designed for long-term archiving of data that may need to be accessed infrequently and within a few hours.  </p>
			<p>Retrieving data from Amazon Glacier works differently, however, as it is not immediately accessible and requires you to initiate a restore request for the data. This restore process can<a id="_idIndexMarker400"/> take some time (between a few minutes to 12 hours) before the data is available to download, and this depends on the retrieval option you choose.  </p>
			<p><strong class="bold">Amazon Glacier Deep Archive</strong>—This is the lowest-cost storage class whereby customers can store very old<a id="_idIndexMarker401"/> historical data to meet compliance and regulatory requirements. Such data may be required to be kept for 7 to 10 years. Retrieval times can take 12 hours or more, depending on the retrieval option chosen.</p>
			<p>We discuss the Amazon S3 Glacier retrieval options in more detail later in this chapter.</p>
			<h3>Unpredictable access patterns</h3>
			<p>Normally, your business will <a id="_idIndexMarker402"/>have a predictable access pattern for most data—for example, newly created data may need to be accessed frequently, such as daily. As data gets older it is accessed less frequently, and sometimes very rarely. Your access pattern, while predictable in this case, changes over time. AWS offers a feature known as lifecycle management that allows you to move data from one storage class to another, depending on changes to your access patterns. We look at lifecyle management and lifecycle rules later in this chapter.  </p>
			<p>Sometimes, however, it is difficult to categorize data as frequently accessed or infrequently accessed, simply because of the nature of that data. You might be frequently accessing a set of objects for an initial period of a few weeks, and those objects may later become infrequently accessed. However, a few months down the line, you may need to access those objects again for analysis or some form of investigation. This data may now need to be frequently accessed over a period once again.</p>
			<p>In these scenarios, AWS offers another storage class called the Intelligent-Tiering storage class. For this privilege, you are charged a small monitoring fee for every object to ensure it is automatically transitioned into the right tier, depending on access patterns.</p>
			<p><strong class="bold">Intelligent-Tiering</strong>—This storage class offers automated tiering of data depending on your access pattern. Objects are <a id="_idIndexMarker403"/>automatically transitioned across four different tiers, two of which are latency access tiers designed to move objects between frequently accessed and infrequently accessed tiers, and the other two being optional archive access tiers: </p>
			<ul>
				<li>Frequent and infrequent tiers—Objects that are frequently accessed (within 30 days) are placed automatically in the frequent access tier (Standard storage class). Any objects not accessed for 30 days are then moved into the infrequent access tier (Standard-IA), thereby incurring lower costs. Remember—the minimum object size for Standard -IA is set to 128 KB, and objects less than this size are treated and charged as if they are a minimum of 128 KB. Any object in the infrequent tier<a id="_idIndexMarker404"/> that later gets accessed is then automatically moved back into the frequent tier and charged accordingly.</li>
				<li>Optional archive access tiers—You can optionally choose to activate the archive access tiers. Once activated, this results in the S3 Intelligent-Tiering service transition when any object is not accessed for 90 days is moved into the Amazon Glacier <strong class="bold">Archive Access tier</strong>. If the object is not accessed for 180 days, it will be moved into the <a id="_idIndexMarker405"/>Amazon <strong class="bold">Deep Archive Access tier</strong>.</li>
			</ul>
			<p>Intelligent-Tiering does not charge a retrieval fee but if objects are archived, retrieval can take some time, depending on the retrieval option chosen. The following table illustrates the retrieval options available:</p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="Images/Table_5.1.jpg" alt="Table 5.1 – Retrieval times for S3 Glacier, Deep Archive, and S3 Intelligent-Tiering archive classes &#13;&#10;" width="1562" height="351"/>
				</div>
			</div>
			<p class="figure-caption">Table 5.1 – Retrieval times for S3 Glacier, Deep Archive, and S3 Intelligent-Tiering archive classes </p>
			<p>As you can see, you have<a id="_idIndexMarker406"/> several retrieval options, and the times will vary depending on which archive storage option you select. </p>
			<h3>S3 on Outposts</h3>
			<p>Amazon Outposts is a fully managed <a id="_idIndexMarker407"/>on-premises service that comes with a 42U rack that can host the same AWS infrastructure and services at your data center. The U refers to rack units or "U-spaces" and is equal to 1.75 inches in height. A standard height is 48U (a 7-foot rack). The service allows you to create a pool of compute, storage, networking, and database services locally on-premises and is ideal if you have workloads running that are very sensitive to low-latency access. Amazon Outposts can also be used as a precursor to migrate your entire data center to the cloud at a later stage.  </p>
			<p>With Amazon Outposts already widely available, AWS offers yet another storage class called <strong class="bold">S3 Outpost</strong>. The service offers<a id="_idIndexMarker408"/> durability and redundancy by storing data across multiple devices and servers hosted on your outposts and is ideal for low-latency access, while also enabling you to meet strict data-residency requirements. Amazon S3 on Outpost allows you to host 48 TB or 96 TB as part of the S3 storage capacity and provides the option to create a maximum of 100 S3 buckets on each outpost. Have a look at the Amazon S3 performance chart shown in the following screenshot:</p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="Images/Table_5.2_a.jpg" alt="" width="1650" height="463"/>
				</div>
			</div>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="Images/Table_5.2_b.jpg" alt="Figure 5.4 – S3 storage class performance and key attributes&#13;&#10;" width="1650" height="704"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 – S3 storage class performance and key attributes</p>
			<p>As shown in the preceding<a id="_idIndexMarker409"/> screenshot, you can choose which storage class to store your objects in depending on your use case. When making this decision, you need to consider durability and availability, as well as the minimum size of your objects, and ascertain whether you require instant access to those objects.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor105"/>Versioning</h2>
			<p>To protect against accidental deletions or overwriting, AWS also offers a feature called S3 Versioning. By default, when you create a<a id="_idIndexMarker410"/> bucket versioning is disabled, which means that if you were to upload an object with the same name (which, as mentioned earlier, is called a <strong class="bold">key</strong> on AWS) as an<a id="_idIndexMarker411"/> existing object in an S3 bucket, then the original object in the bucket will get overwritten. Sometimes this may be exactly what you want, but in most cases, you might want to preserve the original version. Often, objects are overwritten simply because the name of the object was not changed before performing the upload, and this results in an accidental overwrite.</p>
			<p>Amazon S3 offers a feature where you can enable versioning on a bucket. The setting is applied to the entire bucket and will therefore affect all objects. Once versioning is enabled, any object that is uploaded with the same name as an existing object will be tagged with a new version ID. Accessing the object will yield the latest/current version, but a toggle switch in the console will allow you to also see all previous versions in case you need to access an earlier version of the object. </p>
			<p>If you try to delete an<a id="_idIndexMarker412"/> object (without specifying the version ID) in a bucket that has had versioning enabled, then the object is not deleted. AWS adds a delete marker to the object and hides it from the S3 management console view. Subsequently if you need to <em class="italic">restore</em> the object again to make it visible in the S3 management console, you will simply have to delete the delete marker itself. </p>
			<p>You should note that buckets can be in one of three states, outlined as follows:</p>
			<ul>
				<li>Unversioned (default)</li>
				<li>Versioning-enabled</li>
				<li>Versioning-suspended</li>
			</ul>
			<p>Once you enable versioning on a bucket, you can never return to the Unversioned state, but you can suspend versioning if you do not want new versions of objects being created.</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor106"/>Cross-Region and same-Region replication</h2>
			<p>As previously discussed, to help customers comply with compliance and data-residency laws, AWS will never replicate your objects outside of the Region in which you create them. However, there is nothing to stop you from replicating your data outside of the Region in which you uploaded it if there are no regulatory requirements that prevent you from doing so.</p>
			<p><strong class="bold">Amazon Cross-Region Replication</strong> <strong class="bold">(CRR)</strong> is used to asynchronously copy objects across AWS buckets in different AWS Regions. You can use CRR to do the following:</p>
			<ul>
				<li><strong class="bold">Reduce latency</strong>—By copying<a id="_idIndexMarker413"/> objects closer to where end users are based, you can minimize latency in accessing those objects.</li>
				<li><strong class="bold">Increase operational efficiency</strong>—If you run applications across multiple Regions that need access to the same set of data, maintaining multiple copies in those Regions increases efficiency.</li>
				<li><strong class="bold">Meet regulatory and compliance requirements</strong>—Your organization compliance and<a id="_idIndexMarker414"/> regulatory requirements may require you to store copies of data thousands of kilometers away for DR purposes.</li>
			</ul>
			<p>In addition to CRR, Amazon S3 also enables you to configure replication services between buckets in the same<a id="_idIndexMarker415"/> Region. This is known as <strong class="bold">Same-Region Replication</strong> (<strong class="bold">SRR</strong>), which can help you achieve the following:</p>
			<ul>
				<li><strong class="bold">Log Data Aggregation</strong>—You may be collecting log data from several sources and applications. You can collate these datasets in a single log management bucket via replication.</li>
				<li><strong class="bold">Replicating between development and production accounts</strong>—If you use separate development and production accounts and need to use datasets in each, you can use replication to move objects from your development accounts to production accounts.</li>
				<li><strong class="bold">Compliance requirements</strong>—You may be required to maintain multiple copies of your data to adhere to data-residency laws. SRR can help you copy data between multiple buckets to ensure you have more than one copy for compliance purposes.</li>
			</ul>
			<p>It is important to note that both buckets must be configured with versioning enabled in order to set up CRR or SRR. You can also replicate objects across Regions or within the same Region, in either the same AWS account or across multiple AWS accounts. Finally, you can replicate objects into different storage classes from its original storage class, which means that your replicated objects can reside in a cheaper storage class, if perhaps they are being used simply as a backup copy—for example, your original objects can reside in the Standard storage class, but the replicated objects can be placed into the Standard-IA storage class. This will reduce your overall costs of storage.</p>
			<p>Another feature of the replication service is support for multiple destination buckets. You can configure S3 Replication (multi-destination), which enables you to replicate data from one source bucket to multiple destination buckets, either within the same Region, across Regions, or a combination of both.</p>
			<p>Amazon S3 Lifecycle Management Amazon S3 offers unlimited amount of storage. This means that it is very easy to upload any amount of data you create and simply forget about it. At the same time, let's not forget that Amazon S3 charges you for the total amount of storage consumed, and the cost also depends on the storage class in which you place your objects.</p>
			<p>Often, the bulk of your data is going to be infrequently accessed, especially after the initial period in which the data was created. This makes it essential to have some mechanism for moving objects you no longer need frequent access to into a cheaper storage class, to manage your storage costs effectively. In addition, you may also host a lot of archive data that you no longer require after a period, even for compliance and auditing purposes. For example, some regulations state that certain types of data need only be kept for a maximum of 7 years.  </p>
			<p>Manually trying to manage vast quantities of data can be a tiresome affair, often involving the creation of scripts and tools to review the data stored in the cloud. Instead, you can use a reliable solution by Amazon S3, known as Amazon S3 Lifecycle Management. </p>
			<p>Amazon S3 Lifecycle Management actions can be applied to your Amazon S3 buckets. These can be applied to the entire bucket or a subset of data by defining a prefix. These actions fall into two main categories, outlined as follows:</p>
			<ul>
				<li><strong class="bold">Transition actions</strong>—These allow you to move objects from one storage class to another after a certain period of time has passed. For example, if you know that you are going to be infrequently <a id="_idIndexMarker416"/>accessing a particular dataset after 60 days, you can set a rule to move that data from the Standard storage class to the Standard-IA storage class 60 days after creation.</li>
				<li><strong class="bold">Expiration actions</strong>—These allow you<a id="_idIndexMarker417"/> to delete objects from the S3 storage system after a set number of days. For example, if you do not require old log files after 365 days, you can set a rule to automatically expire those objects after 365 days, which will purge them from the storage platform.</li>
			</ul>
			<p>You can use a combination of transition actions and expiration actions as well. For example, you may have log data that you frequently access for the first 30 days. After that, you may still need to revisit that data for a period of 180 days, post which you no longer require it. You can set a combination rule to transition the log files after 30 days of creation from the Standard class to the Standard-IA class, then create another expiration action to purge the data after 180 days.</p>
			<p>You can also apply different lifecycle actions to versioned data—for example, you can have one set of rules and actions against the current version of your objects and another set for previous versions. This further allows you to manage your objects more effectively. </p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor107"/>S3 encryption</h2>
			<p>All data uploaded to Amazon S3 is encrypted in transit using the HTTPS protocol. However, data stored on S3 is not modified in any way, which means that if you are uploading sensitive data in plaintext, the data is stored unencrypted by default. </p>
			<p>To add an additional layer of security, you can encrypt the data before storing it in S3. This is known as encryption at rest. AWS offers<a id="_idIndexMarker418"/> two options for encrypting your data at rest, outlined as follows:</p>
			<ul>
				<li><strong class="bold">Server-side encryption</strong>—When you upload (create) an object, Amazon S3 encrypts it before saving it to disk, and when you download/request an object, it is automatically <a id="_idIndexMarker419"/>decrypted by the S3 service. You have three mutually exclusive options when deciding to encrypt your objects using server-side encryption, outlined as follows:<p>a) <strong class="bold">Server-side encryption</strong> with <strong class="bold">Amazon S3-managed keys</strong> (<strong class="bold">SSE-S3</strong>)— Amazon encrypts your data with a 256-bit <strong class="bold">Advanced Encryption Standard</strong> (<strong class="bold">AES-256</strong>). Each object is encrypted with a unique key, and the key itself is further encrypted with a master key that AWS rotates and manages for you.  </p><p>b) <strong class="bold">Server-side encryption</strong> with <strong class="bold">customer master keys</strong> (<strong class="bold">CMKs</strong>) stored in AWS <strong class="bold">Key Management Service</strong> (<strong class="bold">SSE-KMS</strong>)—This is similar to SSE-S3 but with added features, including the ability to create and manage your own CMKs, as well as an auditing feature that shows when your CMK was used and by whom.</p><p>c) <strong class="bold">Server-side encryption</strong> with <strong class="bold">customer provided keys</strong> (<strong class="bold">CPKs</strong>) (<strong class="bold">SSE-C</strong>)—Encryption is <a id="_idIndexMarker420"/>performed by Amazon S3, but with CPKs. This is ideal if you need to follow regulatory requirements of creating and managing your own keys.</p></li>
				<li><strong class="bold">Client-side encryption</strong>—This is where data is encrypted on the client side and the encrypted data is then<a id="_idIndexMarker421"/> uploaded to Amazon S3. The full encryption process is therefore managed by the customer.</li>
			</ul>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor108"/>Static website hosting</h2>
			<p>In addition to storing data, Amazon S3 also offers a service for hosting complete websites for your company. The only limitation is that the web hosting service is designed to host<a id="_idIndexMarker422"/> static websites only, as opposed to dynamic websites. </p>
			<p>The primary difference is that while the content stored on Amazon S3 to deliver the complete website can be changed and updated, it remains constant and <em class="italic">static</em>, and all users accessing the site will see the same content. </p>
			<p>Dynamic websites use server-side scripting to deliver dynamic content that changes in-flight depending on various parameters, and generally connect to a backend database to fetch content. </p>
			<p>Nevertheless, static websites can also provide complete end-to-end solutions and serve several use cases at a fraction of the cost. In addition to hosting and delivering digital assets such as <strong class="bold">HyperText Transfer Markup</strong> (<strong class="bold">HTML</strong>) files, <strong class="bold">Cascading Style Sheets</strong> (<strong class="bold">CSS</strong>), <strong class="bold">Portable Document Format</strong> (<strong class="bold">PDF</strong>) documents, images, and videos, you can also host client-side scripts that run in the client browser to offer additional features that can include interactive elements—for example, you can run a client-side script on a S3 static website to collect email addresses for potential customers and store them with a third-party email-campaign service provider. You can also host scripts that access additional AWS services. Having lambda functions and, potentially, <strong class="bold">Elastic Container Service</strong> (<strong class="bold">ECS</strong>) or <strong class="bold">Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>) as backends allows you to run anything starting from a static website. Ultimately, you have a wide range of use cases for hosting static websites on Amazon S3. Owing to its highly available and scalable <a id="_idIndexMarker423"/>nature to handle large volumes of traffic, Amazon S3 can prove to be a better solution than hosting static sites across a fleet of EC2 instances, depending on your use case.</p>
			<p>Some examples of considering the Amazon S3 static website hosting service include the following:</p>
			<ul>
				<li><strong class="bold">Developing a product-prelaunch website</strong>—Often, when you need to advertise a pre-launch campaign of a new product range, you may not be able to gauge the amount <a id="_idIndexMarker424"/>of traffic you might generate. Hosting the same solution on a fleet of EC2 instances may be more costly since you may need to scale out fast and utilize a large server farm if your marketing campaigns have been particularly successful. By way of contrast, the scalable nature of S3 will ensure that demand is met automatically with the inflow of large amounts of traffic.</li>
				<li><strong class="bold">Offloading</strong>—With Amazon S3, you get a highly scalable, reliable, and low-latency data storage solution. Even if you host dynamic websites that run on expensive EC2 instances and EBS volumes, you are likely to have a large volume of static content (such as documents, images, and videos, for example). You could offload such static content to an S3 bucket and reference it via API calls from all sites hosted on EC2 instances. The benefits include low storage costs for your digital assets and, because your servers are kept lean, you also get better performance.</li>
				<li><strong class="bold">"Lite" version of website or "Under Maintenance" banners</strong>—Sometimes, you need to host an alternative version of your site when you are performing upgrades or rolling out major updates. By hosting a <em class="italic">lite</em> static version of your site on an S3 bucket, you can easily redirect request to the S3 buckets during periods of maintenance or major updates.</li>
			</ul>
			<p>In this section, we looked at the fundamentals of Amazon S3 on AWS and learned about its various features. In the next section, we look at some additional services, covering transfer of data and archiving.</p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor109"/>Amazon S3TA</h2>
			<p>If you host an S3<a id="_idIndexMarker425"/> bucket in a specific Region but require users across the globe to upload objects to it, your users may experience longer and unexpected variable speeds for uploads and downloads over the internet, depending on where they are based. S3TA reduces this speed variability that is often experienced due to the architecture of the public internet. S3TA routes your uploads via Amazon CloudFront's globally distributed edge locations and AWS backbone networks. This, in turn, gives faster speeds and consistently low latency for your data transfers.</p>
			<p>Here is a screenshot of a S3TA speed checker that uploads a sample file from your browser to various S3 Regions and compares the speed results between standard internet upload versus uploads via S3TA. You can try out the speed test at <a href="https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html">https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html</a>:</p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="Images/B17124_05_05.jpg" alt="Figure 5.5 – Amazon S3TA speed test&#13;&#10;" width="919" height="640"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.5 – Amazon S3TA speed test</p>
			<p>In this section, we<a id="_idIndexMarker426"/> introduced you to the Amazon S3 service and discussed its various feature sets. Amazon S3 is an object storage solution designed to help customers store any amount of data in the cloud. With features such as versioning, CRR/SRR, encryption, and static website hosting, you can use Amazon S3 for a wide range of use cases at affordable storage costs.  </p>
			<p>In the next section, we look at some additional features of the Amazon Glacier services, which offer an archival storage solution.</p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor110"/>Learning about archiving solutions with Amazon S3 Glacier</h1>
			<p>Earlier in this chapter, we<a id="_idIndexMarker427"/> introduced you to the Amazon S3 Glacier and Glacier Deep Archive storage classes. Amazon Glacier offers long-term storage at very a low cost and is intended to be used for archival storage. The architecture offers the same 99.999999999% (eleven 9s) of durability so that in the event of a major disaster, you can rest assured that your old archives will be available to recover if the need arises. The technology works differently from standard S3 storage. The archives need to be requested before you can access/download them, which involves a two-step process of first creating a retrieval job and then downloading your data once the job is complete.</p>
			<p>This retrieval process can take some time and depends on your chosen retrieval options, as previously discussed. However, the upside to this delay in being able to access your data is that you get some of the cheapest storage options on the Amazon platform.</p>
			<h3>Archives and vaults</h3>
			<p>As with <a id="_idIndexMarker428"/>other Amazon S3 storage classes, you can store any amount of data in the Glacier class. However, your objects are stored as archives, and an archive can contain either a single file or multiple files clubbed together in a <strong class="source-inline">.zip</strong> or <strong class="source-inline">.tar</strong> format. The size of your archive can between 1 byte and 40 terabytes. On Amazon S3, a single object can only be a maximum of 5 TB.</p>
			<p>Furthermore, archives can be grouped and stored in vaults. When you create a vault, you need to specify the Region in which it will be created. Vaults also let you define access and notification policies, and you can have up to 1,000 vaults per Region. A vault policy enables you to define who can access it and which actions can be performed on it. You can also define vault lock <a id="_idIndexMarker429"/>policies, such as <strong class="bold">Write Once Read Many</strong> (<strong class="bold">WORM)</strong>, or time-based record-retention policies for regulatory archives.</p>
			<h3>Retrieval options</h3>
			<p>As previously discussed, access to your<a id="_idIndexMarker430"/> archives in Amazon Glacier is not instantaneous. Depending on the Glacier storage class you choose (Glacier versus Deep Archive), you have the following different retrieval options available: </p>
			<p>a) <strong class="bold">Amazon S3 Glacier Retrieval Options</strong></p>
			<ul>
				<li><strong class="bold">Standard</strong>—This is the default <a id="_idIndexMarker431"/>retrieval option and typically takes between 3 and 5 hours to complete, before your data is made available to download.</li>
				<li><strong class="bold">Expedited</strong>—If you need urgent<a id="_idIndexMarker432"/> access to just a subset of your archives, you can opt for the Expedited retrieval option. Naturally, the cost of retrieval is higher than the other options. Furthermore, Expedited retrievals are made available within 1 to 5 minutes for archives of up to 250 <strong class="bold">megabytes</strong> (<strong class="bold">MB</strong>). In addition, two types of expedited retrieval options are available: On-Demand and Provisioned. With On-Demand, your retrieval requests are generally fulfilled within a 5-minute period, although during periods of high demand, this may take longer. Optionally, you can purchase Provisioned capacity, which ensures available retrieval capacity when you need it the most.</li>
				<li><strong class="bold">Bulk</strong> —This is designed to <a id="_idIndexMarker433"/>help you retrieve large amounts of<a id="_idIndexMarker434"/> data at the lowest-cost retrieval option and typically takes between 5 to 12 hours to complete.</li>
			</ul>
			<p>b) <strong class="bold">Amazon S3 Glacier Deep Archive Retrieval Options</strong></p>
			<ul>
				<li><strong class="bold">Standard</strong>—Retrieval of your <a id="_idIndexMarker435"/>deep archives can be achieved within 12 hours.</li>
				<li><strong class="bold">Bulk</strong>—Retrieval of <strong class="bold">petabytes</strong> (<strong class="bold">PB</strong>) of data within 48 hours can be achieved, and it is also the lowest-cost option <a id="_idIndexMarker436"/>available.</li>
			</ul>
			<p>In this section, we looked at archiving solutions with the Amazon S3 Glacier service and how you can store data for many years to fulfill compliance and regulatory requirements. In the next section, we look at how you can connect your on-premises storage services to Amazon S3. </p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor111"/>Connecting your on-premises storage to AWS with Amazon Storage Gateway</h1>
			<p>Amazon Storage<a id="_idIndexMarker437"/> Gateway is an on-premises solution that enables you to connect your on-premises servers and storage systems to the Amazon S3 cloud environment. The service involves installing the Storage Gateway <strong class="bold">virtual machines</strong> (<strong class="bold">VMs</strong>) on-premises and connecting your servers to them. The gateway uses industry-standard protocols to then transfer data <a id="_idIndexMarker438"/>between your servers and the Amazon S3 platform. The VM can be deployed on either VMware ESXi or a Microsoft Hyper-V hypervisor. Optionally, you can also order a hardware appliance, which is a physical server that comes pre-installed and configured with the Storage Gateway software. This reduces the administration time involved in setting up your <a id="_idIndexMarker439"/>own VMs and integrates with your existing storage<a id="_idIndexMarker440"/> systems over protocols such as <strong class="bold">Network File System</strong> (<strong class="bold">NFS</strong>), SMB, and <strong class="bold">Internet Small Computer Systems Interface</strong> (<strong class="bold">iSCSI</strong>).</p>
			<p>Amazon Storage Gateway enables your on-premises application to connect to the AWS storage systems <em class="italic">transparently</em> over standard<a id="_idIndexMarker441"/> protocols such as NFS/SMB, <strong class="bold">Virtual Transport Layer</strong> (<strong class="bold">VTL</strong>), and iSCSI. Connectivity between the Storage Gateway VMs or hardware appliance to the AWS platform can be established over<a id="_idIndexMarker442"/> the internet, through secure IPsec <strong class="bold">virtual private network</strong> (<strong class="bold">VPN</strong>) tunnels or via AWS Direct Connect. </p>
			<p>Amazon S3 Storage Gateway supports different use cases with the following deployment options:</p>
			<ul>
				<li><strong class="bold">File Gateway</strong>—Enables you<a id="_idIndexMarker443"/> to use standard NFS SMB protocols to store data in Amazon S3. Data is also cached locally, enabling low-latency access. Here, there are two options available, as follows:<p>a) <strong class="bold">Amazon S3 File Gateway</strong>—This service enables you to present a file-server solution to your on-premises servers and access Amazon S3, where you can store and retrieve objects in Amazon S3 using industry-standard file protocols such as NFS and SMB. Furthermore, because the data is ultimately stored in S3, you benefit from all its features such as versioning, bucket policies, CRR, and so on.</p><p>b) <strong class="bold">Amazon FSx File Gateway</strong>—This service allows you to connect your on-premises Windows servers or <a id="_idIndexMarker444"/>Windows-based applications (as well as Linux and macOS systems) to the cloud-hosted Amazon FSx for Windows File Server with low latency, and the ability to set up and access a virtually unlimited number of Windows file shares in the cloud. Amazon FSx File Gateway offers full support for SMB protocol support, as well as integration with <strong class="bold">Active Directory</strong> (<strong class="bold">AD</strong>) and the ability to configure access controls using ACLs. </p></li>
				<li><strong class="bold">Volume Gateway</strong>—Enables you to <a id="_idIndexMarker445"/>present block storage volumes to your on-premises servers over the iSCSI protocol. Volume Gateway can be used to asynchronously back up your data to Amazon S3 and comes in two different modes, outlined as follows:</li>
				<li><strong class="bold">Cache mode</strong>—The bulk of your <a id="_idIndexMarker446"/>data is stored in Amazon S3, with only the most frequently accessed data stored locally in the cachefor low-latency connectivity. This <a id="_idIndexMarker447"/>means that you do not need very large amounts of local storage, which helps reduce your capital expenditure.</li>
				<li><strong class="bold">Stored mode</strong>—Your data is<a id="_idIndexMarker448"/> stored locally and available for low-latency access on premises. This is particularly useful if your application is sensitive to latency for data access. The data is then asynchronously backed up to Amazon S3 and can be used for DR purposes.</li>
				<li><strong class="bold">Tape Gateway</strong>—Many <a id="_idIndexMarker449"/>organizations use backup software solutions for their on-premises backup needs (for <a id="_idIndexMarker450"/>example, Veritas Backup Exec and NetBackup). Often, these applications back up data to physical tapes, which most companies store off-site. However, the tape drives, tapes, and off-site storage facilities can become very costly. The Tape Gateway solution comes to the rescue by enabling you to present virtual tapes to your backup software applications over iSCSI. Tape Gateway stores these virtual tapes in a <strong class="bold">virtual tape library</strong> (<strong class="bold">VTL</strong>), which is backed up by Amazon S3. Data is written to these virtual tapes, which results in the Tape Gateway solution asynchronously uploading the <a id="_idIndexMarker451"/>data to Amazon S3. When you need to restore the data, it is downloaded to the local cache, and the backup application can restore it to a location you specify on premises.</li>
			</ul>
			<p>To manage long-term storage of old data, you can transition virtual tapes between Amazon S3 and Amazon S3 Glacier or Amazon S3 Glacier Deep Archive. If you later need to access the data on an <strong class="bold">archived virtual tape</strong>, you need to retrieve the tape and present it to your <a id="_idIndexMarker452"/>Tape Gateway solution. </p>
			<p>Note that retrieval<a id="_idIndexMarker453"/> of an archived tape from Glacier will take between 3 and 5 hours and from Deep Archive can take up to 12 hours.</p>
			<p>In this section, we looked at <a id="_idIndexMarker454"/> how you connect your on-premises applications to the Amazon S3 storage service using the Storage Gateway solution. In the next section, we look at how you can migrate large volumes of data to the cloud using alternative offline methods, which is particularly useful when you have limited internet bandwidth.</p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor112"/>Migrating large datasets to AWS with the AWS Snow Family</h1>
			<p>Many companies<a id="_idIndexMarker455"/> looking to move to the cloud generally host vast amounts of data on premises. While it is possible to transfer data over the public internet into Amazon S3, customers with vast amounts of data may need to consider offline methods of transfer due to bandwidth limitations. </p>
			<p>AWS offers rugged devices that can be delivered to your on-premises location. These include the <strong class="bold">Snowcone</strong>, <strong class="bold">Snowball</strong>, and <strong class="bold">Snowmobile</strong> devices.</p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor113"/>AWS Snowball  </h2>
			<p>At its very basic <a id="_idIndexMarker456"/>offering, you simply copy large amounts of data to the device and ship it back to AWS to have the data imported into Amazon S3. These devices are known as AWS Snowball devices and are part of the Snow Family of devices.  </p>
			<p>These edge devices come with compute and storage capabilities contained in highly rugged, tamper-proof devices. The devices feature a <strong class="bold">Trusted Platform Module</strong> (<strong class="bold">TPM</strong>) chip that detects<a id="_idIndexMarker457"/> unauthorized modifications to hardware, software, or firmware. These devices can be used for storage and data processing at your on-premises locations. Often, customers will use Snowball edge devices for migrations, data collections, and processing with or without internet connectivity.</p>
			<p>Amazon Snowball comes in <em class="italic">two flavors</em>, as follows:</p>
			<ul>
				<li><strong class="bold">Snowball Edge Compute Optimized</strong>—These devices offer both storage and computing resources and can be used for <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>), analytics, and any<a id="_idIndexMarker458"/> local computing tasks. The devices come with 52 <strong class="bold">virtual central processing units</strong> (vCPUs), 208 GB of memory, and an optional NVIDIA Tesla V100 <strong class="bold">graphics processing unit</strong> (<strong class="bold">GPU</strong>). In terms of storage, the device offers 42 TB of <strong class="bold">hard-disk drive</strong> (<strong class="bold">HDD</strong>) capacity and 7.68 TB of <strong class="bold">solid-state drive</strong> (<strong class="bold">SSD</strong>) capacity.</li>
				<li><strong class="bold">Snowball Edge Storage Optimized</strong>—These devices offer larger storage capacity and are ideal for<a id="_idIndexMarker459"/> data migration tasks. With 80 TB of HDD and 1 TB of <strong class="bold">serial advanced technology attachment</strong> (<strong class="bold">SATA</strong>) SDD volumes, you can start moving large volumes of data to the cloud. The device also comes with 40 vCPUs and 80 GB of memory.</li>
			</ul>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor114"/>Amazon Snowcone</h2>
			<p>The smallest member of the <a id="_idIndexMarker460"/>AWS Snow Family, these devices are the smallest ever and weigh just 4.5 <strong class="bold">pounds</strong> (<strong class="bold">lb</strong>) (2.1 <strong class="bold">kilograms</strong> (<strong class="bold">kg</strong>)). Snowcone devices come with 8 TB of usable storage and are designed for outside use in areas of low network connectivity. Examples include IoT, vehicular, and drone use cases.  </p>
			<p>The device also offers compute capabilities with two vCPUs and 4 GB of memory, as well as USB-C power using a cord and optional battery.</p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor115"/>Amazon Snowmobile</h2>
			<p>If you need to transfer exabyte-scale data to the cloud, then you are going need an extremely large 45-foot-long <em class="italic">rugged</em> shipping container. Amazon Snowmobile can transfer up to 100<a id="_idIndexMarker461"/> PB of data to the cloud and assist in all your data center migration efforts.</p>
			<p>The shipping container is delivered on-site and an AWS team member will work with your team to connect a high-speed switch from Snowmobile to your local network.</p>
			<p>With 24/7 video surveillance, optional escort security, and 256-bit encryption, you have access to the most secure way of transferring sensitive data to Amazon S3.  </p>
			<p>In this section, we reviewed offline methods to transfer large amounts of data to the cloud and assist in your data migration efforts. The Amazon Snow Family offers more than just storage containers—these devices come with high levels of compute capabilities to perform data processing, analytics, and ML tasks as you copy data to them as well.</p>
			<p>In the next section, we review some of the key points highlighted in this chapter.</p>
			<h1 id="_idParaDest-116"><a id="_idTextAnchor116"/>Exercise 5.1 – Setting up an Amazon S3 bucket</h1>
			<p>In this exercise, we<a id="_idIndexMarker462"/> will create an Amazon S3 bucket and upload an object to it. More specifically, we will upload a single web page document and test access to it after the upload. Since we plan to later <em class="italic">use</em> this bucket to host a static website and make content accessible to anyone on the internet, you will need to disable the <strong class="bold">Block Public Access</strong> setting, as discussed in the access permissions settings earlier in this chapter. Proceed as follows:</p>
			<ol>
				<li>On your computer, create a new file using a standard text editor of your choice (Notepad on Windows or TextEdit on Mac).</li>
				<li>Add the following lines of code to the document:<p class="source-code"><strong class="bold">&lt;html&gt;</strong></p><p class="source-code"><strong class="bold">&lt;title&gt;Blueberry Muffin Recipe&lt;/title&gt;</strong></p><p class="source-code"><strong class="bold">&lt;Body&gt;</strong></p><p class="source-code"><strong class="bold">&lt;h1&gt;Blueberry Muffin Recipe&lt;/h1&gt;&lt;p&gt;</strong></p><p class="source-code"><strong class="bold">Bake the ultimate blueberry muffins for your guests and loved ones. This recipe shows you how to create cafe-style blueberry muffins in your own kitchen.&lt;/p&gt;</strong></p><p class="source-code"><strong class="bold">&lt;p&gt;&lt;strong&gt;Ingredients:&lt;/strong&gt;&lt;/p&gt;</strong></p><p class="source-code"><strong class="bold">&lt;ul&gt;&lt;li&gt;100g fresh &lt;span style="color: rgb(85, 57, 130);"&gt;&lt;strong&gt;blueberries&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;</strong></p><p class="source-code"><strong class="bold">&lt;li&gt;300g flour&lt;/li&gt;</strong></p><p class="source-code"><strong class="bold">&lt;li&gt;150g granulated sugar&lt;/li&gt;</strong></p><p class="source-code"><strong class="bold">&lt;li&gt;1 tsp. vanilla&lt;/li&gt;</strong></p><p class="source-code"><strong class="bold">&lt;li&gt;60 ml vegetable oil&lt;/li&gt;</strong></p><p class="source-code"><strong class="bold">&lt;li&gt;50g of butter&lt;/li&gt;</strong></p><p class="source-code"><strong class="bold">&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;</strong></p><p class="source-code"><strong class="bold">&lt;/body&gt;</strong></p><p class="source-code"><strong class="bold">&lt;/html&gt;</strong></p><p>The preceding <a id="_idIndexMarker463"/>code is also available in our GitHub repository for this book <a href="https://github.com/PacktPublishing/AWS-Certified-Cloud-Practitioner-Exam-Guide">https://github.com/PacktPublishing/AWS-Certified-Cloud-Practitioner-Exam-Guide</a>, and you can simply download the <strong class="source-inline">index.html</strong> file to your desktop as well.</p></li>
				<li>Next, save it with a filename of <strong class="source-inline">index</strong> with a <strong class="source-inline">.html</strong> extension—so, the filename with the extension should be <strong class="source-inline">index.html</strong>. This will create a simple web page object for you. You may need to set the <strong class="bold">Save as type</strong> option to <strong class="bold">All Files</strong>, as illustrated in the following screenshot:<div id="_idContainer084" class="IMG---Figure"><img src="Images/B17124_05_06.jpg" alt="Figure 5.6 – Saving a file with a .html extension to create a web page&#13;&#10;" width="847" height="372"/></div><p class="figure-caption">Figure 5.6 – Saving a file with a .html extension to create a web page</p></li>
				<li>Next, log in to<a id="_idIndexMarker464"/> your AWS account as the IAM user, <strong class="bold">Alice</strong>.</li>
				<li>Navigate to the Amazon S3 console.</li>
				<li>Click <strong class="bold">Create bucket</strong>, as illustrated in the following screenshot:<div id="_idContainer085" class="IMG---Figure"><img src="Images/B17124_05_07.jpg" alt="Figure 5.7 – List of buckets&#13;&#10;" width="1357" height="347"/></div><p class="figure-caption">Figure 5.7 – List of buckets</p></li>
				<li>For the name of the bucket, type in your name followed by a hyphen (<strong class="source-inline">-</strong>) and the word <strong class="source-inline">webpage</strong>. Make sure there are no spaces in the name and that all lowercase letters are used. Assuming that the name you have chosen has not already been taken by another customer of AWS, you should be able to use this bucket name. If you get an error when you create the bucket, stating that the name is not available, you will simply need to choose a different name.</li>
				<li>For <strong class="bold">AWS Region</strong>, select <strong class="source-inline">us-east-1</strong>.</li>
				<li>Next, under the <strong class="bold">Block Public Access settings for bucket</strong> sub-heading, uncheck he box for <strong class="bold">Block all public access</strong>. Note that for general use cases, you do not want to unblock public access unless your use case demands it, such as when trying to configure static website hosting, which we will look at later in <em class="italic">Exercise 5.4</em>. If you <a id="_idIndexMarker465"/>do not need anonymous access such as that from end users on the public internet, you must always correctly configure your permissions using bucket policies, ACLs, or access point policies, to ensure<a id="_idIndexMarker466"/> you leverage the <strong class="bold">principal of least privilege</strong> (<strong class="bold">PoLP</strong>).</li>
				<li>Next, check the box to state that you acknowledge that the preceding settings could make the bucket and its objects publicly accessible, as illustrated in the following screenshot:<div id="_idContainer086" class="IMG---Figure"><img src="Images/B17124_05_08.jpg" alt="Figure 5.8 – Turning off block all public access on your bucket&#13;&#10;" width="1156" height="241"/></div><p class="figure-caption">Figure 5.8 – Turning off block all public access on your bucket</p></li>
				<li>Leave all other settings as default and click on the <strong class="bold">Create bucket</strong> button at the bottom of the screen. Your Amazon S3 bucket has now been created.  </li>
				<li>Next, in your list of buckets in the main S3 console, select the bucket you just created. This will take you to the current list of objects in the bucket. You will note that there will be none at present.</li>
				<li>You will notice an <strong class="bold">Upload</strong> button. Click on this button and you will have the option to add files and folders, as per the following screenshot:<div id="_idContainer087" class="IMG---Figure"><img src="Images/B17124_05_09.jpg" alt="Figure 5.9 – Uploading object to your bucket&#13;&#10;" width="1217" height="528"/></div><p class="figure-caption">Figure 5.9 – Uploading object to your bucket</p></li>
				<li>Add the <strong class="source-inline">index.html</strong> file you created/downloaded earlier.</li>
				<li>Scroll toward the <a id="_idIndexMarker467"/>bottom of the screen and click the <strong class="bold">Upload</strong> button.</li>
				<li>Your file will be uploaded.</li>
				<li>You will note a green banner at the top of the screen to say that the upload has been successful. Now, go ahead and click <strong class="bold">Exit</strong>.</li>
				<li>You are then presented with the contents of the bucket you just created.</li>
				<li>You can now click on the <strong class="source-inline">index.html</strong> file in the <strong class="bold">Objects</strong> list, which will take you to the Amazon S3 properties page of the file itself.</li>
				<li>Note that each object has its own URL accessible from the internet (as long as the permissions are correctly set), as we can see in the following screenshot:<div id="_idContainer088" class="IMG---Figure"><img src="Images/B17124_05_10.jpg" alt="Figure 5.10 – Uploading the index.html object to your bucket&#13;&#10;" width="1322" height="549"/></div><p class="figure-caption">Figure 5.10 – Uploading the index.html object to your bucket</p></li>
				<li>If you try to click<a id="_idIndexMarker468"/> on this object URL to open it up in another browser window, you will find that you cannot access it. Instead, you get an <strong class="bold">Access Denied</strong> error message. This is because access to an object via its URL has the same effect as trying to anonymously read the object over the public internet.</li>
			</ol>
			<p>Although we disabled the <strong class="bold">Block all public access</strong> setting earlier, your buckets and objects still need an explicit <strong class="source-inline">Allow</strong> rule to grant access to them. You could click on the <strong class="bold">Permissions</strong> tab of the object itself and set up an ACL to enable public access for this object. However, as discussed previously, using bucket policies is a better option as these offer more features and granular control.  </p>
			<p>In the next exercise, we will set up a bucket policy to see how we can allow public access to this file.</p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor117"/>Exercise 5.2 – Configuring public access to S3 buckets</h1>
			<p>In this exercise, we will configure the Amazon <a id="_idIndexMarker469"/>S3 bucket with a <strong class="bold">bucket policy</strong> (resource policy) that will allow users on the public internet to be able to access and read the <strong class="source-inline">index.html</strong> web page you created earlier. </p>
			<p>Remember that you <a id="_idIndexMarker470"/>could choose to restrict access to only a set of known users—for example, if you wanted only IAM users in your AWS account to have access to the objects. You can also configure cross-account access, in which you define principals that belong to another AWS account and grant them specific levels of access.</p>
			<p>In this exercise, we want to grant anonymous access to the <strong class="source-inline">index.html</strong> page because ultimately, we will be building out a static website hosting service using this bucket in later exercises. Proceed as follows:</p>
			<ol>
				<li value="1">Navigate back to the S3 console and click on the bucket you just created, as illustrated in the following screenshot:<div id="_idContainer089" class="IMG---Figure"><img src="Images/B17124_05_11.jpg" alt="Figure 5.11 – Successful upload&#13;&#10;" width="1388" height="571"/></div><p class="figure-caption">Figure 5.11 – Successful upload</p></li>
				<li>Click on the <strong class="bold">Permissions</strong> tab.</li>
				<li>You will note that the <strong class="bold">Block public access</strong> has been disabled and is in an <strong class="bold">Off</strong> state.</li>
				<li>Scroll further down until you get to <strong class="bold">Bucket Policies</strong>, and then click <strong class="bold">Edit</strong>.</li>
				<li>Add the following policy, replacing the values in the placeholder <strong class="source-inline">Your-Bucket-Name</strong> with the name<a id="_idIndexMarker471"/> of your S3 bucket: <p class="source-code"><strong class="bold">{</strong></p><p class="source-code"><strong class="bold">  "Id": "Policy1613735718314",</strong></p><p class="source-code"><strong class="bold">  "Version": "2012-10-17",</strong></p><p class="source-code"><strong class="bold">  "Statement": [</strong></p><p class="source-code"><strong class="bold">    {</strong></p><p class="source-code"><strong class="bold">      "Sid": "Stmt1613735715412",</strong></p><p class="source-code"><strong class="bold">      "Action": [</strong></p><p class="source-code"><strong class="bold">        "s3:GetObject"</strong></p><p class="source-code"><strong class="bold">      ],</strong></p><p class="source-code"><strong class="bold">      "Effect": "Allow",</strong></p><p class="source-code"><strong class="bold">      "Resource": "arn:aws:s3:::Your-Bucket-Name/*",</strong></p><p class="source-code"><strong class="bold">      "Principal": "*"</strong></p><p class="source-code"><strong class="bold">    }</strong></p><p class="source-code"><strong class="bold">  ]</strong></p><p class="source-code"><strong class="bold">}</strong></p></li>
				<li>Click <strong class="bold">Save Changes</strong>. If you copied the policy correctly, the policy validator will not throw up any errors.</li>
				<li>You should then get a confirmation that the policy has been saved and, more importantly, you will note that the bucket's contents are now publicly accessible, as illustrated in the following screenshot:<div id="_idContainer090" class="IMG---Figure"><img src="Images/B17124_05_12.jpg" alt="Figure 5.12 – S3 Bucket permissions tab&#13;&#10;" width="1254" height="641"/></div><p class="figure-caption">Figure 5.12 – S3 Bucket permissions tab</p></li>
				<li>Next, click on the <strong class="bold">Objects</strong> tab again.</li>
				<li>Click on the <strong class="source-inline">index.html</strong> file to open its <strong class="bold">S3 Properties</strong> pane.</li>
				<li>Right-mouse click <a id="_idIndexMarker472"/>on the object URL and open it in a new browser tab.</li>
				<li>You should find that the web page is now accessible from your browser, as illustrated in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="Images/B17124_05_13.jpg" alt="Figure 5.13 – Your index.html page in a browser window&#13;&#10;" width="931" height="357"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.13 – Your index.html page in a browser window</p>
			<p>So far, we have disabled block public access on this bucket as we will eventually be configuring it for static website hosting. In this exercise, we also uploaded the object, <strong class="source-inline">index.html</strong>, which is a recipe document written in HTML code. </p>
			<p>In the next exercise, you will learn how to configure versioning on your bucket. Versioning will help you create<a id="_idIndexMarker473"/> previous copies of an object so that new uploads of updated content for the same object are stored as new versions of the object. This will enable you to prevent against accidental changes to your objects by being able to restore a previous version, as we will see in the upcoming exercises. </p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor118"/>Exercise 5.3 – Enabling versioning on your bucket</h1>
			<p>In this exercise, we <a id="_idIndexMarker474"/>will enable versioning on the Amazon S3 bucket. As you update existing objects with newer versions, you can rest assured that if you need to revert to an older version, those versions will still exist in your bucket. Obviously, if you try to delete a specific version of the object itself, then it will be purged from the S3 platform. However, enabling versioning can help prevent against accidental deletions and overwrites. Proceed as follows:</p>
			<ol>
				<li value="1">Navigate back to the S3 console.</li>
				<li>Click on the bucket you created earlier in <em class="italic">Exercise 5.1</em>.</li>
				<li>Click on the <strong class="bold">Properties</strong> tab.</li>
				<li>You will see an <strong class="bold">Edit the Bucket Versioning</strong> option to edit the state. At present, the versioning will be set to <strong class="bold">disabled</strong>. Note that once again you can suspend versioning actions, but you will not be able to disable them.</li>
				<li>Click <strong class="bold">Edit</strong> in the <strong class="bold">Bucket Versioning</strong> section.</li>
				<li>Select <strong class="bold">Enable</strong>.</li>
				<li>Click <strong class="bold">Save Changes</strong>.</li>
			</ol>
			<p>Let's try to test the versioning feature next, as follows:</p>
			<ol>
				<li value="1">Navigate to the location where you saved the <strong class="source-inline">index.html</strong> web page on your computer. Open<a id="_idIndexMarker475"/> the file with your text editor using Notepad or TextEdit (for Mac).</li>
				<li>Replace the word <strong class="source-inline">Blueberry</strong> in the existing <strong class="source-inline">&lt;H1&gt;</strong> tag within the document to <strong class="source-inline">Chocolate</strong>.</li>
				<li>Save the file without changing the format or extension.</li>
				<li>Navigate back to your Amazon S3 console in your AWS account and click on the bucket you created earlier.</li>
				<li>Click on <strong class="bold">Objects</strong>, as illustrated in the following screenshot:<div id="_idContainer092" class="IMG---Figure"><img src="Images/B17124_05_14.jpg" alt="Figure 5.14 – List of objects in your Amazon S3 bucket&#13;&#10;" width="1237" height="757"/></div><p class="figure-caption">Figure 5.14 – List of objects in your Amazon S3 bucket</p></li>
				<li>Click <strong class="bold">Upload</strong>.</li>
				<li>Click <strong class="bold">Add Files</strong>.</li>
				<li>Select the<a id="_idIndexMarker476"/> same <strong class="source-inline">index.html</strong> file you updated moments ago and click <strong class="bold">Upload</strong>.</li>
				<li>Click <strong class="bold">Exit</strong>.</li>
				<li>Click on the <strong class="source-inline">index.html</strong> fileagain to open up its <strong class="bold">S3 Properties</strong> window.</li>
				<li>Under <strong class="bold">Object URL</strong>, right-mouse click on the URL and open in a new browser window.</li>
				<li>You should find that the web page has been updated with the word <strong class="bold">Chocolate</strong>, as illustrated in the following screenshot:<div id="_idContainer093" class="IMG---Figure"><img src="Images/B17124_05_15.jpg" alt="Figure 5.15 – Your index.html page showcasing the recipe&#13;&#10;" width="666" height="402"/></div><p class="figure-caption">Figure 5.15 – Your index.html page showcasing the recipe</p></li>
				<li>In the Amazon S3 <a id="_idIndexMarker477"/>management console, click on the <strong class="bold">Versions</strong> tab, as illustrated in the following screenshot:<div id="_idContainer094" class="IMG---Figure"><img src="Images/B17124_05_16.jpg" alt="Figure 5.16 – Bucket version tab&#13;&#10;" width="1593" height="646"/></div><p class="figure-caption">Figure 5.16 – Bucket version tab</p></li>
				<li>Note that there are two versions—the original version, which has a version ID of <strong class="source-inline">null</strong>, and a newer version with a <strong class="bold">string of letters and numbers</strong>, called <strong class="bold">Current version</strong>. The reason why the first version has a version ID of <strong class="source-inline">null</strong> is because it was created/uploaded to the bucket before we enabled versioning. Going forward, all new updates to this file will be assigned a new version ID, allowing you to preserve older versions if you ever need to access them again.</li>
			</ol>
			<p>In this exercise, you<a id="_idIndexMarker478"/> learned how to configure versioning on your bucket. You were able to upload and manage multiple versions of the same object, and you discovered how unversioned objects have a version ID of <strong class="source-inline">null</strong>, whereas versioned objects have a version ID comprised of a series of characters unique to that version. You also discovered how to display a list of available versions of your objects in a version-enabled bucket.</p>
			<h1 id="_idParaDest-119"><a id="_idTextAnchor119"/>Exercise 5.4 – Setting up static website hosting</h1>
			<p>In this exercise, we<a id="_idIndexMarker479"/> will configure the bucket to host a static website. When configured with a static website hosting service, the bucket will be configured with a website endpoint that you can distribute to your users, who can then access all the pages (assuming they are linked) using the standard HTML protocol.  </p>
			<p>To configure your bucket for static website hosting, you need a minimum of two files— an <strong class="source-inline">index.html</strong> file and an <strong class="source-inline">error.html</strong> file. An error file is simply a file that the S3 static website hosting service will redirect to if there is a problem with the <strong class="source-inline">index.html</strong> file—for example, if it cannot find the <strong class="source-inline">index.html</strong> page. You could use the <strong class="source-inline">error.html</strong> file to broadcast the fact that perhaps the site is under maintenance. Proceed as follows:</p>
			<ol>
				<li value="1">Create a new HTML file using your text editor as before (either Notepad on Windows or TextEdit on a Mac). However, in this file, simply add a line of text along the lines of <strong class="source-inline">This site is under maintenance</strong>.</li>
				<li>Save the file as <strong class="source-inline">error.html</strong>, making sure to set the file types to <strong class="bold">All Types</strong> if you are using a Windows machine.</li>
				<li>Navigate<a id="_idIndexMarker480"/> back to the S3 console and click on your S3 bucket.</li>
				<li>Click on <strong class="bold">Properties</strong> and then scroll toward the bottom of the page, until you find the <strong class="bold">Static website hosting</strong> section heading.</li>
				<li>Click <strong class="bold">Edit</strong> and select the <strong class="bold">Enable</strong> option.</li>
				<li>For <strong class="bold">Hosting type</strong>, select <strong class="bold">Host a static website</strong>.</li>
				<li>Under the <strong class="bold">Index document</strong> sub-heading, type the name of your index file—in this case, <strong class="source-inline">index.html</strong>.</li>
				<li>Under the <strong class="bold">Error document</strong> sub-heading, type the name of your new error file—in this case, <strong class="source-inline">error.html</strong>.</li>
				<li>Leave all the remaining settings at their defaults and click <strong class="bold">Save changes</strong> at the bottom of the page.</li>
				<li>Next, click on the <strong class="bold">Objects</strong> tab.</li>
				<li>Click <strong class="bold">Upload</strong> and click <strong class="bold">Add files</strong>.</li>
				<li>Select the <strong class="source-inline">error.html</strong> file and click <strong class="bold">Upload</strong>. You should then see a screen like this: <div id="_idContainer095" class="IMG---Figure"><img src="Images/B17124_05_17.jpg" alt="Figure 5.17 – Upload of updated index page to your bucket&#13;&#10;" width="1570" height="570"/></div><p class="figure-caption">Figure 5.17 – Upload of updated index page to your bucket</p></li>
				<li>Click <strong class="bold">Exit</strong>.</li>
				<li>At this stage, your bucket has been configured for static website hosting. To test it, you need to access your website via the S3 website URL endpoint.</li>
				<li>In the S3 console, while still viewing the contents of the buckets (under <strong class="bold">Objects</strong>), click on the <strong class="bold">Properties</strong> tab again.</li>
				<li>Scroll down till <a id="_idIndexMarker481"/>you reach the <strong class="bold">Static website hosting</strong> section, and you will note the URL is provided under the <strong class="bold">Bucket website endpoint</strong> heading, as illustrated in the following screenshot:<div id="_idContainer096" class="IMG---Figure"><img src="Images/B17124_05_18.jpg" alt="Figure 5.18 – Enabling static website hosting on your bucket&#13;&#10;" width="1416" height="412"/></div><p class="figure-caption">Figure 5.18 – Enabling static website hosting on your bucket</p></li>
				<li>Navigate to the <a id="_idIndexMarker482"/>provided URL in a new browser window, and you should find that the website opens with the recipe web page, as illustrated in the following screenshot:<div id="_idContainer097" class="IMG---Figure"><img src="Images/B17124_05_19.jpg" alt="Figure 5.19 – Updated index.html page with the wrong heading (Chocolate)&#13;&#10;" width="665" height="404"/></div><p class="figure-caption">Figure 5.19 – Updated index.html page with the wrong heading (Chocolate)</p></li>
				<li>As you will note, the recipe is for a blueberry muffin, but the heading has changed to <strong class="bold">Chocolate</strong>. Assuming that this was an error in the update, we can easily revert to the previous version of this web page because we have already configured the bucket for versioning.</li>
				<li>Navigate back to the Amazon S3 bucket so that you are looking at the actual contents of the bucket under the <strong class="bold">Objects</strong> tab, as illustrated in the following screenshot:<div id="_idContainer098" class="IMG---Figure"><img src="Images/B17124_05_20.jpg" alt="Figure 5.20 – List of updated objects in your bucket&#13;&#10;" width="1650" height="670"/></div><p class="figure-caption">Figure 5.20 – List of updated objects in your bucket</p></li>
				<li>Notice the <strong class="bold">List versions</strong> toggle just below the <strong class="bold">Objects</strong> sub-heading.</li>
				<li>Click this toggle switch to list <a id="_idIndexMarker483"/>out all versions of all objects in your bucket, as illustrated in the following screenshot:<div id="_idContainer099" class="IMG---Figure"><img src="Images/B17124_05_21.jpg" alt="Figure 5.21 – List of objects and their individual versions. &#13;&#10;" width="1351" height="543"/></div><p class="figure-caption">Figure 5.21 – List of objects and their individual versions. </p></li>
				<li>As you will note, there are two versions of the <strong class="source-inline">index.html</strong> page. The latest version has got a version ID and contains an incorrect recipe heading. </li>
				<li>Click on the checkbox to select this version and then click the <strong class="bold">Delete</strong> button.</li>
				<li>You are then prompted to confirm your delete request by typing in the phrase <strong class="source-inline">permanently delete</strong> in the provided textbox, as illustrated in the following screenshot:<div id="_idContainer100" class="IMG---Figure"><img src="Images/B17124_05_22.jpg" alt="Figure 5.22 – Deleting incorrect version of the index.html page&#13;&#10;" width="1213" height="313"/></div><p class="figure-caption">Figure 5.22 – Deleting incorrect version of the index.html page</p></li>
				<li>Next, click <strong class="bold">Delete objects</strong>.</li>
				<li>Click <strong class="bold">Exit</strong>, and you will note that the version has now been deleted.  </li>
				<li>Click on <strong class="bold">Properties</strong> again and then scroll down to the <strong class="bold">Static website hosting</strong> section. Open up the <a id="_idIndexMarker484"/>website URL in a new browser tab and you should see that the older, correct version of the web page is now displayed, as illustrated in the following screenshot: </li>
			</ol>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="Images/B17124_05_23.jpg" alt="Figure 5.23 – Previous recipe page with the correct heading (Blueberry)&#13;&#10;" width="667" height="400"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.23 – Previous recipe page with the correct heading (Blueberry)</p>
			<p>In this exercise, you<a id="_idIndexMarker485"/> learned how to configure your bucket with static website hosting. You learned how, in this particular lab exercise, we made an error in the title of our web page and we were able to revert to an older versioning of the same document, thanks to having versioning enabled earlier. </p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor120"/>Summary</h1>
			<p>Amazon S3 is one of AWS's flagship storage products and comes with unlimited amounts of storage capacity that is highly scalable and durable.  </p>
			<p>In this chapter, you learned about the core feature of Amazon S3, including versioning, lifecycle management, and replication services, and how Amazon S3 meets a wide range of use cases. You also learned how you can build and deploy static website hosting on an Amazon S3 bucket and its various applications in the real world.</p>
			<p>We also discussed how Amazon S3 comes with a wide range of security tools such as the ability to create granular access permissions via bucket policies and ACLs, as well as encryption of data in transit and at rest. You have also learned how you can connect your on-premises workloads to the Amazon S3 platform using Amazon Storage Gateway via the internet, a VPN, or AWS Direct Connect services.</p>
			<p>If you are looking to migrate large amounts of data to the cloud, you can use the Amazon Snowball service to help you transfer large volumes of data, using rugged and tamper-resistant devices that are shipped to your on-premises location. Once you get the data copied, you simply ship it back to AWS to have your data imported into your S3 environment.</p>
			<p>In the next chapter, we discuss networking services, focusing on the <strong class="bold">Amazon Virtual Private Cloud</strong> (<strong class="bold">VPC</strong>) service, among others. We also look at the Amazon Direct Connect service and at how you can connect your on-premises network with your AWS cloud services using VPN technologies. </p>
			<p>In the next section, we look at some review questions for this chapter to test your knwoledge.</p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor121"/>Questions</h1>
			<p>Here are a few questions to test your knowledge:</p>
			<ol>
				<li value="1">Which of the following is true regarding Amazon S3? (Select 2 answers)<ol><li>Amazon S3 is object-based storage.</li><li>Amazon S3 is an example of file storage.</li><li>Amazon S3 is an example of block storage.</li><li>The Amazon S3 One Zone-IA storage class offers 99.5% of availability. Amazon S3 can be configured as shared mount volumes for Linux-based EC2 instances.</li></ol></li>
				<li>You wish to enforce a policy on an S3 bucket that grants anonymous access to its content if users connect to the data from the corporate and branch offices as part of your security strategy. Which S3 configuration feature will enable you to define the IP ranges from where you will allow access to the data?<ol><li>Security groups</li><li>Bucket policy</li><li>NTFS permissions</li><li><strong class="bold">Network ACLs</strong> (<strong class="bold">NACLs</strong>)</li></ol></li>
				<li>Which AWS service is the most cost-effective if you need to host static website content for an upcoming product launch?<ol><li>Amazon EC2</li><li>Amazon EFS</li><li>Amazon S3</li><li>Azure ExpressRoute</li></ol></li>
				<li>Which Amazon S3 storage class enables you to optimize costs by automatically moving data to the most cost-effective access tier, while ensuring that frequently accessed data is made available immediately?<ol><li>Amazon S3 Standard</li><li>Amazon S3 One-Zone IA</li><li>Amazon Snowball</li><li>Amazon S3 Intelligent-Tiering</li></ol></li>
				<li>Which Amazon S3 service can be configured to automatically migrate data from one storage class to another after a set number of days as a means of reducing your costs, especially where frequent instant access may not be required to that subset of data?<ol><li>Static website hosting</li><li>Lifecycle management</li><li>Storage transition</li><li>S3 migration</li></ol></li>
				<li>When retrieving data from Amazon Glacier, what is the typical time taken by a Standard retrieval option to make the archive available for download?<ol><li>20 minutes</li><li>24 hours</li><li>3 to 5 hours</li><li>90 seconds</li></ol></li>
				<li>Which feature of the Amazon S3 platform enables you to upload content to a centralized bucket from across any location via Amazon edge locations, ensuring faster transfer speeds and avoidance of public internet congestion?<ol><li>Amazon S3TA</li><li>AWS S3 Storage Gateway</li><li>Amazon VPC</li><li>CloudFront</li></ol></li>
				<li>Your on-premises applications require access to a centrally managed cloud storage service. The application running on your servers need to be able to store and retrieve files as durable objects on Amazon S3 over standard NFS-based access with local caching. Which AWS service can help you deliver a solution to meet the aforementioned requirements?<ol><li>AWS Storage Gateway— Amazon S3File Gateway</li><li>AWS EFS</li><li>Amazon Redshift</li><li>EBS volumes</li></ol></li>
				<li>You are looking to migrate your on-premises data to the cloud. As part of a one-time data migration effort, you need to transfer over 900 TB of data to Amazon S3 in a couple of weeks. Which is the most cost-effective strategy to transfer this amount of data to the cloud?<ol><li>Use the Amazon RDS service</li><li>Use the Amazon Snowball service</li><li>Use the Amazon VPN connection between your on-premises network and AWS</li><li>Use AWS Rain</li></ol></li>
			</ol>
		</div>
	</div></body></html>