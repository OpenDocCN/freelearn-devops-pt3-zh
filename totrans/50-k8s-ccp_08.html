<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer094">&#13;
			<h1 id="_idParaDest-137" class="chapter-number"><a id="_idTextAnchor138"/>6</h1>&#13;
			<h1 id="_idParaDest-138"><a id="_idTextAnchor139"/>Kubernetes Deployment– Same Game, Next Level</h1>&#13;
			<p>In the previous chapter, you dove into different deployment scenarios and how you should think not only about controllers, but also about upgrading apps, different types of apps to deploy, and different methods for getting an app up and running. In this chapter, you’re going to dive a bit deeper into the different styles of deploying and troubleshooting versus just doing <span class="No-Break">the deployment.</span></p>&#13;
			<p>The first step in any type of deployment is figuring out what you’re doing – what type of application it is, what type of Kubernetes resource you want to use, and the different plugins that you may want to use, such as a CSI. After you figure out the logistics of what you want to deploy, the next step is to think about how you want <span class="No-Break">to deploy.</span></p>&#13;
			<p>With Kubernetes, there are many different deployment methods – automated deployments, manual deployments, and something in between automated and manual. There’s a vast number of different ways to perform deployments, so you won’t learn about all of them because that could take up more than six chapters of a book in itself, but you will learn about the primary ways to deploy and package up <span class="No-Break">Kubernetes Manifests.</span></p>&#13;
			<p>After you learn about deployments, thinking about how to troubleshoot once something inevitably goes wrong is a good, logical next step. Typically, engineers will learn troubleshooting on the fly, but it’s a good approach to think about troubleshooting techniques prior to something <span class="No-Break">going wrong.</span></p>&#13;
			<p>After learning about troubleshooting and deploying containerized apps, you’re going to wrap up with how to manage network connectivity between apps running on Kubernetes and how to migrate existing, more <span class="No-Break">monolithic-style applications.</span></p>&#13;
			<p>In this chapter, we’re going to cover the <span class="No-Break">following topics:</span></p>&#13;
			<ul>&#13;
				<li>Getting to know Helm charts <span class="No-Break">and Kustomize</span></li>&#13;
				<li>Deploying with CI/CD <span class="No-Break">and GitOps</span></li>&#13;
				<li>Troubleshooting <span class="No-Break">application deployments</span></li>&#13;
				<li>Service meshes <span class="No-Break">and Ingresses</span></li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-139"><a id="_idTextAnchor140"/>Technical requirements</h1>&#13;
			<p>In this section, you’re going to take what you learned about the different types of deployments and methodologies for thinking about Kubernetes resources from the last chapter and expand upon that knowledge in this chapter. You should have a brief understanding of automated deployment methodologies such as CI/CD, and have a high-level understanding of what a service mesh is, along with some application architecture knowledge. As usual, you’ll find the code for this chapter on <span class="No-Break">GitHub: </span><a href="https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch6"><span class="No-Break">https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch6</span></a><a href="https://github.com/AdminTurnedDevOps/Packt/tree/main/ch6%0D"/></p>&#13;
			<h1 id="_idParaDest-140"><a id="_idTextAnchor141"/>Getting to know Helm charts and Kustomize</h1>&#13;
			<p>When you’re working with Kubernetes, unless it’s a dev environment for your testing, there’s an extremely slim chance that you only have one Kubernetes Manifest. You most likely have multiple for various resources such as Deployments, Services, DaemonSets, ConfigMaps, Ingresses, and a ton of the other Kubernetes resources out there. Utilizing almost every single Kubernetes platform or tool that’s deployed to your cluster uses a <span class="No-Break">Kubernetes Manifest.</span></p>&#13;
			<p>With all those Kubernetes Manifests, there are a ton of different values and parameters that you need to pass at runtime to make it all work. In this section, you’ll learn about two different methods of managing <a id="_idIndexMarker339"/>Kubernetes<a id="_idIndexMarker340"/> Manifests – <strong class="bold">Helm charts</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="bold">Kustomize</strong></span><span class="No-Break">.</span></p>&#13;
			<h2 id="_idParaDest-141"><a id="_idTextAnchor142"/>Why think about deployment methods for manifests?</h2>&#13;
			<p>Before diving into different deployment methods, it makes sense to understand why you’d want to<a id="_idIndexMarker341"/> consider deployment methods other than using the terminal for the deploying <span class="No-Break">Manifests first.</span></p>&#13;
			<p>There are three primary points, which we discuss in the <span class="No-Break">following subsections.</span></p>&#13;
			<h3>Scale</h3>&#13;
			<p>When thinking about scale, there’s absolutely no way to scale a deployment if an engineer is always doing it from their laptop. The engineer could be using different plugins, different IDEs, different terminal settings, and even a different operating system. With all of that, the uncertainty alone of the environment can cause a massive amount of error. If every engineer is relying on their computer to deploy an environment, what happens if their laptop crashes? Or there’s a random update during the day? Or someone is out of office? There are so many variables that come into play that <a id="_idIndexMarker342"/>make utilizing a local computer a bad idea when it comes to deployments. Instead, it makes far more sense to have a central location from which you conduct your deployments. The environment stays the same, everyone can use it, you can customize it to your team’s needs, and you don’t have to worry about anyone being out <span class="No-Break">of office.</span></p>&#13;
			<h3>Anything can go wrong</h3>&#13;
			<p>Going into the second point, which echoes the first point in a sense, anything can go wrong. The goal of every organization is to have a successful deployment all the time, zero hiccups, and the ability to deploy at any time. Marketing teams paint this picture in our heads of “deploy 20 times per day with this tool and it’ll always work,” but as all engineers know, that’s not reality. Anything as simple as a network hiccup or making an error when entering a variable name can lead to a failed deployment, and, in turn, an application being down. Because of that, having a proper deployment strategy is key not only to repeatability with Kubernetes but also to repeatability in general. Having a proper process and <em class="italic">rules</em> in place of how something is deployed and when or where it’s deployed is the make or break between a successful update and everyone on the engineering team sitting in the office fixing an issue until <span class="No-Break">1:00 A.M.</span></p>&#13;
			<h3>It’s manual</h3>&#13;
			<p>The last point, which goes without saying, is that it’s an incredibly manual process to sit at a terminal and run commands to deploy a configuration. In today’s world, engineers want to spend their time focusing on value-driven work, not putting out fires. In fact, that’s a huge reason why automation and repeatability exist in the first place. Engineers wanted to get their time back and stop working on mundane tasks. If you’re constantly deploying on your computer to an environment, you’re putting the “this is awful” back into manual work. Now, there are circumstances where you’d want to deploy from your localhost. For example, when I’m deploying to a dev environment or testing a new config for the first time, I’m not going to create a repeatable solution around it because I’m unsure whether it even works yet. However, once I know that it works and my initial dev testing is complete, I’ll automate <span class="No-Break">the workflow.</span></p>&#13;
			<p>Going forward<a id="_idIndexMarker343"/> in this chapter, keep in mind that the reason why you want to think about deployment workflows is to mitigate as much of the three aforementioned points <span class="No-Break">as possible.</span></p>&#13;
			<h2 id="_idParaDest-142"><a id="_idTextAnchor143"/>Helm charts</h2>&#13;
			<p>The idea <a id="_idIndexMarker344"/>behind repeatable deployment strategies is to make your life easier, but with new strategies comes the need to learn about different methods of implementation. The first method to learn about is <span class="No-Break">Helm charts.</span></p>&#13;
			<p>Helm is an open source project originally created by DeisLabs and donated to the CNCF; the CNCF now maintains the project. The objective of Helm when it first came out was to provide engineers with a better way to manage all the Kubernetes Manifests created. Helm was built with Kubernetes in mind and it’s a tool and platform specifically for Kubernetes, so it’s the same YAML you’re used to, just packaged differently – literally just YAML. Kubernetes was meant to give you a way to declaratively deploy containerized apps. It wasn’t necessarily meant to give you a meaningful way to package a bunch of Kubernetes Manifests so you could use them together. That’s where Helm comes into play. In addition, Helm keeps a release history of all deployed charts. This means you could go back to a previous release if something <span class="No-Break">went wrong.</span></p>&#13;
			<p>In January 2016, the project merged with a GCS tool called Kubernetes Deployment Manager, and the project was moved under Kubernetes. Helm was promoted from a Kubernetes subproject to a CNCF project in <span class="No-Break">June 2018.</span></p>&#13;
			<p>In short, Helm is a way to take a bunch of Kubernetes Manifests and package them up to be deployed like <span class="No-Break">an application.</span></p>&#13;
			<h3>Using Helm charts</h3>&#13;
			<p>Now that you know <a id="_idIndexMarker345"/>about Helm, let’s go ahead and dive into it from a <span class="No-Break">hands-on perspective:</span></p>&#13;
			<ol>&#13;
				<li>The first thing that you’ll need to do is install Helm. Because it varies based on the operating system, you<a id="_idIndexMarker346"/> can find a few different installation methods <span class="No-Break">here: </span><a href="https://helm.sh/docs/intro/install/"><span class="No-Break">https://helm.sh/docs/intro/install/</span></a><span class="No-Break">.</span></li>&#13;
				<li>Once Helm is installed, find or create a directory in which you want your first Helm chart to exist. Preferably, this will be an <span class="No-Break">empty directory:</span><pre class="console">&#13;
<strong class="bold">mkdir myfirsthelmchart</strong></pre></li>&#13;
				<li>Next, go into <a id="_idIndexMarker347"/>that directory on <span class="No-Break">your terminal.</span></li>&#13;
				<li>In the new directory, run the following command to create a <span class="No-Break">Helm chart:</span><pre class="console">&#13;
<strong class="bold">helm create name_of_chart</strong></pre></li>&#13;
			</ol>&#13;
			<p>Once you do that, you should see a directory structure similar to the following screenshot. In this case, the chart was <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">newchart</strong></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer070" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_01.jpg" alt="Figure 6.1 – Helm chart&#13;&#10;" width="306" height="153"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Helm chart</p>&#13;
			<p>If you open up the <strong class="source-inline">templates</strong> directory, you’ll see a bunch of examples for Deployments, Ingresses, and a <span class="No-Break">lot more.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer071" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_02.jpg" alt="Figure 6.2 – Example Helm&#13;&#10;" width="968" height="453"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Example Helm</p>&#13;
			<p>If you <a id="_idIndexMarker348"/>open up <strong class="source-inline">values.yaml</strong>, you’ll see where you can start adding values that you want to pass into <span class="No-Break">your templates.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer072" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_03.jpg" alt="Figure 6.3 – Values file&#13;&#10;" width="867" height="415"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Values file</p>&#13;
			<ol>&#13;
				<li value="5">To deploy a Helm chart, run the <span class="No-Break">following command:</span><pre class="console">&#13;
<strong class="bold">helm install nginxapp .</strong></pre></li>&#13;
				<li>To install the Helm chart, run the following command. The <strong class="source-inline">.</strong> symbol indicates the current directory, which is where the Helm <span class="No-Break">chart exists:</span><pre class="console">&#13;
<strong class="bold">helm install mynewapp .</strong></pre></li>&#13;
			</ol>&#13;
			<p>Of course, this isn’t everything<a id="_idIndexMarker349"/> there is to know about Helm. In fact, there are literally entire books on Helm. The goal of this section was to get you on the <span class="No-Break">right path.</span></p>&#13;
			<h3>Helm chart best practices</h3>&#13;
			<p>The following is a<a id="_idIndexMarker350"/> list of best practices to follow in production when <span class="No-Break">using Helm:</span></p>&#13;
			<ul>&#13;
				<li>When storing Helm charts, ensure that they’re set to be public or private as required. The last thing you want is to push a Helm chart to a registry that’s public-facing when it’s not supposed <span class="No-Break">to be.</span></li>&#13;
				<li>Document what your <span class="No-Break">charts do.</span></li>&#13;
				<li>Ensure you store charts in <span class="No-Break">source control.</span></li>&#13;
				<li>Always test Helm charts after a change <span class="No-Break">is made.</span></li>&#13;
			</ul>&#13;
			<h2 id="_idParaDest-143"><a id="_idTextAnchor144"/>Kustomize</h2>&#13;
			<p>Helm and<a id="_idIndexMarker351"/> Kustomize are pretty similar but have some unique differences. One of the primary use cases of Helm is to have a <strong class="source-inline">values.yaml</strong> file to store values to pass into a Kubernetes Manifest. Kustomize has the same type <span class="No-Break">of idea.</span></p>&#13;
			<p>With Kustomize, you have a template, typically called a base. The base is the template that you want to use. It could be for a Kubernetes Deployment, Service, Pod, or anything else you’d like. The template is the literal base where your values get pushed into. Along with the template, you have a <strong class="source-inline">kustomization.yaml</strong> file, which tells Kustomize which templates to use. For example, let’s say you have a <strong class="source-inline">deployment.yaml</strong> and <strong class="source-inline">service.yaml</strong> file. You would put those two filenames into the <strong class="source-inline">kustomization.yaml</strong> file so Kustomize knows it should push values into those <span class="No-Break">two files.</span></p>&#13;
			<p><em class="italic">Values</em> were mentioned a few times already, but not thoroughly <a id="_idIndexMarker352"/>explained. A value can be anything that you want to essentially pass in at runtime. For example, let’s say you have three environments – dev, staging, and prod. In dev, you have one replica. In staging, you have two replicas. In prod, you have three to four replicas. You can use Kustomize to pass those values into one template, so instead of having three Manifests that have different replica values, you have one template that you pass the <span class="No-Break">values into.</span></p>&#13;
			<p>But how do you pass in <span class="No-Break">the values?</span></p>&#13;
			<p>Within a <a id="_idIndexMarker353"/>Kustomize directory, you typically have two directories – base and overlays. The base is where the template goes. The overlay directory is where each environment goes with specific values. For example, let’s say you have a <strong class="source-inline">dev</strong>, <strong class="source-inline">staging</strong>, and <span class="No-Break"><strong class="source-inline">prod</strong></span><span class="No-Break"> overlay.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer073" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_04.jpg" alt="Figure 6.4 – Base configuration&#13;&#10;" width="620" height="308"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Base configuration</p>&#13;
			<p>The <strong class="source-inline">dev</strong> overlay, along <a id="_idIndexMarker354"/>with the others, would have a <span class="No-Break"><strong class="source-inline">kustomization.yaml</strong></span><span class="No-Break"> file.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer074" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_05.jpg" alt="Figure 6.5 – Dev overlay configuration&#13;&#10;" width="856" height="378"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – Dev overlay configuration</p>&#13;
			<p>Within the <strong class="source-inline">kustomization.yaml</strong> file is where you’d find the config for the <span class="No-Break">replica count.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer075" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_06.jpg" alt="Figure 6.6 – Kustomization file&#13;&#10;" width="806" height="364"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – Kustomization file</p>&#13;
			<p>Notice how the <strong class="source-inline">resources</strong> map is <a id="_idIndexMarker355"/>pointing to the <strong class="source-inline">base</strong> directory, and the <strong class="source-inline">replicas</strong> map is specifying the deployment along with the <span class="No-Break">replica count.</span></p>&#13;
			<p>The primary difference between Helm and Kustomize is that Helm’s primary purpose is to package up a bunch of Kubernetes Manifests and deploy them like an app, whereas the primary purpose of Kustomize is to have a template that you push your values into (such as the replica count). Helm does this as well, but it’s not the primary purpose <span class="No-Break">of Helm.</span></p>&#13;
			<h3>Using Kustomize configurations</h3>&#13;
			<p>Now that you know about<a id="_idIndexMarker356"/> Kustomize, let’s dive into it from a <span class="No-Break">hands-on perspective:</span></p>&#13;
			<ol>&#13;
				<li value="1">The first thing that you’ll need to do is install Kustomize. Because it varies based on the operating system, you <a id="_idIndexMarker357"/>can find a few different installation methods <span class="No-Break">here: </span><a href="https://kubectl.docs.kubernetes.io/installation/kustomize/"><span class="No-Break">https://kubectl.docs.kubernetes.io/installation/kustomize/</span></a><span class="No-Break">.</span></li>&#13;
				<li>Once Kustomize is installed, find or create a new directory in which you want your Kustomize config to live. You can call <span class="No-Break">it </span><span class="No-Break"><strong class="source-inline">kustomize</strong></span><span class="No-Break">.</span></li>&#13;
				<li>Create two <a id="_idIndexMarker358"/>new directories under the <strong class="source-inline">kustomize</strong> directory called <strong class="source-inline">overlays</strong> and <strong class="source-inline">base</strong>. Inside the <strong class="source-inline">overlays</strong> directory, create a new child directory called <strong class="source-inline">dev</strong>. It should look similar to the <span class="No-Break">following screenshot.</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer076" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_07.jpg" alt="Figure 6.7 – Dev overlay configuration&#13;&#10;" width="391" height="73"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – Dev overlay configuration</p>&#13;
			<ol>&#13;
				<li value="4">Inside the <strong class="source-inline">base</strong> directory, create a new file called <strong class="source-inline">deployment.yaml</strong> and paste the following code <span class="No-Break">into it:</span><pre class="console">&#13;
apiVersion: apps/v1</pre><pre class="console">&#13;
kind: Deployment</pre><pre class="console">&#13;
metadata:</pre><pre class="console">&#13;
  name: nginx-deployment</pre><pre class="console">&#13;
spec:</pre><pre class="console">&#13;
  selector:</pre><pre class="console">&#13;
    matchLabels:</pre><pre class="console">&#13;
      app: nginxdeployment</pre><pre class="console">&#13;
  replicas: 2</pre><pre class="console">&#13;
  template:</pre><pre class="console">&#13;
    metadata:</pre><pre class="console">&#13;
      labels:</pre><pre class="console">&#13;
        app: nginxdeployment</pre><pre class="console">&#13;
    spec:</pre><pre class="console">&#13;
      containers:</pre><pre class="console">&#13;
      - name: nginxdeployment</pre><pre class="console">&#13;
        image: nginx:latest</pre><pre class="console">&#13;
        ports:</pre><pre class="console">&#13;
        - containerPort: 80</pre></li>&#13;
				<li>Next, create<a id="_idIndexMarker359"/> a new file in the <strong class="source-inline">base</strong> directory called <strong class="source-inline">kustomization.yaml</strong> and paste the following configuration into it, which tells Kustomize which Kubernetes Manifest <span class="No-Break">to utilize:</span><pre class="console">&#13;
apiVersion: kustomize.config.k8s.io/v1beta1</pre><pre class="console">&#13;
kind: Kustomization</pre><pre class="console">&#13;
resources:</pre><pre class="console">&#13;
  - deployment.yaml</pre></li>&#13;
				<li>For the last step, inside of <strong class="source-inline">overlays</strong> | <strong class="source-inline">dev</strong>, create a new file, call it <strong class="source-inline">kustomization.yaml</strong>, and paste the following Manifest <span class="No-Break">into it:</span><pre class="console">&#13;
apiVersion: kustomize.config.k8s.io/v1beta1</pre><pre class="console">&#13;
kind: Kustomization</pre><pre class="console">&#13;
resources:</pre><pre class="console">&#13;
- ../../base/</pre><pre class="console">&#13;
replicas:</pre><pre class="console">&#13;
- name: nginx-deployment</pre><pre class="console">&#13;
  count: 1</pre></li>&#13;
				<li>Once the directories and configurations are in place, <strong class="source-inline">cd</strong> into the <strong class="source-inline">base</strong> | <strong class="source-inline">dev</strong> directory and run the <span class="No-Break">following command:</span><pre class="console">&#13;
<strong class="bold">kubectl kustomize</strong></pre></li>&#13;
			</ol>&#13;
			<p>You’ll see<a id="_idIndexMarker360"/> an output similar to the following screenshot, which gives you a config with one replica, instead of two, which is what the <span class="No-Break">template contains.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer077" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_08.jpg" alt="Figure 6.8 – Kustomize output&#13;&#10;" width="375" height="348"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – Kustomize output</p>&#13;
			<p>As with Helm charts, the topic of Kustomize could fill a small book itself, which means this section couldn’t cover it all. It should, however, get you started in the <span class="No-Break">right direction.</span></p>&#13;
			<h3>Kustomize best practices</h3>&#13;
			<p>The following <a id="_idIndexMarker361"/>is a list of best practices to follow in production when <span class="No-Break">using Kustomize:</span></p>&#13;
			<ul>&#13;
				<li>Ensure that you put overlays into their own directories. You don’t have to do this, but it makes for a much <span class="No-Break">cleaner config.</span></li>&#13;
				<li>Ensure that all code is stored in <span class="No-Break">source control.</span></li>&#13;
				<li>Follow a standard directory structure – <strong class="source-inline">base</strong> for the directory where the template goes and <strong class="source-inline">overlays</strong> for values that you wish to pass into <span class="No-Break">the template.</span></li>&#13;
			</ul>&#13;
			<p>In the next section, you’ll<a id="_idIndexMarker362"/> learn about the two primary deployment methods when it comes to <span class="No-Break">containerized apps.</span></p>&#13;
			<h1 id="_idParaDest-144"><a id="_idTextAnchor145"/>Deploying with CI/CD and GitOps</h1>&#13;
			<p>Kubernetes<a id="_idIndexMarker363"/> deployments come in <span class="No-Break">three stages:</span></p>&#13;
			<ul>&#13;
				<li>Deploying Manifests on your <span class="No-Break">local computer</span></li>&#13;
				<li>Deploying Manifests with an automated solution such as CI/CD, which ultimately just runs <strong class="source-inline">kubectl apply -f</strong> commands, the same as your <span class="No-Break">local computer</span></li>&#13;
				<li>A new and completely automated solution that’s (usually) 100% hands-off from a <span class="No-Break">deployment perspective</span></li>&#13;
			</ul>&#13;
			<p>With the<a id="_idIndexMarker364"/> first stage, it wasn’t scalable at all. A bunch of engineers were running commands on their localhost to deploy a containerized app, and they were all doing it in different ways with different code editors and different plugins. It was a mess and didn’t allow scalability for the deployment process. It also held engineers up from doing value-driven work and instead, they had to sit on their terminals and run commands <span class="No-Break">all day.</span></p>&#13;
			<p>In this section, you’ll learn about more common, automated, and new approaches to deploying apps, which will be around CI/CD <span class="No-Break">and GitOps.</span></p>&#13;
			<h2 id="_idParaDest-145"><a id="_idTextAnchor146"/>What is CI/CD?</h2>&#13;
			<p>When it<a id="_idIndexMarker365"/> comes to CI/CD, it’s assumed that if you’re reading this book, you’re already doing work in CI/CD and know what it is. Because of that, there won’t be an entire breakdown of CI/CD. Let’s do a <span class="No-Break">brief overview.</span></p>&#13;
			<p>By definition, CI/CD is a way to create an artifact of your application and deploy it to the desired destination in an automated fashion. As CI/CD increased in popularity, engineers started using it for other purposes – for example, packaging up Terraform code into an artifact and running it so infrastructure<a id="_idIndexMarker366"/> can be <span class="No-Break">created automatically.</span></p>&#13;
			<p>In the CI process, engineers are worried about <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break">Testing code</span></li>&#13;
				<li>Packaging <span class="No-Break">up code</span></li>&#13;
				<li>Ensuring that all prerequisites and dependencies <span class="No-Break">are met</span></li>&#13;
				<li>Building <span class="No-Break">container images</span></li>&#13;
			</ul>&#13;
			<p>In the CD process, engineers are worried about <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break">Deploying workloads</span></li>&#13;
				<li>Ensuring that they reached the <span class="No-Break">correct destination</span></li>&#13;
				<li>Ensuring that the app or services and infrastructure are up and running <span class="No-Break">as expected</span></li>&#13;
			</ul>&#13;
			<h2 id="_idParaDest-146"><a id="_idTextAnchor147"/>Using CI/CD for Kubernetes deployments</h2>&#13;
			<p>As with everything <a id="_idIndexMarker367"/>else in tech, there are what feels like a million ways to do one thing. Because of that, we cannot specify every CI/CD, automation, and cloud scenario here. To make things simplistic, Terraform code for GKE and YAML pipelines for GitHub Actions will be shown. This is considered pseudocode, but it’ll actually work in the <span class="No-Break">right environments.</span></p>&#13;
			<p>First, let’s start <a id="_idIndexMarker368"/>with the Terraform code and break <span class="No-Break">it down:</span></p>&#13;
			<ol>&#13;
				<li value="1">You’ll start with the Google provider, specifying <span class="No-Break">the region:</span><pre class="console">&#13;
provider "google" {</pre><pre class="console">&#13;
  project     = var.project_id</pre><pre class="console">&#13;
  region      = var.region</pre><pre class="console">&#13;
}</pre></li>&#13;
				<li>Next, <strong class="source-inline">google_container_cluster</strong> will be specified so you can implement the VPC you want to use, subnet, and worker <span class="No-Break">node count:</span><pre class="console">&#13;
resource "google_container_cluster" "primary" {</pre><pre class="console">&#13;
  name     = var.cluster_name</pre><pre class="console">&#13;
  location = var.region</pre><pre class="console">&#13;
  remove_default_node_pool = true</pre><pre class="console">&#13;
  initial_node_count       = 1</pre><pre class="console">&#13;
  network    = var.vpc_name</pre><pre class="console">&#13;
  subnetwork = var.subnet_name</pre><pre class="console">&#13;
}</pre></li>&#13;
				<li>The last<a id="_idIndexMarker369"/> resource is for <strong class="source-inline">google_container_node_pool</strong>, which implements the needed Google APIs for GKE, the <a id="_idIndexMarker370"/>node count, node names, and node size <span class="No-Break">or type:</span><pre class="console">&#13;
resource "google_container_node_pool" "nodes" {</pre><pre class="console">&#13;
  name       = "${google_container_cluster.primary.name}-node-pool"</pre><pre class="console">&#13;
  location   = var.region</pre><pre class="console">&#13;
  cluster    = google_container_cluster.primary.name</pre><pre class="console">&#13;
  node_count = var.node_count</pre><pre class="console">&#13;
  node_config {</pre><pre class="console">&#13;
    oauth_scopes = [</pre><pre class="console">&#13;
      "https://www.googleapis.com/auth/logging.write",</pre><pre class="console">&#13;
      "https://www.googleapis.com/auth/monitoring",</pre><pre class="console">&#13;
    ]</pre><pre class="console">&#13;
    labels = {</pre><pre class="console">&#13;
      env = var.project_id</pre><pre class="console">&#13;
    }</pre><pre class="console">&#13;
    machine_type = "n1-standard-1"</pre><pre class="console">&#13;
    tags         = ["gke-node", "${var.project_id}-gke"]</pre><pre class="console">&#13;
    metadata = {</pre><pre class="console">&#13;
      disable-legacy-endpoints = "true"</pre><pre class="console">&#13;
    }</pre><pre class="console">&#13;
  }</pre><pre class="console">&#13;
}</pre></li>&#13;
			</ol>&#13;
			<p>With the<a id="_idIndexMarker371"/> Terraform code, you’ll want a way to deploy it. The best way in today’s world is with CI/CD. When it comes to deploying infrastructure and services, CI/CD is a great and <span class="No-Break">repeatable process.</span></p>&#13;
			<p>To deploy <a id="_idIndexMarker372"/>the code, you can use any CI/CD platform of your choosing, but the code here is an example of how you can deploy the Terraform code via <span class="No-Break">GitHub Actions.</span></p>&#13;
			<p>The pipeline does <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li>Specifies <strong class="source-inline">workflow_dispatch</strong>, which means the code will only run if you click the <span class="No-Break"><strong class="source-inline">Deployment</strong></span><span class="No-Break"> button</span></li>&#13;
				<li>Uses an Ubuntu container to run <span class="No-Break">the pipeline</span></li>&#13;
				<li>Checks out the code (clones it) to the <span class="No-Break">Ubuntu container</span></li>&#13;
				<li>Configures Terraform in the <span class="No-Break">Ubuntu container</span></li>&#13;
				<li>Configures the GCP SDK in the <span class="No-Break">Ubuntu container</span></li>&#13;
				<li>Runs <strong class="source-inline">terraform init</strong>, and formats, plans, and applies it to the directory where <a id="_idIndexMarker373"/>the <a id="_idIndexMarker374"/>GKE <span class="No-Break">code lives:</span></li>&#13;
			</ul>&#13;
			<pre class="source-code">&#13;
name: GKE Kubernetes Deployment&#13;
on:&#13;
  workflow_dispatch:&#13;
jobs:&#13;
  build:&#13;
    runs-on: ubuntu-latest&#13;
    steps:&#13;
      - uses: actions/checkout@v3&#13;
      - name: Setup Terraform&#13;
        uses: hashicorp/setup-terraform@v1&#13;
      - name: Set up gcloud Cloud SDK environment&#13;
        uses: google-github-actions/setup-gcloud@v0.6.0&#13;
        with:&#13;
          service_account_email:&#13;
          service_account_key:&#13;
          project_id:&#13;
      - name: Terraform Init&#13;
        working-directory: where_the_gke_code_lives&#13;
        run: terraform init&#13;
      - name: Terraform Format&#13;
        working-directory: where_the_gke_code_lives&#13;
        run: terraform fmt&#13;
      - name: Terraform Plan&#13;
        working-directory: where_the_gke_code_lives&#13;
        run: terraform plan&#13;
      - name: Terraform Apply&#13;
        working-directory: where_the_gke_code_lives&#13;
        run: terraform apply -auto-approve</pre>&#13;
			<p>When<a id="_idIndexMarker375"/> using CI/CD, it makes the most sense to use it in <a id="_idIndexMarker376"/>this type of way for Kubernetes. You rarely ever want to use CI/CD to deploy a Kubernetes Manifest and instead, you’d want to use something such as a GitOps solution, as it’s far more efficient, manages the state, monitors the workloads, and a <span class="No-Break">lot more.</span></p>&#13;
			<h2 id="_idParaDest-147"><a id="_idTextAnchor148"/>What is GitOps?</h2>&#13;
			<p>By definition, GitOps is a <a id="_idIndexMarker377"/>set of tools that utilizes Git repositories as a source of truth to deliver Kubernetes resources as code. It’s an operational best practice used for app development, collaboration, compliance, and CI/CD, and applies the best practices to infrastructure automation. Now, let’s see a simpler explanation. It’s configuration management for Kubernetes; that’s it, plain and simple. Configuration management is all about ensuring that the desired state is the current state, which is what GitOps <span class="No-Break">gives us.</span></p>&#13;
			<p>Now that you know the definition of GitOps, let’s talk about what it actually does for Kubernetes. First things first, you have a source control repository. The repository contains your Kubernetes manifests that you wish to deploy to Kubernetes to run your applications. You also have a Kubernetes cluster, which is running on any environment you’d like. It could be on-premises, in a raw Kubernetes cluster, or even in a cloud-based service such as GKE or EKS. Now, you have the Kubernetes Manifests that you want to run in your production environment and the Kubernetes cluster that you want to run the Kubernetes manifests on, but how do you deploy them? The typical way is running something such as <strong class="source-inline">kubectl apply -f</strong> against the Kubernetes manifests, but that requires manual effort and leaves a lot to be desired. Instead, you can implement GitOps. To implement GitOps, there<a id="_idIndexMarker378"/> are a few solutions. You decide to implement a GitOps solution, and that GitOps solution needs access to both the Kubernetes cluster that you’re running and the source control system, such as GitHub or any other Git system where your source code is stored. To do that, you install the GitOps solution on the Kubernetes cluster and while doing that, you give the GitOps solution access to your source control system with some type <a id="_idIndexMarker379"/>of <strong class="bold">Personal Access Token</strong> (<strong class="bold">PAT</strong>) or another type of authentication or authorization method. After that, you use the GitOps solution to deploy the Kubernetes Manifests that live in source control. At this stage, you’re not using <strong class="source-inline">kubectl apply -f</strong> or <strong class="source-inline">kubectl create -f</strong> anymore. Instead, you’re using the CLI or whatever other solution comes with the GitOps platform to deploy the Kubernetes Manifests – and boom, just like that, you have an application deployed! Now, of course, we all wish it were that easy. A couple of sentences to explain and poof, you’re up and running in a production environment. However, it’s not that simple, which is why GitOps is in such high demand and isn’t the easiest thing <span class="No-Break">to crack.</span></p>&#13;
			<p>At the time of writing this, the most popular GitOps platforms <a id="_idIndexMarker380"/>are <strong class="bold">ArgoCD</strong> <span class="No-Break">and Flux.</span></p>&#13;
			<h2 id="_idParaDest-148"><a id="_idTextAnchor149"/>Using GitOps for automated deployments</h2>&#13;
			<p>Knowing the process<a id="_idIndexMarker381"/> to create the Kubernetes infrastructure, you can now deploy and manage a containerized app using GitOps. To follow this section, you’ll need a Kubernetes environment up and running, as ArgoCD will be deployed to <span class="No-Break">the cluster.</span></p>&#13;
			<p>This section is going to be more of a step-by-step guide because regardless of where you’re running Kubernetes, these are the steps to get ArgoCD up and running. Unlike with the CI/CD section, there aren’t tons of different platforms, cloud environments, or configuration code choices that can come into play, and because of that, the following solution can work in <span class="No-Break">any environment.</span></p>&#13;
			<h3>Configuring ArgoCD</h3>&#13;
			<ol>&#13;
				<li value="1">First, create <a id="_idIndexMarker382"/>a namespace for ArgoCD in your <span class="No-Break">Kubernetes </span><span class="No-Break"><a id="_idIndexMarker383"/></span><span class="No-Break">cluster:</span><pre class="console">&#13;
<strong class="bold">kubectl create namespace argocd</strong></pre></li>&#13;
				<li>Install ArgoCD using the preconfigured Kubernetes Manifest from ArgoCD that provides a highly <span class="No-Break">available installation:</span><pre class="console">&#13;
<strong class="bold">kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/ha/install.yaml</strong></pre><div id="_idContainer078" class="IMG---Figure"><img src="Images/B19116_06_09.jpg" alt="Figure 6.9 – ArgoCD creation output&#13;&#10;" width="1210" height="894"/></div></li>&#13;
			</ol>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – ArgoCD creation output</p>&#13;
			<ol>&#13;
				<li value="3">Get the initial admin password to log in <span class="No-Break">to ArgoCD:</span><pre class="console">&#13;
<strong class="bold">kubectl get secret -n argocd argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d</strong></pre></li>&#13;
				<li>Open up <a id="_idIndexMarker384"/>ArgoCD’s UI via Kubernetes port forwarding. That way, you can access the frontend of ArgoCD without having to attach a load balancer to <span class="No-Break">the service:</span><pre class="console">&#13;
<strong class="bold">kubectl port-forward -n argocd service/argocd-server :80</strong></pre></li>&#13;
				<li>Now<a id="_idIndexMarker385"/> that you know the UI works, log in to the server via the CLI. That way, you can deploy containerized apps with ArgoCD via the CLI to create a repeatable process instead of doing it through the UI, which is manual <span class="No-Break">and repetitive.</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer079" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_10.jpg" alt="Figure 6.10 – ArgoCD portal&#13;&#10;" width="969" height="554"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – ArgoCD portal</p>&#13;
			<ol>&#13;
				<li value="6">The port is what ArgoCD is hosting from the <strong class="source-inline">kubectl port-forward</strong> command that you ran in the previous step. Use the following command to log in <span class="No-Break">to ArgoCD:</span><pre class="console">&#13;
<strong class="bold">argocd login 127.0.0.1:argocd_port_here</strong></pre></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer080" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_11.jpg" alt="Figure 6.11 – Login output&#13;&#10;" width="1209" height="128"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11 – Login output</p>&#13;
			<ol>&#13;
				<li value="7">In the<a id="_idIndexMarker386"/> Argo CD UI, go to <strong class="bold">User Info</strong> | <strong class="bold">Update Password</strong>. Change the password from the initial admin password to a password of <span class="No-Break">your choosing.</span></li>&#13;
			</ol>&#13;
			<p>You now have<a id="_idIndexMarker387"/> officially deployed ArgoCD and have the ability to work with the GitOps platform on your terminal and locally on <span class="No-Break">your computer.</span></p>&#13;
			<h3>Deploying an app</h3>&#13;
			<p>In this <a id="_idIndexMarker388"/>section, you’re going to deploy an app. The app that you’ll use is a very popular demo-related app that a lot of folks use to showcase how an environment <span class="No-Break">will work:</span></p>&#13;
			<ol>&#13;
				<li value="1">Create a namespace for your <span class="No-Break">new app:</span><pre class="console">&#13;
<strong class="bold">kubectl create namespace sock-shop</strong></pre></li>&#13;
			</ol>&#13;
			<p>The Sock Shop is a popular microservice demo that you can find <span class="No-Break">here: </span><a href="https://microservices-demo.github.io/deployment/kubernetes-start.html"><span class="No-Break">https://microservices-demo.github.io/deployment/kubernetes-start.html</span></a><span class="No-Break">.</span></p>&#13;
			<ol>&#13;
				<li value="2">Deploy the Sock Shop in ArgoCD. To deploy the app, you will need to do <span class="No-Break">the following:</span><ol><li>Create a new <span class="No-Break">ArgoCD app.</span></li><li>Point to the repo where the <span class="No-Break">app exists.</span></li><li>Point to the destination server, which is the server or service that you’re running <span class="No-Break">Kubernetes on.</span></li><li>Specify the <span class="No-Break">destination namespace:</span></li></ol><pre class="console">&#13;
<strong class="bold">argocd app create socks --repo https://github.com/microservices-demo/microservices-demo.git --path deploy/kubernetes --dest-server https://kubernetes.default.svc --dest-namespace sock-shop</strong></pre></li>&#13;
				<li>Now <a id="_idIndexMarker389"/>that the app is deployed, you can check the status of <span class="No-Break">the app:</span><pre class="console">&#13;
<strong class="bold">argocd app get socks</strong></pre></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer081" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_12.jpg" alt="Figure 6.12 – Sock Shop resources&#13;&#10;" width="1209" height="754"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – Sock Shop resources</p>&#13;
			<p>You can now check that the app was deployed in the <span class="No-Break">ArgoCD UI.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer082" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_13.jpg" alt="Figure 6.13 – Sock Shop app connection&#13;&#10;" width="935" height="728"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.13 – Sock Shop app connection</p>&#13;
			<p>You’ll see the<a id="_idIndexMarker390"/> health of the app, whether it’s synced, and whether the status of the application is <span class="No-Break">as expected.</span></p>&#13;
			<h2 id="_idParaDest-149"><a id="_idTextAnchor150"/>Production use cases for CI/CD and GitOps</h2>&#13;
			<p>Two ways to think about CI/CD and GitOps in production are <span class="No-Break">as follows:</span></p>&#13;
			<ul>&#13;
				<li>CI/CD should be used to deploy <span class="No-Break">the cluster</span></li>&#13;
				<li>GitOps should be used to manage the Kubernetes resources inside <span class="No-Break">the cluster</span></li>&#13;
			</ul>&#13;
			<p>In other <a id="_idIndexMarker391"/>words, CI/CD deploys the infrastructure and clusters and <a id="_idIndexMarker392"/>GitOps deploys and manages the apps. Use the best tool for the job, which is the infrastructure deployment type <span class="No-Break">of workflow.</span></p>&#13;
			<p>Regardless of which GitOps and CI/CD solution you use, you always want to keep in mind that your goal is to automate and create repeatable workflows that work for you and your team. Regardless of what <em class="italic">hot</em> tool or platform is out right now, you want to use what’s best for your team, not whatever is the <span class="No-Break"><em class="italic">new thing</em></span><span class="No-Break">.</span></p>&#13;
			<p>In the next section, you’ll dive into multiple methods of troubleshooting containerized apps running in your <span class="No-Break">Kubernetes cluster.</span></p>&#13;
			<h1 id="_idParaDest-150"><a id="_idTextAnchor151"/>Troubleshooting application deployments</h1>&#13;
			<p>Troubleshooting <a id="_idIndexMarker393"/>environments and applications typically always looks the same and follows a <span class="No-Break">typical order:</span></p>&#13;
			<ul>&#13;
				<li>When was the <span class="No-Break">last deployment?</span></li>&#13;
				<li>What <span class="No-Break">has changed?</span></li>&#13;
				<li>Look at <span class="No-Break">the logs</span></li>&#13;
				<li>Who can access the app and who cannot, if anyone <span class="No-Break">at all?</span></li>&#13;
			</ul>&#13;
			<p>With <a id="_idIndexMarker394"/>Kubernetes, it’s pretty similar when it comes to application troubleshooting. The usual workflow is <span class="No-Break">as follows:</span></p>&#13;
			<ol>&#13;
				<li value="1">Check the app itself running in <span class="No-Break">the container.</span></li>&#13;
				<li>Check the overall health of <span class="No-Break">the Pod(s).</span></li>&#13;
				<li>Check the Service <span class="No-Break">or route.</span></li>&#13;
			</ol>&#13;
			<p>With these three steps, you can usually get to the bottom of what’s happening because, in reality, there can’t be any other problems. It’s either that the app itself isn’t working, the Pod itself has an issue, or the service or route isn’t working <span class="No-Break">as expected.</span></p>&#13;
			<p>Although there could only be three potential problems at a high level, when you dive deeper into those problems, there could be various ways to troubleshoot the current issue you’re facing, which you’ll learn about in <span class="No-Break">this section.</span></p>&#13;
			<p>As with all troubleshooting techniques, you should think about it in the <span class="No-Break">following order:</span></p>&#13;
			<ul>&#13;
				<li>What’s <span class="No-Break">the problem?</span></li>&#13;
				<li><span class="No-Break">What’s changed?</span></li>&#13;
				<li>What could be the problem in the problem? As in, a Pod may be down, but it might not be because of the app. It could be because of a problem with the <span class="No-Break">replication controller.</span></li>&#13;
			</ul>&#13;
			<h2 id="_idParaDest-151"><a id="_idTextAnchor152"/>Troubleshooting Pods</h2>&#13;
			<p>The two commands that’ll help you debug Pods are <span class="No-Break">as follows:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break"><strong class="source-inline">kubectl describe</strong></span></li>&#13;
				<li><span class="No-Break"><strong class="source-inline">kubectl logs</strong></span></li>&#13;
			</ul>&#13;
			<p>Take the<a id="_idIndexMarker395"/> following Kubernetes Manifest and deploy it. Notice how, for <a id="_idIndexMarker396"/>the container tag, it’s spelled as <strong class="source-inline">lates</strong>. That’s on purpose, as you want the container <span class="No-Break">to fail:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
  name: nginx-deployment&#13;
spec:&#13;
  selector:&#13;
    matchLabels:&#13;
      app: nginxdeployment&#13;
  replicas: 2&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: nginxdeployment&#13;
    spec:&#13;
      containers:&#13;
      - name: nginxdeployment&#13;
        image: nginx:lates&#13;
        ports:&#13;
        - containerPort: 80</pre>&#13;
			<p>Retrieve the name of the Pod with the <span class="No-Break">following command:</span></p>&#13;
			<pre class="console">&#13;
kubectl get pods</pre>&#13;
			<p>You’ll see<a id="_idIndexMarker397"/> an output similar to <span class="No-Break">the following:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer083" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_14.jpg" alt="Figure 6.14 – Error container image pull&#13;&#10;" width="745" height="71"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.14 – Error container image pull</p>&#13;
			<p>Notice <a id="_idIndexMarker398"/>how, right off the bat, you can start the troubleshooting process. The status states that there was an error pulling the image. Now you know that there’s an issue with the image, let’s dive a <span class="No-Break">bit deeper.</span></p>&#13;
			<p>Run the <span class="No-Break">following command:</span></p>&#13;
			<pre class="console">&#13;
kubectl describe pods pod_name</pre>&#13;
			<p>You’ll see an output similar to the <span class="No-Break">following screenshot:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer084" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_15.jpg" alt="Figure 6.15 – Pod description&#13;&#10;" width="1390" height="571"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.15 – Pod description</p>&#13;
			<p>The great thing about the <strong class="source-inline">describe</strong> command is that it gives you a log output underneath the <strong class="source-inline">Events</strong> section. You can now see that the issue is that it couldn’t pull the container image based on the name and tag that <span class="No-Break">you gave.</span></p>&#13;
			<p>The last step would be to run the <strong class="source-inline">logs</strong> command to see whether there’s any other data you <span class="No-Break">can use:</span></p>&#13;
			<pre class="console">&#13;
kubectl logs pod_name</pre>&#13;
			<div>&#13;
				<div id="_idContainer085" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_16.jpg" alt="Figure 6.16 – Pod logs&#13;&#10;" width="845" height="44"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.16 – Pod logs</p>&#13;
			<p>You can <a id="_idIndexMarker399"/>see from the screenshot here that there isn’t much more to go <a id="_idIndexMarker400"/>off of other than what was given in the <strong class="source-inline">describe</strong> command, so the troubleshooting has been <span class="No-Break">successfully completed.</span></p>&#13;
			<h2 id="_idParaDest-152"><a id="_idTextAnchor153"/>Troubleshooting Services</h2>&#13;
			<p>When<a id="_idIndexMarker401"/> troubleshooting Services, the first thing that you always want to<a id="_idIndexMarker402"/> confirm is whether the Service exists. If you don’t have a Service running in a Kubernetes cluster, you can use this <span class="No-Break">example Manifest:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
  name: nginx-deployment&#13;
spec:&#13;
  selector:&#13;
    matchLabels:&#13;
      app: nginxdeployment&#13;
  replicas: 2&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: nginxdeployment&#13;
    spec:&#13;
      containers:&#13;
      - name: nginxdeployment&#13;
        image: nginx:latest&#13;
        ports:&#13;
        - containerPort: 80&#13;
---&#13;
apiVersion: v1&#13;
kind: Service&#13;
metadata:&#13;
  name: nginxservice&#13;
spec:&#13;
  selector:&#13;
    app: nginxdeployment&#13;
  ports:&#13;
    - protocol: TCP&#13;
      port: 80&#13;
  type: LoadBalancer</pre>&#13;
			<p>Because Pod <a id="_idIndexMarker403"/>networks are separate from a host network, you’ll need <a id="_idIndexMarker404"/>a Pod to exec or SSH into so you can do the troubleshooting. The following is a command that you can use to configure a Pod for troubleshooting purposes based on a <strong class="source-inline">busybox</strong> container image, which is a popular image used for <span class="No-Break">troubleshooting purposes:</span></p>&#13;
			<pre class="console">&#13;
kubectl run -it --rm --restart=Never busybox --image=gcr.io/google-containers/busybox sh</pre>&#13;
			<p>First, see whether the service is running. You’ll do this outside of the <strong class="source-inline">busybox</strong> <span class="No-Break">container image:</span></p>&#13;
			<pre class="console">&#13;
kubectl get service</pre>&#13;
			<p>You should get the <span class="No-Break">following output.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer086" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_17.jpg" alt="Figure 6.17 – Service configuration&#13;&#10;" width="669" height="94"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.17 – Service configuration</p>&#13;
			<p>If the <a id="_idIndexMarker405"/>service <a id="_idIndexMarker406"/>is running, confirm that you can reach the service <span class="No-Break">via DNS:</span></p>&#13;
			<pre class="console">&#13;
nslookup service_name</pre>&#13;
			<p>You’ll see an output similar to the <span class="No-Break">following configuration:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer087" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_18.jpg" alt="Figure 6.18 – nslookup of Pod&#13;&#10;" width="546" height="107"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.18 – nslookup of Pod</p>&#13;
			<p>If the standard <strong class="source-inline">nslookup</strong> command doesn’t work, or if you want another type of confirmation, try <span class="No-Break">an FQDN:</span></p>&#13;
			<pre class="console">&#13;
nslookup service_name.namespace_name.svc.cluster.local</pre>&#13;
			<p>You’ll see an output similar to the <span class="No-Break">following screenshot.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer088" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_19.jpg" alt="Figure 6.19 – FQDN service lookup&#13;&#10;" width="522" height="125"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.19 – FQDN service lookup</p>&#13;
			<p>Check to confirm that the service is <span class="No-Break">defined correctly:</span></p>&#13;
			<pre class="console">&#13;
kubectl get service name_of_service -o json</pre>&#13;
			<p>You’ll see<a id="_idIndexMarker407"/> an <a id="_idIndexMarker408"/>output similar to the <span class="No-Break">following screenshot.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer089" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_20.jpg" alt="Figure 6.20 – JSON output of service&#13;&#10;" width="1142" height="547"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.20 – JSON output of service</p>&#13;
			<p>Check that the service has endpoints – as in, confirm that there are Pods that the service is <span class="No-Break">pointing to:</span></p>&#13;
			<pre class="console">&#13;
kubectl get pods -l app=name_of_deployment</pre>&#13;
			<p>You’ll see an output similar to the <span class="No-Break">following screenshot:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer090" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_21.jpg" alt="Figure 6.21 – Retrieving Pods based on label&#13;&#10;" width="591" height="87"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.21 – Retrieving Pods based on label</p>&#13;
			<p>Finally, which should already be known, but just in case, check to confirm that the Pods that the service is pointing to <span class="No-Break">are working:</span></p>&#13;
			<pre class="console">&#13;
kubectl get pods</pre>&#13;
			<p>The final piece, which you’ll learn about in the next section, is implementing a service mesh for troubleshooting. A service mesh <a id="_idIndexMarker409"/>has several jobs, and one of <a id="_idIndexMarker410"/>the jobs is making it easier to troubleshoot latency issues between Services, along with ensuring that Services are working <span class="No-Break">as expected.</span></p>&#13;
			<h2 id="_idParaDest-153"><a id="_idTextAnchor154"/>Troubleshooting Deployments</h2>&#13;
			<p>The primary <a id="_idIndexMarker411"/>command that’ll help you debug Deployments is similar to <span class="No-Break">Pod debugging:</span></p>&#13;
			<p><strong class="source-inline">kubectl </strong><span class="No-Break"><strong class="source-inline">describe deployment</strong></span></p>&#13;
			<p>Unless there’s <a id="_idIndexMarker412"/>something wrong with the Deployment controller itself, there usually isn’t a problem with the actual Deployment. It’s typically a problem with the Pods running inside of the Deployment. However, you still may want to check the <span class="No-Break">Deployment itself.</span></p>&#13;
			<p>To do that, you would run <span class="No-Break">the following:</span></p>&#13;
			<pre class="console">&#13;
kubectl describe deployment deployment_name</pre>&#13;
			<p>You should get an output similar to the <span class="No-Break">following screenshot:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer091" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_22.jpg" alt="Figure 6.22 – Describing the nginx Deployment&#13;&#10;" width="691" height="493"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.22 – Describing the nginx Deployment</p>&#13;
			<p>The <a id="_idIndexMarker413"/>goal of the <strong class="source-inline">describe</strong> command isn’t to tell <a id="_idIndexMarker414"/>you about logs or events or what’s happening – it’s to help you fully understand what’s deployed and how it’s deployed. That way, you can backtrack and see whether what’s deployed is actually supposed to <span class="No-Break">be there.</span></p>&#13;
			<p>In the next section, we’ll wrap up this chapter by talking about what a service mesh is, what an Ingress is, and how to think about <span class="No-Break">implementing them.</span></p>&#13;
			<h1 id="_idParaDest-154"><a id="_idTextAnchor155"/>Service meshes and Ingresses</h1>&#13;
			<p>Almost every containerized application needs to be routed in one way or another – whether it’s so outside users can use the application, applications can talk to each other, or one application needs to connect to another. Routes and Services are extremely important in Kubernetes, which is why service meshes and Ingresses play a <span class="No-Break">huge part.</span></p>&#13;
			<p>In many cases, you’ll need better visuals into what’s happening with services, how traffic is being routed, and what applications are routing to which load balancers and IP addresses. You’ll <a id="_idIndexMarker415"/>also eventually want a way to encrypt traffic between services, which Kubernetes doesn’t give you out of <span class="No-Break">the box.</span></p>&#13;
			<p>Service meshes<a id="_idIndexMarker416"/> and Ingresess are typically more advanced-level topics, but in this book and possibly at this stage in your career, you’ll be ready to dive in and fully understand the pros and cons of using these two tools, plugins, <span class="No-Break">and platforms.</span></p>&#13;
			<h2 id="_idParaDest-155"><a id="_idTextAnchor156"/>Why Ingress?</h2>&#13;
			<p>At this point in your Kubernetes journey, it’s almost certain that you’ve seen a Kubernetes Service. In fact, you’ve seen them throughout this book. A lot of the time, a Kubernetes Service <a id="_idIndexMarker417"/>comes with a frontend app that’s attached to it, in which you need a way for users to interact with the Kubernetes Service. It’s typically in front of a <span class="No-Break">load balancer.</span></p>&#13;
			<p>The problem with that is if you have a load balancer in front of your service, you have to do <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li>Pay extra for the cloud load balancer if you’re using a cloud <span class="No-Break">Kubernetes service</span></li>&#13;
				<li>Set up a virtual load balancer if you’re using an on-premises <span class="No-Break">Kubernetes cluster</span></li>&#13;
				<li>Have a bunch of load balancers <span class="No-Break">to manage</span></li>&#13;
			</ul>&#13;
			<p>With an Ingress controller, you don’t have to worry <span class="No-Break">about that.</span></p>&#13;
			<p>You can have several different Kubernetes Services and have an Ingress controller point to all of them, and each of the services can be reached by a <span class="No-Break">different path.</span></p>&#13;
			<p>Ingress controllers save time, money, management, and effort <span class="No-Break">for engineers.</span></p>&#13;
			<h3>Using Ingresses</h3>&#13;
			<p>Now that you<a id="_idIndexMarker418"/> know about Ingress controllers, let’s see how one can be configured using (at the time of writing) the most popular option, <span class="No-Break">Nginx Ingress.</span></p>&#13;
			<p>First things first – you’ll need a Kubernetes Deployment and Service to deploy. If you don’t already have them and would like to keep things simple, you can use the following Kubernetes Manifest, which is a sample Azure app. It doesn’t have to be running in Azure <span class="No-Break">to work:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
  name: aks-helloworld-one&#13;
spec:&#13;
  replicas: 1&#13;
  selector:&#13;
    matchLabels:&#13;
      app: aks-helloworld-one&#13;
  template:&#13;
    metadata:&#13;
      labels:&#13;
        app: aks-helloworld-one&#13;
    spec:&#13;
      containers:&#13;
      - name: aks-helloworld-one&#13;
        image: mcr.microsoft.com/azuredocs/aks-helloworld:v1&#13;
        ports:&#13;
        - containerPort: 80&#13;
        env:&#13;
        - name: TITLE&#13;
          value: "Welcome to Azure Kubernetes Service (AKS)"&#13;
---&#13;
apiVersion: v1&#13;
kind: Service&#13;
metadata:&#13;
  name: aks-helloworld-one&#13;
spec:&#13;
  type: ClusterIP&#13;
  ports:&#13;
  - port: 80&#13;
  selector:&#13;
    app: aks-helloworld-one</pre>&#13;
			<p>Once the app<a id="_idIndexMarker419"/> itself is deployed, you can deploy the Ingress controller. The Ingress controller is part of the native Kubernetes API set from the named group, so you don’t have to worry about installing other CRDs <span class="No-Break">or controllers:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: networking.k8s.io/v1&#13;
kind: Ingress&#13;
metadata:&#13;
  name: hello-world-ingress-static&#13;
  annotations:&#13;
    nginx.ingress.kubernetes.io/ssl-redirect: "false"&#13;
spec:&#13;
  ingressClassName: nginx&#13;
  rules:&#13;
  - http:&#13;
      paths:&#13;
      - path: /&#13;
        pathType: Prefix&#13;
        backend:&#13;
          service:&#13;
            name: aks-helloworld-one&#13;
            port:&#13;
              number: 80</pre>&#13;
			<p>The<a id="_idIndexMarker420"/> last step is to forward to the port for the app’s service so you can reach the <span class="No-Break">app locally:</span></p>&#13;
			<pre class="source-code">&#13;
kubectl port-forward service/aks-helloworld-one :80</pre>&#13;
			<p>You should get an output similar to the <span class="No-Break">following screenshot:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer092" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_23.jpg" alt="Figure 6.23 – Port-forwarding service communication&#13;&#10;" width="1188" height="117"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.23 – Port-forwarding service communication</p>&#13;
			<p>You should be able to reach out to the app <span class="No-Break">over localhost.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer093" class="IMG---Figure">&#13;
					<img src="Images/B19116_06_24.jpg" alt="Figure 6.24 – AKS app&#13;&#10;" width="1210" height="528"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.24 – AKS app</p>&#13;
			<p>Now that you know what an Ingress is from a theoretical and practical perspective, let’s move on to service meshes and look at how communication can occur <span class="No-Break">more securely.</span></p>&#13;
			<h2 id="_idParaDest-156"><a id="_idTextAnchor157"/>Why service meshes?</h2>&#13;
			<p>When you deploy <a id="_idIndexMarker421"/>containerized applications into a Kubernetes cluster, there are two primary ways that those <span class="No-Break">applications communicate:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break">Services</span></li>&#13;
				<li><span class="No-Break">Pod-to-Pod communication</span></li>&#13;
			</ul>&#13;
			<p>Pod-to-Pod communication isn’t recommended because Pods are ephemeral, which means they aren’t permanent. They are designed to go down at any time and only if they were part of a StatefulSet would they keep any type of unique identifier. However, Pods still need to be able to communicate with each other. Backends need to talk to frontends, middleware needs to talk to backends and frontends, and <span class="No-Break">so on.</span></p>&#13;
			<p>The next communication method, which is the primary, is service-to-service. Service-to-service is the preferred method because a Service isn’t ephemeral and only gets deleted if manually deleted. Pods can connect to Services with Selectors or Tags. If a Pod goes down, but the Selector in the Kubernetes Manifest that deployed the Pod doesn’t change, the new Pod will be connected to <span class="No-Break">the Service.</span></p>&#13;
			<p>Here’s the primary concern with everything described so far – all this traffic is unencrypted. Pod-to-Pod communication, or as some people like to call it, East-West traffic, is unencrypted. That means if for any reason a Pod is compromised or you have some segregation issues, there’s nothing out of the box that you <span class="No-Break">can do.</span></p>&#13;
			<p>That’s where a service mesh comes into play. A service mesh has the ability to do <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li>Encrypt traffic <span class="No-Break">between microservices</span></li>&#13;
				<li>Help with network <span class="No-Break">latency troubleshooting</span></li>&#13;
				<li>Securely connect <span class="No-Break">Kubernetes Services</span></li>&#13;
				<li>Perform observability for tracing <span class="No-Break">and alerting</span></li>&#13;
			</ul>&#13;
			<h3>Using a service mesh</h3>&#13;
			<p>Now that <a id="_idIndexMarker422"/>you know about a service mesh, let’s learn how to set one up. There are a ton of different service mesh platforms out there, all of which have their own method of being installed <span class="No-Break">and configured.</span></p>&#13;
			<p>Because it’s a complicated topic in itself, there’s no way to get through it all in this section. In fact, there are literal books for just service meshes. Let’s learn how to set up an Istio <span class="No-Break">service mesh.</span></p>&#13;
			<p>First, <span class="No-Break">download Istio:</span></p>&#13;
			<pre class="console">&#13;
curl -L https://istio.io/downloadIstio | sh</pre>&#13;
			<p>Next, export the path to the <strong class="source-inline">$</strong><span class="No-Break"><strong class="source-inline">PATH</strong></span><span class="No-Break"> variable:</span></p>&#13;
			<pre class="console">&#13;
export PATH=$PWD/bin:$PATH</pre>&#13;
			<p>Output the <strong class="source-inline">$PATH</strong> variable <span class="No-Break">int, </span><span class="No-Break"><strong class="source-inline">bashrc</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="console">&#13;
echo "export PATH=$PATH:$HOME/istio-1.15.0/bin" &gt;&gt; ~/.bashrc</pre>&#13;
			<p>Install Istio on<a id="_idIndexMarker423"/> your Kubernetes cluster. Notice how Ingress is set to <strong class="source-inline">false</strong>. You can set it to <strong class="source-inline">true</strong> if you want to use the Istio Ingress. If you’re using another Ingress controller, such as Nginx Ingress, you can leave it <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">false</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="console">&#13;
istioctl install --set values.gateways.istio-ingressgateway.enabled=false</pre>&#13;
			<p>Istio is a great service mesh but doesn’t have a UI out of the box. One of the most popular ways to look at your service mesh graphically is by using Kiali, which is a <span class="No-Break">simple install:</span></p>&#13;
			<pre class="console">&#13;
kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/addons/kiali.yaml</pre>&#13;
			<p>Set up port forwarding to Kiali so you can reach the <span class="No-Break">UI locally:</span></p>&#13;
			<pre class="console">&#13;
kubectl port-forward -n istio-system service/kiali :20001</pre>&#13;
			<p>The last step is to take a Kubernetes Manifest, like the one you used in this chapter, and inject the sidecar (the service mesh container) into your <span class="No-Break">Kubernetes Deployment:</span></p>&#13;
			<pre class="console">&#13;
istioctl kube-inject -f nginx.yaml | kubectl apply -f –</pre>&#13;
			<p>At this point, you now have the theoretical grounding and a bit of hands-on knowledge for how to move forward on your service <span class="No-Break">mesh journey.</span></p>&#13;
			<h1 id="_idParaDest-157"><a id="_idTextAnchor158"/>Summary</h1>&#13;
			<p>Overall, containerized application deployment, troubleshooting, and third-party tooling are going to be the core pieces of what your Kubernetes cluster looks like. Without proper troubleshooting, you won’t have successful deployments. Without third-party tooling such as Ingress controllers, you won’t be able to properly manage frontend apps. Out of the box, Kubernetes gives you a ton to use to make things work. However, there are more steps you need to take. For better or for worse, Kubernetes isn’t one of those platforms where you can just deploy it and walk away. It takes management and engineering skills to ensure it’s working <span class="No-Break">as expected.</span></p>&#13;
			<p>In the next chapter, you’ll learn about how to monitor the workloads you’ve been deploying throughout <span class="No-Break">this book.</span></p>&#13;
			<h1 id="_idParaDest-158"><a id="_idTextAnchor159"/>Further reading</h1>&#13;
			<ul>&#13;
				<li><em class="italic">Learn Helm</em> by Andrew Block and Austin <span class="No-Break">Dewey: </span><a href="https://www.packtpub.com/product/learn-helm/9781839214295%0D"><span class="No-Break">https://www.packtpub.com/product/learn-helm/9781839214295</span></a></li>&#13;
				<li><em class="italic">Mastering Service Mesh</em> by Anjali Khatri and Vikram <span class="No-Break">Khatri: </span><a href="https://www.packtpub.com/product/mastering-service-mesh/9781789615791?_ga=2.161313023.37784508.1672298745-664251363.1663254593"><span class="No-Break">https://www.packtpub.com/product/mastering-service-mesh/9781789615791?_ga=2.161313023.37784508.1672298745-664251363.1663254593</span></a></li>&#13;
			</ul>&#13;
		</div>&#13;
	</div></body></html>