<html><head></head><body>
<div epub:type="chapter" id="_idContainer081">
<h1 class="chapter-number" id="_idParaDest-337"><a id="_idTextAnchor337"/><span class="koboSpan" id="kobo.1.1">13</span></h1>
<h1 id="_idParaDest-338"><a id="_idTextAnchor338"/><span class="koboSpan" id="kobo.2.1">How Do You Know It All Works?</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Testing our code is how we ensure that our changes are both fit for purpose and that they don’t regress any existing functionality. </span><span class="koboSpan" id="kobo.3.2">In a cloud native environment, our complexity increasingly lives in areas beyond the scope of our code, so testing our application in a meaningful way can become complex. </span><span class="koboSpan" id="kobo.3.3">Let’s explore how we can test cloud native code in ways that are both time-efficient and meaningful while avoiding some </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">common anti-patterns.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">In this chapter, we’re going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">main topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.7.1">General </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">testing anti-patterns</span></span></li>
<li><span class="koboSpan" id="kobo.9.1">Lack of </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">contract testing</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.11.1">Manual testing</span></span></li>
<li><span class="koboSpan" id="kobo.12.1">Trying to recreate </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">the cloud</span></span></li>
<li><span class="koboSpan" id="kobo.14.1">Poorly </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">structured code</span></span></li>
</ul>
<h1 id="_idParaDest-339"><a id="_idTextAnchor339"/><span class="koboSpan" id="kobo.16.1">General testing anti-patterns</span></h1>
<p><span class="koboSpan" id="kobo.17.1">Before we explore the types </span><a id="_idIndexMarker1276"/><span class="koboSpan" id="kobo.18.1">of tests commonly used in cloud native applications, we must first explore some general testing anti-patterns that we must avoid. </span><span class="koboSpan" id="kobo.18.2">These anti-patterns typically result from the evolution of the application’s testing strategy as it is migrated to the cloud. </span><span class="koboSpan" id="kobo.18.3">While most of these anti-patterns apply to unit tests, it’s essential to be mindful of them when testing other patterns </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">as well.</span></span></p>
<p><span class="koboSpan" id="kobo.20.1">First, we will look at some testing anti-patterns and how they surface in a cloud native environment. </span><span class="koboSpan" id="kobo.20.2">The specific anti-patterns we will explore are </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.22.1">Tests that have </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">never failed</span></span></li>
<li><span class="koboSpan" id="kobo.24.1">Coverage </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">badge tests</span></span></li>
<li><span class="koboSpan" id="kobo.26.1">Testing </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">implementation details</span></span></li>
<li><span class="koboSpan" id="kobo.28.1">Tests that </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">intermittently fail</span></span></li>
<li><span class="koboSpan" id="kobo.30.1">Tests with side effects or </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">coupled tests</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.32.1">Multi-stage</span></span><span class="No-Break"><a id="_idIndexMarker1277"/></span><span class="No-Break"><span class="koboSpan" id="kobo.33.1"> tests</span></span></li>
</ul>
<h2 id="_idParaDest-340"><a id="_idTextAnchor340"/><span class="koboSpan" id="kobo.34.1">Tests that have never failed</span></h2>
<p><span class="koboSpan" id="kobo.35.1">When we think about</span><a id="_idIndexMarker1278"/><span class="koboSpan" id="kobo.36.1"> testing, we might think that a test that has never failed is good. </span><span class="koboSpan" id="kobo.36.2">That means our code and changes have always complied with our expected behavior, right? </span><span class="koboSpan" id="kobo.36.3">Without the test failing, how can we be sure that the test fails when its contract </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">is breached?</span></span></p>
<p><span class="koboSpan" id="kobo.38.1">To illustrate this situation, I will use my experience with some of our teams in a previous role. </span><span class="koboSpan" id="kobo.38.2">The teams had just finished writing their functionality and were in the process of writing tests. </span><span class="koboSpan" id="kobo.38.3">They were working with an asynchronous code base in Node.js, and a quirk of asynchronous programming in Node.js is that when an asynchronous function is called and it contains asynchronous code, without a top-level await on the function call in the test, the test will exit before the asynchronous code executes. </span><span class="koboSpan" id="kobo.38.4">This means any assertions in the asynchronous code would only throw errors after the test, and because no assertions were thrown during test execution, the test passes. </span><span class="koboSpan" id="kobo.38.5">From an untrained perspective, the test appears to test the functionality expected. </span><span class="koboSpan" id="kobo.38.6">However, in practice, the test is useless. </span><span class="koboSpan" id="kobo.38.7">Unsurprisingly, many tests started failing when we sprinkled in some </span><strong class="source-inline"><span class="koboSpan" id="kobo.39.1">async</span></strong><span class="koboSpan" id="kobo.40.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.41.1">await</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.42.1">syntactic sugar.</span></span></p>
<p><span class="koboSpan" id="kobo.43.1">In this example, a lack of understanding of asynchronous programming principles contributed to functionally useless tests that gave the impression everything </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">was okay.</span></span></p>
<p><span class="koboSpan" id="kobo.45.1">This anti-pattern is an easy trap to fall into in cloud computing. </span><span class="koboSpan" id="kobo.45.2">As systems become asynchronous, decoupled, and eventually consistent, our testing strategy must match the system’s complexity. </span><span class="koboSpan" id="kobo.45.3">You will notice that the entire situation could have been avoided had the team </span><a id="_idIndexMarker1279"/><span class="koboSpan" id="kobo.46.1">followed </span><strong class="bold"><span class="koboSpan" id="kobo.47.1">test-driven development</span></strong><span class="koboSpan" id="kobo.48.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.49.1">TDD</span></strong><span class="koboSpan" id="kobo.50.1">). </span><span class="koboSpan" id="kobo.50.2">The common TDD approach I like to utilize is </span><em class="italic"><span class="koboSpan" id="kobo.51.1">Red</span></em><span class="koboSpan" id="kobo.52.1">, </span><em class="italic"><span class="koboSpan" id="kobo.53.1">Green</span></em><span class="koboSpan" id="kobo.54.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.56.1">Refactor</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.58.1">Red</span></strong><span class="koboSpan" id="kobo.59.1">: First, create the minimum</span><a id="_idIndexMarker1280"/><span class="koboSpan" id="kobo.60.1"> structure required to support your test. </span><span class="koboSpan" id="kobo.60.2">This might be an empty function block, method, or object. </span><span class="koboSpan" id="kobo.60.3">Second, write a test (or tests) that you believe tests your expected behavior. </span><span class="koboSpan" id="kobo.60.4">When you run your tests, they should fail, </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">showing red.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.62.1">Green</span></strong><span class="koboSpan" id="kobo.63.1">: Fill </span><a id="_idIndexMarker1281"/><span class="koboSpan" id="kobo.64.1">out your empty placeholder with the logic to make your test pass, </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">showing green.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.66.1">Refactor</span></strong><span class="koboSpan" id="kobo.67.1">: Create new tests and functionality to handle edge cases. </span><span class="koboSpan" id="kobo.67.2">In these scenarios, it is best to create positive and negative test cases and purposefully break the test a</span><a id="_idIndexMarker1282"/><span class="koboSpan" id="kobo.68.1"> few times to ensure it behaves </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">as expected.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.70.1">In the cloud native world, typically, these tests would form part of our automated integration pipeline, such </span><a id="_idIndexMarker1283"/><span class="koboSpan" id="kobo.71.1">as in AWS CodePipeline, GCP Cloud Build, or Azure </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">DevOps Pipelines.</span></span></p>
<h2 id="_idParaDest-341"><a id="_idTextAnchor341"/><span class="koboSpan" id="kobo.73.1">Coverage badge tests</span></h2>
<p><span class="koboSpan" id="kobo.74.1">Another anti-pattern</span><a id="_idIndexMarker1284"/><span class="koboSpan" id="kobo.75.1"> that often comes up</span><a id="_idIndexMarker1285"/><span class="koboSpan" id="kobo.76.1"> is </span><strong class="bold"><span class="koboSpan" id="kobo.77.1">coverage badge tests</span></strong><span class="koboSpan" id="kobo.78.1">. </span><span class="koboSpan" id="kobo.78.2">When attempting a cloud migration or refactoring of existing cloud code, a common goal we see added to the agenda is to increase test coverage. </span><span class="koboSpan" id="kobo.78.3">This mentality is putting the cart before the horse. </span><span class="koboSpan" id="kobo.78.4">Good test coverage should arise from writing good, comprehensive tests. </span><span class="koboSpan" id="kobo.78.5">It is perfectly possible to have high test coverage but poor-quality tests. </span><span class="koboSpan" id="kobo.78.6">A test that simply checks that an HTTP server returns a </span><strong class="source-inline"><span class="koboSpan" id="kobo.79.1">200</span></strong><span class="koboSpan" id="kobo.80.1"> status code might give you good test coverage, but is it a good test? </span><span class="koboSpan" id="kobo.80.2">What about the semantic structure of the data? </span><span class="koboSpan" id="kobo.80.3">Does the output match the expected input? </span><span class="koboSpan" id="kobo.80.4">The behavior of the endpoint is completely untested in this scenario. </span><span class="koboSpan" id="kobo.80.5">We haven’t guaranteed that any future changes won’t result in unexpected behaviors, just that they will return a status code </span><span class="No-Break"><span class="koboSpan" id="kobo.81.1">of </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.82.1">200</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.84.1">Incentivizing code coverage in isolation will not give you greater certainty of the emergent behaviors of your application. </span><span class="koboSpan" id="kobo.84.2">Instead, you must incentivize writing proper tests that have been peer-reviewed to describe the expected behavior of the system. </span><span class="koboSpan" id="kobo.84.3">A simple litmus test for good testing practice is whether the test ensures that the emergent behavior of the system more closely aligns with the behavior in our mental model of </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">the system.</span></span></p>
<h2 id="_idParaDest-342"><a id="_idTextAnchor342"/><span class="koboSpan" id="kobo.86.1">Testing implementation details</span></h2>
<p><span class="koboSpan" id="kobo.87.1">Requiring developers </span><a id="_idIndexMarker1286"/><span class="koboSpan" id="kobo.88.1">to hit a code coverage threshold set too high can also lead to another anti-pattern: testing implementation details. </span><span class="koboSpan" id="kobo.88.2">This anti-pattern can be particularly insidious in the cloud native domain as we are more concerned with the result and emergent system behaviors than the method used to achieve them, as implementation details can be very fluid as we leverage new architectural and technological patterns. </span><span class="koboSpan" id="kobo.88.3">For example, if we need to sort an array, we might first check that the input is an array of numbers, then call a bubble sort function if it is. </span><span class="koboSpan" id="kobo.88.4">Let’s say we write two </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">tests here:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.90.1">Check that the bubble sort function is not called when the array is not an array of numbers and the result is </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">an error</span></span></li>
<li><span class="koboSpan" id="kobo.92.1">Check that the bubble sort function is called when the array is an array of numbers and the result is a </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">sorted array</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.94.1">Later, someone removes the initial check to see whether the array is an array of numbers and replaces the bubble sort with a merge sort function that already has built-in type checking. </span><span class="koboSpan" id="kobo.94.2">This is what happens to </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">our test:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.96.1">Our first test passes, even though we now call the sort function on every execution because our merge sort function differs from our bubble </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">sort function</span></span></li>
<li><span class="koboSpan" id="kobo.98.1">Our second test fails because we did not call the bubble </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">sort function</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.100.1">In this case, we have not changed the emergent behavior of the system; we have only changed the implementation details. </span><span class="koboSpan" id="kobo.100.2">Instead, we could design our test to look </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">like this:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.102.1">Check that we get an error on anything other than an array </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">of numbers</span></span></li>
<li><span class="koboSpan" id="kobo.104.1">Check that we correctly sort an array </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">of numbers</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.106.1">These tests check solely the exhibited behavior, not how we achieved it. </span><span class="koboSpan" id="kobo.106.2">Under this new testing framework, both tests will pass when we perform </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">our refactor.</span></span></p>
<h2 id="_idParaDest-343"><a id="_idTextAnchor343"/><span class="koboSpan" id="kobo.108.1">Intermittently failing tests</span></h2>
<p><span class="koboSpan" id="kobo.109.1">I have often asked </span><a id="_idIndexMarker1287"/><span class="koboSpan" id="kobo.110.1">clients about a failing test pipeline only to be told, “</span><em class="italic"><span class="koboSpan" id="kobo.111.1">Yeah, it does that sometimes. </span><span class="koboSpan" id="kobo.111.2">Just rerun it.</span></em><span class="koboSpan" id="kobo.112.1">” Intermittently failing tests </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">breed ambiguity.</span></span></p>
<p><span class="koboSpan" id="kobo.114.1">When a test pipeline fails, our first instinct is to rerun it. </span><span class="koboSpan" id="kobo.114.2">This ambiguity means that our mean time to identify failures in our pipeline goes through the roof, as we don’t know whether the culprit is a failing test or whether the pipeline is just acting up. </span><span class="koboSpan" id="kobo.114.3">It is essential to be not only confident in the success of your passing tests but also in your </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">failing tests.</span></span></p>
<p><span class="koboSpan" id="kobo.116.1">Let us imagine a hypothetical intermittently failing series of tests. </span><span class="koboSpan" id="kobo.116.2">These tests would block production deployments, PR reviews, and local testing. </span><span class="koboSpan" id="kobo.116.3">It always seems to sort itself by the next run, it only happens a few times a year, and it’s an infrequently updated micro-frontend, so why bother </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">fixing it?</span></span></p>
<p><span class="koboSpan" id="kobo.118.1">After triaging the issue, we found the culprit pretty quickly: someone asserted in a test that the current UTC minute of the hour was less than 59 instead of less than or equal to. </span><span class="koboSpan" id="kobo.118.2">This change, in line with probability, was pushed and merged successfully. </span><span class="koboSpan" id="kobo.118.3">The expectation was buried deep in a block that prevented a precursory glance from diagnosing the problem from the test output. </span><span class="koboSpan" id="kobo.118.4">This also creates a compelling argument for verbose and well-formatted test outputs. </span><span class="koboSpan" id="kobo.118.5">As you can imagine, someone’s pipeline failed after working locally; they decided to rerun it, and it passed. </span><span class="koboSpan" id="kobo.118.6">It became known that that particular pipeline was flaky and we could fix it with a rerun. </span><span class="koboSpan" id="kobo.118.7">What effect do you think that has </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">on developers?</span></span></p>
<p><span class="koboSpan" id="kobo.120.1">When I ran into this situation in my work, we found that the number of failed reruns significantly outpaced the actual number of </span><em class="italic"><span class="koboSpan" id="kobo.121.1">flaky</span></em><span class="koboSpan" id="kobo.122.1"> runs due to a lack of confidence in the failures of the underlying pipeline. </span><span class="koboSpan" id="kobo.122.2">Cloud native delivery allows us to push incremental changes to our code base rapidly. </span><span class="koboSpan" id="kobo.122.3">This process means that a high-performing team will run these pipelines multiple </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">times daily.</span></span></p>
<p><span class="koboSpan" id="kobo.124.1">Therefore, in a cloud native environment, having faith in your pipelines, both in success and failure, is imperative. </span><span class="koboSpan" id="kobo.124.2">Another common way that tests become flaky is by relying on test side effects or </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">coupled tests.</span></span></p>
<h2 id="_idParaDest-344"><a id="_idTextAnchor344"/><span class="koboSpan" id="kobo.126.1">Tests with side effects or coupled tests</span></h2>
<p><span class="koboSpan" id="kobo.127.1">Relying on side effects </span><a id="_idIndexMarker1288"/><span class="koboSpan" id="kobo.128.1">or coupling tests is an easy trap, especially as we refactor code and add existing tests, as other tests may already cause side effects that our new tests may unknowingly come to </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">depend on.</span></span></p>
<p><span class="koboSpan" id="kobo.130.1">For illustrative purposes, let us consider tests that ensure user behavior. </span><span class="koboSpan" id="kobo.130.2">We have two endpoints: one to create users and one to delete users. </span><span class="koboSpan" id="kobo.130.3">We have one test that generates a random email, creates a user with that email, and saves it as a global variable in the test file. </span><span class="koboSpan" id="kobo.130.4">Then, another test reads the global variable and deletes the user, checking whether the user is deleted correctly. </span><span class="koboSpan" id="kobo.130.5">We have broken both rules here. </span><span class="koboSpan" id="kobo.130.6">Not only do we have a side effect by modifying the global state but we have also coupled two tests through that side effect. </span><span class="koboSpan" id="kobo.130.7">It’s essential to understand what we have </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">lost here:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.132.1">Isolated testing</span></strong><span class="koboSpan" id="kobo.133.1">: Because of the coupling, if we want to run only the </span><em class="italic"><span class="koboSpan" id="kobo.134.1">user delete</span></em><span class="koboSpan" id="kobo.135.1"> test, it will always fail because it needs to be run in concert with the </span><em class="italic"><span class="koboSpan" id="kobo.136.1">user create</span></em><span class="koboSpan" id="kobo.137.1"> test. </span><span class="koboSpan" id="kobo.137.2">We can now only run the entire </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">test file.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.139.1">Ability to refactor</span></strong><span class="koboSpan" id="kobo.140.1">: If we move the tests to different files or change their execution order, they will fail. </span><span class="koboSpan" id="kobo.140.2">This makes refactoring harder, as we now need to understand its coupled tests to refactor the test for the functionality we are </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">interested in.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.142.1">Parallel execution</span></strong><span class="koboSpan" id="kobo.143.1">: As our test base grows, it becomes apparent that we need to optimize our pipeline execution. </span><span class="koboSpan" id="kobo.143.2">The first tool people will usually reach for is parallel execution. </span><span class="koboSpan" id="kobo.143.3">When we couple tests, parallel execution can cause you to lose the deterministic execution of your test suite. </span><span class="koboSpan" id="kobo.143.4">This lack of determinism means that your tests may intermittently fail, contributing to “flaky” pipelines as the tests may or may not execute in the </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">correct order.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.145.1">How can we remove the coupling and side effects from our example? </span><span class="koboSpan" id="kobo.145.2">A simple indicator for a single test is to run our test in isolation and check that it still passes. </span><span class="koboSpan" id="kobo.145.3">This check ensures that our test</span><a id="_idIndexMarker1289"/><span class="koboSpan" id="kobo.146.1"> has no upstream coupling; it does not test for side effects or </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">downstream coupling.</span></span></p>
<p><span class="koboSpan" id="kobo.148.1">The next step is to refactor our test files. </span><span class="koboSpan" id="kobo.148.2">Ideally, there should be no global variables. </span><span class="koboSpan" id="kobo.148.3">This concept can be controversial as many test implementations will have static data in global variables. </span><span class="koboSpan" id="kobo.148.4">Still, strictly controlled generated data will always beat </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">static data.</span></span></p>
<p><span class="koboSpan" id="kobo.150.1">The driving force behind this is simple: having generated data means that you are testing the bounds of your system to a greater extent. </span><span class="koboSpan" id="kobo.150.2">It can contribute to intermittently failing test pipelines, but if you hit an intermittent failure, take it as a blessing, not a curse. </span><span class="koboSpan" id="kobo.150.3">Hitting an intermittent failure means the data you generated to match your expected production data does not behave as expected! </span><span class="koboSpan" id="kobo.150.4">If you had used static data, you would never have found this edge case </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">before production.</span></span></p>
<p><span class="koboSpan" id="kobo.152.1">The other issue with static data is that teams tend to get lazy. </span><span class="koboSpan" id="kobo.152.2">The usual culprit is UUIDs. </span><span class="koboSpan" id="kobo.152.3">I’ve seen production systems go down because someone had used the same UUID to index two different values and then created a correlation in code where no correlation existed in the production data. </span><span class="koboSpan" id="kobo.152.4">The cause was that rather than generate a new UUID, a developer saw a UUID generated for a different entity and decided to copy the already compliant UUID to save about 20 seconds of development effort. </span><span class="koboSpan" id="kobo.152.5">As you can imagine, saving those 20 seconds was massively outweighed by the impacts of the </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">eventual downtime.</span></span></p>
<p><span class="koboSpan" id="kobo.154.1">Most testing libraries have pre-test and post-test hooks to set up your data and application components. </span><span class="koboSpan" id="kobo.154.2">A level of granularity is also usually provided. </span><span class="koboSpan" id="kobo.154.3">You can run before and after </span><em class="italic"><span class="koboSpan" id="kobo.155.1">all</span></em><span class="koboSpan" id="kobo.156.1"> tests or before and after </span><em class="italic"><span class="koboSpan" id="kobo.157.1">each</span></em><span class="koboSpan" id="kobo.158.1"> test. </span><span class="koboSpan" id="kobo.158.2">The deciding factor on when to use them is based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">application component.</span></span></p>
<p><span class="koboSpan" id="kobo.160.1">If the component has an internal state modified by tests, then that component should be created and disposed of before and after each test. </span><span class="koboSpan" id="kobo.160.2">Examples include local caches and persistence layers. </span><span class="koboSpan" id="kobo.160.3">If the component does not have an internal state, it is probably safe to optimize by setting it up once for all tests and tearing it down when all tests </span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">have finished.</span></span></p>
<p><span class="koboSpan" id="kobo.162.1">Examples might include authentication layers (unless you’re storing sessions in this layer!), request routing layers, or utility components. </span><span class="koboSpan" id="kobo.162.2">When we look at avoiding side effects and </span><a id="_idIndexMarker1290"/><span class="koboSpan" id="kobo.163.1">ordering in tests, we might think of putting our entire flow in a single test. </span><span class="koboSpan" id="kobo.163.2">Then, we’re not breaking the boundaries between our tests! </span><span class="koboSpan" id="kobo.163.3">However, this leads us to our next non-functional antipattern: </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">multistage tests.</span></span></p>
<h2 id="_idParaDest-345"><a id="_idTextAnchor345"/><span class="koboSpan" id="kobo.165.1">Multistage tests</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.166.1">Multistage tests</span></strong><span class="koboSpan" id="kobo.167.1"> often</span><a id="_idIndexMarker1291"/><span class="koboSpan" id="kobo.168.1"> come about because we see actions</span><a id="_idIndexMarker1292"/><span class="koboSpan" id="kobo.169.1"> as being related. </span><span class="koboSpan" id="kobo.169.2">However, we need to keep in mind that the purpose of testing is usually to test a unit of behavior, even in integration tests, albeit with a broader definition of our unit of behavior. </span><span class="koboSpan" id="kobo.169.3">To understand why this is an anti-pattern, we need to look at our failure modes. </span><span class="koboSpan" id="kobo.169.4">When we have many atomic tests, we can easily see which functionality is broken. </span><span class="koboSpan" id="kobo.169.5">With a smaller number of multistage tests, we might cover the same amount of behavior, but we lose fidelity in </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">our reporting.</span></span></p>
<p><span class="koboSpan" id="kobo.171.1">Early errors in a multistage test can also cause the test to fail early, masking errors from later in the multistage test. </span><span class="koboSpan" id="kobo.171.2">It might be a logical fallacy, but if we replaced all our tests with one large multistage test, we would have either a pass or fail for the entire system, which makes the search area on failure very broad. </span><span class="koboSpan" id="kobo.171.3">At the other extreme, where we make our tests as atomic as possible, we get extremely high fidelity and know precisely which units of behavior are broken. </span><span class="koboSpan" id="kobo.171.4">A pattern to follow in this area is to use </span><strong class="bold"><span class="koboSpan" id="kobo.172.1">arrange, act, and </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.173.1">assert</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.174.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.175.1">AAA</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.176.1">):</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.177.1">Arrange</span></strong><span class="koboSpan" id="kobo.178.1">: Set up</span><a id="_idIndexMarker1293"/><span class="koboSpan" id="kobo.179.1"> everything required for the test to run (data, authentication, application </span><span class="No-Break"><span class="koboSpan" id="kobo.180.1">instances, etc.).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.181.1">Act</span></strong><span class="koboSpan" id="kobo.182.1">: Perform the behavior under test. </span><span class="koboSpan" id="kobo.182.2">This action might be calling an endpoint or method, or performing an </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">integration flow.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.184.1">Assert</span></strong><span class="koboSpan" id="kobo.185.1">: Check that the results of your behavior match what </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">you expect.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.187.1">The key here is that this pattern should only occur in order once in a test. </span><span class="koboSpan" id="kobo.187.2">For example, a test that does not follow this pattern might go like this: arrange, act, assert, act, assert, act, assert. </span><span class="koboSpan" id="kobo.187.3">Failures in higher asserts mask all actions after the first assert. </span><span class="koboSpan" id="kobo.187.4">Therefore, our tests should have the correct level of atomicity to provide as much detail </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">as possible.</span></span></p>
<p><span class="koboSpan" id="kobo.189.1">So far, we have</span><a id="_idIndexMarker1294"/><span class="koboSpan" id="kobo.190.1"> mainly focussed on unit testing, but we should not unit test to the exclusion of all else. </span><span class="koboSpan" id="kobo.190.2">Next, we will look at another critical type of testing to ensure semantic correctness: </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">contract testing.</span></span></p>
<h1 id="_idParaDest-346"><a id="_idTextAnchor346"/><span class="koboSpan" id="kobo.192.1">Lack of contract testing</span></h1>
<p><span class="koboSpan" id="kobo.193.1">In a cloud native environment, we often have loose coupling between components, with functionality exposed through a combination of APIs and events while consumed by other microservices, user interfaces, third parties, and every combination and permutation. </span><span class="koboSpan" id="kobo.193.2">When developing system components, worrying about the immediate application is no longer enough. </span><span class="koboSpan" id="kobo.193.3">Instead, we need to provide confidence about the communications between our services. </span><span class="koboSpan" id="kobo.193.4">This is where contract testing comes </span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">into play.</span></span></p>
<h2 id="_idParaDest-347"><a id="_idTextAnchor347"/><span class="koboSpan" id="kobo.195.1">Contract testing</span></h2>
<p><span class="koboSpan" id="kobo.196.1">At the core of </span><strong class="bold"><span class="koboSpan" id="kobo.197.1">contract testing</span></strong><span class="koboSpan" id="kobo.198.1"> is the</span><a id="_idIndexMarker1295"/><span class="koboSpan" id="kobo.199.1"> concept of a contract. </span><span class="koboSpan" id="kobo.199.2">A </span><strong class="bold"><span class="koboSpan" id="kobo.200.1">contract</span></strong><span class="koboSpan" id="kobo.201.1"> is a specification that explains </span><a id="_idIndexMarker1296"/><span class="koboSpan" id="kobo.202.1">precisely how data will be shared between services and its format, and it may even make some assurances around non-functional requirements. </span><span class="koboSpan" id="kobo.202.2">This contract may exist as an OpenAPI specification, JSON Schema, Protobuf definition, Smithy interface, or similarly in</span><a id="_idIndexMarker1297"/><span class="koboSpan" id="kobo.203.1"> any </span><strong class="bold"><span class="koboSpan" id="kobo.204.1">interface definition </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.205.1">language</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.206.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.207.1">IDL</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.209.1">The other piece of the data contract puzzle is that it should also give the semantic meaning of the data being transferred. </span><span class="koboSpan" id="kobo.209.2">The key is providing consumers with a clear definition of what to expect. </span><span class="koboSpan" id="kobo.209.3">Now that we have a contract, we can examine our application’s output and ensure it agrees with our published schema. </span><span class="koboSpan" id="kobo.209.4">In other words, we test our application against </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">the contract.</span></span></p>
<p><span class="koboSpan" id="kobo.211.1">We can now decouple the development of different parts of our application. </span><span class="koboSpan" id="kobo.211.2">By defining our communication patterns in advance and defining tests that allow us to check our compliance with that pattern, we can build multiple parts of the application if we agree on the contracts we align to. </span><span class="koboSpan" id="kobo.211.3">As teams grow and functionality development grows beyond the scope of one developer, these types of tests become increasingly important. </span><span class="koboSpan" id="kobo.211.4">If one developer is working on a vertical slice of application functionality, they might iteratively design the communication patterns between the application components as they progress. </span><span class="koboSpan" id="kobo.211.5">This allows for agile development; however, it falls over when that developer needs to collaborate on that functionality with other parties. </span><span class="koboSpan" id="kobo.211.6">The iterative changes they are keeping in their head suddenly become impediments to the system’s progress as a whole, as these frequent changes need to </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">be communicated.</span></span></p>
<p><span class="koboSpan" id="kobo.213.1">While it may sound slightly waterfall-like to define your communication patterns up front, it’s important to note that the level of upfront planning is minimal. </span><span class="koboSpan" id="kobo.213.2">We’re operating at atomic units of functionality here, one or two API endpoints at a time, not a monolithic definition of a system. </span><span class="koboSpan" id="kobo.213.3">Putting in the time up front to build a shared understanding of the communication model will pay dividends in the future, as rather than iterative, rapid changes to data exchange models, we are now only making changes to the model as </span><a id="_idIndexMarker1298"/><span class="koboSpan" id="kobo.214.1">functionally required by and agreed upon by </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">both parties.</span></span></p>
<h2 id="_idParaDest-348"><a id="_idTextAnchor348"/><span class="koboSpan" id="kobo.216.1">Beyond the initial contract</span></h2>
<p><span class="koboSpan" id="kobo.217.1">As we build out </span><a id="_idIndexMarker1299"/><span class="koboSpan" id="kobo.218.1">these contracts for data exchange methods, we can start publishing these artifacts for other parties to consume. </span><span class="koboSpan" id="kobo.218.2">By ensuring that we remain faithful to our data contracts through contract testing, we ensure that our current and future consumers can enjoy the continued operation of their dependencies. </span><span class="koboSpan" id="kobo.218.3">New users can easily onboard as consumers of the system as it </span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">is documented.</span></span></p>
<p><span class="koboSpan" id="kobo.220.1">The question then becomes, what happens when we need to change a contract? </span><span class="koboSpan" id="kobo.220.2">This is where two other anti-patterns present themselves. </span><span class="koboSpan" id="kobo.220.3">The first anti-pattern is not maintaining a service dependency map. </span><span class="koboSpan" id="kobo.220.4">A service dependency map tells us exactly which services consume functionality from the service we have built to the </span><span class="No-Break"><span class="koboSpan" id="kobo.221.1">contract specification.</span></span></p>
<p><span class="koboSpan" id="kobo.222.1">This allows us to assess the blast radius of the service we are making a contract change to and ensure that any changes we make to the contract are compatible with other services that consume it. </span><span class="koboSpan" id="kobo.222.2">Many cloud service providers will have distributed traceability of transactions through inbuilt observability tooling, or we may be able to build one through any of the third-party tools that offer a similar service. </span><span class="koboSpan" id="kobo.222.3">Without a service dependency map, we don’t have any visibility into the blast radius of changes we plan on making. </span><span class="koboSpan" id="kobo.222.4">Let’s look at an example of a simple </span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">service diagram.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer077">
<span class="koboSpan" id="kobo.224.1"><img alt="" role="presentation" src="image/B22364_13_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.225.1">Figure 13.1 - A simple example of a user service, exposed through an API gateway, called by two upstream services</span></p>
<p><span class="koboSpan" id="kobo.226.1">In this example, we have a user endpoint called by both the messaging service and the backend for </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">frontend services.</span></span></p>
<p><span class="koboSpan" id="kobo.228.1">From the preceding example, we can see that a change to the contract of </span><strong class="source-inline"><span class="koboSpan" id="kobo.229.1">/user</span></strong><span class="koboSpan" id="kobo.230.1"> on the user service will impact two upstream services that may also have to be updated to ensure continuity of service. </span><span class="koboSpan" id="kobo.230.2">When we define the new contract, we can use it to test the upstream services and, if they all pass, safely make the change. </span><span class="koboSpan" id="kobo.230.3">How can we make contracts that don’t break upstream services when we </span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">change them?</span></span></p>
<p><span class="koboSpan" id="kobo.232.1">This brings us to our second anti-pattern, which directly manipulates the existing data contract. </span><span class="koboSpan" id="kobo.232.2">We can extend the data contract to include new functionality instead of modifying the semantic meaning of existing fields or functionality. </span><span class="koboSpan" id="kobo.232.3">Consider an object used by the preceding messaging service that returns a </span><strong class="source-inline"><span class="koboSpan" id="kobo.233.1">name</span></strong><span class="koboSpan" id="kobo.234.1"> field from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.235.1">/user</span></strong><span class="koboSpan" id="kobo.236.1"> endpoint. </span><span class="koboSpan" id="kobo.236.2">Our data contract specifies that this field is the first name of the person, for example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.237.1">Alice</span></strong><span class="koboSpan" id="kobo.238.1">. </span><span class="koboSpan" id="kobo.238.2">The messaging service might also want to provide a salutation, for example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.239.1">Ms. </span><span class="koboSpan" id="kobo.239.2">Alice</span></strong><span class="koboSpan" id="kobo.240.1">. </span><span class="koboSpan" id="kobo.240.2">With no changes to the messaging service, we could change the semantic meaning of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.241.1">/user</span></strong><span class="koboSpan" id="kobo.242.1"> endpoint data contract so that </span><em class="italic"><span class="koboSpan" id="kobo.243.1">name</span></em><span class="koboSpan" id="kobo.244.1"> now means </span><em class="italic"><span class="koboSpan" id="kobo.245.1">salutation plus name</span></em><span class="koboSpan" id="kobo.246.1">. </span><span class="koboSpan" id="kobo.246.2">However, this might have unexpected effects on other consumers of the service. </span><span class="koboSpan" id="kobo.246.3">Let’s say</span><a id="_idIndexMarker1300"/><span class="koboSpan" id="kobo.247.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.248.1">backend for frontend</span></strong><span class="koboSpan" id="kobo.249.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.250.1">BFF</span></strong><span class="koboSpan" id="kobo.251.1">) service gets information about multiple users and sorts their names alphabetically. </span><span class="koboSpan" id="kobo.251.2">Now, we sort by salutation instead of name. </span><span class="koboSpan" id="kobo.251.3">We have unintentionally modified behavior by changing the </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">semantic meaning.</span></span></p>
<p><span class="koboSpan" id="kobo.253.1">This contrived example</span><a id="_idIndexMarker1301"/><span class="koboSpan" id="kobo.254.1"> may seem easy to avoid; however, even simple changes to data contracts can have unintended consequences. </span><span class="koboSpan" id="kobo.254.2">There are two options here: either we change the data contract and deal with the fallout (usually hard to predict, discover, and rectify), or we extend our data contract. </span><span class="koboSpan" id="kobo.254.3">When we extend our data contract, we rely on services not involved in the change to ignore the extensions. </span><span class="koboSpan" id="kobo.254.4">For example, rather than changing the semantic meaning of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.255.1">name</span></strong><span class="koboSpan" id="kobo.256.1"> field, we add a new field called </span><strong class="source-inline"><span class="koboSpan" id="kobo.257.1">salutation</span></strong><span class="koboSpan" id="kobo.258.1">. </span><span class="koboSpan" id="kobo.258.2">The messaging service can consume this field to provide the required functionality, and the BFF service can continue using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.259.1">name</span></strong><span class="koboSpan" id="kobo.260.1"> field as expected, ignoring the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.261.1">salutation</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.262.1"> field.</span></span></p>
<p><span class="koboSpan" id="kobo.263.1">If we really must change the underlying semantics of the data contract, then we can still follow our principle of not modifying the behavior expected by other systems. </span><span class="koboSpan" id="kobo.263.2">This may seem counter-intuitive. </span><span class="koboSpan" id="kobo.263.3">However, by utilizing API versioning, we can fundamentally change the structure and semantics of our data contract by adding a v2 of our API. </span><span class="koboSpan" id="kobo.263.4">This preserves the data contract between our old systems while allowing us to make considerable changes to support new functionality. </span><span class="koboSpan" id="kobo.263.5">We can retroactively update the dependent services by aligning them with the new data contract by utilizing contract testing. </span><span class="koboSpan" id="kobo.263.6">Eventually deprecating the original endpoint without any material impact, we have essentially decoupled the modification of data contracts from the adoption of the new data contracts, which, in turn, changes a highly synchronous deployment exercise and likely downtime into </span><a id="_idIndexMarker1302"/><span class="koboSpan" id="kobo.264.1">an asynchronous process that can be undertaken as the business </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">needs arise.</span></span></p>
<h2 id="_idParaDest-349"><a id="_idTextAnchor349"/><span class="koboSpan" id="kobo.266.1">Contract enforcement</span></h2>
<p><span class="koboSpan" id="kobo.267.1">It’s all good to define </span><a id="_idIndexMarker1303"/><span class="koboSpan" id="kobo.268.1">the data contract we use between services, but the next stage is contract enforcement. </span><span class="koboSpan" id="kobo.268.2">It is not enough to define the contracts that our services communicate in. </span><span class="koboSpan" id="kobo.268.3">Ideally, at both ends, we should check that the data we transfer aligns with our understanding of the contract. </span><span class="koboSpan" id="kobo.268.4">An important aspect here is to validate what we know and discard what we don’t; this leaves us the option of contract expansion, as we discussed earlier. </span><span class="koboSpan" id="kobo.268.5">Contract validation at runtime can save us from unexpected data behaviors and alert us to mismatches </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">between contracts.</span></span></p>
<p><span class="koboSpan" id="kobo.270.1">A good practice here is to complement our contract testing with fuzzing, injecting corrupted or invalid data to ensure our application rejects it. </span><span class="koboSpan" id="kobo.270.2">In the cloud environment, rejecting the wrong data is just as important as accepting the </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">right data!</span></span></p>
<p><span class="koboSpan" id="kobo.272.1">To provide a good user experience, enforcing our data contract at the application layer is often useful before sending it to our services. </span><span class="koboSpan" id="kobo.272.2">Not only does this provide faster feedback to the users but every error we catch in the application is a request we don’t need to serve, reducing the load on the underlying resources. </span><span class="koboSpan" id="kobo.272.3">The cheapest computer you can use is usually at the closest edge to </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">the user.</span></span></p>
<p><span class="koboSpan" id="kobo.274.1">On the flip side, though, we want to validate our data when we receive it for both correctness and security purposes. </span><span class="koboSpan" id="kobo.274.2">Anyone could send anything they want to our endpoints, and it is our responsibility to work out what to do with it. </span><span class="koboSpan" id="kobo.274.3">If we enforce contracts on both the backend and frontend, though, we require our data contract to </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">be portable.</span></span></p>
<h2 id="_idParaDest-350"><a id="_idTextAnchor350"/><span class="koboSpan" id="kobo.276.1">Portability</span></h2>
<p><span class="koboSpan" id="kobo.277.1">In these scenarios, it should go </span><a id="_idIndexMarker1304"/><span class="koboSpan" id="kobo.278.1">without saying that the format of your data contracts should aim to be as technology-agnostic as possible. </span><span class="koboSpan" id="kobo.278.2">Framework- and language-specific libraries often have valuable features. </span><span class="koboSpan" id="kobo.278.3">However, locking us into a framework can make it challenging to operate across technologies. </span><span class="koboSpan" id="kobo.278.4">In like-for-like execution environments, say a frontend in React and a backend in Node.js, both run JavaScript under the hood, so it might be tempting to use a specialized solution. </span><span class="koboSpan" id="kobo.278.5">However, what if your company acquires a product with a code base in C#? </span><span class="koboSpan" id="kobo.278.6">How will they access contracts and ensure data integrity? </span><span class="koboSpan" id="kobo.278.7">Hence, the requirements for portability, which are a feature of all formats mentioned earlier in the chapter, should always be at the forefront of </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">the mind.</span></span></p>
<p><span class="koboSpan" id="kobo.280.1">A mature standard (if you are using JSON, which feels like the de facto cloud native standard, except for perhaps Protobuf in GCP!) is JSON Schema. </span><span class="koboSpan" id="kobo.280.2">It is maintained through the </span><strong class="bold"><span class="koboSpan" id="kobo.281.1">Internet Engineering Task Force</span></strong><span class="koboSpan" id="kobo.282.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.283.1">ITEF</span></strong><span class="koboSpan" id="kobo.284.1">), and any precursory web search will reveal them as the </span><a id="_idIndexMarker1305"/><span class="koboSpan" id="kobo.285.1">stewards of many standards we take for granted today. </span><span class="koboSpan" id="kobo.285.2">You can typically find very mature libraries to generate, validate, and test JSON schemas in the language and framework of your choice. </span><span class="koboSpan" id="kobo.285.3">It also allows for clear delineation between the data schema to test against (JSON Schema) and the interface definition through a standard such as OpenAPI or AsyncAPI. </span><span class="koboSpan" id="kobo.285.4">If the schema is the definition of the data, the interface definition is the metastructure that defines the relationships between our schemas and </span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">service endpoints.</span></span></p>
<h2 id="_idParaDest-351"><a id="_idTextAnchor351"/><span class="koboSpan" id="kobo.287.1">Code generation</span></h2>
<p><span class="koboSpan" id="kobo.288.1">If we have both </span><a id="_idIndexMarker1306"/><span class="koboSpan" id="kobo.289.1">our schemas and our interface definitions predefined, then there exist multiple open source projects that allow for this information to be used to generate code. </span><span class="koboSpan" id="kobo.289.2">Typically, this code generation consists of three </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">discrete components:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.291.1">Type generation</span></strong><span class="koboSpan" id="kobo.292.1">: Generating</span><a id="_idIndexMarker1307"/><span class="koboSpan" id="kobo.293.1"> types from our schemas for consumption in our code. </span><span class="koboSpan" id="kobo.293.2">This generation is typically a prerequisite for the other </span><span class="No-Break"><span class="koboSpan" id="kobo.294.1">two types.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.295.1">Client generation</span></strong><span class="koboSpan" id="kobo.296.1">: From our</span><a id="_idIndexMarker1308"/><span class="koboSpan" id="kobo.297.1"> interface definitions and our generated types, we can automatically build SDKs to interact with our services, without having to worry about needing to make API requests, marshal and unmarshal data, and </span><span class="No-Break"><span class="koboSpan" id="kobo.298.1">so on.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.299.1">Server stub generation</span></strong><span class="koboSpan" id="kobo.300.1">: From our interface definition, we can generate server stubs that allow us to conform to our interface definition, only requiring us to build out the</span><a id="_idIndexMarker1309"/> <span class="No-Break"><span class="koboSpan" id="kobo.301.1">business logic.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.302.1">When we look at the big three cloud providers, they use this methodology to maintain the SDKs that they provide for such a wide range of languages. </span><span class="koboSpan" id="kobo.302.2">AWS uses the Smithy IDL 2.0, which was custom-made for defining interfaces and code generation for AWS but is open source. </span><span class="koboSpan" id="kobo.302.3">Azure uses OpenAPI specifications, which we have discussed in depth already. </span><span class="koboSpan" id="kobo.302.4">Finally, GCP uses Protobuf definitions for all its services, which can encode in both JSON or a custom and compact binary format. </span><span class="koboSpan" id="kobo.302.5">By using code generation, they can make a change to the underlying contract and apply it across all their subsequent client SDKs by </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">regenerating them.</span></span></p>
<p><span class="koboSpan" id="kobo.304.1">So, contract testing ensures we don’t break functionality and semantics for upstream services and ensures we have confidence in calling our downstream services. </span><span class="koboSpan" id="kobo.304.2">But how do we ensure continuity</span><a id="_idIndexMarker1310"/><span class="koboSpan" id="kobo.305.1"> in our user interface? </span><span class="koboSpan" id="kobo.305.2">This is where an anti-pattern is so prevalent that it deserves its own section: </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">manual testing.</span></span></p>
<h1 id="_idParaDest-352"><a id="_idTextAnchor352"/><span class="koboSpan" id="kobo.307.1">Manual testing</span></h1>
<p><span class="koboSpan" id="kobo.308.1">When beginning this </span><a id="_idIndexMarker1311"/><span class="koboSpan" id="kobo.309.1">section, a quote of disputed origin springs to mind: “</span><em class="italic"><span class="koboSpan" id="kobo.310.1">I didn’t have time to write you a short letter, so I wrote you a long one.</span></em><span class="koboSpan" id="kobo.311.1">” As counter-intuitive as this may seem, people often have the same mentality about manual testing. </span><span class="koboSpan" id="kobo.311.2">They are so caught up in the process of testing the long way that they do not pause to consider the possibilities of automation. </span><span class="koboSpan" id="kobo.311.3">This anti-pattern is typically heavily ingrained in organizations right down to the team structure. </span><span class="koboSpan" id="kobo.311.4">This section will look at the case for transitioning to test automation in a cloud native environment and the practices you can use to migrate your manual testing processes to </span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">automated tests.</span></span></p>
<h2 id="_idParaDest-353"><a id="_idTextAnchor353"/><span class="koboSpan" id="kobo.313.1">Typical company testing archetypes</span></h2>
<p><span class="koboSpan" id="kobo.314.1">Usually, companies</span><a id="_idIndexMarker1312"/><span class="koboSpan" id="kobo.315.1"> are convinced that unit testing will provide tangible benefits and agree that these can be automated. </span><span class="koboSpan" id="kobo.315.2">If you are a company that manually performs unit testing, your engineers must have </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">unlimited patience.</span></span></p>
<p><span class="koboSpan" id="kobo.317.1">Integration tests form the middle ground, and companies approach this differently. </span><span class="koboSpan" id="kobo.317.2">Some companies believe that integration tests are optional if they write enough unit tests (more on that in the next section). </span><span class="koboSpan" id="kobo.317.3">Some companies have some integration tests, but they don’t form part of the deployment pipeline or are only run manually once in a </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">blue moon.</span></span></p>
<p><span class="koboSpan" id="kobo.319.1">Finally, we have the companies that have integration tests, have them automated, and they form part of the deployment pipeline. </span><span class="koboSpan" id="kobo.319.2">There are other approaches/levels of maturity, but these are some common integration testing archetypes we see. </span><span class="koboSpan" id="kobo.319.3">At the final tier, we have our end-to-end tests, which may be automated and form part of the deployment process; if this is the case in your company, this section may be preaching to the choir. </span><span class="koboSpan" id="kobo.319.4">However, these tests are much more likely to exist in the form of a dedicated QA function, clicking through user interfaces, following steps in a spreadsheet or document, and then reporting back on the result, either pre- </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">or post-deployment.</span></span></p>
<p><span class="koboSpan" id="kobo.321.1">So, at the crux, we are looking at three separate kinds </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">of tests:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.323.1">Unit tests</span></strong><span class="koboSpan" id="kobo.324.1">: Testing </span><a id="_idIndexMarker1313"/><span class="koboSpan" id="kobo.325.1">atomic units of functionality within a single service </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">or component</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.327.1">Integration tests</span></strong><span class="koboSpan" id="kobo.328.1">: Testing the</span><a id="_idIndexMarker1314"/><span class="koboSpan" id="kobo.329.1"> interactions </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">between components</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.331.1">End-to-end tests</span></strong><span class="koboSpan" id="kobo.332.1">: Testing the</span><a id="_idIndexMarker1315"/><span class="koboSpan" id="kobo.333.1"> system’s functionality from the end </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">user’s context</span></span></li>
</ul>
<h2 id="_idParaDest-354"><a id="_idTextAnchor354"/><span class="koboSpan" id="kobo.335.1">The case for test automation</span></h2>
<p><span class="koboSpan" id="kobo.336.1">With these three</span><a id="_idIndexMarker1316"/><span class="koboSpan" id="kobo.337.1"> forms of test in mind, I would also like to call back to the top of your working memory the </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">DORA metrics:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.339.1">Deployment frequency</span></span></li>
<li><span class="koboSpan" id="kobo.340.1">Lead time </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">for changes</span></span></li>
<li><span class="koboSpan" id="kobo.342.1">Change </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">failure rate</span></span></li>
<li><span class="koboSpan" id="kobo.344.1">Time to </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">restore service</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.346.1">Tests involve optimizing one metric: </span><em class="italic"><span class="koboSpan" id="kobo.347.1">change failure rate</span></em><span class="koboSpan" id="kobo.348.1">. </span><span class="koboSpan" id="kobo.348.2">The more testing we do before we deploy a change, the lower our change failure rate. </span><span class="koboSpan" id="kobo.348.3">Note that this eliminates an entire swath of the testing archetypes we discussed earlier in </span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">this subsection.</span></span></p>
<p><span class="koboSpan" id="kobo.350.1">If your testing does not occur on your deployment path, you are not protecting your change failure rate! </span><span class="koboSpan" id="kobo.350.2">You might have a faster time to restore service as you may uncover errors or their source earlier with post-deployment tests, but this is an entirely different area of expertise (see </span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">Chapter 10</span></span><span class="koboSpan" id="kobo.352.1"> for observing your deployed architecture). </span><span class="koboSpan" id="kobo.352.2">So, we have established the requirement that for tests to have a meaningful impact on the performance of your software teams, they need to be on the critical path for deployment </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">to production.</span></span></p>
<p><span class="koboSpan" id="kobo.354.1">When we have manual processes, we end up batching together our changes so that they can keep up with the pace of change in our code bases. </span><span class="koboSpan" id="kobo.354.2">This protects the change failure rate. </span><span class="koboSpan" id="kobo.354.3">However, in reality, batching changes together increases our change failure rate because the chances of any of the changes we have batched together negatively impacting the application significantly increased compared to if we deploy those </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">changes individually.</span></span></p>
<p><span class="koboSpan" id="kobo.356.1">Let’s say 5 of our changes fail if we deploy 100 changes individually. </span><span class="koboSpan" id="kobo.356.2">Then, we have a 5% change failure rate. </span><span class="koboSpan" id="kobo.356.3">If we deploy batches of 10 changes 10 times, we might get lucky, and those 5 failures across those 100 changes are all batched into 1 segment, but that’s still a 10% change failure rate. </span><span class="koboSpan" id="kobo.356.4">More than likely, those 5 failures spread throughout those 10 segments, and now, up to half of those segments fail, resulting in a change failure rate of up to 50%. </span><span class="koboSpan" id="kobo.356.5">If we just do one significant change, then what ends up happening is every change</span><a id="_idIndexMarker1317"/><span class="koboSpan" id="kobo.357.1"> has a failure. </span><span class="koboSpan" id="kobo.357.2">It’s just a matter of magnitude, so batching things together, even though tests are on our critical path, can still cause issues with our change </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">failure rate.</span></span></p>
<p><span class="koboSpan" id="kobo.359.1">So, we have established that batches are bad for our change failure rate. </span><span class="koboSpan" id="kobo.359.2">Let’s now look at our other metrics: our deployment frequency and lead time for changes. </span><span class="koboSpan" id="kobo.359.3">Both of these functions depend on our total pipeline time. </span><span class="koboSpan" id="kobo.359.4">Introducing manual stages into our pipeline significantly increases the time it takes to complete. </span><span class="koboSpan" id="kobo.359.5">Longer pipeline cycle times mean developers are less likely to deploy small incremental changes; instead, they are more likely to batch together changes, leading to the same problem we discussed before batching together changes for testing. </span><span class="koboSpan" id="kobo.359.6">This impacts our </span><span class="No-Break"><span class="koboSpan" id="kobo.360.1">deployment frequency.</span></span></p>
<p><span class="koboSpan" id="kobo.361.1">Our other metric, lead time for changes, is a function of all the linear steps that must occur before a change Is deployed to production. </span><span class="koboSpan" id="kobo.361.2">By increasing the pipeline time, even if we kept our changes atomic and deployed frequently, the lead time for changes would still be more significant because one of its components takes a long time to complete. </span><span class="koboSpan" id="kobo.361.3">So, manual testing is destructive for our change failure rate and affects our other metrics, lead time for changes, and deployment frequency. </span><span class="koboSpan" id="kobo.361.4">We discussed earlier on in the book that introducing stages on the deployment path that have long cycle times or increase the times that deployment also means that we are unlikely to perform the same checks when the service is heavily impacted, so changes that are hotfixes or are intended to be fixes for urgent issues in production tend not to be as rigorously tested as of the code that initially caused the problem in the </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">first place.</span></span></p>
<p><span class="koboSpan" id="kobo.363.1">So, if we follow our process to the letter, we will see that we negatively impact our time to restore services as well. </span><span class="koboSpan" id="kobo.363.2">We can improve our time to restore service only through workarounds and avenues outside of our standard operating procedures. </span><span class="koboSpan" id="kobo.363.3">This negates any benefit that might be achieved through the earlier detection of issues through testing production or outside the critical </span><span class="No-Break"><span class="koboSpan" id="kobo.364.1">deployment path.</span></span></p>
<p><span class="koboSpan" id="kobo.365.1">As soon as we introduce humans into our process, we introduce variability. </span><span class="koboSpan" id="kobo.365.2">Humans are very good at taking the unknown, applying their knowledge and heuristics, and solving problems they have not encountered before. </span><span class="koboSpan" id="kobo.365.3">Testing is the exact opposite of this process. </span><span class="koboSpan" id="kobo.365.4">We know the issues we want to test for and how to test for them. </span><span class="koboSpan" id="kobo.365.5">Therefore, humans are poorly suited to the task of manual testing. </span><span class="koboSpan" id="kobo.365.6">We can accelerate this process significantly through automation. </span><span class="koboSpan" id="kobo.365.7">As soon as we take humans out of the equation and introduce automated over manual processes, the function of how much testing we can perform does not become a question of </span><em class="italic"><span class="koboSpan" id="kobo.366.1">human</span></em><span class="koboSpan" id="kobo.367.1"> resources but of </span><em class="italic"><span class="koboSpan" id="kobo.368.1">compute</span></em><span class="koboSpan" id="kobo.369.1"> resources. </span><span class="koboSpan" id="kobo.369.2">With the advent of the cloud, on-demand compute resources can quickly be provisioned and deprovisioned as needed to perform testing. </span><span class="koboSpan" id="kobo.369.3">This process accelerates our feedback cycle, allowing us not only to have certainty that the changes we are applying will not cause failures but also to have all of our developers empowered to perform adequate testing on all of the code they push into a </span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">production environment.</span></span></p>
<p><span class="koboSpan" id="kobo.371.1">Now, this may sound</span><a id="_idIndexMarker1318"/><span class="koboSpan" id="kobo.372.1"> like humans don’t add value to the testing process in any way; however, I would like to postulate that humans add unique value in how they can define and envision test suites rather than the execution of those test suites. </span><span class="koboSpan" id="kobo.372.2">The definition and creation of test suites is a unique skill; they are variable and nuanced, and humans are great at that task. </span><span class="koboSpan" id="kobo.372.3">A great joke goes like this: a developer walks into a bar and orders 1 beer; a tester walks into a bar and orders 1 beer, 10,000 beers, negative 1 beers, a sofa, and so on. </span><span class="koboSpan" id="kobo.372.4">Still, the part of testing that we value is the creative side, understanding the problem space, and coming up with unique edge cases to ensure consistency in behavior. </span><span class="koboSpan" id="kobo.372.5">The actual execution of these tests is something that testers are wasted on. </span><span class="koboSpan" id="kobo.372.6">This section won’t tell you to make your entire testing team redundant. </span><span class="koboSpan" id="kobo.372.7">This section tells you to put your testing team to the best use possible</span><a id="_idIndexMarker1319"/><span class="koboSpan" id="kobo.373.1"> by allowing them to exercise </span><span class="No-Break"><span class="koboSpan" id="kobo.374.1">their creativity.</span></span></p>
<h2 id="_idParaDest-355"><a id="_idTextAnchor355"/><span class="koboSpan" id="kobo.375.1">Migrating manual testing processes</span></h2>
<p><span class="koboSpan" id="kobo.376.1">As discussed, manual testing </span><a id="_idIndexMarker1320"/><span class="koboSpan" id="kobo.377.1">processes typically exist in the end-to-end space. </span><span class="koboSpan" id="kobo.377.2">The migration process for manual integration tests puts them on the critical path, as they likely already exist as code-driven tests. </span><span class="koboSpan" id="kobo.377.3">If they don’t, then the integration tests can be created using the existing skill set of your development teams. </span><span class="koboSpan" id="kobo.377.4">Manual end-to-end tests, on the other hand, can seem like a much more daunting task to migrate. </span><span class="koboSpan" id="kobo.377.5">Our testing function may not have coding skills. </span><span class="koboSpan" id="kobo.377.6">However, that does not mean we must revamp our entire testing department. </span><span class="koboSpan" id="kobo.377.7">Instead, we can perform three </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">key actions:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.379.1">Lean on our </span><span class="No-Break"><span class="koboSpan" id="kobo.380.1">development function</span></span></li>
<li><span class="koboSpan" id="kobo.381.1">Utilize tooling to accelerate </span><span class="No-Break"><span class="koboSpan" id="kobo.382.1">the migration</span></span></li>
<li><span class="koboSpan" id="kobo.383.1">Upskill our </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">testing teams</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.385.1">As I said before, humans can deal with variability. </span><span class="koboSpan" id="kobo.385.2">Our development function may have exploited this not maliciously but inadvertently by relying on visual cues to the tester performing the manual testing. </span><span class="koboSpan" id="kobo.385.3">When we migrate to automated testing, typically, we must depend on properties in our user interface that are invisible to the tester but visible to our testing framework. </span><span class="koboSpan" id="kobo.385.4">For example, when we change a button in our interface to a hyperlink but keep the same styling, the tester is unlikely to register a change. </span><span class="koboSpan" id="kobo.385.5">Still, this is a significant change for an automated test suite looking for a </span><span class="No-Break"><span class="koboSpan" id="kobo.386.1">button element.</span></span></p>
<p><span class="koboSpan" id="kobo.387.1">Therefore, our development function needs to improve its working methods to ensure that the artifacts it produces are testable. </span><span class="koboSpan" id="kobo.387.2">In the web world, this may look like leveraging ARIA labels to provide meaning to specific elements. </span><span class="koboSpan" id="kobo.387.3">In this way, a hyperlink and a button that share an ARIA label can be treated similarly. </span><span class="koboSpan" id="kobo.387.4">Regarding aria labels, not only will your testers thank you for making your UI more testable but suitable aria labels also make your site more accessible. </span><span class="koboSpan" id="kobo.387.5">Hence, it’s something you should be doing anyway. </span><span class="koboSpan" id="kobo.387.6">Our development function is already likely well versed in adding tests to the pipeline to production. </span><span class="koboSpan" id="kobo.387.7">So, we can lean on our development teams to help integrate this new test suite into the path to production, removing the requirement for this capability within our </span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">testing teams.</span></span></p>
<p><span class="koboSpan" id="kobo.389.1">We still need help writing the tests. </span><span class="koboSpan" id="kobo.389.2">However, it’s unlikely that our development teams will want to go through all of the documentation produced in the past by a manual testing team and convert them into automated tests. </span><span class="koboSpan" id="kobo.389.3">This is also not future-proof; any new test we want to add will depend on the development team. </span><span class="koboSpan" id="kobo.389.4">This is where we can utilize tooling to accelerate the migration. </span><span class="koboSpan" id="kobo.389.5">Many testing suites we would use for end-to-end testing include functionality allowing us to record tests directly from the browser. </span><span class="koboSpan" id="kobo.389.6">Using this functionality, we can do one last manual run of our tests, record them, and then save them for use in our automated </span><span class="No-Break"><span class="koboSpan" id="kobo.390.1">testing framework.</span></span></p>
<p><span class="koboSpan" id="kobo.391.1">Our source of truth is no longer copious pieces of documentation but codified tests with no ambiguity. </span><span class="koboSpan" id="kobo.391.2">This process gets us significantly closer to automated end-to-end testing without involving the development team. </span><span class="koboSpan" id="kobo.391.3">For this initial migration, interfacing with the development team may be beneficial in getting the project off the ground. </span><span class="koboSpan" id="kobo.391.4">However, in the long run, the testing team must complete this </span><span class="No-Break"><span class="koboSpan" id="kobo.392.1">process autonomously.</span></span></p>
<p><span class="koboSpan" id="kobo.393.1">We must upskill our testing teams in the framework that we use for creating tests. </span><span class="koboSpan" id="kobo.393.2">This does not mean that every tester needs to become a developer. </span><span class="koboSpan" id="kobo.393.3">However, every tester needs the capability to define, record, and integrate tests into the test suite autonomously. </span><span class="koboSpan" id="kobo.393.4">This process is a much smaller ask, but utilizing tooling and leaning on our development function prevents us from needing to change the structure of our teams. </span><span class="koboSpan" id="kobo.393.5">The one case in which I recommend changing the structure of your teams is to shift toward the structure we mentioned earlier in the book that allows teams to </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">be self-sufficient.</span></span></p>
<p><span class="koboSpan" id="kobo.395.1">If your testing function is a standalone unit of your business, consider integrating them into your delivery teams to enable them to be fully autonomous. </span><span class="koboSpan" id="kobo.395.2">Not only will this break down the adversarial nature between a standalone testing function and a development function but it will also allow end-to-end ownership of the delivery of the team’s outcomes. </span><span class="koboSpan" id="kobo.395.3">This closer</span><a id="_idIndexMarker1321"/><span class="koboSpan" id="kobo.396.1"> alignment means that testers can lean upon the development resources within their teams as they upskill to become </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">fully self-sufficient.</span></span></p>
<h1 id="_idParaDest-356"><a id="_idTextAnchor356"/><span class="koboSpan" id="kobo.398.1">Trying to recreate the cloud</span></h1>
<p><span class="koboSpan" id="kobo.399.1">In the previous </span><a id="_idIndexMarker1322"/><span class="koboSpan" id="kobo.400.1">section, we discussed the overuse of unit tests to compensate for the lack of integration tests. </span><span class="koboSpan" id="kobo.400.2">Good coding practices drive good testing. </span><span class="koboSpan" id="kobo.400.3">Our business logic, the part of our code that drives value, should be unit-tested. </span><span class="koboSpan" id="kobo.400.4">However, unit testing for this part of our code should not involve extensive mocking of the environment in which it runs. </span><span class="koboSpan" id="kobo.400.5">The anti-pattern we typically see in this space is that people try to recreate the cloud on their local environment through third-party tooling, extensive mocking, or some </span><span class="No-Break"><span class="koboSpan" id="kobo.401.1">other method.</span></span></p>
<p><span class="koboSpan" id="kobo.402.1">To dissect this anti-pattern, we will look at the traditional testing paradigm, what testing looks like in a cloud native world, and how we can best leverage cloud services to test our code. </span><span class="koboSpan" id="kobo.402.2">Previously, we focused on end-to-end, contract, and unit tests, so it should be no surprise that this</span><a id="_idIndexMarker1323"/><span class="koboSpan" id="kobo.403.1"> section will focus heavily on </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">integration tests.</span></span></p>
<h2 id="_idParaDest-357"><a id="_idTextAnchor357"/><span class="koboSpan" id="kobo.405.1">The traditional testing paradigm</span></h2>
<p><span class="koboSpan" id="kobo.406.1">The traditional </span><a id="_idIndexMarker1324"/><span class="koboSpan" id="kobo.407.1">testing paradigm typically consists of a large</span><a id="_idIndexMarker1325"/><span class="koboSpan" id="kobo.408.1"> number of unit tests because they’re cheap, a few integration tests because they’re a little bit harder to write and a little bit harder to run, and just a couple of end-to-end tests because, as discussed previously, this is often a manual function. </span><span class="koboSpan" id="kobo.408.2">This typically gives us a pattern referred to as the </span><span class="No-Break"><span class="koboSpan" id="kobo.409.1">testing pyramid.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer078">
<span class="koboSpan" id="kobo.410.1"><img alt="" role="presentation" src="image/B22364_13_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.411.1">Figure 13.2 - The testing pyramid</span></p>
<p><span class="koboSpan" id="kobo.412.1">In the initial premise for this section, I mentioned that our unit test should focus on testing the parts of our code that are unique to our business: our business logic. </span><span class="koboSpan" id="kobo.412.2">In the cloud world, resources are cheap, and much of the complexity that used to live inside our application can now be farmed out to the cloud service provider itself. </span><span class="koboSpan" id="kobo.412.3">This presents an interesting problem: if our logic is pushed out to the cloud service provider, less and less of our functionality becomes testable through unit tests. </span><span class="koboSpan" id="kobo.412.4">Typically, we see developers start relying on extensive mocking in this scenario. </span><span class="koboSpan" id="kobo.412.5">It’s not uncommon to enter a code base at a client and see eight or more cloud services mocked out to test a piece of business logic. </span><span class="koboSpan" id="kobo.412.6">Third-party tools have also sprung up and promise to provide cloud-like functionality inside your test pipelines or </span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">local environment.</span></span></p>
<p><span class="koboSpan" id="kobo.414.1">If we continue in our traditional mindset of unit tests first, then these all look like attractive propositions. </span><span class="koboSpan" id="kobo.414.2">When we look at the testing pyramid, it may feel that resorting to an integration test is a failure on behalf of the developer: “</span><em class="italic"><span class="koboSpan" id="kobo.415.1">I wasn’t good enough to write a unit test for this.</span></em><span class="koboSpan" id="kobo.416.1">” We may feel that integration tests are reserved explicitly for very complex cross-service behaviors, but this leads us to integrated test territory, not integration test territory. </span><span class="koboSpan" id="kobo.416.2">Much like the producers of a popular nature documentary, we want to observe</span><a id="_idIndexMarker1326"/><span class="koboSpan" id="kobo.417.1"> the</span><a id="_idIndexMarker1327"/><span class="koboSpan" id="kobo.418.1"> behavior of our system in its natural habitat. </span><span class="koboSpan" id="kobo.418.2">In our case, its natural habitat just happens to be </span><span class="No-Break"><span class="koboSpan" id="kobo.419.1">the cloud.</span></span></p>
<h2 id="_idParaDest-358"><a id="_idTextAnchor358"/><span class="koboSpan" id="kobo.420.1">The testing honeycomb</span></h2>
<p><em class="italic"><span class="koboSpan" id="kobo.421.1">Spotify R&amp;D</span></em><span class="koboSpan" id="kobo.422.1"> published </span><a id="_idIndexMarker1328"/><span class="koboSpan" id="kobo.423.1">an excellent article in 2018 examining the testing </span><a id="_idIndexMarker1329"/><span class="koboSpan" id="kobo.424.1">honeycomb (</span><a href="https://engineering.atspotify.com/2018/01/testing-of-microservices/"><span class="koboSpan" id="kobo.425.1">https://engineering.atspotify.com/2018/01/testing-of-microservices/</span></a><span class="koboSpan" id="kobo.426.1">). </span><span class="koboSpan" id="kobo.426.2">In this honeycomb, we remove our overdependence on unit tests as the base level of testing and rely instead on integration or service tests. </span><span class="koboSpan" id="kobo.426.3">Spotify specifically talks about the removal of integrated tests, which are tests that span multiple services. </span><span class="koboSpan" id="kobo.426.4">However, we believe that end-to-end tests can still produce value even if they span numerous services. </span><span class="koboSpan" id="kobo.426.5">They should not be taken as an indication of an individual service’s health but as an overall system health check </span><span class="No-Break"><span class="koboSpan" id="kobo.427.1">before deployment.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer079">
<span class="koboSpan" id="kobo.428.1"><img alt="" role="presentation" src="image/B22364_13_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.429.1">Figure 13.3 - The testing honeycomb</span></p>
<p><span class="koboSpan" id="kobo.430.1">Using integration tests, we more accurately represent the real-world deployed environment than in unit tests. </span><span class="koboSpan" id="kobo.430.2">Instead of testing against a simulacrum of the cloud, we deploy our services to the cloud and then test them in their natural habitat. </span><span class="koboSpan" id="kobo.430.3">This was fine in the traditional model, where a large amount of our functionality existed within the context of </span><span class="No-Break"><span class="koboSpan" id="kobo.431.1">our application.</span></span></p>
<p><span class="koboSpan" id="kobo.432.1">However, as we have said, more of the common parts of our application are being outsourced to managed services in the cloud. </span><span class="koboSpan" id="kobo.432.2">Therefore, it can be easy to produce tight coupling</span><a id="_idIndexMarker1330"/><span class="koboSpan" id="kobo.433.1"> between</span><a id="_idIndexMarker1331"/><span class="koboSpan" id="kobo.434.1"> cloud services and the logic we want to test. </span><span class="koboSpan" id="kobo.434.2">In the next section, we will go into more detail on structuring our code, but for now, let’s focus on </span><span class="No-Break"><span class="koboSpan" id="kobo.435.1">integration testing.</span></span></p>
<h2 id="_idParaDest-359"><a id="_idTextAnchor359"/><span class="koboSpan" id="kobo.436.1">Testing in the cloud versus testing for the cloud</span></h2>
<p><span class="koboSpan" id="kobo.437.1">Earlier in this </span><a id="_idIndexMarker1332"/><span class="koboSpan" id="kobo.438.1">book, we discussed development in ephemeral environments. </span><span class="koboSpan" id="kobo.438.2">The same concept can be used in our testing pipeline. </span><span class="koboSpan" id="kobo.438.3">Using the structure of the testing honeycomb, we have many integration tests that specify how our application interacts with the cloud environment. </span><span class="koboSpan" id="kobo.438.4">These tests can be run in a temporary cloud environment. </span><span class="koboSpan" id="kobo.438.5">This allows us to test our code in the cloud, using actual cloud services rather than mocking them. </span><span class="koboSpan" id="kobo.438.6">When we mock out services in the cloud, we are testing our code against our mental model of the cloud. </span><span class="koboSpan" id="kobo.438.7">When we use actual cloud services, there is no transitive mental model that our code needs to pass through to </span><span class="No-Break"><span class="koboSpan" id="kobo.439.1">be tested.</span></span></p>
<p><span class="koboSpan" id="kobo.440.1">There are some core concepts that we need to have implemented to be able to test our code in </span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">ephemeral environments:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.442.1">We must have </span><a id="_idIndexMarker1333"/><span class="koboSpan" id="kobo.443.1">solid </span><strong class="bold"><span class="koboSpan" id="kobo.444.1">infrastructure as code</span></strong><span class="koboSpan" id="kobo.445.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.446.1">IaC</span></strong><span class="koboSpan" id="kobo.447.1">) foundations to spin up and tear down environments </span><span class="No-Break"><span class="koboSpan" id="kobo.448.1">as required</span></span></li>
<li><span class="koboSpan" id="kobo.449.1">We need to understand which parts of our infrastructure take longer to provision and supply pre-provisioned resources for testing purposes to keep cycle </span><span class="No-Break"><span class="koboSpan" id="kobo.450.1">times low</span></span></li>
<li><span class="koboSpan" id="kobo.451.1">Our testing pipeline must have access to a </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">cloud environment</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.453.1">When discussing solid IaC foundations, we mean following good practices when implementing IaC. </span><span class="koboSpan" id="kobo.453.2">To test our applications effectively, we need to pull up just the part of our infrastructure required for testing instead of our entire application. </span><span class="koboSpan" id="kobo.453.3">Typically, we need firm domain boundaries between different application areas to test our system effectively with the cloud in isolation from other application components. </span><span class="koboSpan" id="kobo.453.4">For more information on providing firm boundaries between application components and strong cohesion within application components, we recommend reviewing the </span><em class="italic"><span class="koboSpan" id="kobo.454.1">Tight coupling, low </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.455.1">cohesion</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.456.1"> section.</span></span></p>
<p><span class="koboSpan" id="kobo.457.1">The other interesting </span><a id="_idIndexMarker1334"/><span class="koboSpan" id="kobo.458.1">part of IaC that is typically exposed through this practice is the solidification and codification of specific IaC properties. </span><span class="koboSpan" id="kobo.458.2">When we need to deploy multiple copies of our application to run tests, sometimes numerous copies simultaneously, we can quickly highlight any areas of our infrastructure that have solidified around a single deployment. </span><span class="koboSpan" id="kobo.458.3">Hence, testing this way can also highlight gaps in our resiliency plan and ability to bring up new </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">application instances.</span></span></p>
<p><span class="koboSpan" id="kobo.460.1">Some parts of IaC configurations can be provisioned very quickly. </span><span class="koboSpan" id="kobo.460.2">Things such as serverless functions or API gateways can be provisioned in minimal time. </span><span class="koboSpan" id="kobo.460.3">On the other hand, more traditional resources such as relational database instances or virtual machines may require more time to be created. </span><span class="koboSpan" id="kobo.460.4">Typically, we can use common resources between our test environments and partition them by namespaces or any other supported partitioning method. </span><span class="koboSpan" id="kobo.460.5">For example, suppose we had a relational database service. </span><span class="koboSpan" id="kobo.460.6">In that case, each test environment might use the same database instance, which takes a long time to provision. </span><span class="koboSpan" id="kobo.460.7">However, create a separate database within that instance to perform its test and then delete it upon completion. </span><span class="koboSpan" id="kobo.460.8">An in-memory key store might use a single instance with keys prefixed with namespaces unique to the test suite execution. </span><span class="koboSpan" id="kobo.460.9">This process ensures that we keep our cycle times low and provide fast feedback to our developers while also allowing us to maintain a high deployment frequency and low lead time </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">for changes.</span></span></p>
<p><span class="koboSpan" id="kobo.462.1">Fundamental to all of this is that our testing environment needs to be a real cloud environment. </span><span class="koboSpan" id="kobo.462.2">This requirement might mean linking our testing pipeline with cloud credentials, infrastructure pipelines, and CI/CD processes. </span><span class="koboSpan" id="kobo.462.3">This increases complexity; however, the benefit is increased certainty in our deployments. </span><span class="koboSpan" id="kobo.462.4">Applying the same best cloud practices described elsewhere in this book to the cloud environment used for testing is also essential. </span><span class="koboSpan" id="kobo.462.5">We can still apply the practices of good cloud governance, FinOps, DevSecOps, and platform engineering to make this cloud environment a first-class citizen in our cloud estate. </span><span class="koboSpan" id="kobo.462.6">By practicing good hygiene in this cloud environment, we not only make it easier for the developers who need to run tests in this environment but also gain</span><a id="_idIndexMarker1335"/><span class="koboSpan" id="kobo.463.1"> increased certainty in the tests we run, avoiding the issues of flaky pipelines, long pipeline runtimes, and long lead times </span><span class="No-Break"><span class="koboSpan" id="kobo.464.1">for changes.</span></span></p>
<h2 id="_idParaDest-360"><a id="_idTextAnchor360"/><span class="koboSpan" id="kobo.465.1">Testing non-functional requirements</span></h2>
<p><span class="koboSpan" id="kobo.466.1">Now that we are testing</span><a id="_idIndexMarker1336"/><span class="koboSpan" id="kobo.467.1"> in a real cloud environment and have mature integration tests, we can also test for properties that were previously unfeasible. </span><span class="koboSpan" id="kobo.467.2">Some of the key properties that are great to test for in this space include </span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.469.1">Latency</span></strong><span class="koboSpan" id="kobo.470.1">: This ensures our requests are completed in a reasonable amount </span><span class="No-Break"><span class="koboSpan" id="kobo.471.1">of time</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.472.1">Consistency</span></strong><span class="koboSpan" id="kobo.473.1">: Many cloud systems operate on the principle of eventual consistency, but we might have non-functional requirements regarding time </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">to consistency</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.475.1">Scalability</span></strong><span class="koboSpan" id="kobo.476.1">: We might want to perform load testing to ensure that our services can handle the expected </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">traffic shapes</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.478.1">Resilience</span></strong><span class="koboSpan" id="kobo.479.1">: Assuming we have resiliency strategies, we will want to test them based on the reasons we discussed earlier in </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">the book</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.481.1">At this point, you need to apply your judgment. </span><span class="koboSpan" id="kobo.481.2">Previously, we talked about testing needing to be on the critical path to be useful. </span><span class="koboSpan" id="kobo.481.3">Testing non-functional requirements is not always feasible to perform on the critical path and often deals with slowly changing properties of our application. </span><span class="koboSpan" id="kobo.481.4">Therefore, running this sort of testing on a schedule can occasionally be better due to its complex nature. </span><span class="koboSpan" id="kobo.481.5">Typically, these tests are used to test for regression from previous executions. </span><span class="koboSpan" id="kobo.481.6">We can also apply the same rigor of checking for regressions of non-functional requirements on our </span><span class="No-Break"><span class="koboSpan" id="kobo.482.1">other tests.</span></span></p>
<p><span class="koboSpan" id="kobo.483.1">We can certainly check test execution times for regressions on the critical path. </span><span class="koboSpan" id="kobo.483.2">In a recent case, a manually discovered regression uncovered a vulnerability in XZ, a popular compression utility. </span><span class="koboSpan" id="kobo.483.3">A developer noticed regressions in SSH execution times, which, in the subsequent investigation, revealed a complex multi-year-long plot to backdoor the utility. </span><span class="koboSpan" id="kobo.483.4">The full story sounds like the plot of a spy movie and is worth additional research by any </span><span class="No-Break"><span class="koboSpan" id="kobo.484.1">interested readers.</span></span></p>
<p><span class="koboSpan" id="kobo.485.1">Even though these</span><a id="_idIndexMarker1337"/><span class="koboSpan" id="kobo.486.1"> were manually discovered regressions, had they not been found, they could have had potentially catastrophic effects for many projects built on </span><span class="No-Break"><span class="koboSpan" id="kobo.487.1">these tools.</span></span></p>
<h1 id="_idParaDest-361"><a id="_idTextAnchor361"/><span class="koboSpan" id="kobo.488.1">Poorly structured code</span></h1>
<p><span class="koboSpan" id="kobo.489.1">One of the key anti-patterns </span><a id="_idIndexMarker1338"/><span class="koboSpan" id="kobo.490.1">we see in writing cloud native software is a false equivalency between 100% code coverage and code quality. </span><span class="koboSpan" id="kobo.490.2">It’s important to remember that high code quality and good coding practices should naturally result in sufficient code coverage to guarantee the behavior we want to test. </span><span class="koboSpan" id="kobo.490.3">As professionals, we must ensure that we adhere to these practices. </span><span class="koboSpan" id="kobo.490.4">One of the main impediments to writing good tests is poorly structured code, or, to put it another way, low-quality code. </span><span class="koboSpan" id="kobo.490.5">Therefore, in this section, we will explore some common anti-patterns that can arise when writing cloud native software and how that impacts our ability </span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">to test.</span></span></p>
<h2 id="_idParaDest-362"><a id="_idTextAnchor362"/><span class="koboSpan" id="kobo.492.1">Key terms</span></h2>
<p><span class="koboSpan" id="kobo.493.1">Before we discuss code structure, we</span><a id="_idIndexMarker1339"/><span class="koboSpan" id="kobo.494.1"> need to define some key terms to understand the topic </span><span class="No-Break"><span class="koboSpan" id="kobo.495.1">at hand:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.496.1">Business logic</span></strong><span class="koboSpan" id="kobo.497.1"> is anything our application does that transforms the information between our user and the persistence layer. </span><span class="koboSpan" id="kobo.497.2">Business logic might consist of evaluating custom rules to determine whether a customer is eligible for a product or assigning inventory to a new order that has just entered a purchasing system. </span><span class="koboSpan" id="kobo.497.3">Fundamentally, business logic is the part of our application that presents our unique business proposition. </span><span class="koboSpan" id="kobo.497.4">If we connect the user directly to the persistence layer, are we adding any value for the customer? </span><span class="koboSpan" id="kobo.497.5">Other non-business logic areas of the company still derive value by providing things such as a good user experience, reliability, and fulfillment. </span><span class="koboSpan" id="kobo.497.6">But, in a software sense, the codifying and repeatability of processes through business logic is usually one of the core elements through which we </span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">derive value.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.499.1">Side effects</span></strong><span class="koboSpan" id="kobo.500.1"> are anything our application does that affects other parts of the system and relies on behavior outside the defined function. </span><span class="koboSpan" id="kobo.500.2">For example, a side effect might be creating a new record in the database or sending a notification to a user’s phone. </span><span class="koboSpan" id="kobo.500.3">Anything that our function does other than returning a value based on its arguments is a side effect. </span><span class="koboSpan" id="kobo.500.4">Side effects are not inherently wrong. </span><span class="koboSpan" id="kobo.500.5">Instead, they are an essential part of our application, allowing us to perform actions such as persistence, evolution, </span><span class="No-Break"><span class="koboSpan" id="kobo.501.1">and eventing.</span></span></li>
</ul>
<h2 id="_idParaDest-363"><a id="_idTextAnchor363"/><span class="koboSpan" id="kobo.502.1">The monolith</span></h2>
<p><span class="koboSpan" id="kobo.503.1">Just because we escaped </span><a id="_idIndexMarker1340"/><span class="koboSpan" id="kobo.504.1">the monolithic application </span><a id="_idIndexMarker1341"/><span class="koboSpan" id="kobo.505.1">through microservices or serverless functions does not mean we’ve escaped the conceptual idea of the monolith within our code. </span><span class="koboSpan" id="kobo.505.2">I defined the previous two terms because they represent two significant but very different actions an application must perform. </span><span class="koboSpan" id="kobo.505.3">The critical difference is that a pure function can typically represent our business logic. </span><span class="koboSpan" id="kobo.505.4">This function has no side effects and relies solely on its arguments to produce a return value. </span><span class="koboSpan" id="kobo.505.5">To maintain the results of this function, we must rely on side effects to communicate with other parts of our system, such as </span><span class="No-Break"><span class="koboSpan" id="kobo.506.1">our database.</span></span></p>
<p><span class="koboSpan" id="kobo.507.1">This is where we can once again fall into the monolithic trap. </span><span class="koboSpan" id="kobo.507.2">It can be tempting to intersperse our business logic with side effects as we require them. </span><span class="koboSpan" id="kobo.507.3">This makes sense from a logical perspective, and from structuring our code, we add effects as we need them where we need them. </span><span class="koboSpan" id="kobo.507.4">However, this leads us down the path of high coupling and low cohesion, which we had previously in the monolithic structure. </span><span class="koboSpan" id="kobo.507.5">Instead, what we should look to do is separate our concerns from our business logic. </span><span class="koboSpan" id="kobo.507.6">The rules that define how we operate should be written as pure functions. </span><span class="koboSpan" id="kobo.507.7">They shouldn’t have any side effects, making our company’s unique value proposition </span><span class="No-Break"><span class="koboSpan" id="kobo.508.1">directly testable.</span></span></p>
<p><span class="koboSpan" id="kobo.509.1">When we start introducing side effects directly alongside our business logic, we suddenly run into the requirement to provide mocking that mimics these side effects simply to test the rules by which we run our business. </span><span class="koboSpan" id="kobo.509.2">This can turn the practice of testing our business logic from a 10-minute exercise testing a pure function into a multi-hour exercise where most of our time is spent setting up the environment to run our tests by mocking out the side effects. </span><span class="koboSpan" id="kobo.509.3">Recalling the testing honeycomb from the previous section, we can test our side effects through a different type of test. </span><span class="koboSpan" id="kobo.509.4">In that case, we should use integration tests and test our code in the cloud rather than extensive mocking and unit tests. </span><span class="koboSpan" id="kobo.509.5">The logical extension of this is writing our business logic as a pure function and testing only our business logic to ensure correctness against our business rules and expectations. </span><span class="koboSpan" id="kobo.509.6">Then, when we want to test our system’s side effects, we can begin integration testing against the </span><span class="No-Break"><span class="koboSpan" id="kobo.510.1">deployed service.</span></span></p>
<p><span class="koboSpan" id="kobo.511.1">So, now we’ve managed to separate the concerns of our business logic from the side effects required to make it useful. </span><span class="koboSpan" id="kobo.511.2">A lot of functional glue still binds our business logic with our side effects. </span><span class="koboSpan" id="kobo.511.3">While this could be tested through integration testing, other alternatives allow</span><a id="_idIndexMarker1342"/><span class="koboSpan" id="kobo.512.1"> us to increase our code coverage </span><a id="_idIndexMarker1343"/><span class="koboSpan" id="kobo.513.1">without replicating the cloud in our unit tests. </span><span class="koboSpan" id="kobo.513.2">This is advantageous because unit tests have lower complexity, faster execution, and faster feedback cycles than </span><span class="No-Break"><span class="koboSpan" id="kobo.514.1">integration tests.</span></span></p>
<h2 id="_idParaDest-364"><a id="_idTextAnchor364"/><span class="koboSpan" id="kobo.515.1">Hexagonal architecture</span></h2>
<p><span class="koboSpan" id="kobo.516.1">In 2005, Alistair </span><a id="_idIndexMarker1344"/><span class="koboSpan" id="kobo.517.1">Cockburn </span><a id="_idIndexMarker1345"/><span class="koboSpan" id="kobo.518.1">introduced the concept of hexagonal architecture. </span><span class="koboSpan" id="kobo.518.2">Broadly speaking, hexagonal architecture provides a methodology for decoupling the implementation of our side effects from their usage. </span><span class="koboSpan" id="kobo.518.3">I’ll provide a diagram for hexagonal architecture and then we can go into it in </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">more detail.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer080">
<span class="koboSpan" id="kobo.520.1"><img alt="" role="presentation" src="image/B22364_13_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.521.1">Figure 13.4 - Conceptual diagram of the hexagonal architecture model</span></p>
<p><span class="koboSpan" id="kobo.522.1">At the core of our application, we have our application code that glues our side effects and business logic together; this bundle is our entity. </span><span class="koboSpan" id="kobo.522.2">The side effects are exposed through standard interfaces </span><a id="_idIndexMarker1346"/><span class="koboSpan" id="kobo.523.1">referred to as </span><strong class="bold"><span class="koboSpan" id="kobo.524.1">ports</span></strong><span class="koboSpan" id="kobo.525.1">. </span><span class="koboSpan" id="kobo.525.2">For example, we might have a persistence port or a notification port. </span><span class="koboSpan" id="kobo.525.3">What’s important is that the entity is agnostic of the implementation of these ports. </span><span class="koboSpan" id="kobo.525.4">All it knows is the interface by which this functionality is exposed. </span><span class="koboSpan" id="kobo.525.5">Adapters implement these interfaces or ports. </span><span class="koboSpan" id="kobo.525.6">The adapter contains all the knowledge to interact with the external system. </span><span class="koboSpan" id="kobo.525.7">For example, our database port may connect to an adapter that provides a database through a PostgreSQL-compatible service. </span><span class="koboSpan" id="kobo.525.8">Our entity is unaware of Postgres; it could be DynamoDB, SQL Server, MySQL, or any other database engine. </span><span class="koboSpan" id="kobo.525.9">What’s important is that it exposes the functionality expected by the entity and defined in the port. </span><span class="koboSpan" id="kobo.525.10">Likewise, our notification port could use SMS email push notifications or carrier pigeons; it doesn’t matter to </span><span class="No-Break"><span class="koboSpan" id="kobo.526.1">the entity.</span></span></p>
<p><span class="koboSpan" id="kobo.527.1">Similarly, we have </span><a id="_idIndexMarker1347"/><span class="koboSpan" id="kobo.528.1">ports driven by external adapters </span><a id="_idIndexMarker1348"/><span class="koboSpan" id="kobo.529.1">for incoming traffic to our entity. </span><span class="koboSpan" id="kobo.529.2">Whether our entity is triggered by an event from an event queue or by a direct HTTP request, we have ports that represent the interface of the request and then adapters that connect those ports to our entity. </span><span class="koboSpan" id="kobo.529.3">This is a crucial distinction: we have driving ports, external forces that act upon our entity, and driven ports, which our entity uses to act on </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">external systems.</span></span></p>
<p><span class="koboSpan" id="kobo.531.1">This might seem unrelated to testing; however, one of the key benefits of this architecture pattern is that it makes our entities, our application code, agnostic of where it’s being run. </span><span class="koboSpan" id="kobo.531.2">The complexity of actually interacting with actual services is hidden away in the adapters. </span><span class="koboSpan" id="kobo.531.3">Mocking our side effects becomes much easier through the simplified interface presented through our ports, as we can produce a new adapter that implements the expected behavior rather than trying to mock out cloud native services. </span><span class="koboSpan" id="kobo.531.4">This also prevents us from tying our unit testing and application code to specific libraries or SDKs, as all of that is taken care of in our adapters and will eventually be tested through our </span><span class="No-Break"><span class="koboSpan" id="kobo.532.1">integration tests.</span></span></p>
<p><span class="koboSpan" id="kobo.533.1">So, here, we not only get a benefit in the testability of our code but we also gain portability of our code if we need to change an integration with an external system; it is a simple matter of writing a new adapter that agrees with the interface for the existing port. </span><span class="koboSpan" id="kobo.533.2">This negates one of the key arguments against writing cloud native software: it will cause vendor lock-in. </span><span class="koboSpan" id="kobo.533.3">By utilizing hexagonal architecture, we can ensure the code we are writing is agnostic </span><a id="_idIndexMarker1349"/><span class="koboSpan" id="kobo.534.1">of where it’s being run, increasing the</span><a id="_idIndexMarker1350"/><span class="koboSpan" id="kobo.535.1"> portion of our code base that will be utilized if we decide to migrate </span><span class="No-Break"><span class="koboSpan" id="kobo.536.1">cloud providers.</span></span></p>
<h2 id="_idParaDest-365"><a id="_idTextAnchor365"/><span class="koboSpan" id="kobo.537.1">Structuring your code correctly from day one</span></h2>
<p><span class="koboSpan" id="kobo.538.1">We have covered test-driven</span><a id="_idIndexMarker1351"/><span class="koboSpan" id="kobo.539.1"> development in a few sections of this chapter, but I want to discuss it in a different context. </span><span class="koboSpan" id="kobo.539.2">When we talk about structuring our code to be testable and about good structure in general, TDD can help us achieve this outcome. </span><span class="koboSpan" id="kobo.539.3">If the first thing we write in our code base for new functionality is a test, then, by default, the code we write to fulfill this test will be </span><span class="No-Break"><span class="koboSpan" id="kobo.540.1">testable implicitly.</span></span></p>
<p><span class="koboSpan" id="kobo.541.1">I will use Java to paint a picture of testable versus untestable code, as it has some insidious anti-patterns. </span><span class="koboSpan" id="kobo.541.2">Let’s assume we’re testing some business logic, and we have a class that contains everything we need for our feature to run. </span><span class="koboSpan" id="kobo.541.3">We might be tempted to implement our business logic as a private method in this class to call it from within our application logic that is exposed to the outside world as a public method. </span><span class="koboSpan" id="kobo.541.4">If we’re already following some of the practices in this section, we might also mark our private business logic method as static to indicate that it doesn’t rely on this class’s </span><span class="No-Break"><span class="koboSpan" id="kobo.542.1">internal state.</span></span></p>
<p><span class="koboSpan" id="kobo.543.1">Now, it comes time to test our code; of course, the main function we want to test is our business logic to ensure that the business rules we are solidifying in the code are correctly implemented. </span><span class="koboSpan" id="kobo.543.2">However, due to the structure of our class, this is one of the least testable parts of our code because it’s private and only exposed to our </span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">class’s internals.</span></span></p>
<p><span class="koboSpan" id="kobo.545.1">What can happen in this scenario is that the developer can be tempted to do one of </span><span class="No-Break"><span class="koboSpan" id="kobo.546.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.547.1">Make the </span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">method public</span></span></li>
<li><span class="koboSpan" id="kobo.549.1">Test the application code in a way that tests all bounds of </span><span class="No-Break"><span class="koboSpan" id="kobo.550.1">business logic</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.551.1">The first method is not preferable because we’re changing the visibility of class internals specifically for testing purposes. </span><span class="koboSpan" id="kobo.551.2">Other people relying on this business logic may call it directly from this class, which is not its primary purpose, violating the single </span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">responsibility principle.</span></span></p>
<p><span class="koboSpan" id="kobo.553.1">The second is not preferable because we are testing the code through a proxy, which makes the test brittle to application changes. </span><span class="koboSpan" id="kobo.553.2">It also causes us more work on the testing side as we have to mock out everything required for the application code </span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">to run.</span></span></p>
<p><span class="koboSpan" id="kobo.555.1">Now, consider if we had written a test that expected a method that would implement our business logic. </span><span class="koboSpan" id="kobo.555.2">What might our code look like in this scenario? </span><span class="koboSpan" id="kobo.555.3">We’re free from the constraints of the application so it’s unlikely that we would try to test it through the application code. </span><span class="koboSpan" id="kobo.555.4">We could make a public method, but it’s also likely our application code doesn’t exist yet because we want to refine the business logic. </span><span class="koboSpan" id="kobo.555.5">So, rather than add it to the class for the application code, we instead produce a static class that solely implements our business logic, is directly testable, has a single responsibility, and is consumable within our </span><span class="No-Break"><span class="koboSpan" id="kobo.556.1">application code.</span></span></p>
<p><span class="koboSpan" id="kobo.557.1">Therefore, TDD is not only</span><a id="_idIndexMarker1352"/><span class="koboSpan" id="kobo.558.1"> a tool for writing productive tests but also for helping drive well-structured code. </span><span class="koboSpan" id="kobo.558.2">This doesn’t mean you need to write every test before starting to write code, just that you define the core behavior that you want to achieve </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">in advance.</span></span></p>
<h1 id="_idParaDest-366"><a id="_idTextAnchor366"/><span class="koboSpan" id="kobo.560.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.561.1">Testing is one of the greatest tools we have in the cloud native toolbox. </span><span class="koboSpan" id="kobo.561.2">It prevents regressions, ensures compatibility, and allows us to have more confidence that the behavior of our system closely matches the behavior of our mental model. </span><span class="koboSpan" id="kobo.561.3">Hopefully, you have picked up some tips on how to build meaningful tests without blowing your development timelines. </span><span class="koboSpan" id="kobo.561.4">Good testing practices are critical to scaling cloud native applications, and by avoiding the anti-patterns in this chapter, you will be well on your way to deploying quickly and with confidence. </span><span class="koboSpan" id="kobo.561.5">We have covered a lot so far. </span><span class="koboSpan" id="kobo.561.6">Next up, we will look at how to get started on your cloud </span><span class="No-Break"><span class="koboSpan" id="kobo.562.1">native journey.</span></span></p>
</div>
</body></html>