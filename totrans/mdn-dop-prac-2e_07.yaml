- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Containers as a Service (CaaS) and Serverless Computing for Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last two chapters, we covered Kubernetes and how it helps manage your
    containers seamlessly. Now, let’s look at other ways of automating and managing
    container deployments—**Containers as a Service** (**CaaS**) and **serverless
    computing for containers**. CaaS provides container-based virtualization that
    abstracts away all management behind the scenes and helps you manage your containers
    without worrying about the underlying infrastructure and orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: For simple deployments and less complex applications, CaaS can be a savior.
    Serverless computing is a broad term that encompasses applications that can be
    run without us having to worry about the infrastructure behind the scenes. It
    has an additional benefit that you can focus purely on the application. We will
    discuss CaaS technologies such as **Amazon Elastic Container Service** (**Amazon
    ECS**) with **Amazon Web Services Fargate** (**AWS Fargate**) in detail and briefly
    discuss other cloud-based CaaS offerings such as **Azure Kubernetes Services**
    (**AKS**), **Google Kubernetes Engine** (**GKE**), and **Google Cloud Run**. We
    will then delve into the popular open source serverless CaaS solution known as
    **Knative**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The need for serverless offerings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon ECS with **Elastic Compute Cloud** (**EC2**) and Fargate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other CaaS services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open source CaaS with Knative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need an active AWS subscription for this chapter’s exercises. AWS is
    the market’s most popular, feature-rich cloud platform. Currently, AWS is offering
    a free tier for some products. You can sign up for this at [https://aws.amazon.com/free](https://aws.amazon.com/free).
    This chapter uses some paid services, but we will try to minimize how many we
    use as much as possible during the exercises.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will also need to clone the following GitHub repository for some of the
    exercises:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Modern-DevOps-Practices-2e](https://github.com/PacktPublishing/Modern-DevOps-Practices-2e%0D)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to clone the repository into your home directory.
    Then, `cd` into the `ch7` directory to access the required resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As the repository contains files with placeholder strings, you must replace
    the `<your_dockerhub_user>` string with your actual Docker Hub user. Use the following
    commands to substitute the placeholders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: So, let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: The need for serverless offerings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Numerous organizations, so far, have been focusing a lot on infrastructure provisioning
    and management. They optimize the number of resources, machines, and infrastructure
    surrounding the applications they build. However, they should focus on what they
    do best—software development. Unless your organization wants to invest heavily
    in an expensive infrastructure team to do a lot of heavy lifting behind the scenes,
    you’d be better off concentrating on writing and building quality applications
    rather than focusing on where and how to run and optimize them.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless offerings come as a reprieve for this problem. Instead of concentrating
    on how to host your infrastructure to run your applications, you can declare what
    you want to run, and the serverless offering manages it for you. This has become
    a boon for small enterprises that do not have the budget to invest heavily in
    infrastructure and want to get started quickly without wasting too much time standing
    up and maintaining infrastructure to run applications.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless offerings also offer automatic placement and scaling for container
    and application workloads. You can spin from 0 to 100 instances in minutes, if
    not seconds. The best part is that you pay for what you use in some services rather
    than what you allocate.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will concentrate on a very popular AWS container management offering
    called **ECS** and AWS’s container serverless offering, **AWS Fargate**. We will
    then briefly examine offerings from other cloud platforms and, finally, the open
    source container-based serverless solution known as **Knative**.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s go ahead and look at Amazon ECS.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon ECS with EC2 and Fargate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon ECS is a container orchestration platform that AWS offers. It is simple
    to use and manage, uses Docker behind the scenes, and can deploy your workloads
    to **Amazon EC2**, a **virtual machine** (**VM**)-based solution, or **AWS Fargate**,
    a serverless offering.
  prefs: []
  type: TYPE_NORMAL
- en: It is a highly scalable solution that deploys containers in seconds. It makes
    hosting, running, stopping, and starting your containers easy. Just as Kubernetes
    offers **pods**, ECS offers **tasks** that help you run your container workloads.
    A task can contain one or more containers grouped according to a logical relationship.
    You can also group one or more tasks into **services**. Services are similar to
    Kubernetes controllers, which manage tasks and can ensure that the required number
    of replicas of your tasks are running in the right place at the right time. ECS
    uses simple API calls to provide many functionalities, such as creating, updating,
    reading, and deleting tasks and services.
  prefs: []
  type: TYPE_NORMAL
- en: ECS also allows you to place your containers according to multiple placement
    strategies while keeping **high availability** (**HA**) and resource optimization
    in mind. You can tweak the placement algorithm according to your priority—cost,
    availability, or a mix of both. So, you can use ECS to run one-time batch workloads
    or long-running microservices, all using a simple-to-use API interface.
  prefs: []
  type: TYPE_NORMAL
- en: ECS architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we explore the ECS architecture, it is important to understand some
    common AWS terminologies to follow it. Let’s look at some AWS resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '`us-east-1`, `us-west-1`, `ap-southeast-1`, `eu-central-1`, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`us-east-1a`, `us-east-1b`, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS virtual private cloud (VPC)**: An AWS VPC is an isolated network resource
    you create within AWS. You associate a dedicated private IP address range to it
    from which the rest of your resources, such as EC2 instances, can derive their
    IP addresses. An AWS VPC spans an AWS Region.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Subnet**: A subnet, as the name suggests, is a subnetwork within the VPC.
    You must subdivide the IP address ranges you provided to the VPC and associate
    them with subnets. Resources normally reside within subnets, and each subnet spans
    an AZ.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Route table**: An AWS route table routes traffic within the VPC subnets and
    to the internet. Every AWS subnet is associated with a route table through **subnet
    route** **table associations**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Internet gateways**: An internet gateway allows connection to and from the
    internet to your AWS subnets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Identity Access Management (IAM)**: AWS IAM helps you control access to resources
    by users and other AWS resources. They help you implement **role-based access
    control** (**RBAC**) and the **principle of least** **privilege** (**PoLP**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon EC2**: EC2 allows you to spin up VMs within subnets, also known as
    instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Auto Scaling groups (ASGs)**: An AWS ASG works with Amazon EC2 to provide
    HA and scalability to your instances. It monitors your EC2 instances and ensures
    that a defined number of healthy instances are always running. It also takes care
    of autoscaling your instances with increasing load in your machines to allow for
    handling more traffic. It uses the **instance profile** and **launch configuration**
    to decide on the properties of new EC2 instances it spins up.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon CloudWatch**: Amazon CloudWatch is a monitoring and observability
    service. It allows you to collect, track, and monitor metrics, log files, and
    set alarms to take automated actions on specific conditions. CloudWatch helps
    understand application performance, health, and resource utilization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ECS is a cloud-based regional service. When you spin up an ECS cluster, the
    instances span multiple AZs, where you can schedule your tasks and services using
    simple manifests. ECS manifests are very similar to `docker-compose` YAML manifests,
    where we specify which tasks to run and which tasks comprise a service.
  prefs: []
  type: TYPE_NORMAL
- en: You can run ECS within an existing VPC. We can schedule tasks in either Amazon
    EC2 or AWS Fargate.
  prefs: []
  type: TYPE_NORMAL
- en: Your ECS cluster can have one or more EC2 instances attached to it. You also
    have the option to attach an existing EC2 instance to a cluster by installing
    the ECS node agent within your EC2 instance. The agent sends information about
    your containers’ state and tasks to the ECS scheduler. It then interacts with
    the container runtime to schedule containers within the node. They are similar
    to `kubelet` in the Kubernetes ecosystem. If you run your containers within EC2
    instances, you pay for the number of EC2 instances you allocate to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: If you plan to use Fargate, the infrastructure is wholly abstracted from you,
    and you must specify the amount of CPU and memory your container is set to consume.
    You pay for the CPU and memory your container consumes rather than the resources
    you allocate to the machines.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Although you only pay for the resources you consume in Fargate, it is more expensive
    than running your tasks on EC2, especially when running long-running services
    such as a web server. A rule of thumb is to run long-running online tasks within
    EC2 and batch tasks with Fargate. That will give you the best cost optimization.
  prefs: []
  type: TYPE_NORMAL
- en: When we schedule a task, AWS spins up the container on a managed EC2 or Fargate
    server by pulling the required container image from a **container registry**.
    Every **task** has an **elastic network interface** (**ENI**) attached to it.
    Multiple tasks are grouped as a **service**, and the service ensures that all
    the required tasks run at once.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon ECS uses a **task scheduler** to schedule containers on your cluster.
    It places your containers in an appropriate node of your cluster based on placement
    logic, availability, and cost requirements. The scheduler also ensures that the
    desired number of tasks run on the node at a given time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram explains the ECS cluster architecture beautifully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – ECS architecture](img/B19877_07_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – ECS architecture
  prefs: []
  type: TYPE_NORMAL
- en: Amazon provides the ECS **command-line interface** (**CLI**) for interacting
    with the ECS cluster. It is a simple command-line tool that you can use to administer
    an ECS cluster and create and manage tasks and services on the ECS cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s go ahead and install the ECS CLI.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the AWS and ECS CLIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The AWS CLI is available as a `deb` package within the public `apt` repositories.
    To install it, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Installing the ECS CLI in the Linux ecosystem is simple. We need to download
    the binary and move to the system path using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following command to check whether `ecs-cli` has been installed correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, `ecs-cli` has been successfully installed on our system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to allow `ecs-cli` to connect with your AWS API. You need
    to export your AWS CLI environment variables for this. Run the following commands
    to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Once we’ve set the environment variables, `ecs-cli` will use them to authenticate
    with the AWS API. In the next section, we’ll spin up an ECS cluster using the
    ECS CLI.
  prefs: []
  type: TYPE_NORMAL
- en: Spinning up an ECS cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use the ECS CLI commands to spin up an ECS cluster. You can run your
    containers in EC2 and Fargate, so first, we will create a cluster that runs EC2
    instances. Then, we will add Fargate tasks within the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'To connect with your EC2 instances, you need to generate a key pair within
    AWS. To do so, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The output of this command will provide the key pair in a JSON file. Extract
    the JSON file’s key material and save that in a separate file called `ecs-keypair.pem`.
    Remember to replace the `\n` characters with a new line when you save the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we’ve generated the key pair, we can use the following command to create
    an ECS cluster using the ECS CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: When we issue this command, in the background, AWS spins up a stack of resources
    using CloudFormation. CloudFormation is AWS’s **Infrastructure-as-Code** (**IaC**)
    solution that helps you deploy infrastructure on AWS through reusable templates.
    The CloudFormation template consists of several resources such as a VPC, a security
    group, a subnet within the VPC, a route table, a route, a subnet route table association,
    an internet gateway, an IAM role, an instance profile, a launch configuration,
    an ASG, a VPC gateway attachment, and the cluster itself. The ASG contains two
    EC2 instances running and serving the cluster. Keep a copy of the output; we will
    need the details later during the exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Now that our cluster is up, we will spin up our first task.
  prefs: []
  type: TYPE_NORMAL
- en: Creating task definitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ECS tasks are similar to Kubernetes pods. They are the basic building blocks
    of ECS and comprise one or more related containers. Task definitions are the blueprints
    for ECS tasks and define what the ECS task should look like. They are very similar
    to `docker-compose` files and are written in YAML format. ECS also uses all versions
    of `docker-compose` to allow us to define tasks. They help you define containers
    and their images, resource requirements, where they should run (EC2 or Fargate),
    volume and port mappings, and other networking requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Using the `docker-compose` manifest to spin up tasks and services is a great
    idea, as it will help you align your configuration with an open standard.
  prefs: []
  type: TYPE_NORMAL
- en: A task is a finite process and only runs once. Even if it’s a long-running process,
    such as a web server, the task still runs once as it waits for the long-running
    process to end (which runs indefinitely in theory). The task’s life cycle follows
    the **Pending** -> **Running** -> **Stopped** states. So, when you schedule your
    task, the task enters the **Pending** state, attempting to pull the image from
    the container registry. Then, it tries to start the container. Once the container
    has started, it enters the **Running** state. When the container has completed
    executing or errored out, it ends up in the **Stopped** state. A container with
    startup errors directly transitions from the **Pending** state to the **Stopped**
    state.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s go ahead and deploy an `nginx` web server task within the ECS cluster
    we just created.
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the resources for this section, `cd` into the following directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll use `docker-compose` task definitions here. So, let’s start by defining
    the following `docker-compose.yml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The YAML file defines a `web` container with an `nginx` image with host port
    `80` mapped to container port `80`. It uses the `awslogs` logging driver, which
    streams logs into Amazon CloudWatch. It will stream the logs to the `/aws/webserver`
    log group in the `us-east-1` region with the `ecs` stream prefix.
  prefs: []
  type: TYPE_NORMAL
- en: 'The task definition also includes the resource definition—that is, the amount
    of resources we want to reserve for our task. Therefore, we will have to define
    the following `ecs-params.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This YAML file defines `cpu_shares` in millicores and `mem_limit` in bytes for
    the container we plan to fire. Now, let’s look at scheduling this task as an EC2
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling EC2 tasks on ECS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s use `ecs-cli` to apply the configuration and schedule our task using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the task has been scheduled and the container is running, let’s list
    all the tasks to get the container’s details and find out where it is running.
    To do so, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the web container is running on `cluster-1` on `34.237.218.7:80`.
    Now, use the following command to curl this endpoint to see what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here, we get the default `nginx` home page! We’ve successfully scheduled a container
    on ECS using the EC2 launch type. You might want to duplicate this task to handle
    more traffic. This is known as horizontal scaling. We’ll see how in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can easily scale tasks using `ecs-cli`. Use the following command to scale
    the tasks to `2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, use the following command to check whether two containers are running
    on the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, two containers are running on the cluster. Now, let’s query CloudWatch
    to get the logs of the containers.
  prefs: []
  type: TYPE_NORMAL
- en: Querying container logs from CloudWatch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To query logs from CloudWatch, we must list the log streams using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, there are two log streams for this—one for each task. `logStreamName`
    follows the convention `<log_stream_prefix>/<task_name>/<task_id>`. So, to get
    the logs for `ecs/b43bdec7/web`, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here, you will see a stream of logs in JSON format in the response. Now, let’s
    look at how we can stop running tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Stopping tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`ecs-cli` uses the friendly `docker-compose` syntax for everything. Use the
    following command to stop the tasks in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s list the containers to see whether the tasks have stopped by using the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, both containers have stopped.
  prefs: []
  type: TYPE_NORMAL
- en: Running tasks on EC2 is not a serverless way of doing things. You still have
    to provision and manage the EC2 instances, and although ECS manages workloads
    on the cluster, you still have to pay for the amount of resources you’ve provisioned
    in the form of EC2 instances. AWS offers Fargate as a serverless solution where
    you pay per resource consumption. Let’s look at how we can create the same task
    as a Fargate task.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling Fargate tasks on ECS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scheduling tasks on Fargate is very similar to EC2\. Here, we need to specify
    the `launch-type` value as `FARGATE`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To schedule the same task on Fargate, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Oops! We have a problem! Well, it’s complaining about the network type. For
    a Fargate task, we must supply the network type as `awsvpc` instead of the default
    bridge network. The `awsvpc` network is an overlay network that implements the
    `awsvpc` network type. But before that, the Fargate task requires a few configurations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the resources for this section, `cd` into the following directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: First, we’ll have to assume a task execution role for the ECS agent to authenticate
    with the AWS API and interact with Fargate.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, create the following `task-execution-assume-role.json` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, use the following command to assume the task execution role:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'ECS provides a default role policy called `AmazonECSTaskExecutionRolePolicy`,
    which contains various permissions that help you interact with CloudWatch and
    **Elastic Container Registry** (**ECR**). The following JSON code outlines the
    permission that the policy has:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We have to assign this role policy to the `ecsTaskExecution` role we assumed
    previously by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we’ve assigned the policy to the `ecsTaskExecution` role, we need to source
    the ID of both subnets and the security group of the ECS cluster when we created
    it. You can find those details in the command-line output from when we created
    the cluster. We will use these details in the following `ecs-params.yml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `ecs-params.yml` file consists of `task_execution_role`, which we created,
    and `ecs_network_mode` set to `awsvpc`, as Fargate requires. We’ve defined `task_size`
    to have `0.5GB` of memory and `256` millicores of CPU. So, since Fargate is a
    serverless solution, we only pay for the CPU cores and memory we consume. The
    `run_params` section consists of `network_configuration`, which contains `awsvpc_configuration`.
    Here, we specify both subnets created when we created the ECS cluster. We must
    also specify `security_groups`, which we created with the ECS cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Use the subnets and security groups of your ECS cluster instead of copying the
    ones in this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’re ready to fire the task on Fargate, let’s run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s check whether the task is running successfully by using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the task is running on `3.80.173.230:80` as a Fargate task.
    Let’s `curl` this URL to see whether we get a response by using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, we get the default `nginx` home page.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s go ahead and delete the task we created by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As we already know, tasks have a set life cycle, and once they stop, they stop.
    You cannot start the same task again. Therefore, we must create a **service**
    to ensure that a certain number of tasks are always running. We’ll create a service
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling services on ECS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`ecs-cli` command line.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Always use services for applications that are long-running, such as web servers.
    For batch jobs, always use tasks, as we don’t want to recreate the job after it
    ends.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the `nginx` web server as a service, we can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the logs, we can see that the service is trying to ensure that the
    task’s desired count matches the task’s running count. If your task is deleted,
    ECS will replace it with a new one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s list the tasks and see what we get by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the service has created a new task that is running on `18.234.123.71:80`.
    Let’s try to access this URL using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We get the default `nginx` home page in the response. Now, let’s try to browse
    the logs of the task.
  prefs: []
  type: TYPE_NORMAL
- en: Browsing container logs using the ECS CLI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apart from using Amazon CloudWatch, you can also use the friendly ECS CLI to
    do this, irrespective of where your logs are stored. This helps us see everything
    from a single pane of glass.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, we can browse the logs for the particular task this service is
    running. Now, let’s go ahead and delete the service.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting an ECS service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To delete the service, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the service has been deleted.
  prefs: []
  type: TYPE_NORMAL
- en: Note that even if we create multiple instances of tasks, they run on different
    IP addresses and can be accessed separately. However, tasks need to be load-balanced,
    and we need to provide a single endpoint. Let’s look at a solution we can use
    to manage this.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing containers running on ECS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Load balancing** is an essential functionality of multi-instance applications.
    They help us serve the application on a single endpoint. Therefore, you can have
    multiple instances of your applications running simultaneously, and the end user
    doesn’t need to worry about which instance they’re calling. AWS provides two main
    load-balancing solutions—**Layer 4** with the **Network Load Balancer** (**NLB**)
    and **Layer 7** with the **Application Load** **Balancer** (**ALB**).'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: While both load balancers have their use cases, a Layer 7 load balancer provides
    a significant advantage for HTTP-based applications. It offers advanced traffic
    management, such as path-based and host-based routing.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let’s go ahead and create an ALB to frontend our tasks using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The output of the preceding command contains values for `LoadBalancerARN` and
    `DNSName`. We will need to use them in the subsequent steps, so keep a copy of
    the output safe.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step will be to create a **target group**. The target group defines
    the group of tasks and the port they will be listening to, and the load balancer
    will forward traffic to it. Use the following command to define a target group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: You will get the `targetGroupARN` value in the response. Keep it safe, as we
    will need it in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will need a **listener** running on the load balancer. This should
    forward traffic from the load balancer to the target group. Use the following
    command to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: You will get the `listenerARN` value in response to this command. Please keep
    that handy; we will need it in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve defined the load balancer, we need to run `ecs-cli compose service
    up` to deploy our service. We will also provide the target group as a parameter
    to associate our service with the load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the resources for this section, `cd` into the following directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following command to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the service and our task are running on Fargate, we can scale our
    service to three desired tasks. To do so, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Since our service has scaled to three tasks, let’s go ahead and hit the load
    balancer DNS endpoint we captured in the first step. This should provide us with
    the default `nginx` response. Run the following command to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, we get a default `nginx` response from the load balancer. This
    shows that load balancing is working well!
  prefs: []
  type: TYPE_NORMAL
- en: ECS provides a host of other features, such as horizontal autoscaling, customizable
    task placement algorithms, and others, but they are beyond the scope of this book.
    Please read the ECS documentation to learn more about other aspects of the tool.
    Now, let’s look at other popular CaaS products available on the market.
  prefs: []
  type: TYPE_NORMAL
- en: Other CaaS services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon ECS provides a versatile way of managing your container workloads. It
    works great when you have a smaller, simpler architecture and don’t want to add
    the additional overhead of using a complex container orchestration engine such
    as Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: ECS is an excellent tool choice if you run exclusively on AWS and don’t have
    a future multi-cloud or hybrid-cloud strategy. Fargate makes deploying and running
    your containers easier without worrying about the infrastructure behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: ECS is tightly coupled with AWS and its architecture. To solve this problem,
    we can use managed services within AWS, such as **Elastic Kubernetes Service**
    (**EKS**). It offers the Kubernetes API to schedule your workloads. This makes
    managing containers even more versatile as you can easily spin up a Kubernetes
    cluster and use a standard, open source solution that you can install and run
    anywhere you like. This does not tie you to a particular vendor. However, EKS
    is slightly more expensive than ECS and adds a *$0.10* per hour cluster management
    charge. That is nothing in comparison to the benefits you get out of it.
  prefs: []
  type: TYPE_NORMAL
- en: If you aren’t running on AWS, there are options from other providers too. The
    next of the big three cloud providers is Azure, which offers **Azure Kubernetes
    Service (AKS)**, a managed Kubernetes solution that can help you get started in
    minutes. AKS provides a fully managed solution with event-driven elastic provisioning
    for worker nodes as and when required. It also integrates nicely with **Azure
    DevOps**, giving you a faster **end-to-end** (**E2E**) development experience.
    As with AWS, Azure also charges *$0.10* per hour for cluster management.
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Kubernetes Engine (GKE)** is one of the most robust Kubernetes platforms.
    Since the Kubernetes project came from Google and is the largest contributor to
    this project in the open source community, GKE is generally quicker to roll out
    newer versions and is the first to release security patches into the solution.
    Also, it is one of the most feature-rich with customizable solutions and offers
    several plugins as a cluster configuration. Therefore, you can choose what to
    install on Bootstrap and further harden your cluster. However, all these come
    at a cost, as GKE charges a *$0.10* cluster management charge per hour, just like
    AWS and Azure.'
  prefs: []
  type: TYPE_NORMAL
- en: You can use **Google Cloud Run** if you don’t want to use Kubernetes if your
    architecture is not complicated, and there are only a few containers to manage.
    Google Cloud Run is a serverless CaaS solution built on the open source **Knative**
    project. It helps you run your containers without any vendor lock-in. Since it
    is serverless, you only pay for the number of containers you use and their resource
    utilization. It is a fully scalable and well-integrated solution with Google Cloud’s
    DevOps and monitoring solutions such as **Cloud Code**, **Cloud Build**, **Cloud
    Monitoring**, and **Cloud Logging**. The best part is that it is comparable to
    AWS Fargate and abstracts all infrastructure behind the scenes. So, it’s a minimal
    Ops or NoOps solution.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve mentioned Knative as an open source CaaS solution, let’s discuss
    it in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Open source CaaS with Knative
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we’ve seen, several vendor-specific CaaS services are available on the market.
    Still, the problem with most of them is that they are tied up to a single cloud
    provider. Our container deployment specification then becomes vendor-specific
    and results in vendor lock-in. As modern DevOps engineers, we must ensure that
    the proposed solution best fits the architecture’s needs, and avoiding vendor
    lock-in is one of the most important requirements.
  prefs: []
  type: TYPE_NORMAL
- en: However, Kubernetes in itself is not serverless. You must have infrastructure
    defined, and long-running services should have at least a single instance running
    at a particular time. This makes managing microservices applications a pain and
    resource-intensive.
  prefs: []
  type: TYPE_NORMAL
- en: But wait! We said that microservices help optimize infrastructure consumption.
    Yes—that’s correct, but they do so within the container space. Imagine that you
    have a shared cluster of VMs where parts of the application scale with traffic,
    and each part of the application has its peaks and troughs. Doing this will save
    a lot of infrastructure by performing this simple multi-tenancy.
  prefs: []
  type: TYPE_NORMAL
- en: However, it also means that you must have at least one instance of each microservice
    running every time—even if there is zero traffic! Well, that’s not the best utilization
    we have. How about creating instances when you get the first hit and not having
    any when you don’t have traffic? This would save a lot of resources, especially
    when things are silent. You can have hundreds of microservices making up the application
    that would not have any instances during an idle period. If you combine it with
    a managed service that runs Kubernetes and then autoscale your VM instances with
    traffic, you can have minimal instances during the silent period.
  prefs: []
  type: TYPE_NORMAL
- en: There have been attempts within the open source and cloud-native space to develop
    an open source, vendor-agnostic, serverless framework for containers. We have
    Knative for this, which the **Cloud Native Computing Foundation** (**CNCF**) has
    adopted.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The Cloud Run service uses Knative behind the scenes. So, if you use Google
    Cloud, you can use Cloud Run to use a fully managed serverless offering.
  prefs: []
  type: TYPE_NORMAL
- en: To understand how Knative works, let’s look at the Knative architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Knative architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Knative project combines elements of existing CNCF projects such as Kubernetes,
    `kubectl` command line. Knative provides its API for developers, which the `kn`
    command-line utility can use. The users are provided access through Istio, which,
    with its traffic management features, is a crucial component of Knative. The following
    diagram describes this graphically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Knative architecture](img/B19877_07_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Knative architecture
  prefs: []
  type: TYPE_NORMAL
- en: Knative consists of two main modules—`serving` and `eventing`. While the `serving`
    module helps us maintain stateless applications using HTTP/S endpoints, the `eventing`
    module integrates with eventing engines such as Kafka and Google Pub/Sub. As we’ve
    discussed mostly HTTP/S traffic, we will scope our discussion to Knative serving
    for this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Knative maintains serving pods, which help route traffic within workload pods
    and act as proxies using the **Istio Ingress Gateway** component. It provides
    a virtual endpoint for your service and listens to it. When it discovers a hit
    on the endpoint, it creates the required Kubernetes components to serve that traffic.
    Therefore, Knative has the functionality to scale from zero workload pods as it
    will spin up a pod when it receives traffic for it. The following diagram shows
    how:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Knative serving architecture](img/B19877_07_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Knative serving architecture
  prefs: []
  type: TYPE_NORMAL
- en: Knative endpoints are made up of three basic parts—`<app-name>`, `<namespace>`,
    and `<custom-domain>`. While `name` and `namespace` are similar to Kubernetes
    Services, `custom-domain` is defined by us. It can be a legitimate domain for
    your organization or a **MagicDNS** solution, such as **sslip.io,** which we will
    use in our hands-on exercises. If you are using your organization domain, you
    must create your DNS configuration to resolve the domain to the Istio Ingress
    Gateway IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s go ahead and install Knative.
  prefs: []
  type: TYPE_NORMAL
- en: For the exercises, we will use GKE. Since GKE is a highly robust Kubernetes
    cluster, it is a great choice for integrating with Knative. As mentioned previously,
    Google Cloud provides a free trial of $300 for 90 days. You can sign up at [https://cloud.google.com/free](https://cloud.google.com/free)
    if you’ve not done so already.
  prefs: []
  type: TYPE_NORMAL
- en: Spinning up GKE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you’ve signed up and are on your console, you can open the Google Cloud
    Shell CLI to run the following commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'You need to enable the GKE API first using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a two-node autoscaling GKE cluster that scales from `1` node to `5`
    nodes, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it! The cluster is up and running.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will also need to clone the following GitHub repository for some of the
    exercises:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Modern-DevOps-Practices-2e](https://github.com/PacktPublishing/Modern-DevOps-Practices-2e)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to clone the repository into your home directory.
    Then, `cd` into the `ch7` directory to access the required resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Now that the cluster is up and running, let’s go ahead and install Knative.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Knative
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will install the CRDs that define Knative resources as Kubernetes API resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the resources for this section, `cd` into the following directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following command to install the CRDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, Kubernetes has installed some CRDs. Next, we must install the
    core components of the Knative `serving` module. Use the following command to
    do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the core serving components have been installed, the next step is
    installing Istio within the Kubernetes cluster. To do so, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that Istio has been installed, we will wait for the Istio Ingress Gateway
    component to be assigned an external IP address. Run the following command to
    check this until you get an external IP in the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, we’ve been assigned an external IP—`35.226.198.46`. We will use
    this IP for the rest of this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will install the Knative Istio controller by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Now that the controller has been installed, we must configure the DNS so that
    Knative can provide custom endpoints. To do so, we can use the MagicDNS solution
    known as `sslip.io`, which you can use for experimentation. The MagicDNS solution
    resolves any endpoint to the IP address present in the subdomain. For example,
    `35.226.198.46.sslip.io` resolves to `35.226.198.46`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Do not use MagicDNS in production. It is an experimental DNS service and should
    only be used for evaluating Knative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to configure the DNS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it provides a batch job that gets fired whenever there is a
    DNS request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s install the **HorizontalPodAutoscaler** (**HPA**) add-on to automatically
    help us autoscale pods on the cluster with traffic. To do so, run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: That completes our Knative installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to install and configure the `kn` command-line utility. Use the
    following commands to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we’ll deploy our first application on Knative.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a Python Flask application on Knative
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand Knative, let’s try to build and deploy a Flask application that
    outputs the current timestamp in the response. Let’s start by building the app.
  prefs: []
  type: TYPE_NORMAL
- en: Building the Python Flask app
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will have to create a few files to build such an app.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `app.py` file looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need the following Dockerfile to build this application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s go ahead and build the Docker container using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the image is ready, let’s push it to Docker Hub by using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: As we’ve successfully pushed the image, we can run this on Knative.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the Python Flask app on Knative
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can use the `kn` command line or create a manifest file to deploy the app.
    Use the following command to deploy the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, Knative has deployed the app and provided a custom endpoint.
    Let’s `curl` the endpoint to see what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the current time in the response. As we already know, Knative should
    detect whether there is no traffic coming into the pod and delete it. Let’s watch
    the pods for some time and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, just after 1 minute of inactivity, Knative starts terminating
    the pod. Now, that’s what we mean by scaling from zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'To delete the service permanently, we can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: We’ve just looked at the imperative way of deploying and managing the application.
    But what if we want to declare the configuration as we did previously? We can
    create a CRD manifest with the `Service` resource provided by `apiVersion`—`serving.knative.dev/v1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create the following manifest file, called `py-time-deploy.yaml`, for
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: As we’ve created this file, we will use the `kubectl` CLI to apply it. It makes
    deployment consistent with Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Though it is a `service` resource, don’t confuse this with the typical Kubernetes
    `Service` resource. It is a custom resource provided by `apiVersion` `serving.knative.dev/v1`.
    That is why `apiVersion` is very important.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go ahead and run the following command to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'With that, the service has been created. To get the service’s endpoint, we
    will have to query the `ksvc` resource using `kubectl`. Run the following command
    to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The URL is the custom endpoint we have to target. Let’s `curl` the custom endpoint
    using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: We get the same response this time as well! So, if you want to keep using `kubectl`
    for managing Knative, you can easily do so.
  prefs: []
  type: TYPE_NORMAL
- en: Knative helps scale applications based on the load it receives—automatic horizontal
    scaling. Let’s run load testing on our application to see that in action.
  prefs: []
  type: TYPE_NORMAL
- en: Load testing your app on Knative
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the `hey` utility to perform load testing. Since your application
    has already been deployed, run the following command to do the load test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the command has executed, run the following command to get the currently
    running instances of the `py-time` pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, Knative has created seven instances of the `py-time` pod. This
    is horizontal autoscaling in action.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at the cluster nodes by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, GKE has created another node in the node pool because of the
    extra burst of traffic it received. This is phenomenal, as we have the Kubernetes
    API to do what we want. We have automatically horizontally autoscaled our pods.
    We have also automatically horizontally autoscaled our cluster worker nodes. This
    means we have a fully automated solution for running containers without worrying
    about the management nuances! That is open source serverless in action for you!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered CaaS and serverless CaaS services. These help us manage
    container applications with ease without worrying about the underlying infrastructure
    and managing them. We used Amazon’s ECS as an example and deep-dived into it.
    Then, we briefly discussed other solutions that are available on the market.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we looked at Knative, an open source serverless solution for containers
    that run on top of Kubernetes and use many other open source CNCF projects.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will delve into IaC with Terraform.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ECS allows us to deploy to which of the following? (Choose two)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. EC2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. AWS Lambda
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Fargate
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. Amazon Lightsail
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ECS uses Kubernetes in the background. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We should always use services in ECS instead of tasks for batch jobs. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We should always use Fargate for batch jobs as it runs for a short period, and
    we only pay for the resources that are consumed during that time. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of the following are CaaS services that implement the Kubernetes API?
    (Choose three)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. GKE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. AKS
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. EKS
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. ECS
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Google Cloud Run is a serverless offering that uses Knative behind the scenes.
    (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which one of the following is offered as a Knative module? (Choose two)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Serving
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Eventing
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Computing
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. Containers
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A, C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A, B, C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A, B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Part 3:Managing Config and Infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part takes a deep dive into infrastructure and configuration management
    in the public cloud, exploring various tools that enable infrastructure automation,
    configuration management, and immutable infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B19877_08.xhtml#_idTextAnchor1010), *Infrastructure as Code (IaC)
    with Terraform*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B19877_09.xhtml#_idTextAnchor1198), *Configuration Management
    with Ansible*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B19877_10.xhtml#_idTextAnchor1330), *Immutable Infrastructure
    with Packer*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
