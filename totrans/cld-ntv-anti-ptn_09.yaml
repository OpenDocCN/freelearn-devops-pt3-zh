- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Connecting It All
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In cloud native environments, networking plays a critical role in ensuring the
    performance, scalability, and security of applications. However, as organizations
    embrace the cloud, they often encounter challenges stemming from misaligned strategies
    and outdated practices. These challenges manifest as anti-patterns—recurring issues
    that undermine the effectiveness of cloud native solutions.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter delves into some of the most common cloud native networking anti-patterns,
    examining their impact and providing actionable insights to avoid them. By understanding
    and addressing these pitfalls, organizations can design resilient, efficient,
    and secure network architectures tailored for the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'The anti-patterns covered in this chapter include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring latency and bandwidth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lack of DNS strategy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monolithic connectivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ignoring cloud native networking features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero Trust application patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By exploring these topics, this chapter equips you with the knowledge to recognize
    and mitigate these anti-patterns, fostering robust cloud native networking practices.
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring latency and bandwidth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When organizations transition to the cloud, the role of networking undergoes
    a significant shift. In traditional on-premises setups, network engineers and
    administrators manage physical hardware, switches, routers, and the meticulous
    planning necessary to ensure low latency, redundancy, and security. This careful
    orchestration is crucial for optimal performance. However, as companies move to
    the cloud, the focus of networking shifts from physical infrastructure management
    to virtualized infrastructure. This shift can lead to the misconception that networking
    becomes a secondary concern, but in reality, it remains just as critical in cloud
    native environments, albeit in a different form. This is where the common cloud
    native anti-pattern of ignoring latency and bandwidth emerges.
  prefs: []
  type: TYPE_NORMAL
- en: The focus shifts from physical hardware to virtualized infrastructure, requiring
    engineers to manage components such as **virtual private clouds** (**VPCs**),
    subnets, security groups, load balancers, and inter-service communication. While
    physical constraints are reduced, the complexity of managing efficient, secure,
    and redundant communication across distributed systems persists. Latency and bandwidth
    issues can be exacerbated, especially in applications built from numerous microservices,
    which must communicate seamlessly across distributed environments.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will examine how to plan and manage connectivity
    to the internet effectively, on-premises systems, and third-party services. This
    will include insights into designing robust, secure network architectures that
    facilitate seamless integration and reliable communication, whether connecting
    cloud resources to legacy infrastructure, external partners, or the broader public
    internet.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud native latency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In cloud environments such as Azure, AWS, and **Google Cloud Platform** (**GCP**),
    network latency refers to the time a data request takes to travel from one point
    to another.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose your application hosted on AWS needs to retrieve data
    from an S3 bucket. In that case, network latency is the delay incurred as the
    request traverses the network, is processed, and the response is returned. Similarly,
    in Azure, if your services span multiple regions, say from East US to West Europe,
    network latency will influence the time it takes for data to travel across these
    regions. Let us focus on the S3 example, as the S3 latency is something we recently
    encountered in an engagement. Let us use the following diagram as a reference
    point for the scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22364_09_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 - AWS networking diagram
  prefs: []
  type: TYPE_NORMAL
- en: Cloud native latency with services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During a consulting engagement, a mid-sized e-commerce company had recently
    migrated a significant portion of its operations to the cloud. As part of their
    architecture, they stored vast amounts of product images, user-generated content,
    and transactional data in Amazon S3\. However, instead of using S3 gateway endpoints
    to access their storage directly within the VPC, they routed all S3 traffic through
    an egress VPC hosted in a separate account. An S3 endpoint is a private connection
    within a VPC that allows direct, secure access to Amazon S3 without traversing
    the public internet, reducing latency and improving security.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initially, everything worked fine. Their network team was familiar with egress
    VPCs from their on-premises days, where routing traffic through specific network
    exits provided centralized control and monitoring. They assumed a similar setup
    would be beneficial in the cloud, ensuring tighter control over internet-bound
    traffic. However, over time, they began noticing performance degradation. The
    following list goes into the details of what said issues were:'
  prefs: []
  type: TYPE_NORMAL
- en: Latency issues became apparent, particularly during peak traffic hours
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Users experienced delays while browsing extensive collections of product images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High volumes of data uploads further exacerbated the response time issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customers faced slow image loads and transaction processing delays
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The team hadn’t accounted for the cost of additional network hops between their
    application VPC and the egress VPC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every request to S3 had to traverse between the two VPCs, adding unnecessary
    latency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data routing through multiple network layers before reaching S3 contributed
    to the delay
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: API calls made to AWS services such as S3 were directed to the internet due
    to the absence of a gateway option
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Without S3 gateway endpoints, which would have allowed for a direct, high-speed
    connection to S3 within the VPC itself, every request took the long way around.
    The solution was simple but impactful. By enabling S3 gateway endpoints within
    their application VPC, they could establish a direct path to S3, eliminating the
    cross-VPC traffic, and the traffic would stay within the AWS account rather than
    reaching out to the internet. Almost immediately, latency dropped and the performance
    issues disappeared. Their customers enjoyed a smoother, faster experience, and
    the engineering team learned an important lesson about the intricacies of cloud
    native networking. The following figure shows the usage of gateway endpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22364_09_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 - S3 gateway endpoint and DynamoDB
  prefs: []
  type: TYPE_NORMAL
- en: It was a costly oversight that could have been avoided had they considered the
    native tools available within the cloud environment. Instead, they had unknowingly
    introduced an anti-pattern by relying on outdated network practices from their
    on-premises days.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-zone latency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A typical pattern in the move to cloud native is found when connecting resources
    across multiple cloud environments or **availability zones** (**AZs**) within
    the same cloud provider, such as AWS, Azure regions, or GCP zones. While cloud
    platforms offer distributed infrastructure and the promise of high availability,
    organizations often underestimate the latency and bandwidth challenges that arise
    when resources are spread geographically. Note that geographical spread also means
    across zones within a specific region.
  prefs: []
  type: TYPE_NORMAL
- en: Take, for example, a typical region in AWS. You may have 3–5 distinct AZs, each
    of which is a grouping of data centers across different diverse locations. This
    allows for better fault tolerance, but latency between these zones is higher than
    between services/apps in the same zone.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, data transfer costs can escalate rapidly when services communicate
    across regions or zones, leading to unexpected financial overhead. This anti-pattern
    reflects a fundamental oversight in cloud native architecture, where organizations
    focus on multi-zone redundancy or cross-cloud integrations without considering
    the performance and cost implications of networking.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22364_09_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 - Example of AWS AZs
  prefs: []
  type: TYPE_NORMAL
- en: 'It is crucial to factor in bandwidth limitations and optimize for low-latency
    interactions, mainly when designing architectures that span multiple zones or
    regions. In-region networking is optimized logically to ensure efficiency and
    performance, but due to the geographic separation designed to support localized
    high availability, it will always face inherent physical limitations. You can
    do the following to resolve this:'
  prefs: []
  type: TYPE_NORMAL
- en: Build your private, data, and public network layers across the same redundancy
    planes. If data resources are in AZ A, so should the other layers that interact.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Account for cross-zone latency when building with **high availability** (**HA**)
    in mind.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider latency tradeoffs for high-performance computing, as *highly available*
    does not mean highly performant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud native bandwidth
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In cloud native environments, bandwidth limitations can significantly impact
    application performance, particularly as services are scaled or distributed across
    regions. Although the cloud abstracts much of the underlying infrastructure, bandwidth
    constraints still persist. Overlooking these limitations can lead to performance
    bottlenecks, especially in high-traffic or data-intensive scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Bandwidth limitations must be carefully addressed when scaling applications
    or managing large amounts of data. For instance, with the big three hyperscalers
    (AWS, GCP, and Azure), services like EC2 and RDS have bandwidth constraints based
    on instance types. Smaller EC2 instances, such as `t2.micro` or `t3.small`, offer
    significantly lower network bandwidth compared to larger instances like `m6a.large`
    or `c6a.xlarge`. Data transfers between regions or even across AZs can exacerbate
    latency and introduce further bandwidth bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: Similar bandwidth constraints exist within Azure and GCP.
  prefs: []
  type: TYPE_NORMAL
- en: Ambiguity – a cause of latency and bandwidth issues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we explored earlier, the choice of instance types in cloud environments has
    become far more critical than it ever was in traditional on-premises settings.
    The flexibility and sheer variety of options available in the cloud are both a
    blessing and a challenge. Consider, for example, the task of selecting an instance
    type in AWS for a Kubernetes node that requires four cores and eight gigabytes
    of RAM. At first glance, it seems we are spoiled for choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick look at AWS Pricing Calculator reveals a list of at least 10 potential
    instance types, each offering different combinations of network speeds, memory
    allocations, and pricing. The following is an example of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22364_09_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 - Extract from AWS Pricing Calculator
  prefs: []
  type: TYPE_NORMAL
- en: However, the real challenge lies in determining which instance best suits your
    specific use case. Do you choose `c6g.xlarge`, which is cost-effective and still
    provides up to 10 gigabits of network throughput? Or do you opt for the more powerful
    `c7g.xlarge`? It’s not simply a matter of weighing performance against cost. A
    deeper consideration is whether your application can even run on ARM processors,
    both of which leverage AWS’s Graviton ARM chips, which, while offering impressive
    performance gains, may not be compatible with all workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond processor compatibility, other technical specifications, such as network
    bandwidth and CPU architecture, require thoughtful consideration. These details
    aren’t just abstract numbers; they directly impact your application’s performance
    and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: As we migrate from on-premises infrastructure to the cloud, the art of selecting
    the right instance type becomes paramount, and this choice in compute extends
    out to other cloud services.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond VMs – bandwidth limitations for containers and serverless
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is essential to recognize that bandwidth limitations are not confined to
    VMs alone. Containerized services and serverless architectures can also suffer
    from bandwidth bottlenecks, seriously impacting application performance in cloud
    native environments. While abstracting infrastructure management, services such
    as AWS Fargate and Google Cloud Run still impose network bandwidth constraints
    that developers must consider when designing scalable, distributed systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, AWS Lambda, a serverless computing service, also experiences
    bandwidth limitations that can affect applications. While Lambda abstracts server
    infrastructure, its network still faces throughput restrictions, especially when
    handling high-volume data transfers between services like S3, DynamoDB, or external
    APIs. Ignoring these limitations can lead to performance degradation in serverless
    applications, which rely heavily on fast, seamless communication across services.
    Some specific points to consider include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**VPC networking in Lambda**: When a Lambda function is configured to run inside
    a VPC, it may experience added latency and bandwidth constraints due to the VPC’s
    network configuration and throughput limits. Lambda is unique in that the higher
    the memory allocation, the higher the background CPU count and network bandwidth.
    Specifically, the more CPU is available, the more accessible the full bandwidth
    of the elastic network interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cold start delays**: While not directly bandwidth-related, Lambda cold starts
    can indirectly affect how quickly an application can process requests, especially
    under high loads, exacerbating bandwidth bottlenecks during initial invocations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**S3 and Lambda data transfers**: Large-scale data transfers between S3 and
    Lambda can hit bandwidth limits, especially when dealing with large files or high
    concurrency, leading to slower execution times or throttling. Note also the serverless
    limitations of Lambda, via 6 MB sync and 20 MB response size limits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outbound bandwidth to external APIs**: When Lambda functions interact with
    external APIs or services outside the AWS ecosystem, bandwidth constraints can
    increase response times or lead to timeouts if data transfer rates exceed limits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As cloud native architectures become more complex and distributed, bandwidth
    considerations must not be overlooked. From VMs to containers and serverless functions,
    all layers of cloud infrastructure face bandwidth limitations that can introduce
    unexpected bottlenecks. Ignoring these limits is a common anti-pattern that can
    significantly degrade performance and lead to unforeseen costs, especially in
    high-traffic environments or applications that process large volumes of data.
    By proactively addressing bandwidth constraints and designing architectures with
    these limits, organizations can ensure their cloud native solutions are optimized
    for performance and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: Across the big three cloud providers, applications designed without accounting
    for these limitations may suffer from high latency, data bottlenecks, and increased
    costs. Cloud native architecture must consider these factors to avoid common anti-patterns
    related to bandwidth and latency. The following section will show us how we can
    avoid the pitfalls of latency and bandwidth being overlooked. Our next section
    will dig into the lack of DNS strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Lack of DNS strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “*It’s not DNS,*
  prefs: []
  type: TYPE_NORMAL
- en: '*There’s no way it’s DNS,*'
  prefs: []
  type: TYPE_NORMAL
- en: '*It was DNS*.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'This now-famous haiku perfectly captures the frustration and irony of one of
    the most overlooked aspects of modern networking: DNS. Often dismissed as a straightforward
    service, DNS is one of those critical components that only garners attention when
    things go wrong. In cloud native environments, where services, systems, and applications
    rely heavily on dynamic and distributed architectures, DNS issues can quickly
    spiral into significant outages, performance bottlenecks, or security vulnerabilities.
    And yet, many organizations treat DNS as an afterthought.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The anti-pattern of inconsistent DNS management is a silent disruptor. Organizations
    moving toward cloud native architectures often inherit a fragmented approach to
    DNS. With legacy systems, hybrid environments, and third-party services all in
    play, DNS strategies become disjointed and poorly aligned. This leads to unpredictable
    issues: slow resolution times, increased latency, and intermittent failures as
    systems struggle to connect across varied infrastructures.'
  prefs: []
  type: TYPE_NORMAL
- en: In the cloud native space, this is a recipe for disaster. Whether services are
    hosted on-premises or in the cloud, a lack of cohesive DNS strategy can destabilize
    even the most well-designed applications. The challenge is compounded when external
    services are involved, creating a tangled web of DNS resolution paths that can
    delay communication, introduce security risks, or lead to outright service failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section explores the causes and consequences of lacking DNS strategy and
    provides a guide for creating a unified, resilient DNS strategy. We’ll cover the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud native DNS management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hybrid environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third-party integrations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Undermining traffic segregation (QoS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Configuring low-performance backup links for high-performance primary links:
    considerations of QoS over backup links'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud native DNS management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In cloud native architectures, DNS is no longer just simply mapping domain names
    to IP addresses. It becomes critical to how services discover one another, how
    traffic is routed efficiently, and how resilience is built into the network. However,
    the complexity of cloud native environments and the ease of spinning new services
    can quickly turn DNS into a tangled mess if not managed properly.
  prefs: []
  type: TYPE_NORMAL
- en: In cloud native environments, services such as Amazon Route 53, Azure DNS, and
    GCP Cloud DNS provide highly scalable DNS services designed specifically for cloud
    native use cases. These services enable fast, reliable routing to VM instances,
    load balancers, API gateways, and external endpoints. When appropriately managed,
    they ensure low-latency access to services, seamless failover, and redundancy
    across regions. However, when DNS configurations are fragmented, even in cloud
    native environments, it can lead to severe performance and connectivity issues.
    These issues and their eventual solution are discussed in the example that follows.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud native and on-premises DNS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We encountered a similar situation with a fintech client that used Amazon Route
    53 to manage DNS for their cloud native microservices. Initially, everything worked
    smoothly, but as their infrastructure expanded, they began integrating services
    that required coordination between their cloud environment and on-premises systems.
    The fintech organization implemented separate DNS zones to manage internal domains,
    with Route 53 handling cloud native services and **Active Directory** (**AD**)
    DNS managing their on-premises resources. However, there was no unified DNS strategy
    in place, resulting in inconsistent DNS records between the two systems.
  prefs: []
  type: TYPE_NORMAL
- en: As traffic increased, these clashing DNS configurations became a problem. Services
    began to fail, not due to application issues but because the conflicting DNS setups
    couldn’t handle proper traffic routing between the cloud and on-premises environments.
    The lack of a centralized DNS strategy led to delays in resolving internal services,
    causing timeouts and degrading the user experience. The fragmented approach to
    DNS management resulted in misrouted traffic and unnecessary latency, affecting
    critical financial operations.
  prefs: []
  type: TYPE_NORMAL
- en: The fragmented DNS management between AD and Route 53 led to delayed lookups,
    inconsistent routing, and broken connections. Services slowed down, causing latency
    spikes and interruptions that took significant troubleshooting time. The root
    of the issue? The erratic and uncoordinated DNS setup across environments.
  prefs: []
  type: TYPE_NORMAL
- en: Overcoming clashing DNS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The organization eventually resolved this issue with the help of **Route 53
    Resolver**, a service designed to bridge on-premises and cloud native DNS environments.
    Route 53 Resolver allowed them to forward DNS queries between their AWS environment
    and their on-premises AD DNS servers. DNS forwarding rules created a seamless
    flow of DNS queries between the two systems, allowing cloud services to resolve
    on-premises domains, and vice versa. This approach eliminated the need for parallel
    DNS systems, centralizing DNS resolution under a single, cohesive architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The introduction of Route 53 Resolver transformed their DNS setup into a unified
    system, leveraging a proper hybrid model. Internal applications could now resolve
    both cloud native and on-premises domain names without the delays or conflicts
    caused by fragmented management. By consolidating their DNS strategy, integrating
    AWS Directory Service with Route 53, and leveraging Route 53 Resolver, they ensured
    that DNS resolution was consistent, fast, and reliable across all environments.
    A simplified version of the solution can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22364_09_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 - Hybrid DNS Resolver
  prefs: []
  type: TYPE_NORMAL
- en: The next section will expand on this as we look at hybrid environments and QoS.
  prefs: []
  type: TYPE_NORMAL
- en: Undermining traffic segregation (QoS) based on application/data criticality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most overlooked aspects of cloud native architecture is the importance
    of traffic segregation based on application and data criticality. Not all traffic
    in a system is equal; some workloads require high-priority, low-latency communication,
    while others can tolerate slower processing times. This concept is fundamental
    to **quality of service** (**QoS**), which prioritizes traffic based on its importance
    to business operations. Unfortunately, a common anti-pattern in cloud native deployments
    is the failure to implement adequate traffic segregation, resulting in performance
    degradation, missed **service-level agreements** (**SLAs**), and unnecessary resource
    consumption.
  prefs: []
  type: TYPE_NORMAL
- en: In traditional networking, QoS policies often prioritize traffic based on its
    type and importance. Critical applications, for example, real-time financial transactions,
    video conferencing, or database replication are prioritized. At the same time,
    non-critical tasks like backups, bulk file transfers, or routine updates are assigned
    lower priority. However, in cloud native environments, this approach is often
    neglected. Without proper QoS implementation, all traffic is treated equally,
    leading to significant issues when high-priority services must compete with less
    critical ones for bandwidth and compute resources.
  prefs: []
  type: TYPE_NORMAL
- en: The cost of ignoring traffic segregation – a fintech case study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During a consulting engagement with a large fintech company, we encountered
    a classic example of the pitfalls of failing to implement proper traffic segregation
    in a cloud environment. The company ran real-time transaction processing alongside
    nightly data backups, which operated in the same shared cloud infrastructure.
    Initially, everything seemed to work fine, but as transaction volumes grew, so
    did the strain on the network.
  prefs: []
  type: TYPE_NORMAL
- en: The lack of a structured traffic prioritization strategy meant that their backup
    operations, scheduled during peak hours, consumed a significant portion of the
    available bandwidth. This interference caused delays in real-time financial transactions,
    leading to missed SLAs and customer dissatisfaction. This is where the need for
    a robust QoS strategy became evident. With proper traffic segregation and prioritization,
    we ensured that critical services, for example, real-time transaction processing,
    were always given priority over less urgent tasks such as nightly backups. By
    isolating bandwidth-heavy operations and allocating resources based on service
    criticality, we helped them avoid these delays altogether.
  prefs: []
  type: TYPE_NORMAL
- en: The risks of failing to segregate traffic based on criticality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When traffic segregation based on application or data criticality is ignored,
    organizations are exposed to several risks, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Degraded performance for critical applications**: Business-critical applications,
    like real-time financial transactions or sensitive data transfers, may experience
    latency or delays if forced to compete for bandwidth with non-essential traffic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missed SLAs**: In environments where uptime, speed, and reliability are key
    performance indicators, the failure to segregate traffic can lead to missed SLAs,
    resulting in penalties or reputational damage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource contention**: Equal treatment of all traffic can cause resource
    contention, where essential processes are starved for bandwidth or compute power,
    while less important tasks consume unnecessary resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security risks**: Some data flows, such as those involving sensitive financial
    or personal information, should be segregated not just for performance reasons
    but also for security. Failure to isolate this traffic can expose critical data
    to vulnerabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for traffic segregation and QoS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To avoid the anti-pattern of undermining traffic segregation, organizations
    should implement a structured QoS strategy tailored to their cloud native infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Best Practice** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Prioritize traffic based on criticality | Define and categorize traffic based
    on its importance to business operations. Latency-sensitive or critical tasks
    should have higher priority over non-urgent processes. |'
  prefs: []
  type: TYPE_TB
- en: '| Use network segmentation | Implement virtual network segmentation (e.g.,
    VPCs or subnets) to separate traffic by priority, ensuring high-priority traffic
    does not compete with lower-priority flows. |'
  prefs: []
  type: TYPE_TB
- en: '| Leverage cloud native QoS tools | Utilize cloud provider tools such as Amazon
    Traffic Mirroring, bandwidth control, Azure Traffic Manager, and Google Cloud
    Network Service Tiers to manage and optimize traffic flow. |'
  prefs: []
  type: TYPE_TB
- en: '| Monitor and adjust QoS policies | Regularly monitor the performance of QoS
    policies and make adjustments as workloads change to maintain optimal performance.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Account for multi-cloud and hybrid setups | Ensure consistent QoS policies
    across multi-cloud or hybrid environments to prevent bottlenecks and maintain
    performance between on-premises and cloud infrastructures. |'
  prefs: []
  type: TYPE_TB
- en: Table 9.1 - QoS best practices
  prefs: []
  type: TYPE_NORMAL
- en: A common anti-pattern in cloud native architectures is relying on low-performance
    backup links to support high-performance primary links without considering how
    QoS will function during failover. Backup links are implemented in many setups
    as a cost-saving measure, typically designed with lower bandwidth and reduced
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: However, if a primary high-performance link fails, critical applications and
    data flows are forced onto these slower links, potentially causing severe performance
    degradation, increased latency, and service outages. Failing to configure appropriate
    QoS policies for these backup links can exacerbate the issue, as critical traffic
    may not be prioritized during the failover, further degrading the user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Key considerations for backup link QoS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To mitigate these risks, it’s essential to plan for the backup links as carefully
    as the primary links, ensuring that they can handle the most critical traffic
    if a failover occurs. Properly configured QoS can help ensure that essential services
    maintain priority during periods of reduced capacity and operate with minimal
    disruption. To ensure consistency, regular checks and testing applications via
    backup links are critical. Untested backups should be treated as inactive until
    tested in some cadence. The following points highlight how to approach backup
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prioritize critical traffic during failover**: Implement QoS policies to
    ensure that high-priority traffic, such as transactional data or real-time services,
    is prioritized over less critical traffic on backup links.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test backup link capacity regularly**: Regularly test the performance of
    backup links to ensure they can handle the critical traffic load during a failover
    scenario without significant degradation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scale backup links based on needs**: Ensure that backup links are appropriately
    scaled to handle the most critical workloads, even if they can’t match the full
    capacity of primary links.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitor link performance**: Continuously monitor both primary and backup
    links to ensure that QoS policies are functioning as intended and traffic is routed
    efficiently during failover events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluate cost vs. performance trade-offs**: Balance cost savings with critical
    application performance requirements. Under-provisioned backup links may reduce
    costs, but they can introduce unacceptable risks to business continuity during
    outages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proper planning and careful configuration of backup links with QoS policies
    can help ensure smooth transitions during failover, preserving the performance
    of critical applications and maintaining business continuity.
  prefs: []
  type: TYPE_NORMAL
- en: In cloud native environments, failing to implement traffic segregation based
    on application and data criticality is a serious anti-pattern that can erode system
    performance, increase latency, and jeopardize the reliability of critical services.
    By establishing a robust QoS strategy that prioritizes high-value workloads, organizations
    can ensure that their cloud native applications are resilient, responsive, and
    capable of meeting even the most demanding business requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Monolithic connectivity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We briefly touched on the role of network engineers and systems admins in managing
    on-premises hardware such as switches, routers, and the like; with that mindset
    came a traditional data center way of planning networking. The individual hardware
    components became a single point of failure for the entire network, whereas if
    a core switch were to fail, the whole network stack would also crumble. The cloud
    native model has a very different networking setup from that of a data center
    of a conventional organization; a traditional data center model may set its subnets
    and network layers across the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Core**: The backbone of the network for core switches'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distribution**: This sits between the cores and handles networking policies/security'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Access**: The access layer for servers, storage arrays, and the typical network
    we see from an admin perspective'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The accompanying diagram offers a more detailed illustration to provide a clearer
    understanding of this concept.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22364_09_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 - Three-tier traditional network
  prefs: []
  type: TYPE_NORMAL
- en: 'Subnetting is managed differently across the three network layers. The following
    table details this:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Network Layer** | **Subnetting Approach** | **Function** **and Focus**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Core layer | Minimal subnetting | Acts as a high-speed interconnect between
    other layers, prioritizing performance over segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Distribution layer | Extensive subnetting to support diverse needs | Handles
    fiber channels, firewalls, and traffic monitoring between layers, requiring flexibility
    and control |'
  prefs: []
  type: TYPE_TB
- en: '| Access layer | Traditional subnetting practices | Supports everyday network
    setups, tailoring subnetting to user and device |'
  prefs: []
  type: TYPE_TB
- en: Table 9.2 - Subnetting across network layers
  prefs: []
  type: TYPE_NORMAL
- en: Monolithic friction with cloud native
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While still focused on high-speed interconnectivity, the core layer may leverage
    virtualized networking solutions that reduce the need for physical infrastructure,
    making subnetting even more minimal and flexible. The distribution layer becomes
    highly dynamic in a cloud native context, with subnetting used to manage VPCs,
    security groups, and service meshes to control traffic flow between services,
    storage, and firewalls across multiple regions or AZs. Meanwhile, the access layer
    shifts toward integrating scalable resources like containerized workloads, where
    traditional subnetting practices give way to automated, software-defined networking
    solutions that dynamically adjust to workload demands.
  prefs: []
  type: TYPE_NORMAL
- en: In an ideal world, organizations transitioning to cloud native environments
    would leave behind the constraints of their old data centers. However, what often
    happens instead is that traditional networking models are simply lifted and shifted
    into the cloud. This creates a common anti-pattern we’ve encountered frequently,
    where outdated practices are applied to modern architectures. The result is a
    system weighed down by limitations, restricting the true potential of cloud native
    infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: This section will explore how cloud native environments transition from monolithic
    connectivity patterns to layered failover strategies across OSI layers. We’ll
    focus on the challenges of synchronous versus asynchronous traffic, mitigating
    single points of failure and configuring packet inspection to meet the unique
    demands of cloud native architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Monolith to layered networking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Monolithic connectivity**, a common anti-pattern in legacy systems, relies
    on tightly coupled, single-tiered network designs where all application components
    communicate internally, often without clear separation or segmentation. While
    this model may have worked for smaller, self-contained applications, it struggles
    to meet the demands of modern cloud native environments, which prioritize scalability,
    flexibility, and resilience.'
  prefs: []
  type: TYPE_NORMAL
- en: Organizations transitioning to cloud native architectures adopt layered networking
    models that separate services and components. This approach aligns closely with
    microservices, where each service operates independently and communicates through
    well-defined network layers. Organizations can address common issues such as lack
    of scalability, difficulty isolating failures, and security vulnerabilities by
    moving away from monolithic connectivity to a more modular, layered structure.
    *Figure 9**.1* shows a perfect example of a modular layered network structure,
    with multiple private subnets segregated within a VPC.
  prefs: []
  type: TYPE_NORMAL
- en: Layered networking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Layered networking in cloud native environments introduces distinct layers,
    each with a specific purpose. This segmentation enhances control, isolating services
    based on their function, priority, or security requirements. For example, frontend
    services can be placed in one network layer, while backend services, for example,
    databases or internal APIs, reside in another. This layered approach improves
    scalability and security by limiting direct access to critical services. By applying
    network policies, organizations can ensure that only authorized services can communicate
    across layers, reducing the risk of lateral movement in case of a security breach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, layered networking supports the independent scaling of services.
    In monolithic architectures, scaling often meant replicating the entire application,
    which can be resource-intensive and inefficient. In contrast, layered architectures
    enable individual services to scale as needed, depending on traffic and performance
    demands. This flexibility ensures that resources are used efficiently and allows
    organizations to adapt quickly to changing workloads. The following table details
    the benefits of the layered networking approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Aspect** | **Monolithic Connectivity** | **Layered Networking (****Cloud
    native)** |'
  prefs: []
  type: TYPE_TB
- en: '| **Scalability** | Scaling requires replicating the entire monolithic application
    | Independent services can be scaled individually, reducing resource use |'
  prefs: []
  type: TYPE_TB
- en: '| **Security** | All components communicate freely within the same network
    tier, posing potential security risks | There is a clear separation of services,
    enabling better security policies and isolation |'
  prefs: []
  type: TYPE_TB
- en: '| **Resilience** | A failure in one system part can bring down the entire application
    | Isolated services reduce the blast radius of failures, enhancing resilience
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Flexibility** | It is difficult to modify or add services without impacting
    the entire system | Services can be added, modified, or replaced without affecting
    the whole architecture |'
  prefs: []
  type: TYPE_TB
- en: '| **Network** **Traffic Control** | There is no clear traffic segmentation;
    all traffic flows freely between components | Traffic is segmented based on service
    layers, allowing for better traffic management and monitoring |'
  prefs: []
  type: TYPE_TB
- en: '| **Development** **Speed** | Changes require complete application testing
    and deployment | Individual services can be updated and deployed independently
    |'
  prefs: []
  type: TYPE_TB
- en: Table 9.3 - Benefits of layered networking
  prefs: []
  type: TYPE_NORMAL
- en: Monolith to microservice – networking-focused example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During a consulting engagement with a government client, we were tasked with
    addressing significant network challenges as part of their transition from a monolithic
    architecture to a cloud native environment. The company’s original network design
    lacked segmentation, with all services, frontend applications, databases, and
    internal APIs residing in a single flat network. This setup led to numerous issues,
    including inefficiencies in traffic flow, security vulnerabilities, and scaling
    challenges, particularly with IP allocation due to a small subnet range.
  prefs: []
  type: TYPE_NORMAL
- en: Their monolithic network architecture made isolating services based on function
    or security requirements difficult. All traffic flowed through the same network,
    exposing critical backend services, such as databases, to unnecessary risk. Without
    proper network segmentation, any breach in the system could quickly spread laterally,
    potentially compromising sensitive data. Moreover, as traffic to their platform
    grew, scaling required replicating the entire system, including components that
    didn’t need to be scaled. This approach was resource-intensive and inefficient.
  prefs: []
  type: TYPE_NORMAL
- en: The approach – three-tier network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We introduced a layered networking model on AWS, following three-tier capabilities
    to bring order and control to their cloud native infrastructure. This model was
    deployed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Presentation** **layer (frontend)**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Purpose**: Handles user requests and public traffic, primarily on user-facing
    components like web servers or APIs.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation**: Place frontend services in a public subnet within the AWS
    VPC, accessible from the internet.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: Use security groups and **web application firewalls** (**WAFs**)
    to protect against external threats while allowing incoming web traffic or load
    balancer traffic.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application layer (****business logic)**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Purpose**: Processes business logic, communicating between the frontend and
    the backend. This layer hosts the internal services, APIs, or microservices.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation**: Deploy application services in a private subnet to isolate
    them from direct internet access while allowing them to communicate with both
    the frontend and backend layers.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: Use security groups to control which frontend services can communicate
    with the application layer, ensuring only authorized traffic flows between these
    layers. Access between Kubernetes Pods was limited using security group references,
    eliminating IP spoofing as an attack vector.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data** **layer (backend)**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Purpose**: Stores and manages data, like databases and internal APIs, which
    must be secure and isolated.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation**: Place databases and other backend services in a separate
    private subnet with strict access controls to ensure that only the application
    layer can access it.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: Implement a VPC gateway endpoint to restrict access and configure
    **network access control lists** (**NACLs**) to further restrict any unauthorized
    access from other subnets.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: On top of the three-tier approach here, we had distributed all three tiers across
    multiple AZs; the architecture was significantly more resilient and scalable,
    allowing the application to continue functioning even if an entire zone went offline.
    When an AZ was created, the application would scale to other zones, and traffic
    would automatically be directed to the new nodes. AZs are isolated data center
    locations (per zone) within an AWS region, each with independent power, networking,
    and cooling. They offer much greater resilience than two traditional data centers
    because they are geographically separate yet closely interconnected; this also
    consists of fully redundant dedicated fiber lines. This ensures that even if one
    zone fails due to a localized issue, the others remain fully operational without
    impacting performance. Where this multiple AZ design was leveraged best was when
    addressing synchronous and asynchronous traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronous versus synchronous traffic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cloud native architecture fundamentally shifts how traffic and communication
    between services are handled. One of the most significant challenges in traditional
    environments is managing synchronous versus asynchronous traffic, which can become
    a bottleneck as systems grow in complexity and demand. Traditional organizations’
    services often rely on synchronous communication, meaning that one service must
    wait for a response from another before continuing. This approach can lead to
    inefficiencies, higher latency, and potential points of failure, particularly
    in distributed environments where network issues or service delays can halt entire
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: Comparatively, cloud native architectures are designed to embrace asynchronous
    communication. This shift resolves a major anti-pattern often seen in traditional
    setups, where systems are tightly coupled and dependent on real-time, synchronous
    responses. These traditional systems struggle under high load or when services
    experience delays, leading to timeouts, failures, and decreased resilience. Let’s
    look at the benefits of asynchronous traffic in a cloud native environment.
  prefs: []
  type: TYPE_NORMAL
- en: Key benefits of asynchronous traffic in cloud native
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following benefits highlight why asynchronous traffic is essential for
    cloud native applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Increased resilience**: Services can continue functioning even if one part
    of the system is delayed or unavailable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved scalability**: Asynchronous systems can handle higher loads because
    they don’t require immediate responses, reducing the strain on services during
    peak traffic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoupled services**: Cloud native systems encourage loose coupling, where
    services operate independently, reducing the risk of cascading failures'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault tolerance**: By using queues and event-driven models, systems can automatically
    retry failed operations without blocking other processes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From strong consistency to eventual consistency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A key aspect of this transition is the shift from *strongly consistent* to *eventually
    consistent* systems, which allows cloud native applications to prioritize availability
    and fault tolerance over immediate consistency. By adopting eventual consistency,
    cloud native systems can handle large-scale, distributed workloads more effectively,
    as they no longer rely on the entire system being perfectly synchronized. This
    approach increases scalability and resilience, enabling systems to operate smoothly
    even when components are temporarily out of sync – an essential trade-off in high-traffic,
    globally distributed environments.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud native architectures resolve this challenge by leveraging asynchronous
    communication models, such as message queues, event-driven architectures, and
    serverless components. In these systems, services publish events or send messages
    without waiting for an immediate response. For example, when a user places an
    order on an e-commerce platform, the order might be processed asynchronously through
    a message queue (e.g., Amazon SQS or Kafka), allowing the frontend to continue
    interacting with the user while the backend processes the order in the background.
    This decoupling improves the application’s resilience, as the failure or delay
    of one service does not impact the overall system’s ability to respond to users
    or continue functioning.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing monolithic connectivity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In traditional systems, the reliance on synchronous communication creates an
    anti-pattern of tight coupling, where services are overly dependent on each other
    and must be available in real time for the system to function properly. This introduces
    fragility, as any delay or failure in one component can ripple through the entire
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud native architectures resolve this by promoting asynchronous communication,
    where services interact without waiting for immediate responses. In doing so,
    the anti-pattern is broken, and systems become more resilient, scalable, and adaptable
    to change. As organizations move to cloud native, they benefit from the flexibility
    of being able to scale individual services independently, handle failures gracefully,
    and process high volumes of traffic more efficiently. This shift not only improves
    the system’s overall performance but also lays the foundation for a more agile,
    adaptable infrastructure that can evolve with the business’s needs.
  prefs: []
  type: TYPE_NORMAL
- en: In moving from monolithic connectivity to layered networking, cloud native architectures
    significantly improve scalability, security, and resilience. By adopting layered
    models, organizations can break away from tightly coupled, synchronous systems
    prone to single points of failure. Instead, services are isolated and scalable,
    allowing greater flexibility and control. With proper segmentation, even the most
    complex infrastructures can maintain high availability, and the risk of lateral
    movement during a security breach is minimized. These benefits make cloud native
    approaches far superior to traditional models, ensuring they remain robust and
    efficient as applications scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll explore another critically overlooked anti-pattern: ignoring cloud
    native networking features. We’ll examine how failing to leverage built-in cloud
    features can limit performance and security and how properly utilizing these features
    can maximize the benefits of a cloud native infrastructure.'
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring cloud native networking features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most common pitfalls when transitioning to cloud native architectures
    is overlooking the powerful networking features inherently built into cloud platforms.
    In traditional on-premises environments, networking is often hardware-centric,
    relying on physical switches, routers, and firewalls. This leads to misconceptions
    and misaligned expectations when dealing with the more dynamic, software-driven
    nature of cloud native networking.
  prefs: []
  type: TYPE_NORMAL
- en: This section will explore how failing to fully embrace **software-defined networking**
    (**SDN**) in the cloud can lead to performance and resilience issues. We will
    also stress the importance of treating network configuration as code through **infrastructure
    as code** (**IaC**), a practice crucial for successfully implementing cloud native
    networking. The risks associated with inadequate network boundary guardrails,
    especially when managing access between environments such as production and non-production
    are also discussed.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these areas presents unique challenges, and a failure to address them
    can limit the potential of cloud native infrastructures, leaving organizations
    vulnerable to security breaches and operational inefficiencies.
  prefs: []
  type: TYPE_NORMAL
- en: SDN in the cloud – the risks from on-premises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SDN is not a concept that is limited to just the cloud native environment; the
    idea has been around for some time. Popularizing this concept arguably has been
    companies such as VMware with their VMware NSX product, released in 2013 – an
    early example of SDN that allows virtualization of network infrastructure, enabling
    the creation, management, and automation of complex networks through software
    rather than traditional hardware. Rather than setting up entire server racks worth
    of hardware from scratch, SDN tools like VMware NSX gave admins a much quicker
    way to deploy and extend their networks to new hardware; cloud vendors adopted
    this concept to do the same without needing the hardware components. SDN in traditional
    environments still requires hardware to deploy; it just makes templating a lot
    easier.
  prefs: []
  type: TYPE_NORMAL
- en: 'SDN thrives in the cloud, shifting control from physical hardware to software-based
    solutions. This transformation allows cloud providers such as AWS, Azure, and
    GCP to offer flexible, scalable, and dynamic networking solutions that adapt to
    the needs of modern applications. Here are some key examples of how SDN is applied
    across these platforms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS**: Amazon VPC enables isolated networks with control over routing, firewalls,
    and access'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure**: Azure VNet uses SDN with tools like NSGs and Azure Firewall for
    traffic segmentation and network policy automation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GCP**: Google Cloud VPC uses SDN for customizable IP ranges, firewall rules,
    and routing, with tools such as Cloud Armor and VPC peering for security and connectivity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Across all three platforms, SDN provides the flexibility to scale, automate,
    and manage network infrastructure programmatically, allowing users to build secure,
    optimized cloud environments without the limitations of traditional hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Networking mindset changes with cloud native
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most common cloud native anti-patterns is the lack of understanding
    of SDN in cloud environments compared to traditional on-premises hardware setups.
    This gap in understanding often leads to unrealistic expectations around performance,
    resilience, and overall network behavior, resulting in misconfigurations that
    compromise both system reliability and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: With the cloud vendors, a common misunderstanding arises when users expect cloud
    networking to behave like traditional hardware-based infrastructure, where dedicated
    physical devices dictate network performance and capacity. Network reliability
    is tied directly to hardware robustness, such as switches and routers in an on-premises
    environment. However, AWS networking, like Amazon VPC, is entirely virtualized.
    Performance and resilience depend on how well subnets, security groups, and multi-AZ
    setups are configured. Misconfigurations in this virtual environment can lead
    to poor fault tolerance and performance bottlenecks, starkly contrasting the expectations
    of physical hardware environments.
  prefs: []
  type: TYPE_NORMAL
- en: Legacy brought to the cloud native networking – a case study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We encountered a common example of a poorly configured AWS networking setup
    during a network uplift engagement with a banking client. However, when we refer
    to “poorly configured,” it’s essential to recognize that what was once considered
    best practice can, with the passage of time and advancements in technology, evolve
    into a suboptimal solution. This client transitioned from an on-premises infrastructure
    to AWS over 3–4 years. Initially, their network architects viewed the three-tier
    AWS network design as too simplistic and believed it introduced too much overhead
    for cross-domain communication and change management.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of designing separate VPCs for each environment or workload, the architects
    implemented a design that centralized networking into a single VPC shared across
    multiple accounts. In this design, subnets were shared between different accounts,
    which seemed logical from a traditional networking perspective. It mirrored the
    idea of a centralized core network sharing access layers across various AWS accounts.
    However, rather than solving overhead issues, this approach introduced significant
    complexity. When a change or flexibility was required, any alteration to the VPC
    structure or route table rules affected all accounts within the shared network.
    Instead of building a fault-tolerant, layered cloud network, they had inadvertently
    created a single point of failure disguised as simplicity. This design was similar
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22364_09_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 - Shared VPC design
  prefs: []
  type: TYPE_NORMAL
- en: In a risk-averse industry such as banking, this design flaw was compounded by
    the fact that even minor changes were heavily scrutinized during change advisory
    board meetings. The result was a rigid, fragile network architecture that stifled
    agility and introduced considerable risk.
  prefs: []
  type: TYPE_NORMAL
- en: Our solution was transitioning from shared subnets to individual VPCs for each
    account, interconnected through AWS Transit Gateway. To preserve the benefits
    of the shared subnet setup, we restructured the network, as shown in *Figure 9**.1*.
    All outbound traffic, such as internet and third-party requests, was routed through
    an egress VPC, where a security appliance such as a FortiGate firewall scanned
    all outbound traffic. This eliminated the need for multiple NAT gateways or instances.
    Each VPC was configured with specific subnets, allowing cloud native features
    to be enabled or restricted based on the use case. For example, data/private subnets
    were limited to accessing only DynamoDB gateway endpoints, ensuring tighter security
    and minimizing unnecessary service access.
  prefs: []
  type: TYPE_NORMAL
- en: The added benefit of this rearchitected solution was a more resilient, dispersed
    network design. Changes were now account-specific, significantly reducing the
    blast radius of any failed modifications. This modular design ensured that any
    impact was limited to individual environments, enhancing agility and fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: As we have touched on changes, this leads us to the next section on inadequate
    network access reviews and missing boundary guardrails.
  prefs: []
  type: TYPE_NORMAL
- en: Inadequate network access reviews and missing boundary guardrails
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With traditional data centers, where physical boundaries naturally limit access,
    cloud infrastructure is dynamic, allowing for easier and potentially dangerous
    access escalation. Without regular, thorough reviews of access privileges, users
    or systems may gain unintended access to critical production environments from
    non-production or development systems. This lack of oversight leaves organizations
    vulnerable to unauthorized lateral movement, exposing sensitive data and core
    systems to significant threats.
  prefs: []
  type: TYPE_NORMAL
- en: 'The absence of solid network boundary guardrails further exacerbates these
    risks. Guardrails, such as security groups, firewall rules, and routing table
    policies, are essential for keeping access within the intended environment. Without
    these controls, the network becomes flat, allowing unrestricted movement across
    environments, which increases the risk of breaches and non-compliance with industry
    regulations. To secure cloud native environments effectively, organizations must
    implement rigorous access reviews and enforce strict boundary controls to prevent
    unauthorized access and escalation. A common sense approach would be to segregate
    resources within their groupings between environments (i.e., for AWS), having
    a production account containing only production networking resources and no connections
    to non-production or testing environments via any means. The following table outlines
    the risks typically found:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Risk** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Access escalation | Users gain unauthorized access to production systems
    from non-production environments |'
  prefs: []
  type: TYPE_TB
- en: '| Weak security posture | The lack of boundary guardrails results in flat network
    structures, allowing unauthorized movement between environments |'
  prefs: []
  type: TYPE_TB
- en: '| Increased attack surface | Poorly defined boundaries create vulnerabilities,
    enabling attackers to move laterally within the network |'
  prefs: []
  type: TYPE_TB
- en: '| Compliance violations | Inadequate control and oversight can lead to non-compliance
    with security and regulatory standards |'
  prefs: []
  type: TYPE_TB
- en: '| Operational risks | Overlapping or misconfigured access can cause outages,
    service disruptions, and, importantly, break compliance measures |'
  prefs: []
  type: TYPE_TB
- en: Table 9.4 - Key risks of inadequate network access reviews and missing guardrails
  prefs: []
  type: TYPE_NORMAL
- en: Organizations can better protect their cloud infrastructure by addressing these
    issues through consistent access reviews and robust boundary guardrails, ensuring
    secure and compliant operations. To better deliver the previously mentioned IaC
    and automation, they are key.
  prefs: []
  type: TYPE_NORMAL
- en: IaC and automation – the networking perspective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the heart of every cloud native organization is IaC. The specific tool you
    choose (Terraform, CloudFormation, or Azure Resource Manager) matters less than
    how you design and implement it. Every IaC tool is both terrible and terrific,
    but what truly defines a successful approach is the architecture and best practices
    behind its use. Standardization is critical to efficient infrastructure deployment
    across cloud native environments. This is especially true for cloud networking,
    where consistency is crucial for managing multiple environments, such as development,
    testing, and production.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without proper standardization and best practices, cloud infrastructure can
    quickly become chaotic. Different teams may deploy similar resources in various
    ways, leading to inefficiencies, inconsistencies, and unnecessary complexity.
    The result is a system that becomes difficult to manage and prone to errors. Standardization
    is not just about keeping things tidy; it’s about ensuring that every deployment
    follows a predictable, efficient pattern that can be repeated and scaled. So,
    what does effective standardization and best practice look like? Consider the
    following best practices:'
  prefs: []
  type: TYPE_NORMAL
- en: '`{environment}-{app}-{resource}-{name}` – for example, `prod-banking-aks-node`
    – helps maintain clarity and avoids confusion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Repeatable patterns**: For repeated deployments, such as setting up a VNet
    in Azure across multiple environments, use reusable modules (e.g., Terraform modules
    or CDK functions). This ensures consistency in how infrastructure is deployed
    and makes management easier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plan before you deploy**: Especially with networking resources, ensure that
    infrastructure changes can be made without disrupting running environments. Some
    changes, such as renaming resources, can trigger replacements that may bring down
    critical systems during change windows, and idempotency safeguards against such
    risks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Version control**: Use Git to store and manage your IaC. Version control
    allows for easy tracking of changes and rollbacks and better collaboration across
    teams, ensuring that infrastructure deployments remain consistent and traceable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated deployment pipelines**: Implement CI/CD pipelines for infrastructure
    deployment rather than relying on manual, localized processes. This reduces human
    error, ensures consistency, and allows for better integration with version control
    systems for streamlined management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By adhering to these principles, organizations can bring order to the complexities
    of cloud deployments, ensuring that infrastructure is scalable, maintainable,
    and efficient. Standardization isn’t just a best practice; it’s the foundation
    for long-term success in the cloud. The following figure provides a simple example
    of what an automated and standardized pipeline looks like when deploying with
    CI/CD:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22364_09_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 - Simple IaC change, check, and deployment pipeline
  prefs: []
  type: TYPE_NORMAL
- en: In a well-automated, IaC-driven, cloud native network, changes to routing rules
    or security policies are scripted, version-controlled, and deployed uniformly
    across environments. This ensures that every environment, whether development,
    testing, or production, has consistent network configurations, reducing the risk
    of miscommunication between services and ensuring tight security controls. Conversely,
    in environments where networking is managed manually, any change is subject to
    human error, creating discrepancies across environments that can lead to outages
    or data breaches.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the risk of misconfiguration, neglecting automation in networking slows
    down an organization’s ability to scale. Cloud native environments demand agility,
    and without automated network deployments, provisioning new environments or scaling
    existing ones becomes a time-consuming, error-prone task. Teams are forced to
    replicate network configurations manually, often introducing inconsistencies that
    can cause service disruptions.
  prefs: []
  type: TYPE_NORMAL
- en: Zero Trust application patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As organizations transition from on-premises environments to cloud native architectures,
    the **Zero Trust** model is one of the most crucial security shifts they must
    adopt. In traditional on-premises environments, security often hinged on perimeter
    defenses; if you were inside the network, you were trusted. However, cloud native
    applications operate in a more dynamic, distributed, and potentially exposed environment.
    The concept of a transparent network boundary dissolves in the cloud, where services
    span regions, multiple VPCs, and often different cloud providers. This is where
    Zero Trust emerges as an essential security framework, built on the premise of
    “*never trust,* *always verify*.”
  prefs: []
  type: TYPE_NORMAL
- en: In its simplest terms, Zero Trust rejects the notion of implicit trust based
    on location or ownership of a network. Instead, it assumes that every user, device,
    and application must continuously prove its legitimacy before accessing resources.
    The core principles of Zero Trust dictate that security should not only focus
    on external threats but also on monitoring and controlling access within the network,
    preventing unauthorized lateral movement, and reducing the attack surface. This
    is particularly relevant in cloud native environments, where the dynamic nature
    of workloads and users necessitates constant verification at every access point.
  prefs: []
  type: TYPE_NORMAL
- en: Zero Trust in cloud native versus on-premises environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In traditional, on-premises setups, applications typically relied on network
    segmentation and firewalls to define security zones, sometimes called DMZs. If
    an application or user was inside the corporate network, they were often granted
    broad access to resources with little scrutiny. This approach, known as **implicit
    trust**, leaves significant room for error. Once an attacker gains access to the
    network, they can move laterally between systems without facing substantial barriers.
    On-premises security models have often prioritized keeping threats out rather
    than scrutinizing every internal interaction.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, cloud native environments treat every component as an untrusted
    entity, whether it’s an internal microservice, user, or external client. For cloud
    native applications, the Zero Trust model aligns more naturally with the distributed
    nature of cloud services, where there are no well-defined internal and external
    perimeters. Applications must verify every request, whether it’s between internal
    microservices, API calls, or user access.
  prefs: []
  type: TYPE_NORMAL
- en: Consider AWS and its implementation of the principle of least privilege. At
    its core, this principle aligns with Zero Trust by ensuring that users and services
    are granted only the permissions they need to perform their tasks and nothing
    more. This means leveraging services such as AWS **Identity and Access Management**
    (**IAM**), where tightly scoped policies control every action. No service or user
    is inherently trusted within a single account or VPC. Each action must be authenticated
    and authorized, minimizing the risk of privilege escalation or misuse.
  prefs: []
  type: TYPE_NORMAL
- en: In Azure, Conditional Access policies and **Azure Active Directory** (**AAD**)
    take on a similar role, verifying each access request based on dynamic conditions,
    such as user location, device health, and behavioral analytics. Access is granted
    only when these factors align with predefined security policies. Meanwhile, Azure
    VNet and **network security groups** (**NSGs**) enable granular segmentation of
    traffic, ensuring that applications and services are isolated and access is controlled
    based on tightly defined security rules.
  prefs: []
  type: TYPE_NORMAL
- en: In GCP, the BeyondCorp model operationalizes Zero Trust by completely removing
    implicit trust from the equation. Google Cloud’s **Identity-Aware Proxy** (**IAP**)
    ensures that each request to an application is authenticated, authorized, and
    encrypted based on user and device identity. No traffic is assumed trustworthy
    simply because it originates from a particular part of the network.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Principle** | **Description** | **Cloud native** **Application Example**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Never trust, always verify | Every request must be authenticated, authorized,
    and encrypted, regardless of origin. No part of the network is trusted by default,
    and access is continuously verified. | Validating API requests, user logins, and
    inter-service communication using AWS IAM policies and AWS Secrets Manager for
    secure communication |'
  prefs: []
  type: TYPE_TB
- en: '| Least privilege | Enforces minimal access, granting users and services only
    the permissions needed to perform their tasks, and nothing more. This limits the
    potential damage from a compromised account or service. | Azure **Role-Based Access**
    **Control** (**RBAC**)ensures least privilege access for cloud native applications,
    keeping internal systems isolated |'
  prefs: []
  type: TYPE_TB
- en: '| Microsegmentation | It breaks down networks into secure segments to limit
    lateral movement. In cloud native environments, this is achieved with virtual
    network constructs, which isolate workloads by default. | AWS VPCs, Azure VNets,
    and Google Cloud VPCs isolate resources, while security groups and NSGs control
    authorized traffic |'
  prefs: []
  type: TYPE_TB
- en: '| Continuous monitoring and auditing | Monitors and audits all interactions
    within the environment in real time to detect anomalies, respond to threats |
    AWS CloudTrail, Azure Monitor, and Google Cloud Operations Suite provide real-time
    insights into access patterns |'
  prefs: []
  type: TYPE_TB
- en: Table 9.5 - Key principles of Zero Trust
  prefs: []
  type: TYPE_NORMAL
- en: Zero Trust in practice – a cloud native example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During a consulting engagement with a financial services company, we were tasked
    with implementing a Zero Trust architecture for a cloud native microservice-based
    application deployed across multiple AWS AZs. Each microservice was deployed as
    an AWS Lambda function, with API Gateway serving as the communication layer between
    services. To ensure robust security, we implemented IAM-based authorization for
    each service call using AWS Signature Version 4 signing, which adds authentication
    details to HTTP requests. This method ensured that access to each API was tightly
    controlled, limiting communication strictly to authorized IAM roles.
  prefs: []
  type: TYPE_NORMAL
- en: We leveraged Amazon Cognito to enforce identity verification for user access,
    applying fine-grained permissions to regulate access to specific data and application
    functions. Additionally, network traffic between the production and staging environments
    was isolated using separate VPCs, preventing direct communication without explicit
    authorization. Real-time monitoring through CloudWatch Logs and VPC Flow Logs
    allowed us to track network activity and quickly flag any unauthorized access
    attempts. Finally, to ensure microsegmentation, we used PrivateLink and VPC gateway
    endpoints for client access. This comprehensive approach ensured that all interactions
    within the system were authenticated, authorized, and monitored, adhering to the
    Zero Trust principles that are critical in cloud native architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22364_09_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 - Example of Zero Trust application in AWS
  prefs: []
  type: TYPE_NORMAL
- en: In this Zero Trust framework, the application is not only secure but also adaptable
    and able to scale or deploy new services without compromising its security posture.
    This approach contrasts sharply with on-premises models, where trust is often
    assumed within the network, creating vulnerabilities once an attacker breaches
    the perimeter.
  prefs: []
  type: TYPE_NORMAL
- en: As cloud native architectures grow in complexity and scale, adopting a Zero
    Trust application pattern is no longer optional, it’s a necessity. By ensuring
    that no user, service, or device is trusted by default and that every interaction
    is authenticated and authorized, organizations can safeguard their cloud infrastructure
    against evolving threats. The Zero Trust model, supported by cloud native tools
    across AWS, Azure, and GCP, helps protect the distributed and dynamic nature of
    modern applications, ensuring security without compromising the agility and innovation
    that the cloud offers. The next section goes beyond Zero Trust and looks at balancing
    the trade-offs within cloud native.
  prefs: []
  type: TYPE_NORMAL
- en: Network defense in depth versus flat networks – balancing security and operability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The debate between network defense in depth and flat networks is critical. When
    trade-offs are not adequately weighed, they often reveal an anti-pattern in architectural
    design. On the one hand, defense in depth (a layered approach to security) prioritizes
    protecting resources at multiple levels, from firewalls and network segmentation
    to access controls and encryption. On the other hand, flat networks, which offer
    minimal segmentation and simpler connectivity, can enhance operability by reducing
    complexity and streamlining communication between services.
  prefs: []
  type: TYPE_NORMAL
- en: Defense in depth is a tried-and-true security model that applies multiple layers
    of protection to cloud native environments. By segmenting workloads across VPCs
    in AWS, Azure **virtual networks** (**VNets**), or Google Cloud VPCs, services
    are logically separated and protected by strict security groups, firewalls, and
    access control policies. This model ensures that even if an attacker breaches
    one layer, additional barriers, such as Azure NSGs, Google Cloud firewall rules,
    or AWS security groups, can prevent lateral movement and further compromise. While
    this layered approach strengthens security, it also increases operational overhead.
    However, the trade-off comes in the form of increased complexity. More segmentation
    means more configuration, potential points of failure, and a more significant
    operational overhead when managing policies across various layers.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, flat networks, which provide minimal segmentation between services,
    simplify the operational burden. In a flat network, communication is less restricted,
    making deploying and scaling services easier. The ease of connectivity reduces
    friction during development and deployment cycles, as developers do not need to
    navigate a web of security layers and access rules. However, while flat networks
    may enhance speed and flexibility, they sacrifice security. With fewer barriers
    between services, an attacker who gains access to any part of the network may
    move laterally with minimal resistance, potentially compromising the entire system.
  prefs: []
  type: TYPE_NORMAL
- en: The key to choosing between network defense in depth and flat networks lies
    in evaluating the organization’s specific needs and the criticality of the data
    and services being managed. Security versus operability is not a binary decision
    but a balancing act. Critical applications may benefit from more stringent security
    measures in some cases, while less sensitive services may tolerate flatter, more
    operationally efficient architectures.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when we were tasked with building microservices across an EKS cluster
    in a cloud native environment handling financial transactions, defense in depth
    was likely the best approach, ensuring that each microservice handling sensitive
    data was tightly secured and isolated. Beyond just the regular AWS tooling, to
    ensure each call was secure, we implemented a service mesh for Mutual TLS and
    Open Policy Agent to refine-grained access policies. Again, the trade-offs between
    security and operability must always be considered, with the understanding that
    flexibility in cloud native environments should never come at the expense of security
    where it truly matters. As any company that handles financial transactions needs
    to comply with PCI-DSS and other compliance standards, we ensured that, at all
    layers of the implementation, best practices have been applied.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ignoring fundamental aspects such as latency and bandwidth can lead to significant
    performance bottlenecks, while a lack of a DNS strategy introduces operational
    inefficiencies and inconsistency in service discovery. Relying on monolithic connectivity
    creates a fragile network structure that is difficult to scale and secure, whereas
    ignoring cloud native networking features overlooks the built-in capabilities
    designed to optimize and secure modern infrastructures. Finally, failing to adopt
    Zero Trust application patterns leaves cloud environments vulnerable, as traditional
    perimeter-based security is insufficient for the dynamic, distributed nature of
    cloud native systems. To build resilient, scalable, and secure cloud native applications,
    it is essential to address these anti-patterns head-on, ensuring that network
    architectures are designed with the unique demands of the cloud in mind.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will go over how to approach observability within the cloud
    native space.
  prefs: []
  type: TYPE_NORMAL
