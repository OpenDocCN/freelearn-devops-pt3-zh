<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer185">
			<h1 id="_idParaDest-188"><em class="italic"><a id="_idTextAnchor189"/></em><a href="B17124_08_Final_SK_ePub.xhtml#_idTextAnchor189"><em class="italic">Chapter 8</em></a>: AWS Database Services</h1>
			<p>Most applications need to store, access, organize, and manipulate data in some way. Normally, the data would reside externally to the actual application in what we call a <em class="italic">database</em> for several reasons, including efficiency improvements. Databases are designed to do more than simply store data, however. Depending on the type of database, data can be organized and stored in a structured or semi-structured manner, offer high-speed access to the data, and give you the ability to perform queries and scans against the data. Data can also be combined from different <em class="italic">tables</em> within the database to help you create complex analytics and reporting. Typical examples of where you would use a database include storing customer records and their orders for your e-commerce website, storing a product listing catalog, and storing temperature information from your home IoT thermostat devices. AWS offers three primary database solutions and several others for specific application types. </p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Managed databases versus unmanaged databases</li>
				<li>Introduction to database concepts and models</li>
				<li>Introduction to <strong class="bold">Amazon Relational Database Service</strong> (<strong class="bold">Amazon RDS</strong>)</li>
				<li>Learning about Amazon DynamoDB (aNoSQL database solution)</li>
				<li>Understand the use cases for Amazon Redshift and data warehousing</li>
				<li>Understanding the importance of in-memory caching options with Amazon Elasticache</li>
				<li>Learning about additional database services for specific niche requirements</li>
				<li><strong class="bold">Database Migration Service</strong> (<strong class="bold">DMS</strong>)</li>
			</ul>
			<p>In this chapter, you will learn about the various managed databases solutions offered by AWS and launch your very first Amazon Relational Database service running the MySQL engine. Later in this book, we will configure a database to store data that's been uploaded via a web application.</p>
			<h1 id="_idParaDest-189"><a id="_idTextAnchor190"/>Technical requirements</h1>
			<p>To complete the exercises in this chapter, you will need to access your AWS account and be logged in as our fictitious administrator, <strong class="bold">Alice</strong>, using her IAM user credentials.</p>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor191"/>Managed databases versus unmanaged databases</h1>
			<p>Traditionally, in an on-premises setup, you would configure a server (physical or virtual) with a base operating system and then proceed to install the database software on it. Because <a id="_idIndexMarker797"/>the primary purpose of a database is to store data, you would also need to ensure that you had <a id="_idIndexMarker798"/>adequate storage attached to your server. Due to the importance of the data store, you would take additional security measures to protect the data and ensure you had adequate backups and copies of the data (ideally stored offsite in another location) in case of disasters.</p>
			<p>On AWS, you <a id="_idIndexMarker799"/>can set up an <strong class="bold">Elastic Compute Cloud</strong> (<strong class="bold">EC2</strong>) instance and install your database, such as <strong class="bold">Microsoft SQL Server</strong> or <strong class="bold">Oracle</strong>, in the same manner to serve your frontend web and application servers as required. In this case, you take full ownership of managing the database, provisioning the required <a id="_idIndexMarker800"/>amount of <strong class="bold">Elastic Block Store</strong> (<strong class="bold">EBS</strong>) volumes for storage, and ensuring adequate backups are made. You also need to design for high availability and performance.  </p>
			<p>Alternatively, AWS also offers <strong class="bold">managed database solutions</strong>. AWS takes care of provisioning your database instances, where you specify certain parameters to ensure the required capacity for your application. AWS will also provision and manage the required storage for your database, as well as perform all backups and replications as required. Ultimately, you get a fully managed solution where AWS takes care of almost every configuration option you choose, except for ensuring that your application is optimized for the chosen database soluti<a id="_idTextAnchor192"/>on.</p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor193"/>Learning about additional database services for specific niche requirements</h2>
			<p>In this section, we introduced you to the concept of unmanaged databases and how traditionally, we <a id="_idIndexMarker801"/>would have to install our database software on a physical or virtual server. However, hosting a database on a server carries additional administrative efforts. While on AWS, you can install a database on an EC2 instance, it makes more sense to consider using AWS managed <a id="_idIndexMarker802"/>database offerings such as <strong class="bold">Amazon RDS</strong> as this reduces the management burden on the customer. In the next section, we will introduce you to database concepts and models.</p>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor194"/>Introduction to database concepts and models</h1>
			<p>Today, there are several types of database models, but the most common are <em class="italic">relational</em> and <em class="italic">non-relational</em> models. Relational databases have existed for years and allow you to efficiently <a id="_idIndexMarker803"/>manage your data with the ability to perform complex queries and analyses. However, they have certain restrictions, such as the fact that you need to define the database schema (its structure) before you can add data, and changing this later can be difficult. Non-relational databases offer a lot more flexibility and are used for many modern-day web and mobile applications. Let's look at the key differences.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor195"/>Relational databases</h2>
			<p>A <strong class="bold">relational database</strong> is often compared to a spreadsheet, although databases offer more capabilities <a id="_idIndexMarker804"/>than just letting you analyze data using complex <a id="_idIndexMarker805"/>calculations and formulas. Like a spreadsheet, a relational database can be composed of one or more <strong class="source-inline">tables</strong>. Within each table, you have rows and columns – columns define <strong class="source-inline">attributes</strong> for your data and rows contain individual <strong class="source-inline">records</strong> in the database. Each row in your table then contains data that relates to the attributes that were defined in the columns. So, for example, in a customer's table, you can have columns such as <strong class="source-inline">First Name</strong> and <strong class="source-inline">Last Name</strong>, then your rows will contain data related to those columns comprised of your customer's first and last names. </p>
			<p>Another important factor to consider with <strong class="bold">relational databases</strong> is the need to define your database schema before you add data to the database. For example, if you have a column called <strong class="source-inline">First Name</strong> and a second column called <strong class="source-inline">Date of Birth</strong>, then you need to define the type of data you will permit in each of those columns prior to adding any data; for the <strong class="source-inline">First Name</strong> column, the type of data will be <strong class="source-inline">string</strong>, whereas for the <strong class="source-inline">Date of Birth</strong> column, you will define the type of data as <strong class="source-inline">date</strong>.</p>
			<p>An important column (<strong class="source-inline">attribute</strong>) that must exist in a relational database is the <strong class="source-inline">Primary Key</strong> field. Each record must have a primary key that is unique across the whole table. This ensures that <a id="_idIndexMarker806"/>each record within the table is unique, allowing <a id="_idIndexMarker807"/>you to easily query specific records in the table. As shown in the following table, the customer records table has a primary key called <strong class="source-inline">CustomerID</strong>:</p>
			<div>
				<div id="_idContainer167" class="IMG---Figure">
					<img src="Images/B17124_08_01.jpg" alt="Figure 8.1 – Customer contact table&#13;&#10;" width="1184" height="229"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Customer contact table</p>
			<p>A database can also host multiple tables for specific record sets. Rather than storing all the records within a single table, you can have separate tables for related data. So, for example, in one table, you can have your <em class="italic">customer contact</em> details and in another, you can have your <em class="italic">customer order</em> details. In most cases, the tables will have some relationship with other tables in the same database. In this example, the tables are related to specific customers; one for their contact details and another for their orders. </p>
			<p>The purpose of separating different sets of data into separate tables is to allow for better management, performance, and to avoid duplicate data. For example, if you have a single table to host customers' contact details as well as their orders, then you would have multiple records relisting the customers' contact information for every single order they placed. By separating the orders from the contact details into separate tables, we can avoid this duplication of data and improve performance. The following is an example of a customers' orders table:</p>
			<div>
				<div id="_idContainer168" class="IMG---Figure">
					<img src="Images/B17124_08_02.jpg" alt="Figure 8.2 – Customer order table&#13;&#10;" width="657" height="206"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – Customer order table</p>
			<p>In the preceding two tables, note that we avoided replicating the data by separating the customer contact details table from the orders table. If we have all the data in one table, then we would have multiple columns with the same pieces of information repeated, such as First Name and Last Name for every order placed by the same customer. </p>
			<p>The tables in a database <a id="_idIndexMarker808"/>can relate to each other, and we need some <a id="_idIndexMarker809"/>form of connection between the two tables to effectively structure the data. In the previous example, rather than repeating all customer address details in the orders table, we simply include the <strong class="source-inline">Customer-ID</strong> column, where we identify which customer the order relates to. Remember that the <strong class="source-inline">Customer-ID</strong> column is the primary key of the <strong class="bold">Customer Contacts Table</strong>, so each Customer ID uniquely identifies a customer.</p>
			<p>Ultimately, we can now query the database by combining data from both columns using the Customer ID as a reference point. We can then produce a report from a query to list all the orders that have been placed by a customer, whose <strong class="source-inline">Customer-ID</strong> is <strong class="source-inline">Cust002</strong>. In this report, we can list the customer contact details, which will be extracted from the first table, and the list of orders placed from the second table, where <strong class="source-inline">Customer-ID</strong> is <strong class="source-inline">Cust002</strong>. This report could then be sent to the customer as a statement of account.</p>
			<p>Relational databases enable you to perform such complex queries, analyses, and reports from large datasets. Performance is directly correlated to the types of queries you need to perform and the volume of data you host. Often, this means that your infrastructure may need to be upgraded from time to time to cope with demanding applications. </p>
			<p>Relational databases use the <strong class="bold">Structured Query Language</strong> (<strong class="bold">SQL</strong>), which is a language <a id="_idIndexMarker810"/>designed to create databases and tables, as well as manage the data within the database using queries. SQL is written out as statements <a id="_idIndexMarker811"/>that you then issue to the database to perform a specific action. For example, the <strong class="source-inline">SELECT</strong> statement enables you to query specific data, whereas the <strong class="source-inline">WhereHERE</strong> statement enables you to restrict your <strong class="source-inline">SELECT</strong> query to match a specific condition. For the <em class="italic">AWS Certified Cloud Practitioner</em> exam, you are not expected to know how to use the SQL language.</p>
			<p>Relational <a id="_idIndexMarker812"/>databases are also known as <strong class="bold">Online Transaction Processing</strong> (<strong class="bold">OLTP</strong>)<strong class="bold"> databases</strong>. <strong class="bold">OLTP databases</strong> are designed <a id="_idIndexMarker813"/>for adding, updating, and deleting small amounts of data in a database regularly. Typical examples include a <em class="italic">customer orders</em> database for an e-commerce website or a student's database for a university.</p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor196"/>Non-relational (NoSQL) databases </h2>
			<p>With relational databases, you store data in a structured format of a defined schema in tables. Each column <a id="_idIndexMarker814"/>of a table (known as an attribute) will only hold one type of data and this needs to be predefined. You usually <a id="_idIndexMarker815"/>query multiple tables of related data and combining queries across your tables can yield required pieces of information.</p>
			<p>However, the problem with relational databases is the lack of flexibility since data needs to be structured. Furthermore, the more tables you have across your database, the more complex the queries tend to be, and the more resources are required to run and manage the databases. Relational databases also do not lend themselves well when trying to perform thousands of reads and writes to the database per second.</p>
			<p>In contrast, non-relational databases do not follow the traditional relational approach to storing data. Non-relational database data is stored using different models, depending on the type of data being stored. These are as follows:</p>
			<ul>
				<li><strong class="bold">Key-value stores</strong>: This is a <a id="_idIndexMarker816"/>collection of key-value pairs contained within an object.</li>
				<li><strong class="bold">Document data stores</strong>: This is <a id="_idIndexMarker817"/>typically a <strong class="bold">JavaScript Object Notation</strong> (<strong class="bold">JSON</strong>) format document (although <a id="_idIndexMarker818"/>other formats can be used) that's used to store data in a structured or semi-structured form. Data can be comprised of nested attributes of key-value pairs. All the documents in the store are not required to maintain identical data structures and this offers greater levels of flexibility.</li>
				<li><strong class="bold">Columnar data stores</strong>: Data is organized into cells grouped by columns rather <a id="_idIndexMarker819"/> than by rows. Furthermore, reads and writes are carried out using columns rather than rows. </li>
			</ul>
			<p>There is no requirement to predefine the schema of the database, and this creates a lot of flexibility because you can freely add fields (attributes) to a document without the need to define them first. Therefore, you have documents with different numbers of fields. For example, one document listing a customer's details could include their name, address details, order history, and credit card information, while another document could include a list of their favorite products.</p>
			<p>Non-relational <a id="_idIndexMarker820"/>databases were developed as an alternative to relational databases where the flexibility of the schema was <a id="_idIndexMarker821"/>required, as well as to handle very large data stores that required thousands of reads/writes per second, something that relational databases have traditionally found difficult to do. Non-relational databases can cope with this kind of load because a query does not have to view several related tables to extract the results. Furthermore, a non-relational database can handle frequent changes to the data.  </p>
			<p>Like relational databases, though, non-relational databases do require you to have at least one primary key field (attribute) and this is the only attribute required. Beyond this, your database tables are effectively schemaless. The primary key is used to ensure that each record of the database is unique. </p>
			<p>In this section, we reviewed the primary differences between relational and non-relational databases. We looked at the use cases for both types of database solutions and compared the key differences between the two. On AWS, both relational databases and non-relational databases are offered. In the next section, we will look at the services that are offered in detail.</p>
			<h1 id="_idParaDest-195"><a id="_idTextAnchor197"/>Introduction to Amazon RDS</h1>
			<p><strong class="bold">Amazon RDS</strong> offers traditional relational databases <a id="_idIndexMarker822"/>as fully managed <a id="_idIndexMarker823"/>services on the AWS platform. Ideal for transactional database requirements, also known as <strong class="bold">OLTP</strong>, AWS offers six different database engines, as follows:</p>
			<ul>
				<li>MySQL</li>
				<li>PostgreSQL</li>
				<li>MariaDB</li>
				<li>Microsoft SQL server</li>
				<li>Oracle</li>
				<li>Amazon Aurora</li>
			</ul>
			<p>Another <a id="_idIndexMarker824"/>term you might have heard of is <strong class="bold">Relational Database Management System</strong> (<strong class="bold">RDBMS</strong>). An RDBMS performs functions to <strong class="bold">create, read, update, and delete</strong> (<strong class="bold">CRUD</strong>) data from the database <a id="_idIndexMarker825"/>using an underlying software component, which we call the database engine.</p>
			<p>An important point to understand here is that when you choose to set up an Amazon RDS database, you are setting up a <em class="italic">database instance</em> with a chosen engine to run on that instance. You can then create one or more databases supported by that engine on your database instance. This means you can have several databases running on an individual database instance. </p>
			<p>Furthermore, on Amazon RDS, when you set up a database instance, you specify hardware capabilities in the form of CPU and memory allocation. The type of instance will also determine the maximum storage bandwidth and network performance that the instance can offer. AWS offers three different types of instance classes with varying virtual hardware specifications and is designed for various uses cases. These are as follows:</p>
			<ul>
				<li><strong class="bold">Standard classes (includes m classes</strong>): These classes offer a balance of compute, memory, and <a id="_idIndexMarker826"/>network resources, and they are ideal for most application requirements. Standard classes offer the following specs:<ul><li>Between 2 and 96 vCPUs</li><li>Up to 384 GB of memory</li></ul></li>
				<li><strong class="bold">Memory-optimized classes (includes r and x classes)</strong>: These classes are ideal for <a id="_idIndexMarker827"/>most demanding applications that require greater levels of memory and are optimized for memory-intensive applications. Memory-optimized classes offer the following specs:<ul><li>Between 4 and 128 vCPUs</li><li>Up to 3,904 GB of memory</li></ul></li>
				<li><strong class="bold">Burstable classes (includes t classes)</strong>: These classes are designed for nonproduction databases and provide a baseline performance level, with the ability to <a id="_idIndexMarker828"/>burst to full CPU usage. Burstable classes are ideal for database workloads with moderate CPU usage that experience occasional spikes. Burstable classes offer the following specs:<ul><li>Between 1 and 8 vCPUs</li><li>Up to 32 GB of memory</li></ul></li>
			</ul>
			<p>The following screenshot shows the different <strong class="bold">DB instance class</strong> options you can select from:</p>
			<div>
				<div id="_idContainer169" class="IMG---Figure">
					<img src="Images/B17124_08_03.jpg" alt="Figure 8.3 – Database instance class options&#13;&#10;" width="898" height="336"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – Database instance class options</p>
			<p>In addition <a id="_idIndexMarker829"/>to the compute resource, Amazon RDS also requires storage capabilities to host all the required data. The storage <a id="_idIndexMarker830"/>platform runs on Amazon <strong class="bold">EBS</strong>, so it is decoupled from the actual database instance class. This allows you to upgrade the storage volumes without necessarily having to upgrade the instance class and vice versa, so long as compatibility is maintained. The volume's throughput <a id="_idIndexMarker831"/>is determined by the instance types chosen, as well as the <strong class="bold">input/output operations per second</strong> (<strong class="bold">IOPS</strong>) that the EBS volume supports. AWS offers the following different storage options for your databases:</p>
			<ul>
				<li><strong class="bold">General Purpose SSD</strong>: Designed for standard workloads and ideal for most <a id="_idIndexMarker832"/>databases, General Purpose SSD volumes offer between 20 GiB to 64 TiB of storage data for MariaDB, MySQL, PostgreSQL, and Oracle databases, and between 20 GiB to 16 TiB for Microsoft SQL Server.<p>The number of IOPS that's achieved is dependent on the size of the storage volume, with a baseline I/O performance of 3 IOPS per GiB (minimum 100 IOPS). The larger the volume size, the higher the performance; for example, a 60 GiB volume would give you 180 IOPS. </p><p>General Purpose SSDs also offer bursts in performance for volumes less than 1 TiB in size for extended periods. This means that smaller volumes will get an additional <a id="_idIndexMarker833"/>performance boost when required and you do not need to allocate unnecessary storage for short-term occasional spikes. Bursting is not relevant for volume sizes above 1 TiB, however.</p></li>
				<li><strong class="bold">Provisioned IOPS SSD</strong>: AWS recommends using Provisioned IOPS SSDs for production <a id="_idIndexMarker834"/>applications that require fast and consistent I/O performance. With Provisioned IOPS SSDs, you specify the IOPS rate and the size of the volume. Like General Purpose SSDs, you can allocate up to 64 TiB of storage, depending on the underlying database engine you use. Provisioned IOPS SSD does not offer any bursting, however.</li>
				<li><strong class="bold">Magnetic</strong>: AWS also <a id="_idIndexMarker835"/>offers magnetic storage volumes for backward compatibility. They are not recommended for any production environments and are limited to 1,000 IOPS and up to 3 TiB of storage.</li>
			</ul>
			<p>Ultimately, you can upgrade your storage if required, but this will usually require a short outage, typically of a few minutes (for Magnetic, this can take much longer), so this must be planned for.</p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor198"/>Deploying in Amazon VPCs</h2>
			<p>Amazon RDS is a regional service, which means that you need to select the Region you want to deploy <a id="_idIndexMarker836"/>your database instance in first. Amazon RDS database instances can only be deployed in a VPC, and like EC2 instances, in a specified subnet. Since a subnet is always only associated with a single Availability Zone, this also means that if there is an Availability Zone failure, your database will not be accessible. With <a id="_idIndexMarker837"/>Amazon RDS, you can only deploy a <strong class="bold">single master database instance</strong>. This type of instance can perform both read and write operations to the database. Amazon does offer various solutions in case the master database instance fails and we look at these options later in this chapter. </p>
			<p>Deploying your RDS database in a VPC gives granular control over how the databases is going to be accessed and allows you to configure various network security components such as the private IP addressing range you will use, security groups to protect your RDS instance, and <strong class="bold">Network Access Control Lists</strong> (<strong class="bold">NACLs</strong>) to protect traffic in the subnet <a id="_idIndexMarker838"/>that will host the database.</p>
			<p>Deploying <a id="_idIndexMarker839"/>your RDS database in a VPC also means that you can configure various architectures from which to access that database. The following are some scenarios that you could configure:</p>
			<ul>
				<li><strong class="bold">A DB instance in a VPC that's accessed by an EC2 instance in the same VPC</strong>: In this configuration, you would need to configure the security group associated with the RDS database to accept inbound traffic from the security group associated with the EC2 instance on the relevant database port; for example, port <strong class="source-inline">3306</strong> for a MySQL RDS instance.</li>
				<li><strong class="bold">A DB instance in a VPC that's accessed by an EC2 instance in a different VPC</strong>: In <a href="B17124_06_Final_SK_ePub.xhtml#_idTextAnchor122"><em class="italic">Chapter 6</em></a>, <em class="italic">AWS Networking Services, VPCs, Route53, and CloudFront</em>, we discussed that you could connect two VPCs using a service known as VPC peering. Instances in either VPC can then communicate with each other over that peering connection using private IP addresses as though they were within the same network. Once the peering connection has been established, you would then need to configure the necessary rules for your security groups to enable traffic to flow between the EC2 instance and your RDS database. VPC peering can allow you to peer connections between VPCs in the same Region, cross-Region, and even across AWS accounts.</li>
				<li><strong class="bold">A DB instance in a VPC that's accessed by a client application through the internet</strong>: While nothing is stopping you from placing your RDS database instance in a public subnet of a VPC, this is not considered a best practice for production environments. Databases are considered backend services that contain critical and maybe sensitive information. They should always be placed in the private subnet of the VPC. Placing your RDS database in a public subnet should only really be done for testing purposes or specific use cases. </li>
				<li><strong class="bold">A DB instance in a VPC that's accessed by a private network</strong>: With a VPC in place, you can set up a VPN tunnel or a Direct Connect service between your on-premises network and the VPC. This allows you to place your RDS database in a private subnet of the VPC, and still be able to access it from your corporate offices via the VPN tunnel or Direct Connect service. </li>
			</ul>
			<p>For production environments, always place your RDS database instances in the private subnet(s) of your VPC. The architecture shown in the following diagram illustrates one such best practice methodology. Here, the RDS database has deployed a private subnet, dubbed a <strong class="bold">database subnet</strong>. To access the database via a standard web application, traffic <a id="_idIndexMarker840"/>from the internet is routed via an <strong class="bold">Elastic Load Balancer</strong> (<strong class="bold">ELB</strong>) (discussed in detail in <a href="B17124_09_Final_SK_ePub.xhtml#_idTextAnchor223"><em class="italic">Chapter 9</em></a>, <em class="italic">High Availability and Elasticity on AWS</em>) and distributed to web servers placed in another set of private subnets within your VPC. Your web servers then connect to the RDS database to perform any data <a id="_idIndexMarker841"/>operations such as adding, updating, or deleting records as required by the application. Traffic can flow based on the rules that have <a id="_idIndexMarker842"/>been defined by your <strong class="bold">Network Access Control Lists</strong> (<strong class="bold">NACLs</strong>) and security groups:</p>
			<div>
				<div id="_idContainer170" class="IMG---Figure">
					<img src="Images/B17124_08_04.jpg" alt="Figure 8.4 – Amazon RDS deployed in a VPC in private subnets&#13;&#10;" width="1140" height="786"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.4 – Amazon RDS deployed in a VPC in private subnets</p>
			<p>In the preceding diagram, users from the internet can access the database via the web/app server <a id="_idIndexMarker843"/>rather than have direct access to the database. Users will connect to the web/app servers via an <strong class="bold">Application Load Balancer</strong> (<strong class="bold">ALB</strong>) (which distributes traffic among healthy EC2 instances in the fleet). The web/app server <a id="_idIndexMarker844"/>will have a process in place to send database operations requests to the RDS database in the backend private subnet.  </p>
			<p>Note that the RDS database will only accept traffic from the EC2 instances that are attached to the appropriate security groups. This ensures that if the EC2 instances are replaced or if additional EC2 instances are attached to the fleet, and if they are attached to the same security sroup, they will be able to communicate with the RDS database.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Your applications connect to the backend database via an RDS DNS endpoint name, rather than the database instance's specific IP addresses. This allows you to easily manage failovers in the event of a disaster, which we will discuss next in this chapter. </p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor199"/>Backup and recovery</h2>
			<p>Your database is going to be very important to you and ensuring that you can recover from failures, data loss, or even data corruption is going to be an important factor in your design <a id="_idIndexMarker845"/>architecture. AWS enables you to address your disaster recovery and business continuity concerns, and Amazon RDS comes with several configuration options to choose from.</p>
			<h3>What are your RPO and RTO requirements?</h3>
			<p>When deciding on a strategy to protect your database on AWS from unexpected failures or data <a id="_idIndexMarker846"/>corruption, you need to consider <a id="_idIndexMarker847"/>what configuration options are going to meet your organization's expectations for recovery. If your business hosts critical data <a id="_idIndexMarker848"/>that needs to be recovered fast in the event of a failure, you need to design an architecture that will support this requirement. To help <a id="_idIndexMarker849"/>you identify how critical your recovery is going to be, you need to determine your <strong class="bold">Recovery Point Objective</strong> (<strong class="bold">RPO</strong>) and <strong class="bold">Recovery Time Objective</strong> (<strong class="bold">RTO</strong>). These two parameters will help you design a recovery strategy that meets your business requirements:</p>
			<ul>
				<li><strong class="bold">RTO</strong>: This represents the time (in hours) it takes to recover from a disaster and return to a working state. The time taken will involve provisioning a new database instance, performing a restore job, and any other administrative or technical tasks that need to be completed. </li>
				<li><strong class="bold">RPO</strong>: This represents how much data (again, as a measure of time, and generally in hours) you will lose in the event of a disaster. The shorter the RPO, the less data you are likely to lose in the event of a failure.</li>
			</ul>
			<p>If your organization stipulates that it can only afford an RTO of 2 hours and an RPO of 4 hours, this means that you need to recover from failure to a working state within 2 hours and the maximum amount of data you can afford to lose (probably because you can create that data) is 4 hours' worth. </p>
			<p>Based on your RPO and RTO levels, you can then choose a disaster recovery strategy that fits <a id="_idIndexMarker850"/>your requirements. For example, if your <a id="_idIndexMarker851"/>RPO is set to 4 hours and your recovery strategy was based on restoring older backups of your database, then you should be performing a backup of your database every 4 hours.</p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor200"/>High availability with Multi-AZ</h2>
			<p>For Amazon RDS database engines running MariaDB, MySQL, PostgreSQL, Oracle, Microsoft SQL, and Amazon Aurora, AWS offers high availability and failover support using the <strong class="bold">RDS Multi-AZ</strong> solution. Multi-AZ is <a id="_idIndexMarker852"/>an architectural design <a id="_idIndexMarker853"/>pattern where a primary (master) copy of your database is deployed in one Availability Zone and a secondary (standby) copy is deployed in <a id="_idIndexMarker854"/>another Availability Zone. Data is then <strong class="bold">synchronously</strong> replicated from the master copy to the standby copy continuously. Normally, with relational databases, only one database can hold the <em class="italic">master</em> status, meaning that data can be both written to and read from it. In the case of Multi-AZ deployments, this is still true, and the standby copy of the database simply receives all the changes that have been made to the master synchronously. However, you cannot write or read from the standby directly. </p>
			<p>If the master copy of your database fails, then AWS will perform a failover operation to the standby copy of the database. The standby copy will be promoted to become the new master, and the previous master will be terminated and replaced with another standby copy. Replication will then be initiated in the opposite direction. During failover, your application may experience a brief outage (about 2 minutes), but then will be able to reconnect to the database (the standby copy that has been promoted to the new master) and continue operating:</p>
			<div>
				<div id="_idContainer171" class="IMG---Figure">
					<img src="Images/B17124_08_05.jpg" alt="Figure 8.5 – Amazon RDS configured with Multi-AZ&#13;&#10;" width="1143" height="785"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.5 – Amazon RDS configured with Multi-AZ</p>
			<p>Failover can be triggered for several reasons other than Availability Zone outages, including patching <a id="_idIndexMarker855"/>the master database or upgrades of the instance. You can also perform a failover test to ensure that the configuration has been set up correctly by performing a reboot of the master database and requesting a failover operation on reboot.</p>
			<p>With Multi-AZ, you can reduce your RTO and RPO levels drastically. Because existing data has already been replicated to a standby copy in another Availability Zone, failover happens in a matter of minutes and data loss is minimized. </p>
			<p>However, in certain circumstances, Multi-AZ alone as a DR strategy may not be enough. For example, what happens if there is data corruption on the master copy of your database? That corrupted data will be replicated across to the standby too.</p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor201"/>Backup and recovery</h2>
			<p>AWS also offers options to perform regular backups of your database, which you can use to perform <a id="_idIndexMarker856"/>point-in-time restores. AWS offers two options here: <em class="italic">automatic backups</em> and <em class="italic">manual snapshots</em>.</p>
			<h3>Automatic backups</h3>
			<p>AWS offers a fully managed automatic backup service free of cost up to the total size of your database. While <a id="_idIndexMarker857"/>performing automatic backups, the first snapshot that's created will be a full backup; subsequent snapshots will be incremental, ensuring that only changes to the data are backed up. Some additional features of automatic backups include the following:</p>
			<ul>
				<li><strong class="bold">Backup window</strong>: Automatic backups are performed during a predefined window <a id="_idIndexMarker858"/>that is configurable for the customer. The default backup time allocated is 30 minutes but you can change this as well. Furthermore, if the backup requires more time than what's been allotted to the backup window, the backup continues after the window ends, until it finishes.</li>
				<li><strong class="bold">Backup retention</strong>: Amazon RDS offers a retention period of 1 day to 35 days. This means <a id="_idIndexMarker859"/>that you can store up to 35 days of backups that you can restore from. You can also set the backup retention to <strong class="source-inline">0</strong> days, which essentially translates to disabling backup operations. If you disabled automatic backups at the time of launching your RDS instance, you can enable this later by setting the backup retention period to a positive non-zero value.</li>
				<li><strong class="bold">Automatic backups and transactional logs</strong>: When you configure your RDS <a id="_idIndexMarker860"/>database for automatic backups, the backup process runs at the predefined backup window. In addition, AWS will also capture transaction logs (as updates to your DB instance are made) up to the last 5-minute interval, which is uploaded to Amazon S3. AWS can then use the daily backups as well as the transaction logs to restore your DB instance to any second during the retention period, up to <strong class="source-inline">LatestRestorableTime</strong>, which is typically the last 5 minutes.</li>
			</ul>
			<h3>Manual snapshots</h3>
			<p>In addition to automatic backups, you can also create manual snapshots of your database, which can provide additional protection. You can then use manual snapshots to restore your DB instance to a known state as frequently as you like. </p>
			<p>Manual snapshots <a id="_idIndexMarker861"/>can be very useful if you plan to make a major change to your database and would like an additional snapshot before making that change.</p>
			<p>Automatic and manual backups are particularly useful if you need to restore due to data corruption. Remember that even if you have Multi-AZ enabled, any data corruption on the master copy will be replicated across to the standby, and having older backups can enable you to revert your database to a time before that data corruption occurred. </p>
			<h3>Cross-Region snapshots</h3>
			<p>You can copy your snapshots across Regions to improve the availability of your backups even further, in the <a id="_idIndexMarker862"/>event of a regional outage or disaster. You can also configure the replication of your automatic backups and transactional logs across to another Region. Amazon RDS initiates a cross-Region copy of all snapshots and transaction logs as soon as they are ready on the DB instance.</p>
			<h3>I/O suspense issue</h3>
			<p>An important behavior pattern to be aware of is that a brief I/O suspension (usually lasting only a few seconds) is experienced when your backup process initializes. In scenarios <a id="_idIndexMarker863"/>where you have a single Database instance deployed, this results in a brief outage when connecting to the database. This means that if your backup operations are taking place during business hours, then users may experience some interruption during the backup process. </p>
			<p>To work around this problem, you can choose to ensure that your backup processes take place outside of business hours, or better still, follow Amazon's recommendations for deploying a Multi-AZ deployment for database, particularly for MariaDB, MySQL, Oracle, and PostgreSQL engines. This is because, in a Multi-AZ configuration, the backup is taken from the standby copy of the database and not the master. Note that for Microsoft <a id="_idIndexMarker864"/>SQL Server, I/O activity is suspended briefly during backup, even for Multi-AZ deployments.</p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor202"/>Horizontal scaling with read replicas</h2>
			<p>Traditionally, relational databases do not scale well horizontally due to their architecture. A relational <a id="_idIndexMarker865"/>database can normally only have one master copy (the copy that you can write data to), which means that if you experience a failure on the master copy, you need to resort to restoring data from backups. AWS offers Multi-AZ as a means to overcome this single point of failure by enabling you to create a standby copy of the database that has data synchronously replicated to it.</p>
			<p>However, you cannot use a standby copy to perform write or read queries since your standby copy is only accessible in the event of the failure of the master copy. If the master copy of your database fails, your standby copy gets promoted to become the new master copy of the database, upon which you will be reading and writing to it.</p>
			<p>When it comes to scaling horizontally, where you can have multiple nodes of your database, AWS offers <a id="_idIndexMarker866"/>an option to scale read copies of your database using a feature called <strong class="bold">read replicas</strong>. AWS RDS can use its built-in replication functionality for Microsoft SQL, MySQL, Oracle, and PostgreSQL to create additional read replicas of the source DB instance. Data is then replicated from the source DB to the replicas using <em class="italic">asynchronous </em>replication. This can help reduce the load on your master copy by redirecting read queries to the read replicas instead. Application components that only need to read from the database can be routed to send requests to the read replicas, allowing your master copy to focus on thos<a id="_idTextAnchor203"/>e applications that need to write to the database:</p>
			<div>
				<div id="_idContainer172" class="IMG---Figure">
					<img src="Images/B17124_08_06.jpg" alt="Figure 8.6 – AWS RDS with read replicas&#13;&#10;" width="1384" height="815"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.6 – AWS RDS with read replicas</p>
			<p>Read replicas can also be configured for cross-Region replication, as depicted in the preceding diagram. (The exception is for the Microsoft SQL Server engine, which doesn't allow Multi-AZ read replicas or cross-Region read replicas.) This means that you can maintain read copies of your database in a different Region that can be used by other applications that <a id="_idIndexMarker867"/>may only require read access to the data. Storing read replicas across Regions can also help you address any compliance or regulatory needs that stipulate that you need to maintain copies of your data at a considerable distance.</p>
			<p>You can also set the read replica as Multi-AZ, which will enable you to use the read replicas as a DR target. This means that if you ever need to promote the read replica to a standalone database, it will already be configured for Multi-AZ. This feature is available for MySQL, MariaDB, PostgreSQL, and Oracle engines. Finally you can add up to five read replicas to each DB instance.</p>
			<p>Furthermore, in the event of a major disaster, a read replica can also be promoted to become the master copy, after which it becomes independent of the original master copy of the database.  </p>
			<p>In this section, we examined Amazon RDS and the key offerings by its managed RDS. We reviewed the various database engines on offer, concepts related to high availability and scalability, as well as backup and recovery.</p>
			<p>In the next <a id="_idIndexMarker868"/>section, we will briefly look at one AWS RDS offering, specifically Amazon Aurora. While Amazon Aurora is an RDS database solution from AWS, it offers several enhanced capabilities.</p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor204"/>A brief introduction to Amazon Aurora</h2>
			<p>Amazon Aurora is AWS's proprietary MySQL- and PostgreSQL-compatible database solution and was designed for enterprise-grade production environments. Amazon Aurora comes <a id="_idIndexMarker869"/>with a vast array of features that enable you to design your database solution with high availability, scalability, and cost-effective deployments to suit a variety of business needs.</p>
			<p>Amazon Aurora is architected to offer high resilience, with copies of the database placed across a minimum of three Availability Zones, It is up to five times faster than standard MySQL databases and three times faster than standard PostgreSQL databases.</p>
			<p>The service offers <em class="italic">fault tolerance</em> and <em class="italic">self-healing storage capabilities</em> that can scale up to 128 TB per database instance. Amazon Aurora also offers the ability to host up to 15 low latency read replicas. Let's review some of the key features of Amazon Aurora.</p>
			<h3>Amazon Aurora DB clusters</h3>
			<p>Amazon Aurora is <a id="_idIndexMarker870"/>deployed as <strong class="bold">DB clusters</strong> that consist of one or more <strong class="bold">DB instances</strong> and a <em class="italic">cluster volume</em>. This cluster volume spans multiple Availability Zones, within which copies of the cluster data are stored.</p>
			<p>The Aurora DB cluster is made of up two types of DB instances, as follows:</p>
			<ul>
				<li><strong class="bold">Primary DB instance</strong>: This instance supports both read and write operations, and it <a id="_idIndexMarker871"/>performs all of the data modifications to the cluster volume. You have <em class="italic">one</em> Primary DB instance.</li>
				<li><strong class="bold">Aurora Replica</strong>: You can have up to 15 Aurora Replicas in addition to the primary <a id="_idIndexMarker872"/>DB instance. Aurora Replicas connect to the same storage volume as the primary DB instance but are only used for read operations. You use Aurora Replicas as a failover option if the primary DB instance fails. You can also offload read queries from the primary DB instance to the replicas.</li>
			</ul>
			<p>With regard to the architecture, DB instances (compute capacity) and cluster volume (storage) are decoupled, as illustrated in the following diagram.</p>
			<div>
				<div id="_idContainer173" class="IMG---Figure">
					<img src="Images/B17124_08_07.jpg" alt="Figure 8.7 – Amazon Aurora DB cluster architecture&#13;&#10;" width="725" height="429"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.7 – Amazon Aurora DB cluster architecture</p>
			<p>This decoupling <a id="_idIndexMarker873"/>of the computer capacity and storage also means that even a single DB instance is still a cluster due to the fact storage volumes are spread across multiple storage nodes, across multiple Availability Zones.</p>
			<p>With regards to provisioning your DB instances, you have a choice of two instance classes. These are memory optimized (designed for memory-intensive workloads) and burstable performance (which provides a baseline performance level with the ability to burst to full CPU usage).</p>
			<p>While the standard Amazon Aurora deployment seems somewhat similar to deploying an Amazon RDS database, where you choose the compute capacity and underlying storage, AWS also offers a serverless alternative, which we will look at briefly next.</p>
			<h3>Amazon Aurora Serverless</h3>
			<p>Amazon Aurora Serverless (version 1) is an on-demand autoscaling configuration for Amazon Aurora. The DB Cluster automatically scales compute capacity up and down based on your <a id="_idIndexMarker874"/>requirements. The serverless alternative automatically starts up, scales compute capacity to match your application's usage, and shuts down when it's not in use. Furthermore, the cluster volume is always encrypted.</p>
			<p>In terms of use cases, Amazon Aurora Serverless is ideal for applications with unpredictable workloads. Another use case is where you have a lightweight application that experiences peaks for 30 minutes to several hours a few times a day or perhaps at regular intervals throughout the year. Examples include budgeting, accounting, and reporting applications. </p>
			<p>In this section, we were briefly introduced to the Amazon Aurora Service. In the next section, we will examine Amazon DynamoDB, which is AWS's non-relational (NoSQL) database offering.</p>
			<h1 id="_idParaDest-202"><a id="_idTextAnchor205"/>Learning about Amazon DynamoDB (NoSQL database solution)</h1>
			<p>Amazon offers a fully managed non-relational database solution called Amazon DynamoDB. Unlike AWS's relational database offerings (excluding Amazon Aurora, which also <a id="_idIndexMarker875"/>has a serverless offering, as discussed earlier), you do not need to worry about provisioning the right DB instance with the right specification for your application. DynamoDB is offered as a serverless solution because you do not need to define any database instance configuration, such as CPU or memory configuration. Amazon manages the underlying infrastructure that hosts the DynamoDB service. </p>
			<p>DynamoDB is a regional service just like Amazon RDS, but it comes with higher levels of scalability and high availability. You do not need to provision a single DB instance in one Availability Zone as you do with a single instance of an Amazon RDS database. Instead, when you provision a DynamoDB table, Amazon provisions the database and automatically <a id="_idIndexMarker876"/>spreads the data across several servers to handle your throughput and storage requirements. All data is stored on <strong class="bold">solid-state disks</strong> (<strong class="bold">SSDs</strong>) and the underlying storage is replicated across multiple Availability Zones.  </p>
			<p>The architecture of DynamoDB means that it can be used for use cases that are similar Amazon RDS's, although <a id="_idIndexMarker877"/>they are more ideal for applications that can have millions of concurrent users and where the application needs to perform thousands of reads and writes per second.  </p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor206"/>Tables, items, and attributes</h2>
			<p>Let's look at the core components of a DynamoDB database:</p>
			<ul>
				<li><strong class="bold">Tables</strong>: Like Amazon <a id="_idIndexMarker878"/>RDS databases, your data is stored <a id="_idIndexMarker879"/>in tables. So, you can have a customers table that will host information about your customers and their orders. Each table will also have a unique primary key, which is crucial for uniquely identifying every record in the table. Records are known as items in DynamoDB Tables.</li>
				<li><strong class="bold">Items</strong>: Items are <a id="_idIndexMarker880"/>like records in Amazon RDS databases. A table can have <a id="_idIndexMarker881"/>one or more items, and each item will be a unique combination of attributes that define that item. Items can be up to <strong class="source-inline">400 KB</strong> in size and can contain key-value pairs called attributes.</li>
				<li><strong class="bold">Attributes</strong>: An attribute <a id="_idIndexMarker882"/>is like a column heading or <a id="_idIndexMarker883"/>a field in an Amazon RDS database. Attributes help define the items in your table. So, in a Customers Table, the attributes could be <strong class="source-inline">First-Name</strong> or <strong class="source-inline">Last-Name</strong> and so on.</li>
			</ul>
			<p>Unlike Amazon RDS, you do not need to predefine the schema of the table. This offers greater flexibility as your table evolves. Other than the primary key, you can add new attributes and define them to expand the table at will.  </p>
			<p>Furthermore, the items don't have to have a value for all attributes – so, for example, you can have a table that contains customer address details, their orders, and their favorite dessert for a restaurant. Some items may have a value set against the favorite dessert, while some may not, and this is perfectly fine.</p>
			<p>However, attributes need to have data types defined. The following are the options that are available here:</p>
			<ul>
				<li><strong class="bold">Scalar</strong>: This only <a id="_idIndexMarker884"/>has one value and it can be a number, string, binary, Boolean, or null.</li>
				<li><strong class="bold">Set</strong>: This represents <a id="_idIndexMarker885"/>multiple scalar values and can be a string set, number set, or binary set.</li>
				<li><strong class="bold">Document</strong>: This is <a id="_idIndexMarker886"/>a complex structure with options for nested attributes. You can use JSON formatted documents and retrieve data you need without having to retrieve the entire document. There are two subtypes of the document data type:<ul><li><strong class="bold">List</strong>: An ordered collection of values</li><li><strong class="bold">Map</strong>: An unordered collection of name-value pairs</li></ul></li>
			</ul>
			<p>In this section, we examined the different components of a DynamoDB database. In the next section, we will learn how to provision the required capacity for our database requirements.</p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor207"/>Provisioning capacity for DynamoDB</h2>
			<p>When it comes to provisioning your database, all you need to provide is the parameters that define the <strong class="bold">read capacity units</strong> (<strong class="bold">RCUs</strong>) and <strong class="bold">write capacity units</strong> (<strong class="bold">WCUs</strong>). These values <a id="_idIndexMarker887"/>enable Amazon to determine <a id="_idIndexMarker888"/>the underlying infrastructure to provision to host <a id="_idIndexMarker889"/>your database and your throughput levels.</p>
			<p>Depending on your RCU and WCU, DynamoDB will provision one or more partitions to store your data in and use the primary key to distribute your items across multiple partitions. Spreading data across multiple partitions enables DynamoDB to achieve ultra-low latency reads and writes, regardless of the number of items you have in the table.</p>
			<p>The two options that are available for provisioning capacity are as follows:</p>
			<ul>
				<li><strong class="bold">On-Demand</strong>: DynamoDB <a id="_idIndexMarker890"/>will provision capacity based on your read and write requests and provision capacity dynamically. This option is ideal when you have unpredictable application traffic and unknown workloads.</li>
				<li><strong class="bold">Provisioned</strong>: You specify the number of reads and writes per second that are required <a id="_idIndexMarker891"/>by your application. This is ideal if you have predictable application access patterns. You can always also enable <em class="italic">auto-scaling</em> to automatically adjust to traffic changes.</li>
			</ul>
			<p>In this section, we looked at Amazon DynamoDB and AWS's non-relational database solution. We discovered how DynamoDB is suitable for modern web applications that require <a id="_idIndexMarker892"/>thousands of reads and writes per second, as well as how DynamoDB is built for this purpose specifically.</p>
			<p>In the next section, we will look at Amazon's data warehousing solution, known as Amazon Redshift.</p>
			<h1 id="_idParaDest-205"><a id="_idTextAnchor208"/>Understanding the use cases for Amazon Redshift and data warehousing</h1>
			<p>A data warehousing solution is a specialized database solution designed to pull data from other <a id="_idIndexMarker893"/>relational databases and enable complex querying and analytics to be performed across different datasets. For example, you can combine data <a id="_idIndexMarker894"/>across customer orders, inventory data, and financial information to analyze product trends, demands, and return on investments.  </p>
			<p>Clients of Amazon <a id="_idIndexMarker895"/>Redshift include <strong class="bold">business intelligence</strong> (<strong class="bold">BI</strong>) applications, reporting, and analytics toolsets.  </p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor209"/>Online Analytical Processing (OLAP)</h2>
			<p>Amazon Redshift is designed for analytics and is optimized for scanning many rows of data for <a id="_idIndexMarker896"/>one or multiple columns. Instead of organizing data as rows, Redshift transparently organizes data by columns; it converts the data into columnar storage for each of the columns. Let's look at what this means.</p>
			<p>In a traditional database, data for each record is stored as rows. The columns represent the attributes of your data, and each row will contain field values for the relevant columns. </p>
			<p>Let's look at a table we saw earlier and see how the data is stored in blocks on disk:</p>
			<div>
				<div id="_idContainer174" class="IMG---Figure">
					<img src="Images/B17124_08_08.jpg" alt="Figure 8.8 – Data stored in blocks on disk&#13;&#10;" width="1144" height="312"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.8 – Data stored in blocks on disk</p>
			<p>Note that the data is stored sequentially for each column that makes up the entire row in blocks on the disk (<em class="italic">block 1, 2, 3, and so on</em>). If the record size is greater than the block size, then the record is stored across more than one block. Similarly, if the record size is smaller than the block size, then the record may consume less than the size of one block. Ultimately, this way of storing data leads to inefficiencies in the use of storage.</p>
			<p>Having said that, in a traditional relational database, most transactions will involve frequent read <a id="_idIndexMarker897"/>and write queries for a small set of records at a time, where the entire record set needs to be retrieved.</p>
			<p>Now, let's look at how Redshift stores data. Using the same customer data table, each data block stores values of a single column for multiple rows, as per the following diagram:</p>
			<div>
				<div id="_idContainer175" class="IMG---Figure">
					<img src="Images/B17124_08_09.jpg" alt="Figure 8.9 – Data stored on Amazon Redshift&#13;&#10;" width="541" height="109"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.9 – Data stored on Amazon Redshift</p>
			<p>Amazon Redshift converts the data as it is added into columnar storage. With this architecture, Amazon Redshift can store column field values for as many as three times the number of records compared to traditional row-based database storage. This means that you only consume a third of the I/O operations when it comes to reading column field values for a given set of records, compared to row-wise storage. Furthermore, because the data that's stored in blocks will be of the same type, you can use a compression method design for the columnar data type to achieve even better I/O and reduce the overall storage space. This architecture works well for data warehousing solutions because, by its very nature, your queries are designed to read only a few columns for <a id="_idIndexMarker898"/>a very large number of rows to extract data for analysis. In addition, queries require a fraction of the memory that would be required for processing row-wise blocks.</p>
			<p>Ultimately, Redshift is designed to host petabytes of data and supports massively parallel data processing for high-performance queries.</p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor210"/>Redshift architecture</h2>
			<p>The Redshift architecture <a id="_idIndexMarker899"/>is built on a cluster model that is comprised of the following:</p>
			<ul>
				<li><strong class="bold">Leader node</strong>: A single <a id="_idIndexMarker900"/>node that manages all communications between client applications and with <em class="italic">compute nodes</em>. The leader node carries out all operations such as the steps required to carry out various complex queries – the leader node will compile code and distribute these to compute nodes.</li>
				<li><strong class="bold">Compute nodes</strong>: Up to <strong class="source-inline">128</strong> compute nodes can be part of a Redshift cluster. The compute <a id="_idIndexMarker901"/>nodes execute the compiled code that was provided by the leader node and sends back intermediate results for the final aggregation. Each compute node will have its own dedicated CPU, memory, and disk type, which determines the node's type:<ul><li><strong class="bold">Dense compute nodes</strong>: These can store up to 326 TB of data on magnetic disks.</li><li><strong class="bold">Dense storage nodes</strong>: Can store <a id="_idIndexMarker902"/>up to 2 PB of data on <strong class="bold">solid-state disks</strong> (<strong class="bold">SSDs</strong>).</li><li><strong class="bold">RA3 instances</strong>: This next generation of Nitro-powered compute instances come with <em class="italic">managed storage </em>(unlike the other previous node types). You choose some nodes based on your performance requirements and only pay for the managed storage that you consume. This architecture has the compute and storage components decoupled. Furthermore, data storage is split, whereby local SSD storage is used for fast access to cached data and Amazon S3 is used to use longer-term durable storage that scales automatically. You will need to upgrade your dense compute and dense storage nodes if you wish to make use of managed storage. </li></ul></li>
			</ul>
			<p>In this section, we introduced you to the Amazon Redshift service, a cloud-hosted data warehousing <a id="_idIndexMarker903"/>solution designed for <strong class="bold">OLAP</strong> operations. In the next section, we will look at another feature of Amazon Redshift known as the Redshift Spectrum service, which allows you to directly query data held in Amazon S3.</p>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor211"/>About Redshift Spectrum</h2>
			<p>Another solution from Amazon Redshift is the <strong class="bold">Redshift Spectrum</strong> service, which allows you <a id="_idIndexMarker904"/>to perform SQL queries against data stored directly on Amazon S3 buckets. This is particularly useful if, for instance, you store frequently accessed data in Redshift and some infrequent data in Amazon S3. Rather than import the infrequent data into Redshift, which will only be queried occasionally, storing them in Amazon S3 and using Redshift Spectrum will be more cost-effective.</p>
			<p>It is also important to note that data in S3 must be structured and you must define the structure to enable Redshift to consume it.</p>
			<h1 id="_idParaDest-209"><a id="_idTextAnchor212"/>Understanding the importance of in-memory caching options with Amazon Elasticache</h1>
			<p>Often, you will find yourself accessing a set of data regularly, which is what we term as frequently <a id="_idIndexMarker905"/>accessed data. Every <a id="_idIndexMarker906"/>time you run a query on the database, you consume resources to perform the query operation and then retrieve that data. Overall, this can add additional load to your database and may even affect performance as you constantly write new data to the database.  </p>
			<p>As part of your overall application architecture, you should consider using in-memory caching engines offered by AWS to alleviate the load on your primary databases. <strong class="bold">Amazon Elasticache</strong> is a web service that offers in-memory caching in the cloud. By caching frequently accessed data on Amazon Elasticache, applications can be configured to retrieve frequently accessed data from it rather than make more expensive database calls.</p>
			<p>AWS offers two in-memory caching engines, as follows:</p>
			<ul>
				<li><strong class="bold">Amazon Elasticache for Redis</strong>: This is built as a cluster, which is a collection of one <a id="_idIndexMarker907"/>or more cache nodes, all of which run an instance of the Redis cache engine software. Redis <a id="_idIndexMarker908"/>is designed for <strong class="bold">complex data types</strong>, offers <a id="_idIndexMarker909"/>Multi-AZ capabilities, encryption of data, and compliance with FedRAMP, HIPAA, and PCI-DSS, as well as high availability and automatic failover options.</li>
				<li><strong class="bold">Amazon Elasticache for Memcached</strong>: This is <a id="_idIndexMarker910"/>designed for <strong class="bold">simple data types</strong>. Here, you <a id="_idIndexMarker911"/>can run large nodes with multiple cores or threads and scale out. It should be used where you require object caching.</li>
			</ul>
			<p>In this <a id="_idIndexMarker912"/>section, we learned about the <a id="_idIndexMarker913"/>various AWS Elasticache services, which offer in-memory caching capabilities for our applications. In-memory caching can be used to alleviate the load on your primary databases by caching frequently access data rather than having to run expensive queries repeatedly. In the next section, we will look at some additional databases offered by AWS that address specific niche market requirements.  </p>
			<h1 id="_idParaDest-210"><a id="_idTextAnchor213"/>Learning about additional database services for specific niche requirements</h1>
			<p>In addition <a id="_idIndexMarker914"/>to Amazon RDS and DynamoDB, AWS also offers additional databases that meet the requirements of specific niche <a id="_idIndexMarker915"/>applications. In this section, we will take a look at two of those <a id="_idIndexMarker916"/>databases: <strong class="bold">Amazon Neptune</strong> and <strong class="bold">Amazon Quantum Ledger Database</strong> (<strong class="bold">QLDB</strong>).</p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor214"/>Introduction to Amazon Neptune</h2>
			<p>Amazon Neptune is a fully managed graph database service and a type of NoSQL database. Graph databases are designed to store data as nodes (person, place, location, and so on) and directions. Each node would have some property and nodes have relationships <a id="_idIndexMarker917"/>between them. So, for example, <strong class="bold">Alice</strong> lives in <strong class="bold">London</strong>, and in <strong class="bold">London</strong>, there is a resident called <strong class="bold">Alice</strong>. This is a simple example, but you can start to imagine how complex your nodes and their relationships can become. These kinds of complex relationships between the nodes are just as important as the data itself and are ideal for a graph database solution. Traditional relational databases would require you to define complex joins between tables and even then, this would result in inefficiencies when trying to extract data. </p>
			<p>Amazon Neptune support well-known graph models such as Property Graph and W3C's RDF and their respective query languages such as Apache TinkerPop, Gremlin, and SPARQL. Amazon Neptune is a highly available database solution that offers point-in-time recovery and continuous backups to Amazon S3 with Availability Zone replication. Typical use cases for Amazon Neptune include applications such as fraud detection, knowledge graphs, drug discovery, and network security.</p>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor215"/>Amazon QLDB</h2>
			<p>Some types of data are highly sensitive and maintaining data integrity is of paramount importance. Examples <a id="_idIndexMarker918"/>of this include bank transaction records, where you need to track the history of credits and debits, or insurance claim applications that require you to maintain a verifiable history of the claim process. Another example is that of having to trace the movement of parts in a supply chain network and being able to prove the journey those items took to reach the customer. </p>
			<p>Although you can use relational databases to host these ledger types of data, you would need to build in an audit trail, which can be very cumbersome and prone to errors. Furthermore, because data stored in relational databases is not inherently immutable, it can become difficult to verify if data has been altered or deleted.  </p>
			<p>An alternative <a id="_idIndexMarker919"/>solution is to build a <strong class="bold">blockchain</strong> network. Blockchain frameworks such as Hyperledger Fabric and Ethereum enable you to build decentralized databases where the data stored is immutably and is cryptographically verifiable. However, blockchain networks are very complex and are designed on a decentralized model where you have multiple nodes that need to verify each record before it is committed to the database. </p>
			<p>Amazon <strong class="bold">QLDB</strong> is a fully managed ledger database that enables <a id="_idIndexMarker920"/>you to store immutable records with cryptographically verifiable transaction logs in a centralized database model. Amazon QLDB can maintain a history of all data changes. The following are the key benefits of <a id="_idIndexMarker921"/>Amazon QLDB:</p>
			<ul>
				<li><strong class="bold">Immutable and transparent</strong>: It enables you to track and maintain a sequence transaction log (journal) of every single change you make to your data. With QLDB, this transaction log is immutable, which means it cannot be altered or deleted. QLDB tracks each application data change and maintains a complete and verifiable history of all changes over time.</li>
				<li><strong class="bold">Cryptographically verifiable</strong>: You can use a SHA 256 cryptographic hash function to generate secure output files of your data's change history. This is also known as a <em class="italic">digest</em>, which acts as proof of any changes that have been made to your data. This can validate the integrity of all your data changes. </li>
				<li><strong class="bold">Easy to use</strong>: It uses a flexible document data model. You can use a SQL-like query language to query your data known as PartiQL. Amazon QLDB transactions are ACID-compliant.</li>
				<li><strong class="bold">Serverless</strong>: Amazon QLDB is a fully managed database service with no need to provision database instances or worry about capacity restraints. You start by creating a ledger and defining your tables. At this point, QLDB will automatically scale as required by your application.</li>
			</ul>
			<p>In this section, we examined a couple of additional database solutions designed for niche applications. In the next section, we will look at the database migration services offered by AWS, enabling you to move your on-premises databases to the cloud.</p>
			<h1 id="_idParaDest-213"><a id="_idTextAnchor216"/>Database Migration Service</h1>
			<p>Amazon <a id="_idIndexMarker922"/>offers a <strong class="bold">Database Migration Service</strong> (<strong class="bold">DMS</strong>) that can be used to migrate data from one database to another. Often, this is used as part of an on-premises to cloud migration strategy, where you need to migrate database services located in your data center to your AWS account in the cloud. AWS DMS offers support for both homogeneous migrations, such as from MySQL to MySQL or Oracle to Oracle, as well as heterogeneous migrations between engines, such as Oracle to Microsoft SQL Server or Amazon Aurora.</p>
			<p>An important <a id="_idIndexMarker923"/>point to be aware of is that, while migrating, you can continue to use your source database, which minimizes downtime for your business operations. In addition, you can also use DMS to perform continuous data replication from your on-premises environment to the cloud to offer high availability or disaster recovery capabilities.</p>
			<h1 id="_idParaDest-214"><a id="_idTextAnchor217"/>Exercise 8.1 – Extending your VPC to host database subnets</h1>
			<p>In <a href="B17124_07_Final_SK_ePub.xhtml#_idTextAnchor157"><em class="italic">Chapter 7</em></a>, <em class="italic">AWS Compute Services</em>, you expanded your VPC to include both private subnets and public subnets. Generally, you would only host services in a public subnet that <a id="_idIndexMarker924"/>would need direct exposure on the internet. Examples include the bastion host server we deployed earlier in <a href="B17124_07_Final_SK_ePub.xhtml#_idTextAnchor157"><em class="italic">Chapter 7</em></a>, <em class="italic">AWS Compute Services </em>(which we will discuss in the next chapter).</p>
			<p>Most applications are deployed across tiers – so, for example, you can have a web tier, an application tier, and a database tier. These different tiers are designed to separate different components of your application stack, allowing you to create a degree of isolation, as well as benefit from a layered security model. In <a href="B17124_07_Final_SK_ePub.xhtml#_idTextAnchor157"><em class="italic">Chapter 7</em></a>, <em class="italic">AWS Compute Services </em>, as part of <em class="italic">Exercise 7.1 – Expanding ProductionVPC so that it includes two public subnets and two private subnets</em>, you also configured two private subnets across two Availability Zones to host your application servers. In this example, the application tier and web tier are the same. However, in many real-world scenarios, they would be separate.</p>
			<p>In this exercise, you will be extending your VPC to add an additional tier, known as the database tier, within which you will be able to launch an Amazon RDS database. Like EC2 instances, Amazon RDS needs to be deployed in a VPC.  </p>
			<p>In the following diagram, you can see that your VPC now has three tiers – a public (DMZ) tier to host bastion host servers, NAT gateways, and Elastic Load Balancers, an application tier comprised of the <strong class="bold">Private Subnet One – App</strong> and <strong class="bold">Private Subnet Two – App</strong> subnets, and finally, a database tier comprised of the <strong class="bold">Private Subnet Three – Data</strong> and <strong class="bold">Private Subnet Four – Data</strong> subnets. Note that the subnets are spread across <a id="_idIndexMarker925"/>two Availability Zones to enable you to offer high availability of services in the event of an Availability Zone failure. We will discuss high availability in more detail in the next chapter:</p>
			<div>
				<div id="_idContainer176" class="IMG---Figure">
					<img src="Images/B17124_08_10.jpg" alt="Figure 8.10 – Extending the VPC to include a database tier&#13;&#10;" width="671" height="682"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.10 – Extending the VPC to include a database tier</p>
			<p>Let's start <a id="_idIndexMarker926"/>by extending your VPC so that it includes our database tier: </p>
			<ol>
				<li>Log back into your AWS account as our administrator, <strong class="bold">Alice</strong>. </li>
				<li>Navigate to the <strong class="bold">VPC</strong> dashboard and ensure you are in the <strong class="bold">US-East-1</strong> Region.</li>
				<li>In the left-hand menu, click on <strong class="bold">Subnets</strong>. </li>
				<li>Next, click the <strong class="bold">Create subnet</strong> button in the top right-hand corner of the screen.</li>
				<li>You will <a id="_idIndexMarker927"/>be presented with the <strong class="bold">Create subnet</strong> wizard page.</li>
				<li>Under <strong class="bold">VPC ID</strong>, select <strong class="bold">ProductionVPC</strong> from the drop-down menu.</li>
				<li>In the <strong class="bold">Subnet settings</strong> section, under <strong class="bold">Subnet 1 of 1</strong>, provide a name for your first database subnet. For this exercise, name your subnet <strong class="bold">Private Subnet Three – Data</strong>.</li>
				<li>Under <strong class="bold">Availability Zone</strong>, select the <strong class="bold">us-east-1a</strong> Availability Zone.</li>
				<li>Next, for <strong class="bold">IPv4 CIDR block</strong>, type in <strong class="source-inline">10.0.5.0/24</strong>.</li>
				<li>Next, rather than create this subnet and repeat the wizard to create the second database subnet, simply click on the <strong class="bold">Add new subnet</strong> button, as per the following screenshot:<div id="_idContainer177" class="IMG---Figure"><img src="Images/B17124_08_11.jpg" alt="Figure 8.11 – Creating multiple subnets&#13;&#10;" width="642" height="698"/></div><p class="figure-caption">Figure 8.11 – Creating multiple subnets</p></li>
				<li>A new <a id="_idIndexMarker928"/>subsection, <strong class="bold">Subnet 2 of 2</strong>, will appear, allowing you to create an additional subnet in the same wizard. Under <strong class="bold">Subnet name</strong>, type in <strong class="bold">Private Subnet Four – Data</strong>.</li>
				<li>For <strong class="bold">Availability Zone</strong>, select the <strong class="bold">us-east-1b</strong> Availability Zone from the drop-down list.</li>
				<li>For <strong class="bold">IPv4 CIDR block</strong>, type in <strong class="source-inline">10.0.6.0/24</strong>.</li>
				<li>Click the <strong class="bold">Create subnet</strong> button at the bottom of this page.</li>
			</ol>
			<p>AWS will successfully create two new subnets, which you will use to host your Amazon RDS database. In the right-hand menu, click on <strong class="bold">Subnets</strong> to view all the subnets now associated with your <strong class="bold">ProductionVPC</strong>, as per the following screenshot:</p>
			<div>
				<div id="_idContainer178" class="IMG---Figure">
					<img src="Images/B17124_08_12.jpg" alt="Figure 8.12 – Subnets in ProductionVPC&#13;&#10;" width="1396" height="354"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.12 – Subnets in ProductionVPC</p>
			<p>Now that you have created an additional two subnets for your <strong class="bold">ProductionVPC</strong>, you can proceed <a id="_idIndexMarker929"/>with the next part of this exercise. Like EC2 instances, Amazon RDS databases require you to configure the necessary security groups that will permit traffic to the database instances.  </p>
			<p>In our layered security model, we wish to ensure that only our application servers will be able to communicate with the databases in the backend. In this part of the exercise, you will create a new security group that will be configured to allow database-relevant traffic from any application servers you deploy later. To do this, you must configure an inbound rule on the new database security group to accept traffic on port <strong class="source-inline">3306</strong> for MySQL traffic from the security group of the application servers, specifically from the <strong class="bold">AppServers-SG</strong> security group. Let's get started:</p>
			<ol>
				<li value="1">Ensure you are currently in the <strong class="bold">VPC</strong> dashboard. Then, from the left-hand menu, click on <strong class="bold">Security Groups</strong>.</li>
				<li>Click the <strong class="bold">Create security group</strong> button in the top right-hand corner of the screen.</li>
				<li>For <strong class="bold">Security group name</strong>, type in <strong class="source-inline">Database-SG</strong>. Under description, type in <strong class="source-inline">Allow MYSQL traffic from AppServer-SG</strong>.</li>
				<li>Under <strong class="bold">VPC</strong>, ensure you select <strong class="bold">ProductionVPC</strong> from the drop-down list.</li>
				<li>Next, in the <strong class="bold">Inbound rules</strong> section, click on the <strong class="bold">Add rule</strong> button.</li>
				<li>Under <strong class="bold">Type</strong>, select <strong class="bold">MySQL/Aurora</strong>.</li>
				<li>Ensure that <strong class="bold">Source</strong> is set to <strong class="bold">Custom</strong>, and in the search box, start typing <strong class="source-inline">sg-</strong>. You should see that a list of all security groups shows up in a list. Select the <strong class="bold">AppServers-SG</strong> security group.</li>
				<li>Provide an optional description if required.</li>
				<li>Click on the <strong class="bold">Create security group</strong> button in the bottom right-hand corner of the screen.</li>
			</ol>
			<p>AWS will <a id="_idIndexMarker930"/>now confirm that the security group has been created successfully.</p>
			<p>In this exercise, you extended your VPC to host two additional private subnets that we will use to host our Amazon RDS database. You also created a new security group, <strong class="bold">Database-SG</strong>, which will be associated with our Amazon RDS database instance. It will also allow MySQL/Aurora traffic on port <strong class="source-inline">3306</strong> from any EC2 instance that is associated with the <strong class="bold">AppServer-SG</strong> security group.  </p>
			<p>In the next exercise, we will configure an Amazon RDS database subnet group that will be used to inform Amazon RDS of which subnets it can deploy our databases to.</p>
			<h1 id="_idParaDest-215"><a id="_idTextAnchor218"/>Exercise 8.2 – Creating a database subnet group</h1>
			<p>Before you can launch an RDS database in your VPC, you need to define a DB subnet group. A <strong class="bold">DB subnet group</strong> is a <a id="_idIndexMarker931"/>collection of two or more subnets within the VPC where you want to deploy your database instance. When creating your <a id="_idIndexMarker932"/>DB subnet group, at least two subnets must be selected in the VPC that are associated with two separate Availability Zones in a Region. Amazon RDS uses the subnet group's IP address CIDR block to assign your RDS database instance(s) with an IP address.  </p>
			<p>Amazon RDS can then deploy the database instance on one of your chosen subnets that is part of the group. In the case of a Multi-AZ deployment, the master copy will be deployed in one subnet in a particular Availability Zone, while the standby copy will be deployed in another subnet that is hosted within another Availability Zone.</p>
			<p>Note that the subnets in a DB subnet group are either public or private, but they cannot be a mix of both public and private subnets. Ideally, you want to configure private subnets as part of your subnet group because you want to deploy any backend databases in the private subnets of your VPC. Your databases should only be accessible from web/application servers and not directly from the internet.</p>
			<p>To set up <a id="_idIndexMarker933"/>the DB subnet group, follow these steps:</p>
			<ol>
				<li value="1">Ensure that you are logged into your <strong class="bold">AWS Management Console</strong> as the IAM user <strong class="bold">Alice</strong>. </li>
				<li>From the top left-hand menu, click on the <strong class="bold">Services</strong> drop-down arrow and select <strong class="bold">RDS</strong> located under the <strong class="bold">Database</strong> category. This will take you to the Amazon RDS dashboard.</li>
				<li>Ensure that you are in the <strong class="bold">us-east-1</strong> Region and from the left-hand menu, click on <strong class="bold">Subnet groups</strong>. </li>
				<li>Next, in the main pane of the screen, click the <strong class="bold">Create DB Subnet Group</strong> button.</li>
				<li>On the page that appears, you will need to define your DB subnet group details:<ul><li>Provide a name for your DB subnet group; for example, <strong class="source-inline">ProductionVPC-DBSubnet</strong>.</li><li>For the description, type in <strong class="source-inline">DB Subnet Group to host RDS Database in Production VPC</strong>.</li><li>Under <strong class="bold">VPC</strong>, select <strong class="bold">ProductionVPC</strong> from the drop-down menu.</li><li>Next, under <strong class="bold">Availability Zone</strong>, choose the Availability Zones that include the subnets you want to add. For this exercise, select the checkboxes next to <strong class="bold">us-east-1a</strong> and <strong class="bold">us-east-1b</strong>.</li></ul></li>
				<li>Next, under <strong class="bold">Subnets</strong>, select the subnets you created earlier for your RDS database. When you click on the drop-down list next to <strong class="bold">Subnets</strong>, you will notice that the subnet names are not visible; instead, the subnets are identified by their subnet IDs. You can determine the correct subnets by cross-referencing the subnet ID against the subnets you created in the VPC dashboard, which you can access in another browser window. Alternatively, you can ensure you select the correct subnets by comparing their relative IPv4 CIDR blocks. So, in our example, the <a id="_idIndexMarker934"/>private DB subnets are in the <strong class="source-inline">10.0.5.0/24</strong> and <strong class="source-inline">10.0.6.0/24</strong> Ipv4 CIDR blocks, as per the following screenshot:<div id="_idContainer179" class="IMG---Figure"><img src="Images/B17124_08_13.jpg" alt="Figure 8.13 – Creating database subnet groups&#13;&#10;" width="1118" height="810"/></div><p class="figure-caption">Figure 8.13 – Creating database subnet groups</p></li>
				<li>Next, click the <strong class="bold">Create</strong> button at the bottom right-hand corner of the screen.</li>
			</ol>
			<p>AWS will create your DB subnet group using the details you provided, as per the following screenshot:</p>
			<div>
				<div id="_idContainer180" class="IMG---Figure">
					<img src="Images/B17124_08_14.jpg" alt="Figure 8.14 – Successfully creating a database subnet group&#13;&#10;" width="1453" height="503"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.14 – Successfully creating a database subnet group</p>
			<p>In this exercise, you learned about RDS DB subnet groups, which allow you to define a minimum of <a id="_idIndexMarker935"/>two subnets across two Availability Zones, where Amazon RDS can deploy your RDS DB instance when you choose to launch your database.</p>
			<p>In the next exercise, we will launch our RDS database in <strong class="bold">ProductionVPC</strong>. We will also use this database to host the backend data of our web application, which we will then deploy in the fourth exercise of this next chapter.  </p>
			<h1 id="_idParaDest-216"><a id="_idTextAnchor219"/>Exercise 8.3 – Launching your Amazon RDS database in ProductionVPC</h1>
			<p>In this <a id="_idIndexMarker936"/>exercise, you will launch an <a id="_idIndexMarker937"/>Amazon RDS MySQL database in the DB subnet group of <strong class="bold">ProductionVPC</strong>. Let's get started:</p>
			<ol>
				<li value="1">Ensure that you are logged into your AWS account as the IAM user <strong class="bold">Alice</strong>.</li>
				<li>Navigate to the Amazon RDS dashboard.</li>
				<li>From the left-hand menu, select <strong class="bold">Databases</strong>.</li>
				<li>On the right-hand side of the pane, click the <strong class="bold">Create database</strong> button.</li>
				<li>Next, you will be presented with the <strong class="bold">Create database</strong> wizard, where you will need to define various parameters of your VPC. Amazon offers the <strong class="source-inline">t2.micro</strong> database instance running the MySQL engine as part of the Free Tier offering, which comes with the following features for up to 12 months:<ul><li>750 hours of Amazon RDS in a Single-AZ db.t2.micro instance.</li><li>20 GB of General Purpose storage (SSD).</li><li>20 GB for automated backup storage and any user-initiated DB snapshots.</li></ul></li>
				<li>For <strong class="bold">Choose a database creation method</strong>, select the option next to <strong class="bold">Standard create</strong>.</li>
				<li>Next, for <a id="_idIndexMarker938"/>the database <a id="_idIndexMarker939"/>engine option, select <strong class="bold">MySQL</strong>.</li>
				<li>Leave the <strong class="bold">Edition</strong> and <strong class="bold">Version</strong> settings as-is. </li>
				<li>Under <strong class="bold">Templates</strong>, select the <strong class="bold">Free Tier</strong> option.</li>
				<li>Next, you need to provide some settings:<ul><li>For the DB instance identifier, type in <strong class="source-inline">productiondb</strong>.</li><li>Under <strong class="bold">Credential Settings</strong>, leave the <strong class="bold">Master</strong> username set to <strong class="bold">admin</strong> and provide a password of your choice. Make sure that you note this password down; otherwise, you will not be able to connect to the database.</li></ul></li>
				<li>Under <strong class="bold">Database instance class</strong>, leave the settings as-is.</li>
				<li>Under <strong class="bold">Storage</strong>, leave the settings as-is except for <strong class="bold">Storage autoscaling</strong>, where you should <em class="italic">disable</em> the option for <strong class="bold">Enable storage autoscaling</strong>.</li>
				<li>Under <strong class="bold">Availability &amp; durability</strong>, you will note that the option to enable <strong class="bold">Multi-AZ</strong> is grayed out. This is because Multi-AZ is not available in the Free Tier.</li>
				<li>Next, under <strong class="bold">Connectivity</strong>, do the following:<ul><li>Ensure that you select <strong class="bold">ProductionVPC</strong> from the drop-down list under <strong class="bold">Virtual private cloud (VPC)</strong>.</li><li>Note that the subnet group has been pre-populated with the DB subnet group we created earlier, which in this case is <strong class="source-inline">productionvpc-dbsubnet</strong>.</li><li>Under <strong class="bold">Public Access</strong>, select <strong class="bold">No</strong>.</li><li>Next, under <strong class="bold">VPC Security Group</strong>, ensure that <strong class="bold">Choose Existing</strong> is selected. Then, from the drop-down list under <strong class="bold">Existing VPC security groups</strong>, select the <strong class="bold">Database-SG</strong> security group you created earlier.</li><li>Next, under <strong class="bold">Availability Zone</strong>, you can select an AZ of your choice or leave it <a id="_idIndexMarker940"/>as the <strong class="bold">No preference</strong> option.  </li><li>Next, expand <a id="_idIndexMarker941"/>the <strong class="bold">Additional configuration</strong> option and ensure that the database port is set to <strong class="source-inline">3306</strong>.</li></ul></li>
				<li>Under <strong class="bold">Database authentication options</strong>, ensure that <strong class="bold">Password authentication</strong> is enabled.</li>
				<li>Under <strong class="bold">Additional configuration</strong>, do the following:<ul><li>Type in the same database name as you did earlier for <strong class="bold">DB instances identifier</strong>. In our example, the name will be <strong class="source-inline">productiondb</strong>.</li><li>Leave <strong class="bold">DB parameters group</strong> and <strong class="bold">Options group</strong> as-is.</li><li>Under <strong class="bold">Backups</strong>, ensure that <strong class="bold">Enable automatic backups</strong> is enabled and then set <strong class="bold">Backup retention period</strong> to <strong class="bold">1 Day</strong>.</li><li>Under <strong class="bold">Backup window</strong>, select the <strong class="bold">No preference</strong> option. For real-world applications, you may wish to set the backup window to a period outside of normal business hours.  </li><li>Under the <strong class="bold">Maintenance</strong> subheading, leave the settings as-is.</li><li>Finally, click the <strong class="bold">Create database</strong> button at the bottom right-hand corner of the screen.</li></ul></li>
			</ol>
			<p>Your RDS database will take a few minutes to launch. As part of the launch process, an initial backup <a id="_idIndexMarker942"/>will also be performed. Once the <a id="_idIndexMarker943"/>database has been successfully launched and ready to use, you will see that its <strong class="bold">Status</strong> will be set to <strong class="bold">Available</strong>, as per the following screenshot:</p>
			<div>
				<div id="_idContainer181" class="IMG---Figure">
					<img src="Images/B17124_08_15.jpg" alt="Figure 8.15 – RDS database created successfully notification&#13;&#10;" width="1470" height="421"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.15 – RDS database created successfully notification</p>
			<p>In the next exercise, you will learn how to deploy a DynamoDB table.</p>
			<h1 id="_idParaDest-217"><a id="_idTextAnchor220"/>Exercise 8.4 – Deploying an Amazon DynamoDB table</h1>
			<p>In this <a id="_idIndexMarker944"/>exercise, you will deploy a very simple DynamoDB table. Let's get started:</p>
			<ol>
				<li value="1">Ensure that you are logged into your AWS account as the IAM user known as <strong class="bold">Alice</strong>.</li>
				<li>Next, navigate to the DynamoDB dashboard. You can search for <strong class="source-inline">DynamoDB</strong> from the top search box of <strong class="bold">AWS Management Console</strong>.</li>
				<li>If this is the first time you have visited the <strong class="bold">DynamoDB console</strong> page, you will be presented with a splash screen.  </li>
				<li>Click the <strong class="bold">Create table</strong> button. </li>
				<li>Provide a name for your table in the text box next to <strong class="bold">Table name</strong>; for example, <strong class="source-inline">Recipes</strong>.</li>
				<li>In the <strong class="bold">Primary key</strong> field, enter <strong class="source-inline">RecipeName</strong> and ensure that the type is set to <strong class="bold">String</strong>.</li>
				<li>Under <strong class="bold">Table settings</strong>, uncheck the box next to <strong class="bold">Use default settings</strong>.</li>
				<li>In the <strong class="bold">Read/write capacity mode</strong> section, select the <strong class="bold">On-demand</strong> option.</li>
				<li>Click the <strong class="bold">Create</strong> button at the bottom of the page. DynamoDB will create a new table for <a id="_idIndexMarker945"/>you in a few seconds, as per the following screenshot:<div id="_idContainer182" class="IMG---Figure"><img src="Images/B17124_08_16.jpg" alt="Figure 8.16 – DynamoDB table – Recipes&#13;&#10;" width="1650" height="737"/></div><p class="figure-caption">Figure 8.16 – DynamoDB table – Recipes</p></li>
				<li>Click on the <strong class="bold">Items</strong> tab.</li>
				<li>You can start adding items in the <strong class="bold">Items</strong> tab. Click the <strong class="bold">Create item</strong> button.</li>
				<li>You will see a dialog box in which you can add a new item (record) to your database.  </li>
				<li>In the text box next to <strong class="bold">RecipeName String</strong>, enter <strong class="source-inline">Vegan Sausage Rolls</strong>.</li>
				<li>Click on the <strong class="bold">Save</strong> button.</li>
				<li>Note that the new item has been added and that the value of the primary key for this item is the name of the recipe, <strong class="source-inline">Vegan Sausage Rolls</strong>.</li>
				<li>Click the <strong class="bold">Create item</strong> button again.</li>
				<li>In the <a id="_idIndexMarker946"/>text box next to <strong class="bold">RecipeName String</strong>, enter <strong class="source-inline">Vegan Peri Peri Burger</strong>.</li>
				<li>Click on the <em class="italic">plus</em> sign and select the <strong class="bold">Append</strong> drop-down list. From the list of options provided, select the <strong class="bold">StringSet</strong> type. A new field will be created for you. In the <strong class="bold">field</strong> box, enter an attribute (field name); for example, <strong class="source-inline">Ingredients</strong>. You will also notice that an additional entry appears below this <strong class="bold">StringSet</strong>, which is where you would input the values for the field you just created. Click on the <em class="italic">plus</em> sign next to the <em class="italic">empty array</em> line and select <strong class="bold">Append</strong>.</li>
				<li>In the <strong class="bold">Value</strong> box, type in the following words, followed by pressing <em class="italic">Enter</em> on your keyboard after each word – <strong class="source-inline">Lettuce</strong> <strong class="source-inline">Tomato</strong> <strong class="source-inline">Cucumber</strong>. Click on a different part of the screen to update the values, as per the following screenshot:<div id="_idContainer183" class="IMG---Figure"><img src="Images/B17124_08_17.jpg" alt="Figure 8.17 – DymanoDB item entry&#13;&#10;" width="553" height="267"/></div><p class="figure-caption">Figure 8.17 – DymanoDB item entry</p></li>
				<li>Click on the <strong class="bold">Save</strong> button.</li>
				<li>At this <a id="_idIndexMarker947"/>point, your table has been updated with a new record. You will see the two items in your table. The <strong class="bold">Vegan Sausage Roll</strong> item only has one field with a value in it, namely the primary key. The <strong class="bold">Vegan Peri Peri Burger</strong> item has two fields associated with it, which are the primary key and an attribute called <strong class="bold">Ingredients</strong>. Review the following screenshot for reference:</li>
			</ol>
			<div>
				<div id="_idContainer184" class="IMG---Figure">
					<img src="Images/B17124_08_18.jpg" alt="Figure 8.18 – DynamoDB Recipes table&#13;&#10;" width="1069" height="536"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.18 – DynamoDB Recipes table</p>
			<p>As you <a id="_idIndexMarker948"/>can see, DynamoDB offers lots of flexibility in not requiring a rigid schema definition before inputting data.</p>
			<p>Next, we will conclude by summarizing this chapter.</p>
			<h1 id="_idParaDest-218"><a id="_idTextAnchor221"/>Summary</h1>
			<p>In this chapter, we learned about the various database services offered by Amazon, comprising both relational and non-relational databases services. You learned how AWS enables you to quickly deploy new RDS databases and offers full management of your database as a service, rather than you having to provision EC2 instances that you will install database software on.</p>
			<p>Amazon RDS comes with six engines – MySQL, PostgreSQL, Microsoft SQL, Oracle, MariaDB, and Amazon Aurora. Amazon RDS is a regional service and must be deployed in your VPC. You have options to configure for high availability using services such as Multi-AZ and backup and restore strategies. You can also scale out read copies of your RDS database to offload read queries away from the primary master copy of your database.</p>
			<p>Amazon Aurora comes with a lot more features and addresses some of the limitations of traditional RDS engines out of the box, including features such as self-healing and high availability.</p>
			<p>We then looked at Amazon DynamoDB, which is a non-relational database designed for the modern web, mobiles, and IoT applications that can read and write thousands of requests per second. Amazon DynamoDB is offered as a completely serverless solution – you do not need to provision database instances or storage yourself. You simply specify <strong class="bold">WCUs</strong> and <strong class="bold">RCUs</strong> and AWS will provision the underlying infrastructure for you.</p>
			<p>In addition, we looked at two in-memory caching engines offered by the Amazon Elasticache service – Redis and Memcached – and compared which engine to use in what scenario.</p>
			<p>Finally, we examined AWS DMS3, which offers both homogenous migrations such as Oracle to Oracle migrations and heterogenous migration such as Oracle to Microsoft SQL type migrations. AWS DMS can be used to migrate on-premises databases to the cloud very easily.</p>
			<p>In the next chapter, we will discuss concepts related to high availability and scalability. In addition, we will carry out various lab exercises that will enable you to learn how to combine the various core services we have learned about so far. You will do this by deploying a multi-tier application architecture.</p>
			<h1 id="_idParaDest-219"><a id="_idTextAnchor222"/>Questions</h1>
			<ol>
				<li value="1">A company plans to migrate its on-premises MySQL database to Amazon RDS. Which AWS service should they use for this task?<ol><li>Amazon Snowball</li><li>AWS Database Migration Service (AWS DMS)</li><li>AWS VM Import/Export</li><li>AWS Server Migration Service</li></ol></li>
				<li>Which of the following is the primary benefit of using an Amazon RDS database instead of installing a MySQL-compatible database on your EC2 instance?<ol><li>Managing the database, including patching and backups, is taken care of by Amazon.</li><li>Managing the database, including patching and backups, is taken care of by the customer.</li><li>You have full access to the operating system layer that the RDS database runs on.</li><li>You can choose which drive and partition to install the RDS database on.</li></ol></li>
				<li>AWS RDS supports six database engines. From the following list, choose <em class="italic">three</em> engines supported by Amazon RDS. <ol><li>Microsoft SQL</li><li>Oracle</li><li>MySQL</li><li>FoxPro</li><li>Db2</li></ol></li>
				<li>You are building an application for a wealth asset management company that will be used to store portfolio data and transactions of stocks, mutual funds, and forex purchased. To that end, you need a backend database solution that will ensure a ledger-like functionality because they want to maintain an accurate history of their applications' data, for example, tracking the history of credits and debits for its customers. Which AWS database solution would you recommend for this business requirement?<ol><li>Amazon RDS</li><li>Amazon DynamoDB</li><li>Amazon QLDB</li><li>Amazon Redshift</li></ol></li>
				<li>Which AWS database solution enables you to build a complete data warehousing solution, capable of handling complex analytic queries against petabytes of structured data using standard SQL and industry-recognized business intelligence tools?<ol><li>AWS DynamoDB</li><li>AWS Redshift</li><li>AWS Neptune</li><li>AWS Pluto</li></ol></li>
				<li>You are looking to host a production-grade enterprise relational database solution that offers high-end features such as self-healing storage systems that are capable of scaling up to 128 TB per database instance. Which of the following AWS database solutions fulfills the requirement?<ol><li>Amazon DynamoDB</li><li>Amazon Aurora</li><li>Amazon Redshift</li><li>Amazon Neptune</li></ol></li>
				<li>Which AWS feature of Amazon Redshift enables you to run SQL queries against data stored directly on Amazon S3 buckets?<ol><li>Redshift DaX</li><li>Athena</li><li>Redshift Spectrum</li><li>Redshift Cache</li></ol></li>
				<li>Which AWS service enables you to migrate an on-premises MySQL database to an Amazon RDS database running the Oracle Engine?<ol><li>AWS Cross-Region Replication</li><li>AWS SMS</li><li>AWS DMS</li><li>AWS EFS</li></ol></li>
				<li>You are running a single RDS DB instance. Which configuration would you recommend so that you can avoid I/O suspension issues when performing backups?<ol><li>Configure RDS read replicas.</li><li>Configure RDS Multi-AZ.</li><li>Configure RDS Cross Region Backup.</li><li>Configure DynamoDB DaX.</li></ol></li>
			</ol>
		</div>
	</div></body></html>