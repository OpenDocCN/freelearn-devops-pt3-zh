<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer065">
			<h1 id="_idParaDest-174" class="chapter-number"><a id="_idTextAnchor866"/>7</h1>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor867"/>Containers as a Service (CaaS) and Serverless Computing for Containers</h1>
			<p>In the last two chapters, we covered Kubernetes and how it helps manage your containers seamlessly. Now, let’s look<a id="_idIndexMarker723"/> at other ways of automating and managing container deployments—<strong class="bold">Containers as a Service</strong> (<strong class="bold">CaaS</strong>) and <strong class="bold">serverless computing for containers</strong>. CaaS provides container-based virtualization that abstracts away all management behind the scenes and helps you manage your containers without worrying about the underlying infrastructure <span class="No-Break">and orchestration.</span></p>
			<p>For simple deployments and less complex applications, CaaS can be a savior. Serverless computing is a broad term that encompasses applications that can be run without us having to worry about the infrastructure behind the scenes. It has an additional benefit that you can<a id="_idIndexMarker724"/> focus purely on the application. We will discuss<a id="_idIndexMarker725"/> CaaS <a id="_idIndexMarker726"/>technologies such as <strong class="bold">Amazon Elastic Container Service</strong> (<strong class="bold">Amazon ECS</strong>) with <strong class="bold">Amazon Web Services Fargate</strong> (<strong class="bold">AWS Fargate</strong>) in detail and briefly discuss other cloud-based CaaS offerings such <a id="_idIndexMarker727"/>as <strong class="bold">Azure Kubernetes Services</strong> (<strong class="bold">AKS</strong>), <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>), and <strong class="bold">Google Cloud Run</strong>. We will <a id="_idIndexMarker728"/>then delve into the<a id="_idIndexMarker729"/> popular open source serverless CaaS solution known <span class="No-Break">as </span><span class="No-Break"><strong class="bold">Knative</strong></span><span class="No-Break">.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>The need for <span class="No-Break">serverless offerings</span></li>
				<li>Amazon ECS with <strong class="bold">Elastic Compute Cloud</strong> (<strong class="bold">EC2</strong>) <span class="No-Break">and Fargate</span></li>
				<li>Other <span class="No-Break">CaaS services</span></li>
				<li>Open source CaaS <span class="No-Break">with K<a id="_idTextAnchor868"/><a id="_idTextAnchor869"/>native</span></li>
			</ul>
			<h1 id="_idParaDest-176"><a id="_idTextAnchor870"/>Technical requirements</h1>
			<p>You will need an active AWS subscription for this chapter’s exercises. AWS is the market’s most popular, feature-rich cloud platform. Currently, AWS is offering a free tier for some products. You can sign up for this at <a href="https://aws.amazon.com/free">https://aws.amazon.com/free</a>. This chapter uses some paid services, but we will try to minimize how many we use as much as possible during <span class="No-Break">the exercises.</span></p>
			<p>You will also need to clone the following GitHub repository for some of <span class="No-Break">the exercises:</span></p>
			<p><a href="https://github.com/PacktPublishing/Modern-DevOps-Practices-2e%0D"><span class="No-Break">https://github.com/PacktPublishing/Modern-DevOps-Practices-2e</span></a></p>
			<p>Run the following command to clone the repository into your home directory. Then, <strong class="source-inline">cd</strong> into the <strong class="source-inline">ch7</strong> directory to access the <span class="No-Break">required resources:</span></p>
			<pre class="console">
$ git clone https://github.com/PacktPublishing/Modern-DevOps-Practices-2e.git \
  modern-devops
$ cd modern-devops/ch7</pre>			<p>As the repository contains files with placeholder strings, you must replace the <strong class="source-inline">&lt;your_dockerhub_user&gt;</strong> string with your actual Docker Hub user. Use the following commands to substitute <span class="No-Break">the placeholders:</span></p>
			<pre class="console">
$ find ./ -type f -exec sed -i -e \
's/&lt;your_dockerhub_user&gt;/&lt;your actual docker hub user&gt;/g' {} \; </pre>			<p>So, let’s <span class="No-Break">get st<a id="_idTextAnchor871"/><a id="_idTextAnchor872"/>arted!</span></p>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor873"/>The need for serverless offerings</h1>
			<p>Numerous organizations, so far, hav<a id="_idTextAnchor874"/>e been focusing<a id="_idIndexMarker730"/> a lot on infrastructure provisioning and management. They optimize the number of resources, machines, and infrastructure surrounding the applications they build. However, they should focus on what they do best—software development. Unless your organization wants to invest heavily in an expensive infrastructure team to do a lot of heavy lifting behind the scenes, you’d be better off concentrating on writing and building quality applications rather than focusing on where and how to run and <span class="No-Break">optimize them.</span></p>
			<p>Serverless offerings come as a reprieve for this problem. Instead of concentrating on how to host your infrastructure to run your applications, you can declare what you want to run, and the serverless offering manages it for you. This has become a boon for small enterprises that do not have the budget to invest heavily in infrastructure and want to get started quickly without wasting too much time standing up and maintaining infrastructure to <span class="No-Break">run applications.</span></p>
			<p>Serverless offerings also offer automatic placement<a id="_idIndexMarker731"/> and scaling for container and application workloads. You can spin from 0 to 100 instances in minutes, if not seconds. The best part is that you pay for what you use in some services rather than what <span class="No-Break">you allocate.</span></p>
			<p>This chapter will concentrate<a id="_idIndexMarker732"/> on a very popular AWS container management offering called <strong class="bold">ECS</strong> and AWS’s <a id="_idTextAnchor875"/>container serverless offering, <strong class="bold">AWS Fargate</strong>. We will then briefly examine offerings<a id="_idIndexMarker733"/> from other cloud platforms and, finally, the o<a id="_idTextAnchor876"/>pen source<a id="_idIndexMarker734"/> container-based serverless solution known <span class="No-Break">as </span><span class="No-Break"><strong class="bold">Knative</strong></span><span class="No-Break">.</span></p>
			<p>Now, let’s go ahead and look at <a id="_idTextAnchor877"/><a id="_idTextAnchor878"/><span class="No-Break">Amazon ECS.</span></p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor879"/>Amazon ECS with EC2 and Fargate</h1>
			<p>Amazon <a id="_idIndexMarker735"/>ECS is a container orchestration platform that AWS offers. It is simple to use and manage, uses Docker behind the scenes, and can deploy your workloads<a id="_idIndexMarker736"/> to <strong class="bold">Amazon E<a id="_idTextAnchor880"/>C2</strong>, a <strong class="bold">virtual machine</strong> (<strong class="bold">VM</strong>)-based solution, or <strong class="bold">AWS Fargate</strong>, a <a id="_idIndexMarker737"/><span class="No-Break">serverless offering.</span></p>
			<p>It is a highly scalabl<a id="_idTextAnchor881"/>e solution that deploys containers in seconds. It makes hosting, running, stopping, and starting your containers easy. Just as Kubernetes<a id="_idIndexMarker738"/> offers <strong class="bold">pods</strong>, ECS offers <a id="_idTextAnchor882"/><strong class="bold">tasks</strong> that help you run your container workloads. A task<a id="_idIndexMarker739"/> can contain one or more containers grouped according to a logical relationship. You can also group one or more tasks into <strong class="bold">services</strong>. Services are similar to Kubernetes<a id="_idIndexMarker740"/> controllers, which manage tasks and can ensure that the required number of replicas of your tasks are running in the right place at the right time. ECS uses simple API calls to provide many functionalities, such as creating, updating, reading, and deleting tasks <span class="No-Break">and services.</span></p>
			<p>ECS also<a id="_idIndexMarker741"/> allows you to place your containers according<a id="_idIndexMarker742"/> to multiple placement strategies while keeping <strong class="bold">high availability</strong> (<strong class="bold">HA</strong>) and resource optimization in mind. You can tweak the placement algorithm according to your priority—cost, availability, or a mix of both. So, you can use ECS to run one-time batch workloads or long-running microservices, all using a simple-to<a id="_idTextAnchor883"/><a id="_idTextAnchor884"/>-use <span class="No-Break">API interface.</span></p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor885"/>ECS architecture</h2>
			<p>Before we explore the <a id="_idIndexMarker743"/>ECS architecture, it is important to understand some common AWS terminologies to follow it. Let’s look at some <span class="No-Break">AWS resources:</span></p>
			<ul>
				<li><strong class="bold">AWS Regions</strong>: An AWS Region is a geographical region<a id="_idIndexMarker744"/> where AWS provides<a id="_idIndexMarker745"/> its services. It is normally a city or a metropolitan region but can sometimes span multiple cities. It comprises multiple <strong class="bold">Availability Zones</strong> (<strong class="bold">AZs</strong>). Some examples of AWS Regions are <strong class="source-inline">us-east-1</strong>, <strong class="source-inline">us-west-1</strong>, <strong class="source-inline">ap-southeast-1</strong>, <strong class="source-inline">eu-central-1</strong>, and <span class="No-Break">so on.</span></li>
				<li><strong class="bold">AWS AZs</strong>: AWS AZs are data centers within an AWS Region<a id="_idIndexMarker746"/> connected with low-latency, high-bandwidth networks. Most resources run within AZs. Examples of AZs are <strong class="source-inline">us-east-1a</strong>, <strong class="source-inline">us-east-1b</strong>, and <span class="No-Break">so on.</span></li>
				<li><strong class="bold">AWS virtual private cloud (VPC)</strong>: An AWS VPC is an isolated network resource you create<a id="_idIndexMarker747"/> within AWS. You associate a dedicated private IP address range to it from which the rest of your resources, such as EC2 instances, can derive their IP addresses. An AWS VPC spans an <span class="No-Break">AWS Region.</span></li>
				<li><strong class="bold">Subnet</strong>: A subnet, as the name suggests, is a subnetwork<a id="_idIndexMarker748"/> within the VPC. You must subdivide the IP address ranges you provided to the VPC and associate them with subnets. Resources normally reside within subnets, and each subnet spans <span class="No-Break">an AZ.</span></li>
				<li><strong class="bold">Route table</strong>: An AWS route table routes traffic<a id="_idIndexMarker749"/> within the VPC subnets and to the internet. Every AWS<a id="_idIndexMarker750"/> subnet is associated with a route table through <strong class="bold">subnet route </strong><span class="No-Break"><strong class="bold">table associations</strong></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Internet gateways</strong>: An internet gateway allows connection<a id="_idIndexMarker751"/> to and from the internet to your <span class="No-Break">AWS subnets.</span></li>
				<li><strong class="bold">Identity Access Management (IAM)</strong>: AWS IAM helps you control access<a id="_idIndexMarker752"/> to resources by users<a id="_idIndexMarker753"/> and other AWS<a id="_idIndexMarker754"/> resources. They help you implement <strong class="bold">role-based access control</strong> (<strong class="bold">RBAC</strong>) and the <strong class="bold">principle of least </strong><span class="No-Break"><strong class="bold">privilege</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">PoLP</strong></span><span class="No-Break">).</span></li>
				<li><strong class="bold">Amazon EC2</strong>: EC2 allows you to spin up VMs <a id="_idIndexMarker755"/>within subnets, also known<a id="_idIndexMarker756"/> <span class="No-Break">as instances.</span></li>
				<li><strong class="bold">AWS Auto Scaling groups (ASGs)</strong>: An AWS ASG works with Amazon EC2 to provide HA and scalability<a id="_idIndexMarker757"/> to your instances. It monitors your EC2 instances and ensures that a defined number of healthy instances<a id="_idIndexMarker758"/> are always running. It also<a id="_idIndexMarker759"/> takes care of autoscaling your instances with increasing load in your machines to allow for handling more traffic. It uses the <strong class="bold">instance profile</strong> and <strong class="bold">launch configuration</strong> to decide on the properties of new EC2 instances it <span class="No-Break">spins up.</span></li>
				<li><strong class="bold">Amazon CloudWatch</strong>: Amazon CloudWatch is<a id="_idIndexMarker760"/> a monitoring and observability<a id="_idIndexMarker761"/> service. It allows you to collect, track, and monitor metrics, log files, and set alarms to take automated actions on specific conditions. CloudWatch helps understand application performance, hea<a id="_idTextAnchor886"/>lth, and <span class="No-Break">resource utilization.</span></li>
			</ul>
			<p>ECS is a cloud-based regional service. When you spin up an ECS cluster, the instances span multiple AZs, where you can schedule your tasks and services using simple manifests. ECS manifests are very similar to <strong class="source-inline">docker-compose</strong> YAML manifests, where we specify which tasks to run and which tasks comprise <span class="No-Break">a service.</span></p>
			<p>You can run ECS within an existin<a id="_idTextAnchor887"/>g VPC. We can schedule tasks in either Amazon EC2 or <span class="No-Break">AWS Fargate.</span></p>
			<p>Your ECS<a id="_idIndexMarker762"/> cluster can have one or more EC2 instances attached to it. You also have the option to attach an existing EC2 instance to a cluster by installing the ECS node agent within your EC2 instance. The agent sends information about your containers’ state and tasks to the ECS scheduler. It then interacts with the container runtime to schedule containers within the node. They are similar to <strong class="source-inline">kubelet</strong> in the Kubernetes ecosystem. If you run your containers within EC2 instances, you pay for the number of EC2 instances you allocate to <span class="No-Break">the cluster.</span></p>
			<p>If you plan to use Fargate, the infrastructure is wholly abstracted from you, and you must specify the amount of CPU and memory your container is set to consume. You pay for the CPU and memory your container consumes rather than the resources you allocate to <span class="No-Break">the machines.</span></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Although you only pay for the resources you consume in Fargate, it is more expensive than running your tasks on EC2, especially when running long-running services such as a web server. A rule of thumb is to run long-running online tasks within EC2 and batch tasks with Fargate. That will give you the best <span class="No-Break">cost optimization.</span></p>
			<p>When we schedule a task, AWS spins<a id="_idIndexMarker763"/> up the container on a managed EC2 or Fargate<a id="_idTextAnchor888"/> server<a id="_idIndexMarker764"/> by pulling the required<a id="_idIndexMarker765"/> container image from a <strong class="bold">container registry</strong>. Every <strong class="bold">task </strong>has an <strong class="bold">elastic network interface</strong> (<strong class="bold">ENI</strong>) attached to it. Multiple tasks are grouped as a <strong class="bold">service</strong>, and the service ensures that all the required tasks run <span class="No-Break">at once.</span></p>
			<p>Amazon ECS uses a <strong class="bold">task scheduler</strong> to schedule containers on your cluster. It places your containers in an appropriate<a id="_idIndexMarker766"/> node of your cluster based on placement logic, availability, and cost requirements. The scheduler also ensures that the desired number of tasks ru<a id="_idTextAnchor889"/>n on the node at a <span class="No-Break">given time.</span></p>
			<p>The following diagram<a id="_idIndexMarker767"/> explain<a id="_idTextAnchor890"/>s the ECS cluster <span class="No-Break">architecture beautifu<a id="_idTextAnchor891"/>lly:</span></p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B19877_07_1.jpg" alt="Figure 7.1 – ECS architecture" width="1530" height="1600"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – ECS architecture</p>
			<p>Amazon provides the ECS <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>) for interacting with the ECS cluster. It is a simple command-line tool<a id="_idIndexMarker768"/> that you can use to administer<a id="_idIndexMarker769"/> a<a id="_idTextAnchor892"/>n ECS cluster and create and manage tasks and services on the <span class="No-Break">ECS cluster.</span><a id="_idTextAnchor893"/><a id="_idTextAnchor894"/></p>
			<p>Now, let’s go ahead and install the <span class="No-Break">ECS CLI.</span></p>
			<h2 id="_idParaDest-180">I<a id="_idTextAnchor895"/>nstalling the AWS and ECS CLIs</h2>
			<p>The AWS CLI is available as a <strong class="source-inline">deb</strong> package<a id="_idIndexMarker770"/> within the <a id="_idTextAnchor896"/>public <strong class="source-inline">apt</strong> repositories. To install it, run the <span class="No-Break">following</span><span class="No-Break"><a id="_idIndexMarker771"/></span><span class="No-Break"> commands:</span></p>
			<pre class="console">
$ sudo apt update &amp;&amp; sudo apt install awscli -y
$ aws --version
aws-cli/1.22.34 Python/3.10.6 Linux/5.19.0-1028-aws botocore/1.23.34</pre>			<p>Installing the ECS CLI in the Linux ecosystem is simple. We need to download the binary and move to the system path using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ sudo curl -Lo /usr/local/bin/ecs-cli \
https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-amd64-latest
$ sudo chmod +x /usr/local/bin/ecs-cli</pre>			<p>Run the following command to check whether <strong class="source-inline">ecs-cli</strong> has been <span class="No-Break">installed correctly:</span></p>
			<pre class="console">
$ ecs-cli --version
ecs-cli version 1.21.0 (bb0b8f0)</pre>			<p>As we can see, <strong class="source-inline">ecs-cli</strong> has been successfully installed on <span class="No-Break">our system.</span></p>
			<p>The next step is to allow <strong class="source-inline">ecs-cli</strong> to connect with your AWS API. You need to export your AWS CLI environment variables for this. Run the following commands to <span class="No-Break">do so:</span></p>
			<pre class="console">
$ export AWS_SECRET_ACCESS_KEY=...
$ export AWS_ACCESS_KEY_ID=..<a id="_idTextAnchor897"/>.
$ export AWS_DEFAULT_REGION=...</pre>			<p>Once we’ve set the environment variables, <strong class="source-inline">ecs-cli</strong> will use the<a id="_idTextAnchor898"/>m to authenticate with the AWS API. In the next section, <a id="_idTextAnchor899"/><a id="_idTextAnchor900"/>we’ll spin up an ECS cluster using the <span class="No-Break">ECS CLI.</span></p>
			<h2 id="_idParaDest-181">Spinning up a<a id="_idTextAnchor901"/>n ECS cluster</h2>
			<p>We can use the ECS CLI commands<a id="_idIndexMarker772"/> to spin up an ECS cluster. You can run your containers in EC2 and Fargate, so first, we will create a cluster that runs EC2 instances. Then, we will add Fargate tasks within <span class="No-Break">the cluster.</span></p>
			<p>To connect with your EC2 instances, you need to generate a key pair within AWS. To do so, run the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ aws ec2 create-key-pair --key-name ecs-keypair</pre>			<p>The output of this command will provide the key pair in a JSON file. Extract the JSON file’s key material and save that in a separate file called <strong class="source-inline">ecs-keypair.pem</strong>. Remember to replace the <strong class="source-inline">\n</strong> characters with a new line when you save <span class="No-Break">the file.</span></p>
			<p>Once we’ve generated the key pair, we can use the following command to create an ECS cluster using the <span class="No-Break">ECS CLI:</span></p>
			<pre class="console">
$ ecs-cli up --keypair ecs-keypair --instance-type t2.micro \
--size 2 --cluster cluster-1 --capability-iam
INFO[0002] Using recommended Amazon Linux 2 AMI with ECS Agent 1.72.0 and Docker version 
20.10.23
INFO[0003] Created cluster cluster=cluster-1 region=us-east-1
INFO[0004] Waiting for your cluster resources to be created...
INFO[0130] Cloudformation stack status stackStatus=CREATE_IN_PROGRESS
VPC created: vpc-0448321d209bf75e2
Security Group created: sg-0e30839477f1c9881
Subnet created: subnet-02200afa6716866fa
Subnet created: subnet-099582f6b0d04e419
Cluster creation succeeded.</pre>			<p>When we issue this command, in the background, AWS<a id="_idIndexMarker773"/> spins up a stack of resources using CloudFormation. CloudFormation is AWS’s <strong class="bold">Infrastructure-as-Code</strong> (<strong class="bold">IaC</strong>) solution that helps you deploy infrastructure on AWS through reusable template<a id="_idTextAnchor902"/>s. The CloudFormation template consists of several resources such as a VPC, a security group, a subnet within the VPC, a route table, a route, a subnet route table associat<a id="_idTextAnchor903"/>ion, an internet gateway, an IAM role, an instance profile, a launch configuration, an ASG, a VP<a id="_idTextAnchor904"/>C gateway attachment, and the cluster itself. The ASG contains two EC2 instances running and serving the cluster. Keep a copy<a id="_idIndexMarker774"/> of the output; we will need the details later during <span class="No-Break">the exercises.</span></p>
			<p>Now that <a id="_idTextAnchor905"/><a id="_idTextAnchor906"/>our cluster is up, we will spin up our <span class="No-Break">first task.</span></p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor907"/>Creating task definitions</h2>
			<p>ECS tasks are similar to Kubernetes<a id="_idIndexMarker775"/> pods. They are the basic building blocks of ECS and comprise one or more related containers. Task definitions are the blueprints for ECS tasks and define what the ECS task should look like. They are very similar to <strong class="source-inline">docker-compose</strong> files and are written in YAML format. ECS also uses all versions of <strong class="source-inline">docker-compose</strong> to allow us to define tasks. They help you define containers and their images, resource requirements, where they should run (EC2 or Fargate), volume and port mappings, and other <span class="No-Break">networking requirements.</span></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Using the <strong class="source-inline">docker-compose</strong> manifest to spin up tasks and services is a great idea, as it will help you align your configuration with an <span class="No-Break">open standard.</span></p>
			<p>A task is a finite process and only<a id="_idIndexMarker776"/> runs once. Even if it’s a long-running process, such as a web server, the task still runs once as it waits for the long-running process to end (which runs indefinitely in theory). The task’s life cycle follows the <strong class="bold">Pending</strong> -&gt; <strong class="bold">Running</strong> -&gt;<strong class="bold"> Stopped</strong> states. So, when you schedule your task, the task enters the <strong class="bold">Pending</strong> state, attempting to pull the image from the container registry. Then, it tries to start the container. Once the container has started, it enters the <strong class="bold">Running</strong> state. When the container has completed executing or errored out, it ends up in the <strong class="bold">Stopped</strong> state. A container with startup errors directly transitions from the <strong class="bold">Pending</strong> state to the <span class="No-Break"><strong class="bold">Stopped</strong></span><span class="No-Break"> state.</span></p>
			<p>Now, let’s go ahead and deploy an <strong class="source-inline">nginx</strong> web server task <a id="_idTextAnchor908"/>within the ECS cluster we <span class="No-Break">just created.</span></p>
			<p>To access the resources for this section, <strong class="source-inline">cd</strong> into the <span class="No-Break">following directory:</span></p>
			<pre class="console">
$ cd ~/modern-devops/ch7/ECS/tasks/EC2/</pre>			<p>We’ll use <strong class="source-inline">docker-compose</strong> task definitions here. So, let’s start by defining the following <span class="No-Break"><strong class="source-inline">docker-compose.yml</strong></span><span class="No-Break"> file:</span></p>
			<pre class="console">
version: '3'
services:
  web:
    image: nginx
    ports:
      - "80:80"
    logging:
      driver: awslogs
      options:
        awslogs-group: /aws/webserver
        awslogs-region: us-east-1
        awslogs-stream-prefix: ecs</pre>			<p>The YAML file defines<a id="_idIndexMarker777"/> a <strong class="source-inline">web</strong> container with an <strong class="source-inline">nginx</strong> image with host port <strong class="source-inline">80</strong> mapped to container port <strong class="source-inline">80</strong>. It uses the <strong class="source-inline">awslogs</strong> logging driver, which s<a id="_idTextAnchor909"/>treams logs into Amazon CloudWatch. It will stream the logs to the <strong class="source-inline">/aws/webserver</strong> log group in the <strong class="source-inline">us-east-1</strong> region with the <strong class="source-inline">ecs</strong> <span class="No-Break">stream prefix.</span></p>
			<p>The task definition also includes the resource definition—that is, the amount of resources we want to reserve for our task. Therefore, we will have to define the following <span class="No-Break"><strong class="source-inline">ecs-params.yaml</strong></span><span class="No-Break"> file:</span></p>
			<pre class="console">
version: 1
task_definition:
  services:
    web:
      cpu_shares: 100
      mem_limit: 524288000</pre>			<p>This YAML file defines <strong class="source-inline">cpu_shares</strong> in millicores and <strong class="source-inline">mem_limit</strong> in bytes <a id="_idTextAnchor910"/>for the container we plan<a id="_idIndexMarker778"/> to fire. N<a id="_idTextAnchor911"/><a id="_idTextAnchor912"/>ow, let’s look at scheduling this task as an <span class="No-Break">EC2 task.</span></p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor913"/>Scheduling EC2 tasks on ECS</h2>
			<p>Let’s use <strong class="source-inline">ecs-cli</strong> to apply the configuration<a id="_idIndexMarker779"/> and schedule our task using<a id="_idIndexMarker780"/> the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ ecs-cli compose up --create-log-groups --cluster cluster-1 --launch-type EC2</pre>			<p>Now that the task has been scheduled and the container is running, let’s list all the tasks to get the container’s details and find out where it is running. To do so, run the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ ecs-cli ps --cluster cluster-1
Name                    State    Ports                TaskDefinition
cluster-1/fee1cf28/web  RUNNING  34.237.218.7:80-&gt;80  EC2:1</pre>			<p>As we can see, the web container is running on <strong class="source-inline">cluster-1</strong> on <strong class="source-inline">34.237.218.7:80</strong>. Now, use the following command to curl this endpoint to see what <span class="No-Break">we get:</span></p>
			<pre class="console">
$ curl 34.237.218.7:80
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welco<a id="_idTextAnchor914"/>me to nginx!&lt;/title&gt;
...
&lt;/html&gt;</pre>			<p>Here, we get the default <strong class="source-inline">nginx</strong> home page! We’ve successfully scheduled a container on ECS using the EC2 launch type. You might<a id="_idIndexMarker781"/> want to duplicate<a id="_idIndexMarker782"/> this task to handle<a id="_idIndexMarker783"/> more<a id="_idTextAnchor915"/> traffic. This is know<a id="_idTextAnchor916"/><a id="_idTextAnchor917"/>n as horizontal scaling. We’ll se<a id="_idTextAnchor918"/>e how in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor919"/>Scaling tasks</h2>
			<p>We can easily scale<a id="_idIndexMarker784"/> tasks using <strong class="source-inline">ecs-cli</strong>. Use the following command to scale the tasks <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">2</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
$ ecs-cli compose scale 2 --cluster cluster-1 --launch-type EC2</pre>			<p>Now, use the following command to check whether two containers are running on <span class="No-Break">the cluster:</span></p>
			<pre class="console">
$ ecs-cli ps --cluster cluster-1
Name                    State    Ports                 TaskDefinition
cluster-1/b43bdec7/web  RUNNING  54.90.208.183:80-&gt;80  EC2:1
cluster-1/fee1cf28/web  RUNNING  34.237.218.7:80-&gt;80   EC2:1</pre>			<p>As we can see, two containers are running on the cluster. No<a id="_idTextAnchor920"/><a id="_idTextAnchor921"/>w, let’s query CloudWatch to get the logs of <span class="No-Break">the containers.</span></p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor922"/>Querying container logs from CloudWatch</h2>
			<p>To query logs from CloudWatch, we must list the log streams using<a id="_idIndexMarker785"/> the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ aws logs describe-log-streams --log-group-name /aws/webserver \
--log-stream-name-prefix ecs | grep logStreamName
  "logStreamName": "ecs/web/b43bdec7",
  "logStreamName": "ecs/web/fee1cf28",</pre>			<p>As we can see, there are two log streams for this—one for each task. <strong class="source-inline">logStreamName</strong> follows the convention <strong class="source-inline">&lt;log_stream_prefix&gt;/&lt;task_name&gt;/&lt;task_id&gt;</strong>. So, to get the logs for <strong class="source-inline">ecs/b43bdec7/web</strong>, run the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ aws logs get-log-events --log-group-name/aws/webserver \
--log-stream ecs/web/b43bdec7</pre>			<p>Here, you will see a stream of logs in JSON format<a id="_idIndexMarker786"/> in t<a id="_idTextAnchor923"/><a id="_idTextAnchor924"/>he response. Now, let’s look at how we can stop <span class="No-Break">running tasks.</span></p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor925"/>Stopping tasks</h2>
			<p><strong class="source-inline">ecs-cli</strong> uses the friendly <strong class="source-inline">doc<a id="_idTextAnchor926"/>ker-compose</strong> syntax for<a id="_idIndexMarker787"/> everything. Use the following command to stop the tasks in <span class="No-Break">the cluster:</span></p>
			<pre class="console">
$ ecs-cli compose down --cluster cluster-1</pre>			<p>Let’s list the containers to see whether the tasks have stopped by using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ ecs-cli ps --cluster cluster-1
INFO[0001] Stopping container... container=cluster-1/b43bdec7/web
INFO[0001] Stopping container... container=cluster-1/fee1cf28/web 
INFO[0008] Stopped container... container=cluster-1/b43bdec7/web 
desiredStatus=STOPPED lastStatus=STOPPED taskDefinition="EC2:1" 
INFO[0008] Stopped container... container=cluster-1/fee1cf28/web 
desiredStatus=STOPPED lastStatus=STOPPED taskDefinition="EC2:1"</pre>			<p>As we can see, both containers <span class="No-Break">have stopped.</span></p>
			<p>Running tasks on EC2 is not a serverless way of doing things. You still have to provision and manage the EC2 instanc<a id="_idTextAnchor927"/>es, and although ECS manages workloads on the cluster, you still have to pay for the amount of resources you’ve provisioned in the form of EC2 instances. AWS offers Fargate as a serverless solution where you pay per resource consumption. L<a id="_idTextAnchor928"/><a id="_idTextAnchor929"/>et’s look at how<a id="_idIndexMarker788"/> we can create the same task as a <span class="No-Break">Fargate task<a id="_idTextAnchor930"/>.</span></p>
			<h2 id="_idParaDest-187">Scheduling Fargate tasks on <a id="_idTextAnchor931"/>ECS</h2>
			<p>Scheduling tasks on Fargate is very similar<a id="_idIndexMarker789"/> to EC2. Here, we need to specify<a id="_idIndexMarker790"/> the <strong class="source-inline">launch-type</strong> value <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">FARGATE</strong></span><span class="No-Break">.</span></p>
			<p>To schedule the same task on Fargate, run the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ ecs-cli compose up --create-log-groups --cluster cluster-1 --launch-type FARGATE
FATA[0001] ClientException: Fargate only supports network mode 'awsvpc'.</pre>			<p>Oops! We have a problem! Well, it’s complaining about the network type. For a Fargate task, we must supply the network type as <strong class="source-inline">awsvpc</strong> instead of the default bridge network. The <strong class="source-inline">awsvpc</strong> network<a id="_idIndexMarker791"/> is an overlay network t<a id="_idTextAnchor932"/>hat implements the <strong class="bold">Container Network Interface</strong> (<strong class="bold">CNI</strong>). To understand more about Docker networking, please refer to <a href="B19877_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">The Modern Way of DevOps</em>. For now, let’s go ahead and configure the <strong class="source-inline">awsvpc</strong> network type. But before that, the Fargate task requires a <span class="No-Break">few configurations.</span></p>
			<p>To access the resources for this section, <strong class="source-inline">cd</strong> into the <span class="No-Break">following directory:</span></p>
			<pre class="console">
$ cd ~/modern-devops/ch7/ECS/tasks/FARGATE/</pre>			<p>First, we’ll have to assume a task execution role for the ECS agent to authenticate with the AWS API and interact <span class="No-Break">with Fargate.</span></p>
			<p>To do so, create the following <span class="No-Break"><strong class="source-inline">task-execution-assume-role.json</strong></span><span class="No-Break"> file:</span></p>
			<pre class="console">
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "",
      "Effect": "Allow",
      "Principal": {
        "Service": "ecs-tasks.amazonaws.com"<a id="_idTextAnchor933"/>
      },
      "Action": "sts:AssumeRole"
    }
  ]
}</pre>			<p>Then, use the following command<a id="_idIndexMarker792"/> to assume the task <span class="No-Break">execution</span><span class="No-Break"><a id="_idIndexMarker793"/></span><span class="No-Break"> role:</span></p>
			<pre class="console">
$ aws iam --region us-east-1 create-role --role-name ecsTaskExecutionRole \
 --assume-role-policy-document file://task-execution-assume-role.json </pre>			<p>ECS provides a default role policy called <strong class="source-inline">AmazonECSTaskExecutionRolePolicy</strong>, which contains various permissions<a id="_idIndexMarker794"/> that help you interact with CloudWatch and <strong class="bold">Elastic Container Registry</strong> (<strong class="bold">ECR</strong>). The following JSON code outlines the permission that the <span class="No-Break">policy has:</span></p>
			<pre class="console">
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ecr:GetAuthorizationToken",
                "ecr:BatchCheckLayerAvailability",
                "ecr:GetDownloadUrlForLayer",
                "ecr:BatchGetImage",
                "logs:CreateLogStream",
                "logs:PutLogEvents"
            ],
            "Resource": "*"
        }
    ]
}</pre>			<p>We have to assign this role policy to the <strong class="source-inline">ecsTaskExecution</strong> role we assumed previously by using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ aws iam attach-role-policy \
--policy-arn arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy \
--role-name ecsTaskExecutionRole</pre>			<p>Once we’ve assigned the policy<a id="_idIndexMarker795"/> to the <strong class="source-inline">ecsTaskExecution</strong> role, we need to source the ID of both subnets <a id="_idIndexMarker796"/>and the security group of the ECS cluster <a id="_idTextAnchor934"/>when we created it. You can find those details in the command-line output from when we created the cluster. We will use these details in the following <span class="No-Break"><strong class="source-inline">ecs-params.yml</strong></span><span class="No-Break"> file:</span></p>
			<pre class="console">
version: 1
task_definition:
  task_execution_role: ecsTaskExecutionRole
  ecs_network_mode: awsvpc
  task_size:
    mem_limit: 0.5GB
    cpu_limit: 256
run_params:
  network_configuration:
    awsvpc_configuration:
      subnets:
        - "subnet-088b52c91a6f40fd7"
        - "subnet-032cd63290da67271"
      security_groups:
        - "sg-097206175813aa7e7"
      assign_public_ip: ENABLED</pre>			<p>The <strong class="source-inline">ecs-params.yml</strong> file consists of <strong class="source-inline">task_execution_role</strong>, which we created, and <strong class="source-inline">ecs_network_mode</strong> set to <strong class="source-inline">awsvpc</strong>, as Fargate requires. We’ve defined <strong class="source-inline">task_size</strong> to have <strong class="source-inline">0.5G<a id="_idTextAnchor935"/>B</strong> of memory and <strong class="source-inline">256</strong> millicores of CPU. So, since Fargate is a serverless solution, we only pay for the CPU cores and memory we consume. The <strong class="source-inline">run_params</strong> section consists of <strong class="source-inline">network_configuration</strong>, which contains <strong class="source-inline">awsvpc_configuration</strong>. <a id="_idTextAnchor936"/>Here, we specify both subnets created when we created the ECS cluster. We must also specify <strong class="source-inline">security_groups</strong>, which we created with the <span class="No-Break">ECS cluster.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Use the subnets and security groups of your ECS cluster instead of copying the ones in <span class="No-Break">this example.</span></p>
			<p>Now that we’re ready<a id="_idIndexMarker797"/> to fire the task on Fargate, let’s run<a id="_idIndexMarker798"/> the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ ecs-cli compose up --create-log-groups --cluster cluster-1 --launch-type FARGATE</pre>			<p>Now, let’s check whether the task is running successfully by using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ ecs-cli ps --cluster cluster-1
Name                    State    Ports           TaskDefinition
cluster-1/8717a149/web  RUNNING  3.80.173.230:80 FARGATE:1</pre>			<p>As we can see, the task is running on <strong class="source-inline">3.80.173.230:80</strong> as a Fargate task. Let’s <strong class="source-inline">curl</strong> this URL to see whether we get a response by using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ curl 3.80.173.230:80
&lt;html&gt;
&lt;he<a id="_idTextAnchor937"/>ad&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
...
&lt;/body&gt;
&lt;/html&gt;</pre>			<p>As we can see, we get the default <strong class="source-inline">nginx</strong> <span class="No-Break">home page.</span></p>
			<p>Now, let’s go ahead and delete the task we created by using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$<a id="_idTextAnchor938"/> ecs-cli compose down --cluster cluster-1</pre>			<p>As we already know, tasks<a id="_idIndexMarker799"/> have a set life cycle, and once they stop, they stop. You cannot start the same task again. Therefore, we must create a <strong class="bold">service</strong> to ensure that a certain numb<a id="_idTextAnchor939"/><a id="_idTextAnchor940"/>er of tasks<a id="_idIndexMarker800"/> are always running. We’ll creat<a id="_idTextAnchor941"/>e a service in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-188">Scheduling services o<a id="_idTextAnchor942"/>n ECS</h2>
			<p><strong class="bold">ECS services</strong> are similar to Kubernetes <strong class="bold">ReplicaSets</strong>. They ensure that a certain<a id="_idIndexMarker801"/> number of tasks are always running<a id="_idIndexMarker802"/> at a particular time. To schedule<a id="_idIndexMarker803"/> a service, we can use the <strong class="source-inline">ecs-cli</strong> <span class="No-Break">command line.</span></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Always use services for applications that are long-running, such as web servers. For batch jobs, always use tasks, <a id="_idTextAnchor943"/>as we don’t want to recreate the job after <span class="No-Break">it ends.</span></p>
			<p>To run the <strong class="source-inline">nginx</strong> web server as a service, we can use the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ ecs-cli compose service up --create-log-groups \
 --cluster cluster-1 --launch-type FARGATE
INFO[0001] Using ECS task definition TaskDefinition="FARGATE:1"
INFO[0002] Auto-enabling ECS Managed Tags
INFO[0013] (service FARGATE) has started 1 tasks: (task 9b48084d).  timestamp="2023-07-03 
11:24:42 +0000 UTC"
INFO[0029] Service status desiredCount=1 runningCount=1 serviceName=FARGATE
INFO[0029] (service FARGATE) has reached a steady state.  timestamp="2023-07-03 11:25:00 
+0000 UTC"
INFO[0029] (service FARGATE) (deployment ecs-svc/94284856) deployment 
completed.  timestamp="2023-07-03 11:25:00 UTC"
INFO[0029] ECS Service has reached a stable state desiredCount=1 runningCount=1 
serviceName=FARGATE
INFO[0029] Created an ECS service service=FARGATE taskDefinition="FARGATE:1"</pre>			<p>Looking at the logs, we can see that the service<a id="_idIndexMarker804"/> is trying to ensure that the task’s desired count matches the task’s running count. If your task is deleted, ECS will replace it with a <span class="No-Break">new one.</span></p>
			<p>Let’s list the tasks and see what we get by using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ ecs-cli ps --cluster cluster-1
Name                    State   Ports             TaskDefinition
cluster-1/9b48084d/web  RUNNING 18.234.123.71:80  FARGATE:1</pre>			<p>As we can see, the service has created a new task that is running on <strong class="source-inline">18.234.123.71:80</strong>. Let’s try to access this URL using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ curl 18.234.123.71
&lt;!DOC<a id="_idTextAnchor944"/>TYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
...
&lt;/html&gt;</pre>			<p>We get the default<a id="_idTextAnchor945"/><a id="_idTextAnchor946"/> <strong class="source-inline">nginx</strong> home page<a id="_idIndexMarker805"/> in the response. Now, let’s try to browse<a id="_idTextAnchor947"/> the logs of <span class="No-Break">the task.</span></p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor948"/>Browsing container logs using the ECS CLI</h2>
			<p>Apart from using Amazon<a id="_idIndexMarker806"/> CloudWatch, you can also use the friendl<a id="_idTextAnchor949"/>y ECS CLI <a id="_idIndexMarker807"/>to do this, irrespective of where your logs<a id="_idIndexMarker808"/> are stored. This helps us see everything from a single pane <span class="No-Break">of glass.</span></p>
			<p>Run the following command to <span class="No-Break">do so:</span></p>
			<pre class="console">
$ ecs-cli logs --task-id 9b48084d --cluster cluster-1
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform 
configuration
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/
default.conf
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2023/07/03 11:24:57 [notice] 1#1: nginx/1.25.1
2023/07/03 11:24:57 [notice] 1#1: built by gcc 12.2.0 (Debian 12.2.0-14)
2023/07/03 11:24:57 [notice] 1#1: OS: Linux 5.10.184-175.731.amzn2.x86_64
2023/07/03 11:24:57 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 65535:65535
2023/07/03 11:24:57 [notice] 1#1: start worker processes
2023/07/03 11:24:57 [notice] 1#1: start worker process 29
2023/07/03 11:24:57 [notice] 1#1: start worker process 30
13.232.8.130 - - [03/Jul/2023:<a id="_idTextAnchor950"/>11:30:38 +0000] "GET / HTTP/1.1" 200 615 "-" "curl/7.81.0" 
"-"</pre>			<p>As we can see, we can browse<a id="_idIndexMarker809"/> the logs for the<a id="_idTextAnchor951"/><a id="_idTextAnchor952"/> particular task this service<a id="_idIndexMarker810"/> is running. Now, let’s <a id="_idTextAnchor953"/>go ahead and delete <span class="No-Break">the service.</span></p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor954"/>Deleting an ECS service</h2>
			<p>To delete the service, run<a id="_idIndexMarker811"/> the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ ecs-cli compose service down --cluster cluster-1
INFO[0001] Deleted ECS service service=FARGATE
INFO[0001] Service status desiredCount=0 runningCount=1 serviceName=FARGATE
INFO[0006] Service status desiredCount=0 runningCount=0 serviceName=FARGATE
INFO[0006] (service FARGATE) has stopped 1 running tasks: (task 
9b48084d11cf49be85141fd9bfe9e1c3).  timestamp="2023-07-03 11:34:10 +0000 UTC"
INFO[0006] ECS Service has reached a stable state desiredCount=0 runningCount=0 
serviceName=FARGATE</pre>			<p>As we can see, the service has <span class="No-Break">been deleted.</span></p>
			<p>Note that even if we create multiple instances of tasks, they run on different IP addresses and can be accessed separately. However, tasks need to be load-balanced, and we<a id="_idTextAnchor955"/><a id="_idTextAnchor956"/> need to provide a single endpoint. Let’s look at a solution we can use<a id="_idTextAnchor957"/> to <span class="No-Break">manage this.</span></p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor958"/>Load balancing containers running on ECS</h2>
			<p><strong class="bold">Load balancing</strong> is an essential functionality of multi-instance<a id="_idIndexMarker812"/> applications. They help us serve the application on a single endpoint. Therefore, you can have multiple instances of your applications running simultaneously, and the end<a id="_idIndexMarker813"/> user doesn’t need to worry about which <a id="_idIndexMarker814"/>instance<a id="_idIndexMarker815"/> they’re calling. AWS provides two main load-balancing solutions—<strong class="bold">Layer 4</strong> with the <strong class="bold">Network Load Balancer</strong> (<strong class="bold">NLB</strong>) and <strong class="bold">Layer 7</strong> with the <strong class="bold">Application Load </strong><span class="No-Break"><strong class="bold">Balancer</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">ALB</strong></span><span class="No-Break">).</span></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">While both load balancers have their use cases, a Layer 7 load balancer provides a significant advantage for HTTP-based applications. It offers advanced traffic management, such as path-based and <span class="No-Break">host-based routing.</span></p>
			<p>So, let’s go ahead and create an ALB to frontend our tasks using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ aws elbv2 create-load-balancer --name ecs-alb --subnets &lt;SUBNET-1&gt; &lt;SUBNET-2&gt; \
--security-groups &lt;SECURITY_GROUP_ID&gt; --region us-east-1</pre>			<p>The output of the preceding command contains values for <strong class="source-inline">LoadBalancerARN</strong> and <strong class="source-inline">DNSName</strong>. We will need to use them in the subsequent steps, so keep a copy of the <span class="No-Break">output safe.</span></p>
			<p>The next step will be to create a <strong class="bold">target group</strong>. The target group defines the group of tasks and the port they will be<a id="_idIndexMarker816"/> listening to, and the load balancer will forward traffic to it. Use the following command to define a <span class="No-Break">target group:</span></p>
			<pre class="console">
$ aws elbv2 create-target-group --name target-group --protocol HTTP \
 --port 80 --target-type ip --vpc-id &lt;VPC_ID&gt; --region us-east-1</pre>			<p>You will get the <strong class="source-inline">targetGroupARN</strong> value in the response. Keep it safe, as we will need it in the <span class="No-Break">next step.</span></p>
			<p>Next, we will need a <strong class="bold">listener</strong> running on the load balancer. This should<a id="_idIndexMarker817"/> forward traffic from the load balancer to the target group. Use the following command to <span class="No-Break">do so:</span></p>
			<pre class="console">
$ aws elbv2 create-listener  --load-balancer-arn &lt;LOAD_BALANCER_ARN&gt; \
 --protocol HTTP --port 80 \
 --default-actions Type=forward,TargetGroupArn=&lt;TARGET_GROUP_ARN&gt; \
 --region us-east-1 </pre>			<p>You will get the <strong class="source-inline">listenerARN</strong> value in response to this<a id="_idTextAnchor959"/> command. Please keep that handy; we will need it in the <span class="No-Break">next step.</span></p>
			<p>Now that we’ve defined the load balancer, we need to run <strong class="source-inline">ecs-cli compose service up</strong> to deploy our service. We will also provide the target<a id="_idIndexMarker818"/> group as a parameter to associate our service with the <span class="No-Break">load balancer.</span></p>
			<p>To access the resources for this section, <strong class="source-inline">cd</strong> into the <span class="No-Break">following directory:</span></p>
			<pre class="console">
$ cd ~/modern-devops/ch7/ECS/loadbalancing/</pre>			<p>Run the following command to <span class="No-Break">do so:</span></p>
			<pre class="console">
$ ecs-cli compose service up --create-log-groups --cluster cluster-1 \
--launch-type FARGATE --target-group-arn &lt;TARGET_GROUP_ARN&gt; \
--container-name web --container-port 80</pre>			<p>Now that the service and our task are running on Fargate, we can scale our service to three desired tasks. To do so, run the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ ecs-cli compose service scale 3 --cluster cluster-1</pre>			<p>Since our service has scaled to three tasks, let’s go ahead and hit the load balancer DNS endpoint we captured in the first step. This should provide us with the default <strong class="source-inline">nginx</strong> response. Run the following command to <span class="No-Break">do so:</span></p>
			<pre class="console">
$ curl ecs-alb-1660189891.us-east-1.elb.amazonaws.com
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
…
&lt;/html&gt;</pre>			<p>As we can see, we get a default <strong class="source-inline">nginx</strong> <a id="_idTextAnchor960"/>response from the load balancer. This shows that load balancing is <span class="No-Break">working well!</span></p>
			<p>ECS provides a host of other features, such as horizontal autoscaling, customizable task placement <a id="_idIndexMarker819"/>algorithms, and others, but they are beyond the scope of this book. <a id="_idTextAnchor961"/>Please read the ECS documentation to learn more about oth<a id="_idTextAnchor962"/><a id="_idTextAnchor963"/>er aspects of the tool. Now, let’s look at other popular CaaS products available on <span class="No-Break">the market.</span></p>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor964"/>Other CaaS services</h1>
			<p>Amazon ECS provides<a id="_idIndexMarker820"/> a ver<a id="_idTextAnchor965"/>satile way of managing your container workloads. It works great when you have a smaller, simpler architecture and don’t want to add the additional overhead of using a complex container orchestration engine such <span class="No-Break">as Kubernetes.</span></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">ECS is an excellent tool choice if you run exclusively on AWS and don’t have a future multi-cloud or hybrid-cloud strategy. Fargate makes deploying and running your containers easier without worrying about the infrastructure behind <span class="No-Break">the scenes.</span></p>
			<p>ECS is tightly <a id="_idIndexMarker821"/>coupled with AWS<a id="_idIndexMarker822"/> and its architecture. To solve this problem, we can use managed serv<a id="_idTextAnchor966"/>ices within AWS, such as <strong class="bold">Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>). It offers the Kubernetes API to schedule your workloads. This makes managing containers even more versatile as you can easily spin up a Kubernetes cluster and use a standard, open source solution that you can install and run anywhere you like. This does not tie you to a particular vendor. However, EKS is slightly more expensive than ECS and adds a <em class="italic">$0.10</em> per hour cluster management charge. That is nothing in comparison to the benefits you get out <span class="No-Break">of it.</span></p>
			<p>If you aren’t running on AWS, there are options from other providers too. The next of the big<a id="_idTextAnchor967"/><a id="_idIndexMarker823"/> three cloud providers is Azure, which offers <strong class="bold">Azure Kubernetes Service (AKS)</strong>, a managed Kubernetes solution that can help you get started in minutes. AKS provides a fully managed solution with event-driven elastic provisioning<a id="_idIndexMarker824"/> for worker nodes<a id="_idIndexMarker825"/> as and when required. It also integrates nicely with <strong class="bold">Azure DevOps</strong>, giving you a faster <strong class="bold">end-to-end</strong> (<strong class="bold">E2E</strong>) development experience. As with AWS, Azure also charges <em class="italic">$0.10</em> per hour for <span class="No-Break">cluster management.</span></p>
			<p><strong class="bold">Google Kubernetes Engine (GKE)</strong> is one of the most robust Kubernetes platforms. Since the Kubernetes project <a id="_idIndexMarker826"/>came from Google and is the largest contributor to this project in the open source community, GKE is generally quicker to roll out newer versions and is the first to release security patches into the solution. Also, it is one of the most feature-rich with customizable solutions and offers several plugins as a cluster configuration. Therefore, you can choose what to install on Bootstrap and further harden your cluster. However, all these come at a cost, as GKE charges a <em class="italic">$0.10</em> cluster management charge per hour, just like AWS <span class="No-Break">and Azure.</span></p>
			<p>You can use <strong class="bold">Google Cloud Run</strong> if you don’t want to use Kubernetes if your architecture is not complicated, and there are only<a id="_idIndexMarker827"/> a few containers to manage. Google Cloud Run<a id="_idIndexMarker828"/> is a serverless CaaS solution built on the open source <strong class="bold">Knative</strong> project. It helps you run your containers without any vendor lock-in. Since it is serverless, you only pay for the number of containers you use<a id="_idIndexMarker829"/> and their resource<a id="_idIndexMarker830"/> utilization. It is a fully<a id="_idIndexMarker831"/> scalable and well-integrated<a id="_idIndexMarker832"/> solution with Google Cloud’s DevOps and monitoring solutions such as <strong class="bold">Cloud Code</strong>, <strong class="bold">Cloud Build</strong>, <strong class="bold">Cloud Monitoring</strong>, and <strong class="bold">Cloud Logging</strong>. The best part is that it is comparable to AWS Fargate and abstracts all infrastructure behind the scenes. So, it’s a minimal Ops<a id="_idIndexMarker833"/> or <span class="No-Break">NoOps<a id="_idTextAnchor968"/><a id="_idTextAnchor969"/> solution.</span></p>
			<p>Now that we’ve mentioned Knative as an open source CaaS solution, let’s discuss it in <span class="No-Break">more detail.</span></p>
			<h1 id="_idParaDest-193"><a id="_idTextAnchor970"/>Open source CaaS with Knative</h1>
			<p>As we’ve seen, several <a id="_idIndexMarker834"/>vendor-specific CaaS services <a id="_idIndexMarker835"/>are available on the market. Still, the problem with most of them is that they are tied up to a single cloud provider. Our container deployment specification then becomes vendor-specific and results in vendor lock-in. As modern DevOps engineers, we must ensure that the proposed solution best fits the architecture’s needs, and avoiding vendor lock-in is one of the most <span class="No-Break">important requirements.</span></p>
			<p>However, Kubernetes in itself is not serverless. You must have infrastructure defined, and long-running services should have at least a single instance running at a particular time. This makes managing microservices applications a pain <span class="No-Break">and resource-intensive.</span></p>
			<p>But wait! We said that microservices help optimize infrastructure consumption. Yes—that’s correct, but they do so within the container space. Imagine that you have a shared cluster of VMs where parts of the application scale with traffic, and each part of the application has its peaks and troughs. Doing this will save a lot of infrastructure by performing this <span class="No-Break">simple multi-tenancy.</span></p>
			<p>However, it also means that you must have at least one instance of each microservice running every time—even if there is zero traffic! Well, that’s not the best utilization we have. How about creating instances when you get the first hit and not having any when you don’t have traffic? This would save a lot of resources, especially when things are silent. You can have hundreds of microservices making up the application that would not have any instances during an idle period. If you combine it with a managed service that runs Kubernetes and then autoscale your VM instances with traffic, you can have minimal instances during the <span class="No-Break">silent period.</span></p>
			<p>There have be<a id="_idTextAnchor971"/>en attempts<a id="_idIndexMarker836"/> within the open source and cloud-native<a id="_idIndexMarker837"/> space to develop <a id="_idIndexMarker838"/>an open source, vendor-agnostic, serverless framework for containers. We have Knative for this, which the <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>) <span class="No-Break">has adopted.</span></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The Cloud Run service uses Knative behind the scenes. So, if you use Google Cloud, you can use Cloud Run to<a id="_idTextAnchor972"/><a id="_idTextAnchor973"/> use a fully managed <span class="No-Break">serverless offering.</span></p>
			<p>To understand how Knative wor<a id="_idTextAnchor974"/>ks, let’s look at the <span class="No-Break">Knative architecture.</span></p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor975"/>Knative architecture</h2>
			<p>The Knative project<a id="_idIndexMarker839"/> combines elements of <a id="_idIndexMarker840"/>existing CNCF<a id="_idIndexMarker841"/> projects such as Kubernetes, <strong class="bold">Istio</strong>, <strong class="bold">Prometheus</strong>, and <strong class="bold">Grafana</strong> and eventing engines such as <strong class="bold">Kafk<a id="_idTextAnchor976"/>a</strong> and <strong class="bold">Google Pub/Sub</strong>. Knative runs as a Kubernetes<a id="_idIndexMarker842"/> operator using Kubernetes <strong class="bold">Custom Resource Definitions</strong> (<strong class="bold">CRDs</strong>), which help operators<a id="_idIndexMarker843"/> administer Knative<a id="_idIndexMarker844"/> using the <strong class="source-inline">kubectl</strong> command line. Knative provides its API for developers, which the <strong class="source-inline">kn</strong> command-line utility<a id="_idIndexMarker845"/> can use. The users are pr<a id="_idTextAnchor977"/>ovided access through Istio, which, wi<a id="_idTextAnchor978"/>th its traffic management<a id="_idIndexMarker846"/> features, is a crucial component of Knative. The following diagram describes <span class="No-Break">this graphically:</span></p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B19877_07_2.jpg" alt="Figure 7.2 – Knative architecture" width="1168" height="1033"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Knative architecture</p>
			<p>Knative consists of two main modules—<strong class="source-inline">serving</strong> and <strong class="source-inline">eventing</strong>. While the <strong class="source-inline">serving</strong> module helps us maintain stateless applications using HTTP/S endpoints, the <strong class="source-inline">eventing</strong> module integrates with eventing engines such as Kafka and Google Pub/Sub. As we’ve discussed mostly HTTP/S traffic, we will scope our discussion to Knative serving for <span class="No-Break">this book.</span></p>
			<p>Knative maintains serving pods, which help route traffic within workload pods and act as proxies using the <strong class="bold">Istio Ingress Gateway</strong> component. It provides a virtual endpoint for your service and listens to it. When it discovers a hit on the endpoint, it creates the required Kubernetes components to serve that traffic. Therefore, Knative has the functionality to <a id="_idTextAnchor979"/>scale from zero workload pods as it will spin up a pod when it receives traffic for it. The following<a id="_idIndexMarker847"/> diagram <span class="No-Break">shows how:</span></p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B19877_07_3.jpg" alt="Figure 7.3 – Knative serving architecture" width="1081" height="805"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Knative serving architecture</p>
			<p>Knative endpoints are made up of three basic parts—<strong class="source-inline">&lt;app-name&gt;</strong>, <strong class="source-inline">&lt;namespace&gt;</strong>, and <strong class="source-inline">&lt;custom-domain&gt;</strong>. While <strong class="source-inline">name</strong> and <strong class="source-inline">namespace</strong> are similar to Kubernetes Services, <strong class="source-inline">custom-domain</strong> is defined<a id="_idIndexMarker848"/> by us. It can be a legitimate domain for your organization or a <strong class="bold">MagicDNS</strong> solution, such as <strong class="bold">sslip.io,</strong> which we will use in our hands-on exercises. If you are using<a id="_idIndexMarker849"/> your organization domain, you must create <a id="_idIndexMarker850"/>your DNS configuration to resolve the domain to the Istio Ingress Gateway <span class="No-Break">IP addresses.</span></p>
			<p>Now, let’<a id="_idTextAnchor980"/>s go ahead and <span class="No-Break">install Knative.</span></p>
			<p>For the exercises, we will use GKE. Since GKE is a highly robust Kubernetes cluster, it is a great choi<a id="_idTextAnchor981"/>ce for integrating with Knative. As mentioned previously, Google Clou<a id="_idTextAnchor982"/><a id="_idTextAnchor983"/>d provides a free trial of $300 for<a id="_idTextAnchor984"/> 90 days. You can sign up at <a href="https://cloud.google.com/free">https://cloud.google.com/free</a> if you’ve not done <span class="No-Break">so already.</span></p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor985"/>Spinning up GKE</h2>
			<p>Once you’ve signed up and <a id="_idIndexMarker851"/>are on your console, you can open the Google Cloud Shell CLI to run the <span class="No-Break">following commands.</span></p>
			<p>You need to enable the GKE API first using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ gcloud services enable container.googleapis.com</pre>			<p>To create a two-node autoscaling GKE cluster that scales from <strong class="source-inline">1</strong> node to <strong class="source-inline">5</strong> nodes, run the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ gcloud container clusters create cluster-1 --num-nodes 2 \ 
--enable-autoscaling --min-nodes 1 --max-nodes 5 --zone us-central1-a</pre>			<p>And that’s it! The cluster is up <span class="No-Break">and running.</span></p>
			<p>You will also need to clone the following GitHub repository for some of <span class="No-Break">the exercises:</span></p>
			<p><a href="https://github.com/PacktPublishing/Modern-DevOps-Practices-2e"><span class="No-Break">https://github.com/PacktPublishing/Modern-DevOps-Practices-2e</span></a></p>
			<p>Run the following command to clone the repository into your home directory. Then, <strong class="source-inline">cd</strong> into the <strong class="source-inline">ch7</strong> directory to access the <span class="No-Break">required resources:</span></p>
			<pre class="console">
$ git clone https://github.com/PacktPublishing/Modern-DevOps-Practices-2e.git \
  modern-devops</pre>			<p>Now that the <a id="_idTextAnchor986"/>cluster is up and runnin<a id="_idTextAnchor987"/>g, let’s go ahead and <span class="No-Break">install Knative.</span></p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor988"/>Installing Knative</h2>
			<p>We will install the CRDs<a id="_idIndexMarker852"/> that define Knative resources as Kubernetes <span class="No-Break">API resources.</span></p>
			<p>To access the resources for this section, <strong class="source-inline">cd</strong> into the <span class="No-Break">following directory:</span></p>
			<pre class="console">
$ cd ~/modern-devops/ch7/knative/</pre>			<p>Run the following command to install <span class="No-Break">the CRDs:</span></p>
			<pre class="console">
$ kubectl apply -f \
 https://github.com/knative/serving/releases/download/knative-v1.10.2/serving-crds.yaml</pre>			<p>As we can see, Kubernetes has installed some CRDs. Next, we must install the core components of the Knative <strong class="source-inline">serving</strong> module. Use the following command to <span class="No-Break">do so:</span></p>
			<pre class="console">
$ kubectl apply -f \
 https://github.com/knative/serving/releases/download/knative-v1.10.2/serving-core.yaml</pre>			<p>Now that the core serving components have been installed, the next step is installing Istio within the Kubernetes cluster. To do so, run the <span class="No-Break">following commands:</span></p>
			<pre class="console">
$ curl -L https://istio.io<a id="_idTextAnchor989"/>/downloadIstio | sh -
$ sudo mv istio-*/bin/istioctl /usr/local/bin
$ istioctl install --set profile=demo -y</pre>			<p>Now that Istio has been installed, we will wait for the Istio Ingress Gateway component to be assigned an external IP address. Run the following command to check this until you get an external IP in <span class="No-Break">the response:</span></p>
			<pre class="console">
$ kubectl -n istio-system get service istio-ingressgateway
NAME                TYPE         EXTERNAL-IP   PORT(S)
istio-ingressgteway LoadBalancer 35.226.198.46 15021,80,443</pre>			<p>As we can see, we’ve been assigned an external IP—<strong class="source-inline">35.226.198.46</strong>. We will use this IP for the rest of <span class="No-Break">this exercise.</span></p>
			<p>Now, we will install the Knative Istio<a id="_idIndexMarker853"/> controller by using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ kubectl apply -f \
  https://github.com/knative/net-istio/releases/download/knative-v1.10.1/net-istio.yaml</pre>			<p>Now that the controller has been installed, we must configure the DNS so that Knative can provide custom endpoints. To do so, we can use the MagicDNS solution known as <strong class="source-inline">sslip.io</strong>, which you can use for experimentation. The MagicDNS solution resolves any endpoint to the IP address present in the subdomain. For example, <strong class="source-inline">35.226.198.46.sslip.io</strong> resolves <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">35.226.198.46</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Do not use MagicDNS in production. It is an experimental DNS service and should only be used for <span class="No-Break">evaluating Knative.</span></p>
			<p>Run the following command to configure <span class="No-Break">the DNS:</span></p>
			<pre class="console">
$ kubectl apply -f \
https://github.com/knative/serving/releases/download/knative-v1.10.2\
/serving-default-domain.yaml</pre>			<p>As you can see, it provides a batch<a id="_idIndexMarker854"/> job that gets fired whenever there is a <span class="No-Break">DNS request.</span></p>
			<p>Now, let’s install the <strong class="bold">HorizontalPodAutoscaler</strong> (<strong class="bold">HPA</strong>) add-on to automatically help us autoscale pods on the cluster with traffic. To do so, run the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ kubectl apply -f \
https://github.com/knative/serving/releases/download/knative-v1.10.2/serving-hpa.yaml</pre>			<p>That completes our <span class="No-Break">Knative installation.</span></p>
			<p>Now, we need to install<a id="_idIndexMarker855"/> and configure the <strong class="source-inline">kn</strong> command-line utility. Use the following commands to <span class="No-Break">do so:</span></p>
			<pre class="console">
$ sudo curl -Lo /usr/local/bin/kn \ 
https://github.com/knative/client/releases/download/knative-v1.10.0/kn-linux-amd64
$ sudo chmod +x /usr/local/bin/kn</pre>			<p>In the next section, w<a id="_idTextAnchor990"/>e’ll deploy our first application <span class="No-Break">on Knative.</span></p>
			<h2 id="_idParaDest-197">Deployin<a id="_idTextAnchor991"/>g a Python Flask application on Knative</h2>
			<p>To understand Knative, let’s try to build<a id="_idIndexMarker856"/> and deploy a Flask application<a id="_idIndexMarker857"/> that outputs the current timestamp in the response. Let’s start by building <span class="No-Break">the app.</span></p>
			<h3>Building the Python Flask app</h3>
			<p>We will have to create<a id="_idIndexMarker858"/> a few files to build such <span class="No-Break">an app.</span></p>
			<p>The <strong class="source-inline">app.py</strong> file looks <span class="No-Break">like this:</span></p>
			<pre class="console">
import os
import datetime
from flask import Flask
app = Flask(__name__)
@app.route('/')
def current_time():
  ct = datetime.datetime.now()
  return 'The current time is : {}!\n'.format(ct)
if __name__ == "__main__":
  app.run(debug=True,host='0.0.0.0')</pre>			<p>We will need the <a id="_idIndexMarker859"/>following Dockerfile to build <span class="No-Break">this application:</span></p>
			<pre class="console">
FROM python:3.7-slim
ENV PYTHONUNBUFFERED True
ENV APP_HOME /app
WORKDIR $APP_HOME
COPY . ./
RUN pip install Flask gunicorn
CMD exec gunicorn --bind :$PORT --workers 1 --threads 8 --timeout 0 app:app</pre>			<p>Now, let’s go ahead and build the Docker container using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ docker build -t &lt;your_dockerhub_user&gt;/py-time .</pre>			<p>Now tha<a id="_idTextAnchor992"/>t the image is ready, let’s push it to Docker Hub by using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ docker push &lt;your_dockerhub_user&gt;/py-time</pre>			<p>As we’ve successfully pushed<a id="_idIndexMarker860"/> the image, we can run this <span class="No-Break">on Knative.</span></p>
			<h3>Deploying the Python Flask app on Knative</h3>
			<p>We can use the <strong class="source-inline">kn</strong> command line or create<a id="_idIndexMarker861"/> a manifest file to deploy the app. Use<a id="_idIndexMarker862"/> the following command to deploy <span class="No-Break">the application:</span></p>
			<pre class="console">
$ kn service create py-time --image &lt;your_dockerhub_user&gt;/py-time
Creating service 'py-time' in namespace 'default':
  9.412s Configuration "py-time" is waiting for a Revision to become ready.
  9.652s Ingress has not yet been reconciled.
  9.847s Ready to serve.
Service 'py-time' created to latest revision 'py-time-00001' is available at URL:
http://py-time.default.35.226.198.46.sslip.io</pre>			<p>As we can see, Knative has deployed the app and provided a custom endpoint. Let’s <strong class="source-inline">curl</strong> the endpoint to see what <span class="No-Break">we get:</span></p>
			<pre class="console">
$ curl http://py-time.default.35.226.198.46.sslip.io
The current time is : 2023-07-03 13:30:20.804790!</pre>			<p>We get the current time in the response. As we already know, Knative should detect whether there is no traffic coming into the pod and delete it. Let’s watch the pods for some time and see <span class="No-Break">what happens:</span></p>
			<pre class="console">
$ kubectl get pod -w
NAME                            READY  STATUS        RESTARTS  A<a id="_idTextAnchor993"/>GE
py-time-00001-deployment-jqrbk  2/2    Running       0         5s
py-time-00001-deployment-jqrbk  2/2    Terminating   0         64s</pre>			<p>As we can see, jus<a id="_idTextAnchor994"/>t after 1 minute of inactivity, Knative starts terminating the pod. Now, that’s what we mean by scaling <span class="No-Break">from zero.</span></p>
			<p>To delete the service permanently, we can use the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ kn service delete py-time</pre>			<p>We’ve just looked at the imperative<a id="_idIndexMarker863"/> way of deploying and managing<a id="_idIndexMarker864"/> the application. But what if we want to declare the configuration as we did previously? We can create a CRD manifest with the <strong class="source-inline">Service</strong> resource provided <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">apiVersion</strong></span><span class="No-Break">—</span><span class="No-Break"><strong class="source-inline">serving.knative.dev/v1</strong></span><span class="No-Break">.</span></p>
			<p>We will create the following manifest file, called <strong class="source-inline">py-time-deploy.yaml</strong>, <span class="No-Break">for this:</span></p>
			<pre class="console">
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: py-time
spec:
  template:
    spec:
      containers:
        - image: &lt;your_dockerhub_user&gt;/py-time</pre>			<p>As we’ve created this file, we will use the <strong class="source-inline">kubectl</strong> CLI to apply it. It makes deployment consistent <span class="No-Break">with Kubernetes.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Though it is a <strong class="source-inline">service</strong> resource, don’t confuse this with the typical Kubernetes <strong class="source-inline">Service </strong>resource. It is a custom resource provided by <strong class="source-inline">apiVersion</strong> <strong class="source-inline">serving.knative.dev/v1</strong>. That is why <strong class="source-inline">apiVersion</strong> is <span class="No-Break">very important.</span></p>
			<p>Let’s go ahead and run the following command to <span class="No-Break">do so:</span></p>
			<pre class="console">
$ kubectl apply -f py-time-deploy.yaml
service.serving.knative.dev/py-time created</pre>			<p>With that, the service has been created. To get the service’s endpoint, we will have to query the <strong class="source-inline">ksvc</strong> resource using <strong class="source-inline">kubectl</strong>. Run the followin<a id="_idTextAnchor995"/>g command to <span class="No-Break">do so:</span></p>
			<pre class="console">
$ kubectl get ksvc py-time
NAM<a id="_idTextAnchor996"/>E      URL
py-time   http://py-time.default.35.226.198.46.sslip.io</pre>			<p>The URL is the custom endpoint we have to target. Let’s <strong class="source-inline">curl</strong> the custom endpoint using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ curl http://py-time.default.35.226.198.46.sslip.io
The current time is : 2023-07-03 13:30:23.345223!</pre>			<p>We get the same response this time as well! So, if you want to keep using <strong class="source-inline">kubectl</strong> for managing Knative, you can easily <span class="No-Break">do so.</span></p>
			<p>Knative helps scale<a id="_idTextAnchor997"/><a id="_idTextAnchor998"/> applications<a id="_idIndexMarker865"/> based on the load it receives—automatic horizontal<a id="_idIndexMarker866"/> scaling. L<a id="_idTextAnchor999"/>et’s run load testing on our application to see that <span class="No-Break">in action.</span></p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor1000"/>Load testing your app on Knative</h2>
			<p>We will use the <strong class="source-inline">hey</strong> utility to perform<a id="_idIndexMarker867"/> load testing. Since your application has already<a id="_idIndexMarker868"/> been deployed, run the following command to do the <span class="No-Break">load test:</span></p>
			<pre class="console">
$ hey -z 30s -c 500 http://py-time.default.35.226.198.46.sslip.io</pre>			<p>Once the command has executed, run the following command to get the currently running instances of the <span class="No-Break"><strong class="source-inline">py-time</strong></span><span class="No-Break"> pods:</span></p>
			<pre class="console">
$ kubectl get pod
NAME                           READY STATUS RESTARTS   AGE
py-time-00001-deployment-52vjv 2/2   Running   0       44s
py-time-00001-deployment-bhhvm 2/2   Running   0       44s
py-time-00001-deployment-h6qr5 2/2   Running   0       42s
py-time-00001-deployment-h92jp 2/2   Running   0       40s
py-time-00001-deployment-p27gl 2/2   Running   0       88s
py-time-00001-deployment-tdwrh 2/2   Running   0       38s
py-time-00001-deployment-zsgcg 2/2   Running   0       42s</pre>			<p>As we can see, Knative has created seven instances of the <strong class="source-inline">py-time</strong> pod. This is horizontal autoscaling <span class="No-Break">in action.</span></p>
			<p>Now, let’s look at the cluster nodes by running the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ kubectl get nodes
NAME                                       STATUS   AGE
gke-cluster-1-default-pool-353b3ed4-js71   Ready   3m17s
gke-cluster-1-default-pool-353b3ed4-mx83   Ready   106m
gke-cluster-1-default-pool-353b3ed4-vf7q   Ready   106m</pre>			<p>As we can see, GKE has created another node in the node pool because of the extra burst of traffic it received. This is phenomenal, as we have the Kubernetes API to do what we want. We have automatically horizontally autoscaled our pods. We have also automatically horizontally autoscaled our cluster worker nodes. This means we have a fu<a id="_idTextAnchor1001"/><a id="_idTextAnchor1002"/>lly automated solution for running<a id="_idIndexMarker869"/> containers without worrying about the management<a id="_idIndexMarker870"/> nuances! That is open source serverless in action <span class="No-Break">for you!</span></p>
			<h1 id="_idParaDest-199"><a id="_idTextAnchor1003"/>Summary</h1>
			<p>This chapter covered CaaS and serverless CaaS services. These help us manage container applications with ease without worrying about the underlying infrastructure and managing them. We used Amazon’s ECS as an example and deep-dived into it. Then, we briefly discussed other solutions that are available on <span class="No-Break">the market.</span></p>
			<p>Finally, we looked at Knative, an open source serverless solution for co<a id="_idTextAnchor1004"/><a id="_idTextAnchor1005"/>ntainers that run on top of Kubernetes and use many other open source <span class="No-Break">CNCF projects.</span></p>
			<p>In the next chapter, we will delve into IaC <span class="No-Break">with Terraform.</span></p>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor1006"/>Questions</h1>
			<ol>
				<li>ECS allows us to deploy to which of the following? (<span class="No-Break">Choose two)</span><p class="list-inset"><span class="No-Break">A. EC2</span></p><p class="list-inset">B. <span class="No-Break">AWS Lambda</span></p><p class="list-inset"><span class="No-Break">C. Fargate</span></p><p class="list-inset">D. <span class="No-Break">Amazon Lightsail</span></p></li>
				<li>ECS uses Kubernetes in the <span class="No-Break">background</span><span class="No-Break">. (True/False)</span></li>
				<li>We should always use services in ECS instead of tasks for batch <span class="No-Break">jobs</span><span class="No-Break">. (True/False)</span></li>
				<li>We should always use Fargate for batch jobs as it runs for a short period, and we only pay for the resources that are consumed during that <span class="No-Break">time</span><span class="No-Break">. (True/False)</span></li>
				<li>Which of the following are CaaS services that implement the Kubernetes API? (<span class="No-Break">Choose three)</span><p class="list-inset"><span class="No-Break">A. GKE</span></p><p class="list-inset"><span class="No-Break">B. AKS</span></p><p class="list-inset"><span class="No-Break">C. EKS</span></p><p class="list-inset"><span class="No-Break">D. ECS</span></p></li>
				<li>Google Cloud Run is a serverless offering that uses Knative behind the <span class="No-Break">scenes</span><span class="No-Break">. (True/False)</span></li>
				<li>Which one of the following is offered as a Knative module? (<span class="No-Break">Choose two)</span><p class="list-inset"><span class="No-Break">A. Serving</span></p><p class="list-inset"><span class="No-Break">B. Eventing</span></p><p class="list-inset"><span class="No-Break">C. Computing</span></p><p class="list-inset"><span class="No-Break">D. Containers</span></p></li>
			</ol>
			<h1 id="_idParaDest-201"><a id="_idTextAnchor1007"/>Answers</h1>
			<ol>
				<li value="1"><span class="No-Break">A, C</span></li>
				<li><span class="No-Break">False</span></li>
				<li><span class="No-Break">False</span></li>
				<li><span class="No-Break">True</span></li>
				<li>A, <span class="No-Break">B, C</span></li>
				<li><span class="No-Break">True</span></li>
				<li><span class="No-Break">A, B</span></li>
			</ol>
		</div>
	</div>
</div>


<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer066" class="Content">
			<h1 id="_idParaDest-202" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor1008"/>Part 3:Managing Config and Infrastructure</h1>
			<p>This part takes a deep dive into infrastructure and configuration management in the public cloud, exploring various tools that enable infrastructure automation, configuration management, and <span class="No-Break">immutable infrastructure.</span></p>
			<p>This part has the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B19877_08.xhtml#_idTextAnchor1010"><em class="italic">Chapter 8</em></a>, <em class="italic">Infrastructure as Code (IaC) with Terraform</em></li>
				<li><a href="B19877_09.xhtml#_idTextAnchor1198"><em class="italic">Chapter 9</em></a>, <em class="italic">Configuration Management with Ansible</em></li>
				<li><a href="B19877_10.xhtml#_idTextAnchor1330"><em class="italic">Chapter 10</em></a>, <em class="italic">Immutable Infrastructure with Packer<a id="_idTextAnchor1009"/></em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer067" class="Basic-Graphics-Frame">
			</div>
		</div>
	</div>
</div>
</body></html>