<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer123">
<h1 class="chapter-number" id="_idParaDest-230"><a id="_idTextAnchor699"/>9</h1>
<h1 id="_idParaDest-231"><a id="_idTextAnchor700"/>Dealing with Databases in DevOps Scenarios</h1>
<p>In the previous chapters, you learned about the continuous integration and continuous deployment of your software. You also learned how the same principles can be applied to the delivery of configuration in infrastructure. Once you have adopted these principles and started increasing the flow of value delivery, you might run into another challenge: managing your database schema changes.</p>
<p>Applying DevOps to databases can feel like trying to change the tires on a running car. You must find some way of coordinating changes between the database schema and application code without taking the system down for maintenance.</p>
<p>In this chapter, you will learn about different approaches for doing just that: managing these schema changes while avoiding downtime. With proper planning and a disciplined approach, this can be achieved in a way that manages risks well. You will see how you can treat your database schema as code, and you will learn about the different approaches that are available to do so. You will also see another approach that avoids database schemas altogether, namely, going schema-less.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Managing a database schema as code</li>
<li>Applying database schema changes </li>
<li>Going schema-less</li>
<li>Other approaches and concerns</li>
</ul>
<h1 id="_idParaDest-232"><a id="_idTextAnchor701"/><a id="_idTextAnchor702"/>Technical requirements</h1>
<p>In order to practice the ideas that are laid out in this chapter, you will need to have the following tools installed:</p>
<ul>
<li>An application with the Entity Framework Core NuGet package installed</li>
<li>Visual Studio with SQL Server Data Tools</li>
<li>Access to Azure Pipelines</li>
<li><a id="_idTextAnchor703"/>An Azure subscription, for accessing Cosmos DB</li>
</ul>
<h1 id="_idParaDest-233"><a id="_idTextAnchor704"/>Managing a database schema as code</h1>
<p>For those of you who are familiar <a id="_idIndexMarker780"/>with working with relational databases from application code, it is very<a id="_idIndexMarker781"/> likely you have been working with an <strong class="bold">object-relational mapper</strong> (<strong class="bold">ORM</strong>).</p>
<p>ORMs were introduced to fill the impedance mismatch between object-oriented programming languages and the relational database schema, which works with tables. Well-known examples are Entity Framework and NHibernate.</p>
<p>ORMs provide a layer of abstraction that allows for the storage and retrieval of objects from a database, without worrying about the underlying table structure when doing so. To perform automated mapping of objects to tables, or the other way around, ORMs often have built-in capabilities for describing a database schema, the corresponding object model, and the mappings between them in a markup language. Most of the time, neither of these have to be written by hand. Often, they can be generated from an object model or an existing database, and the mappings between them are often, by convention, generated or drawn in a visual editor.</p>
<p>While all this allows for the current database schema to be defined as code, this alone does not help with coping with schema changes, yet. For handling schema changes as code, two common approaches are available. The first one describes every change in code; the other one describes only the latest version of the schema in code. These approaches are known as migration-based and state-based approaches, respectively. Both can rely on third-party tooling for applying the changes to the databas<a id="_idTextAnchor705"/><a id="_idTextAnchor706"/><a id="_idTextAnchor707"/>e.</p>
<h2 id="_idParaDest-234"><a id="_idTextAnchor708"/>Migrations</h2>
<p>The first approach<a id="_idIndexMarker782"/> is based on keeping an ordered set of changes that have<a id="_idIndexMarker783"/> to be applied to the database. These changes are often called <strong class="bold">migrations</strong>, and they can be generated by tools such as Microsoft Entity Framework or Redgate SQL Change Automation, or they can be written by hand.</p>
<p>Tools can automatically generate<a id="_idIndexMarker784"/> the migration scripts based on a comparison<a id="_idIndexMarker785"/> of the current schema of the database<a id="_idIndexMarker786"/> and the new schema definition in source control. This is called <strong class="bold">scaffolding</strong>. The scripts generated by tools are not always perfect, and they can be improved by applying the domain knowledge that the programmer has. Once one or more new migrations are scaffolded or written, they can be applied to a database using the chosen tool. The following is a diagram showing how that works:</p>
<div>
<div class="IMG---Figure" id="_idContainer121">
<img alt="Figure 9.1 – Migration approach " height="506" src="image/B18655_09_01.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – Migration approach</p>
<p>Here, we see how an ever-growing series of migrations, labeled <strong class="bold">m1</strong> to <strong class="bold">m4</strong>, are generated to describe incremental changes to the database. To update the database to the latest version, the latest applied migration is determined and all migrations after that are added one after the other.</p>
<p>When editing migration scripts by hand, the following have to be kept in mind:</p>
<ul>
<li>The migration scripts should be ordered. Migrations describe the SQL statements that need to be executed in order to move the database from version <em class="italic">x</em> to version <em class="italic">x+1</em>. The next migration can be started only when this step is completed.</li>
<li>A migration script should migrate not only the schema but also the data. This can mean that a few steps are needed in between migrations. For example, moving two columns to another table often implies that the new columns are first created, then filled with the data from the old columns, and only then are the old columns rem<a id="_idTextAnchor709"/>oved.</li>
<li>It is advisable to include all database objects in the migration scripts. Extra indexes and constraints should be applied not only to the production database but also to test environments. With migrations, there is already a mechanism for delivering those from source control. Having these in the same migration scripts also ensures that indexes and constraints are applied in the same order and cannot unexpectedly block migrations by existing only in production.</li>
<li>If possible, migration scripts should be made idempotent. If there is ever an issue or the suspicion of an issue, being able to just rerun the last migration is a great way to ensure that it is fully applied.</li>
</ul>
<p>One disadvantage of this approach<a id="_idIndexMarker787"/> is the strict ordering requirement that<a id="_idIndexMarker788"/> is imposed on generating and applying the generated migrations. This makes it hard to integrate this approach into a development workflow that relies heavily on the use of branches.</p>
<p>Migrations created in different branches that are merged together only later might break the ordering of migrations or, even worse, merge a split in the migration path. For example, imagine the case where two migrations, <em class="italic">b</em> and <em class="italic">c</em>, in two different branches have been created after an existing migration, <em class="italic">a</em>. How are these going to be merged? Neither order—<em class="italic">a</em>, <em class="italic">b</em>, <em class="italic">c</em> or <em class="italic">a</em>, <em class="italic">c</em>, <em class="italic">b</em>—is correct, since both <em class="italic">b</em> and <em class="italic">c</em> are created to be executed directly after <em class="italic">a</em>. The only way such an error can be fixed is by performing the following steps:</p>
<ol>
<li>Remove all migrations apart from the first new one, which is <em class="italic">c</em> in this case.</li>
<li>Apply all other migrations to a database that has none of the new migrations applied, in this case, only <em class="italic">b</em> if <em class="italic">a</em> was already applied, or both <em class="italic">a</em> and <em class="italic">b</em>.</li>
<li>Generate a new migration for the other migrations, in this case, a replacement for <em class="italic">c</em>.</li>
</ol>
<p>An advantage of this approach is that every individual schema change will be deployed against the database in the same fashion. Irrespective of whether one—or more than one—migration is applied to the production database at the same time, they will still run one by one in a predictable<a id="_idIndexMarker789"/> order and in the same way<a id="_idIndexMarker790"/> in which they ran against<a id="_idTextAnchor710"/> the test environment, even if they were applied there on<a id="_idTextAnchor711"/>e by one.</p>
<h2 id="_idParaDest-235"><a id="_idTextAnchor712"/>End state</h2>
<p>A different approach to managing schema<a id="_idIndexMarker791"/> changes is to not keep track<a id="_idIndexMarker792"/> of the individual changes (or migrations), but only store the latest version of the schema in source control. External tools such as Microsoft Visual Studio and Redgate’s SQL Data Compare tool are then used to compare the current schema in source control with the actual schema of the database, generate migration scripts, and apply these when running. The migration scripts are not stored and are singl<a id="_idTextAnchor713"/>e-use only.</p>
<p>Unlike writing migrations, it is not feasible to execute a task like this by hand. While tracking the newest version of the schema by hand in source control can be managed, the same is not feasible for an end-state approach. Generating a migration script while comparing the existing schema and the new schema and applying this migration script can only be done using a tool. Examples of suitable tools are Redgate SQL Source Control and SQL Server Data Tools. How these tools work is shown in the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer122">
<img alt="Figure 9.2 – Managing schema changes  " height="581" src="image/B18655_09_02.jpg" width="1535"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – Managing schema changes </p>
<p>Here, we see how the current, actual database schema and the description of the desired database schema are compared to generate an upgrade and directly apply a script for making the changes needed to make the actual schema the same as the desired schema.</p>
<p>One advantage of this approach is that there is no series of scripts generated that have to be executed in a specific order. Therefore, this approach combines easily with extensive branching schemas, where changes are integrated more slowly over time. It also removes the need to write migrations by hand for simple scenarios, such as adding or deleting a column, table, or index.</p>
<p>The disadvantage of this approach is that it makes it harder to handle changes that need data operations as well. Again, imagine a scenario of moving two columns to another table. Since the tooling only enforces the new schema, this will lead to data loss if there is no further intervention.</p>
<p>One possible form of intervention to circumvent this is the addition of pre- and post-deployment scripts to the schema package. In the pre-deployment script, the current data is staged in a temporary table. Then, after applying the new schema, the data is copied from the temporary table to the new location in the post-deployment script.</p>
<p>This section was about<a id="_idIndexMarker793"/> managing database schema<a id="_idIndexMarker794"/> changes in a format that can be stored in source control. The next section discusses how these changes can be picked up at deploy time and then applied to<a id="_idTextAnchor714"/><a id="_idTextAnchor715"/><a id="_idTextAnchor716"/> a database.</p>
<h1 id="_idParaDest-236"><a id="_idTextAnchor717"/>Applying database schema changes</h1>
<p>With the database schema and, optionally, a series<a id="_idIndexMarker795"/> of migrations defined in source control, it is time to start thinking about when to apply changes to the database schema. There are two methods to do so. Database schema changes can be applied prior to depl<a id="_idTextAnchor718"/>oyment of the new application version, or by the application<a id="_idTextAnchor719"/> code itself.</p>
<h2 id="_idParaDest-237"><a id="_idTextAnchor720"/>Upgrading as part of the release</h2>
<p>The first approach to applying database changes<a id="_idIndexMarker796"/> is as part of the release pipeline. When this is the case, the tool that is responsible for reading and executing the migration scripts is invoked using a step in the pipeline.</p>
<p>This invocation can be done using a custom script in PowerShell or another scripting language. However, this is error-prone, and with every change of tool, there is a risk that the scripts need to be updated. Luckily, for most migration-based tools, there are Azure Pipelines tasks that are readily available for starting the migration from the release.</p>
<p>For example, there is an Azure Pipelines<a id="_idIndexMarker797"/> extension available for applying Entity Framework Core migrations to a database directly from the <strong class="source-inline">dll</strong> file where they are defined. This task can be added to the release pipeline for updating the database before the new application code is deployed. A link to the <strong class="bold">Build &amp; Release Tools</strong> extension is provided in the <em class="italic">Further reading</em> section at the end of this chapter. </p>
<p>Another variation is a split between the build and release phases of an application. In this case, the migration scripts are exported as a separate build artifact, either directly from the source code—if written in SQL—or after executing a tool that generates the necessary SQL scripts as output. This artifact is then downloaded again in the release pha<a id="_idTextAnchor721"/>se, where it is applied to the database using an Azure Pipelines task for<a id="_idTextAnchor722"/> executing SQL.</p>
<h2 id="_idParaDest-238"><a id="_idTextAnchor723"/>Upgrading with the application code</h2>
<p>Instead of applying schema changes <a id="_idIndexMarker798"/>from the release pipeline, they can also be applied by the application itself. Some of the ORMs, with migration support built in, have the capability to automatically detect whether the database schema matches the latest migration. If not, they can automatically migrate the schema to that latest ver<a id="_idTextAnchor724"/>sion on the spot.</p>
<p>An example of an ORM that supports this is Entity Framework. The core version of Entity Framework does not have support for automatic migrations built in. In Entity Framework Core, a single line of application code can be used to initiate an upgrade at a time that is convenient from the perspective of the application. The code for doing so is shown in the following code snippet:</p>
<pre class="source-code">
using (var context = new MyContext(...))
{
    context.Database.Migrate();
}</pre>
<p>The advantage of this approach is that it is very simple to enable. Just a Boolean switch in the configuration of, for example, Entity Framework can enable this workflow. However, the disadvantage is that most ORMs that support this will enforce a global lock on the database—stopping all database transactions while the migrations are running. For any migration or set of migrations that take more than a few seconds, this approach might be impractical.</p>
<p>This approach is normally only used for migration-based<a id="_idIndexMarker799"/> approaches. Approaches that use an end-state approach require an external third-party tool that is used to generate the necessary migration scripts and apply them. This is normally done<a id="_idTextAnchor725"/> from the release pipeline and is not wrapped in the ap<a id="_idTextAnchor726"/>plication itself.</p>
<h2 id="_idParaDest-239"><a id="_idTextAnchor727"/>Adding a process</h2>
<p>As the previous section illustrated, it is important<a id="_idIndexMarker800"/> to think about how and when changes to the database schema or the application (or applications!) that use that schema are applied. But, no matter how the deployment of schema changes and code deployments are scheduled, there will always be a period where one of the following is true:</p>
<ul>
<li>The new application code is already running while the schema changes are not applied yet or are in the process of being applied.</li>
<li>The old application code is still running while the schema changes are already applied or are in the process of being applied.</li>
<li>The application code is not running while the schema changes are being applied.</li>
</ul>
<p>The third situation is highly undesirable. This is true in general, but especially when practicing DevOps. If changes are shipped often and during working hours, it is unacceptable to take the application down for every schema change.</p>
<p>To prevent having to take the application down while schema changes are being applied, one of the following conditions has to be met:</p>
<ul>
<li>The schema changes are backward-compatible in such a way that the old version of the application code can run without errors against a database where the schema changes have already been applied or are being applied.</li>
<li>The new application code is backward-compatible in such a way that it can run against both the old and new versions of the schema.</li>
</ul>
<p>Meeting the first of these conditions ensures that the old application code can continue to run while the schema changes are being applied. Meeting the second of these conditions ensures that the new version of the application code can be deployed first, and once that is completed, the database can be upgraded while this code is running. While either will work, it is often desirable to aim for the first condition. The reason is that schema changes often support application code changes.</p>
<p>This means that the following<a id="_idIndexMarker801"/> is a safe process for deploying schema changes without downtime:</p>
<ol>
<li value="1">Create a new database.</li>
<li>Apply the database changes.</li>
<li>Verify that the changes have been applied properly or abort the deployment pipeline.</li>
<li>Deploy the new application code.</li>
</ol>
<p>It is important to realize that this process assumes failing forward. This means that if there ever is an issue with the deployment of schema changes, they should be resolved before going forward with the code changes.</p>
<p>Finally, meeting the condition of backward compatibility for schema changes can sometimes be impossible to fulfill for a schema change. If this is the case, the change can often be split into two partial changes that together have the same end result, while they both meet the condition of backward compatibility. For example, renaming a property, or changing the unit in which it stores a distance from feet to meters, can be executed as follows:</p>
<ol>
<li value="1">Generate a migration that adds a new column to a database table, storing the distance in meters.</li>
<li>Add application code that reads from the old column but writes to both columns.</li>
<li>Deploy these cha<a id="_idTextAnchor728"/>nges to production.</li>
<li>Add a new migration that migrates data from the old column to the new column for all cases where the new column is not yet filled, but the old column is.</li>
<li>Update the application code to read and write only the new column.</li>
<li>Deploy these changes to production.</li>
<li>Add a new migration that removes the old column.</li>
</ol>
<p>Using the correct tools<a id="_idIndexMarker802"/> and a proper process, it is possible to execute effective and safe deployments of schema changes. In the next section, another approac<a id="_idTextAnchor729"/>h, using schema-less datab<a id="_idTextAnchor730"/>ases, is introduced.</p>
<h1 id="_idParaDest-240"><a id="_idTextAnchor731"/>Going schema-less</h1>
<p>In the previous sections, the focus<a id="_idIndexMarker803"/> was on relational databases, where strict schemas are applied to every table. A completely different approach to database schema management is to let go of having a database schema altogether. This can be done by using<a id="_idIndexMarker804"/> schema-less or document databases. A well-known example of a schema-less database is <strong class="bold">Azure Cosmos DB</strong>. These databases can store documents<a id="_idIndexMarker805"/> of different forms in the same “table.” Table is in quote marks here since these types of databases often do not use the term “table,” but call this a database, a container, or a collection.</p>
<p>Since these databases can store documents with a different schema in the same collection, schema changes no longer exist from a database’s point of view. But of course, there will be changes to the structure of the corresponding objects in the application code over time. To see how to handle this, it is best to differentiate between storing obje<a id="_idTextAnchor732"/>cts in the database<a id="_idTextAnchor733"/> and reading them back.</p>
<h2 id="_idParaDest-241"><a id="_idTextAnchor734"/>Writing objects to the database</h2>
<p>The documents that are stored in a schema-less database<a id="_idIndexMarker806"/> are often serializations<a id="_idIndexMarker807"/> of objects in application code. When working with a relational database, these objects are often stored using an ORM, such as Entity Framework, Dapper, or NHibernate. When working with a document database, these objects are often serialized and stored in the database. Serialization<a id="_idIndexMarker808"/> is the process of converting an object into a stream of bytes so that it can be saved or transmitted easily across process boundaries. Deserialization<a id="_idIndexMarker809"/> is the reverse process of constructing a data structure or object from a stream of bytes. This means that a change in the definition of that code object will also result in a different document structure when saving the object. Due to the nature of document data<a id="_idTextAnchor735"/>bases, this will work fine.</p>
<p>As an example, take<a id="_idIndexMarker810"/> the following C# class and its JSON<a id="_idIndexMarker811"/> representation.</p>
<p>This code is using the <strong class="source-inline">JsonConstructor</strong> attribute to indicate that the constructor of the class should be used to create an instance of the class during deserialization:</p>
<pre class="source-code">
public class Person
{
   [JsonConstructor] private Person() {}
   public Person(string name) {
      Name = name ?? throw new ArgumentNullException();
   }</pre>
<p>The following code shows a JSON representation of the instance of a <strong class="source-inline">Person</strong> class after serializing it to a document database:</p>
<pre class="source-code">
{
   "Name": "Mark Anderson"
}</pre>
<p>After this code has been running in a production environment for a while, and thousands of people have been saved, a new requirement comes in. Next to the name of the person, the city where they live must also be recorded. For this reason, the <strong class="source-inline">Person</strong> class is extended to include another property. After performing this change and deploying the new code, whenever a person is saved, the following code is used:</p>
<pre class="source-code">
public class Person
{
   [JsonConstructor] private Person() {}
   public Person(string name, string city) { 
   Name = name ?? throw new ArgumentNullException();
   City = city ?? throw new ArgumentNullException();
   }
   [JsonProperty]
   public string Name { get; private set; } [JsonProperty]
   public string City { get; private set; }
}</pre>
<p>The definition of the <strong class="source-inline">Person</strong> class has changed; the corresponding JSON representation of a new instance is shown in the following snippet. Both document variations can be saved into the same collection:</p>
<pre class="source-code">
{
   "Name": "Mark Anderson",
   "City": "Amsterdam"
}</pre>
<p>This shows that from the viewpoint<a id="_idIndexMarker812"/> of writing information<a id="_idIndexMarker813"/> to the database, the schema-less approach is very convenient since developers do not have to think about s<a id="_idTextAnchor736"/><a id="_idTextAnchor737"/><a id="_idTextAnchor738"/>chema change management at all.</p>
<h2 id="_idParaDest-242"><a id="_idTextAnchor739"/>Reading objects from the database</h2>
<p>While schema-less databases<a id="_idIndexMarker814"/> make it extremely easy to write documents of different forms<a id="_idIndexMarker815"/> to the same collection, this can pose problems when reading documents back from that same collection and deserializing them. In reality, the problem of schema management is not removed but deferred to a later point in time.</p>
<p>Continuing the example from the previous section, deserializing the first person that was saved on the new C# <strong class="source-inline">Person</strong> class definition will result in a null value for the city property. This can be unexpected since the C# code guarantees that a person without a city can never be constructed. This is a clear example of the challenges that schema-less databases pose.</p>
<p>In this example, the issue can<a id="_idIndexMarker816"/> be circumvented by updating the <strong class="source-inline">Person</strong> class<a id="_idIndexMarker817"/> to the following:</p>
<pre class="source-code">
public class Person
{
   [JsonConstructor] 
   private Person() {}
   public Person(string name, string city) {
      Name = name ?? throw new ArgumentNullException(); 
      City = city ?? throw new ArgumentNullException();
   }
   [JsonProperty]
   public string Name { get; private set; }
   [JsonIgnore]
   private string _city;
   [JsonProperty] 
   public string City {
      get { return _city; }
      private set { _city = value ?? _city = string.Empty}
   }
}</pre>
<p>Aside from this scenario, where a property was added, there are many other scenarios that require the C# class to be adapted in order to handle deserialization scenarios. Some examples are as follows:</p>
<ul>
<li>Adding a property of a primitive type</li>
<li>Adding a complex property, another objec<a id="_idTextAnchor740"/>t, or an array </li>
<li>Renaming a property</li>
<li>Replacing a property of a primitive type with a complex property </li>
<li>Making nullable properties non-nullable</li>
</ul>
<p>Adding code to objects<a id="_idIndexMarker818"/> to handle these situations increases the size and complexity <a id="_idIndexMarker819"/>of the code base and pollutes the primary code base with the capabilities for coping with past situations. Especially when this happens often, this can lead to unwanted complications in a code base. To prevent this, a possible solution is to go through the following process whenever the schema of an object changes:</p>
<ol>
<li value="1">Change the schema of the object, ensuring that there are only properties added. Even when the goal is to remove a property, at this stage, only a property with the new name is added.</li>
<li>Implement logic on the object to cope with the deserialization of old versions of the object.</li>
<li>Deploy the new version of the object.</li>
<li>Start a background process that loads all objects of the type from the database one by one and saves them back to the database.</li>
<li>Once the background process has processed all existing entities, remove the code that is responsible for coping with the schema change during deserialization from the code base, along with any properties that are no longer used.</li>
</ol>
<p>Using this approach, all changes are propagated to all stored versions of the object over a period of time. The downside to this approach is that the change to the object’s structure is spread over two changes<a id="_idIndexMarker820"/> that must be deployed sepa<a id="_idTextAnchor741"/>rately. Also, deployment<a id="_idIndexMarker821"/> of the second change must wait until all objec<a id="_idTextAnchor742"/>ts in the database have been converted.</p>
<h1 id="_idParaDest-243"><a id="_idTextAnchor743"/>Other approaches and concerns</h1>
<p>Besides the more common approaches that were discussed previously, the following tips and approaches might help with reducing the amou<a id="_idTextAnchor744"/>nt of work in dealing with databases or help reduce the risk a<a id="_idTextAnchor745"/>ssociated with making database changes.</p>
<h2 id="_idParaDest-244"><a id="_idTextAnchor746"/>Minimizing the influence of databases</h2>
<p>A first step in dealing with databases<a id="_idIndexMarker822"/> can be to reduce the chance that a database change has to be made. In many databases, it is possible to write stored procedures—or some other code or script—that executes within the database engine. While stored procedures come with some benefits, changing them can also count as a database schema change, or at the least, result i<a id="_idTextAnchor747"/>n changes that can be difficult to test.</p>
<p>One simple approach for this is to just replace stor<a id="_idTextAnchor748"/>ed procedures with application code that allows for easier sid<a id="_idTextAnchor749"/>e-by-side changes using feature toggles.</p>
<h2 id="_idParaDest-245"><a id="_idTextAnchor750"/>Full side-by-side deployment</h2>
<p>When working in a high-risk <a id="_idIndexMarker823"/>environment, or with a fragile database, there is also another approach to database schema changes that can be taken. This approach is based on applying feature toggles and the blue-green deployment pattern and goes as follows:</p>
<ol>
<li value="1">Change the application code in such a way that it writes any update to not just one but two databases.</li>
<li>In the production environment, create a complete, full copy of the existing database and configure the application code to write all changes to both databases. These databases will be called <em class="italic">old</em> and <em class="italic">new</em>, from now on.</li>
<li>Introduce the required changes to the new database schema and the application code <em class="italic">only</em> in the path that writes to the new database.</li>
<li>Introduce the necessary changes in all code paths that read data in such a way that all queries run against both databases.</li>
<li>Update the application code to detect differences in the query results between the new and old databases and log an error when it finds any discrepancy.</li>
<li>If the changes run without any issues, remove the old database, and the old read and write access paths, from the application code.</li>
<li>If the changes run with errors, fix the issues. Then, restart by restoring the backup of the intended new database, and resume at <em class="italic">step 5</em>.</li>
</ol>
<p>The advantage of this approach<a id="_idIndexMarker824"/> is that it is very lightweight. The downside is that it is very involved, takes a lot of work, and is more exp<a id="_idTextAnchor751"/>ensive. Also, the extra database costs and duration of backup and restor<a id="_idTextAnchor752"/>e operations should be taken into account.</p>
<h2 id="_idParaDest-246"><a id="_idTextAnchor753"/>Testing database changes</h2>
<p>Just as with application code, insights<a id="_idIndexMarker825"/> into the quality of database schema changes can be gathered through testing. Links to performing tests on database schemas can be found at the end of this chapter.</p>
<p>In most cases, in order to fully cover the risks introduced by database changes, system tests are needed that execute against a fully deployed stack of the application. This type of test can cover most of the risks that come from faulty schemas, invalid stored procedures, an<a id="_idTextAnchor754"/>d database and application code mismatches.</p>
<h1 id="_idParaDest-247"><a id="_idTextAnchor755"/>Summary</h1>
<p>In this chapter, you have learned how to manage your database schema and schema changes using source control. You know about both the migration-based approach and state-based approach for storing changes, and how to apply them to your production database in a safe manner.</p>
<p>Additionally, you have learned how schema-less databases can remove the burden of traditional schema management. However, this comes at the price of having to cope with schema differences when reading older versions of an object back from the database.</p>
<p>In the next chapter, you will learn about continuous testing. You will not only learn about testing techniques, but also about which to apply at what point, and how testing is a crucial part of DevOps and a critical enabler <a id="_idTextAnchor756"/>of a continuous flow of value to end users.</p>
<h1 id="_idParaDest-248"><a id="_idTextAnchor757"/>Questions</h1>
<p>As we conclude, here is a list of questions for you to test your knowledge regarding this chapter’s material. You will find the answers in the <em class="italic">Assessments</em> section of the Appendix:</p>
<ol>
<li value="1">True or false: When working with Entity Framework, schema management is built in using migrations-based support.</li>
<li>True or false: When working with a migrations-based approach for schema management, you do not need extra tracking tables in your database schema.</li>
<li>True or false: When working with an end-state-based approach for schema management, you do not need extra tracking tables in your database schema.</li>
<li>What are the benefits of a full side-by-side approach to database schema changes? (Choose multiple answers):<ol><li>The risks are reduced to almost zero.</li><li>You can measure the actual performance impact of changes in a production-like environment.</li><li>Side-by-side migrations reduce cycle time.</li></ol></li>
<li>True or false: Schema-less databases remove the need for thinking about schema changes completely.</li>
<li>What is a possible technology choice that you can make to limit th<a id="_idTextAnchor758"/>e impact of changes on your database schema?</li>
</ol>
<h1 id="_idParaDest-249"><a id="_idTextAnchor759"/>Further reading</h1>
<ul>
<li>More information about Entity Framework and Entity Framework migrations can be found at <a href="https://docs.microsoft.com/nl-nl/ef/">https://docs.microsoft.com/nl-nl/ef/</a>and <a href="https://docs.microsoft.com/en-us/ef/ef6/modeling/code-first/migrations/">https://docs.microsoft.com/en-us/ef/ef6/modeling/code-first/migrations/</a>.</li>
<li>More information about Redgate and its database tooling can be found at <a href="https://www.red-gate.com/">https://www.red-gate.com/</a>.</li>
<li>More information on SQL Server Data Tools can be found at <a href="https://docs.microsoft.com/en-us/sql/ssdt/download-sql-server-data-tools-ssdt?view=sql-server-ver15">https://docs.microsoft.com/en-us/sql/ssdt/download-sql-server-data-tools-ssdt?view=sql-server-ver15</a>.</li>
<li>Build &amp; Release Tools Azure DevOps extension: <a href="https://marketplace.visualstudio.com/items?itemName=bendayconsulting.build-task&amp;ssr=false#overview">https://marketplace.visualstudio.com/items?itemName=bendayconsulting.build-task&amp;ssr=false#overview</a>.</li>
<li>Database changes deployment with Redgate SQL Change Automation and Azure DevOps: <a href="https://azuredevopslabs.com/labs/vstsextend/redgate/">https://azuredevopslabs.com/labs/vstsextend/redgate/</a>.</li>
</ul>
</div>
</div></body></html>