- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes Deployment– Same Game, Next Level
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you dove into different deployment scenarios and how
    you should think not only about controllers, but also about upgrading apps, different
    types of apps to deploy, and different methods for getting an app up and running.
    In this chapter, you’re going to dive a bit deeper into the different styles of
    deploying and troubleshooting versus just doing the deployment.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in any type of deployment is figuring out what you’re doing –
    what type of application it is, what type of Kubernetes resource you want to use,
    and the different plugins that you may want to use, such as a CSI. After you figure
    out the logistics of what you want to deploy, the next step is to think about
    how you want to deploy.
  prefs: []
  type: TYPE_NORMAL
- en: With Kubernetes, there are many different deployment methods – automated deployments,
    manual deployments, and something in between automated and manual. There’s a vast
    number of different ways to perform deployments, so you won’t learn about all
    of them because that could take up more than six chapters of a book in itself,
    but you will learn about the primary ways to deploy and package up Kubernetes
    Manifests.
  prefs: []
  type: TYPE_NORMAL
- en: After you learn about deployments, thinking about how to troubleshoot once something
    inevitably goes wrong is a good, logical next step. Typically, engineers will
    learn troubleshooting on the fly, but it’s a good approach to think about troubleshooting
    techniques prior to something going wrong.
  prefs: []
  type: TYPE_NORMAL
- en: After learning about troubleshooting and deploying containerized apps, you’re
    going to wrap up with how to manage network connectivity between apps running
    on Kubernetes and how to migrate existing, more monolithic-style applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting to know Helm charts and Kustomize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying with CI/CD and GitOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting application deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service meshes and Ingresses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you’re going to take what you learned about the different
    types of deployments and methodologies for thinking about Kubernetes resources
    from the last chapter and expand upon that knowledge in this chapter. You should
    have a brief understanding of automated deployment methodologies such as CI/CD,
    and have a high-level understanding of what a service mesh is, along with some
    application architecture knowledge. As usual, you’ll find the code for this chapter
    on GitHub: [https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch6](https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch6)'
  prefs: []
  type: TYPE_NORMAL
- en: Getting to know Helm charts and Kustomize
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you’re working with Kubernetes, unless it’s a dev environment for your
    testing, there’s an extremely slim chance that you only have one Kubernetes Manifest.
    You most likely have multiple for various resources such as Deployments, Services,
    DaemonSets, ConfigMaps, Ingresses, and a ton of the other Kubernetes resources
    out there. Utilizing almost every single Kubernetes platform or tool that’s deployed
    to your cluster uses a Kubernetes Manifest.
  prefs: []
  type: TYPE_NORMAL
- en: With all those Kubernetes Manifests, there are a ton of different values and
    parameters that you need to pass at runtime to make it all work. In this section,
    you’ll learn about two different methods of managing Kubernetes Manifests – **Helm
    charts** and **Kustomize**.
  prefs: []
  type: TYPE_NORMAL
- en: Why think about deployment methods for manifests?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before diving into different deployment methods, it makes sense to understand
    why you’d want to consider deployment methods other than using the terminal for
    the deploying Manifests first.
  prefs: []
  type: TYPE_NORMAL
- en: There are three primary points, which we discuss in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Scale
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When thinking about scale, there’s absolutely no way to scale a deployment if
    an engineer is always doing it from their laptop. The engineer could be using
    different plugins, different IDEs, different terminal settings, and even a different
    operating system. With all of that, the uncertainty alone of the environment can
    cause a massive amount of error. If every engineer is relying on their computer
    to deploy an environment, what happens if their laptop crashes? Or there’s a random
    update during the day? Or someone is out of office? There are so many variables
    that come into play that make utilizing a local computer a bad idea when it comes
    to deployments. Instead, it makes far more sense to have a central location from
    which you conduct your deployments. The environment stays the same, everyone can
    use it, you can customize it to your team’s needs, and you don’t have to worry
    about anyone being out of office.
  prefs: []
  type: TYPE_NORMAL
- en: Anything can go wrong
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Going into the second point, which echoes the first point in a sense, anything
    can go wrong. The goal of every organization is to have a successful deployment
    all the time, zero hiccups, and the ability to deploy at any time. Marketing teams
    paint this picture in our heads of “deploy 20 times per day with this tool and
    it’ll always work,” but as all engineers know, that’s not reality. Anything as
    simple as a network hiccup or making an error when entering a variable name can
    lead to a failed deployment, and, in turn, an application being down. Because
    of that, having a proper deployment strategy is key not only to repeatability
    with Kubernetes but also to repeatability in general. Having a proper process
    and *rules* in place of how something is deployed and when or where it’s deployed
    is the make or break between a successful update and everyone on the engineering
    team sitting in the office fixing an issue until 1:00 A.M.
  prefs: []
  type: TYPE_NORMAL
- en: It’s manual
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last point, which goes without saying, is that it’s an incredibly manual
    process to sit at a terminal and run commands to deploy a configuration. In today’s
    world, engineers want to spend their time focusing on value-driven work, not putting
    out fires. In fact, that’s a huge reason why automation and repeatability exist
    in the first place. Engineers wanted to get their time back and stop working on
    mundane tasks. If you’re constantly deploying on your computer to an environment,
    you’re putting the “this is awful” back into manual work. Now, there are circumstances
    where you’d want to deploy from your localhost. For example, when I’m deploying
    to a dev environment or testing a new config for the first time, I’m not going
    to create a repeatable solution around it because I’m unsure whether it even works
    yet. However, once I know that it works and my initial dev testing is complete,
    I’ll automate the workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Going forward in this chapter, keep in mind that the reason why you want to
    think about deployment workflows is to mitigate as much of the three aforementioned
    points as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Helm charts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The idea behind repeatable deployment strategies is to make your life easier,
    but with new strategies comes the need to learn about different methods of implementation.
    The first method to learn about is Helm charts.
  prefs: []
  type: TYPE_NORMAL
- en: Helm is an open source project originally created by DeisLabs and donated to
    the CNCF; the CNCF now maintains the project. The objective of Helm when it first
    came out was to provide engineers with a better way to manage all the Kubernetes
    Manifests created. Helm was built with Kubernetes in mind and it’s a tool and
    platform specifically for Kubernetes, so it’s the same YAML you’re used to, just
    packaged differently – literally just YAML. Kubernetes was meant to give you a
    way to declaratively deploy containerized apps. It wasn’t necessarily meant to
    give you a meaningful way to package a bunch of Kubernetes Manifests so you could
    use them together. That’s where Helm comes into play. In addition, Helm keeps
    a release history of all deployed charts. This means you could go back to a previous
    release if something went wrong.
  prefs: []
  type: TYPE_NORMAL
- en: In January 2016, the project merged with a GCS tool called Kubernetes Deployment
    Manager, and the project was moved under Kubernetes. Helm was promoted from a
    Kubernetes subproject to a CNCF project in June 2018.
  prefs: []
  type: TYPE_NORMAL
- en: In short, Helm is a way to take a bunch of Kubernetes Manifests and package
    them up to be deployed like an application.
  prefs: []
  type: TYPE_NORMAL
- en: Using Helm charts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that you know about Helm, let’s go ahead and dive into it from a hands-on
    perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that you’ll need to do is install Helm. Because it varies based
    on the operating system, you can find a few different installation methods here:
    [https://helm.sh/docs/intro/install/](https://helm.sh/docs/intro/install/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once Helm is installed, find or create a directory in which you want your first
    Helm chart to exist. Preferably, this will be an empty directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, go into that directory on your terminal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the new directory, run the following command to create a Helm chart:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once you do that, you should see a directory structure similar to the following
    screenshot. In this case, the chart was called `newchart`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Helm chart'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.1 – Helm chart
  prefs: []
  type: TYPE_NORMAL
- en: If you open up the `templates` directory, you’ll see a bunch of examples for
    Deployments, Ingresses, and a lot more.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Example Helm'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.2 – Example Helm
  prefs: []
  type: TYPE_NORMAL
- en: If you open up `values.yaml`, you’ll see where you can start adding values that
    you want to pass into your templates.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Values file'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.3 – Values file
  prefs: []
  type: TYPE_NORMAL
- en: 'To deploy a Helm chart, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To install the Helm chart, run the following command. The `.` symbol indicates
    the current directory, which is where the Helm chart exists:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Of course, this isn’t everything there is to know about Helm. In fact, there
    are literally entire books on Helm. The goal of this section was to get you on
    the right path.
  prefs: []
  type: TYPE_NORMAL
- en: Helm chart best practices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following is a list of best practices to follow in production when using
    Helm:'
  prefs: []
  type: TYPE_NORMAL
- en: When storing Helm charts, ensure that they’re set to be public or private as
    required. The last thing you want is to push a Helm chart to a registry that’s
    public-facing when it’s not supposed to be.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document what your charts do.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure you store charts in source control.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always test Helm charts after a change is made.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kustomize
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Helm and Kustomize are pretty similar but have some unique differences. One
    of the primary use cases of Helm is to have a `values.yaml` file to store values
    to pass into a Kubernetes Manifest. Kustomize has the same type of idea.
  prefs: []
  type: TYPE_NORMAL
- en: With Kustomize, you have a template, typically called a base. The base is the
    template that you want to use. It could be for a Kubernetes Deployment, Service,
    Pod, or anything else you’d like. The template is the literal base where your
    values get pushed into. Along with the template, you have a `kustomization.yaml`
    file, which tells Kustomize which templates to use. For example, let’s say you
    have a `deployment.yaml` and `service.yaml` file. You would put those two filenames
    into the `kustomization.yaml` file so Kustomize knows it should push values into
    those two files.
  prefs: []
  type: TYPE_NORMAL
- en: '*Values* were mentioned a few times already, but not thoroughly explained.
    A value can be anything that you want to essentially pass in at runtime. For example,
    let’s say you have three environments – dev, staging, and prod. In dev, you have
    one replica. In staging, you have two replicas. In prod, you have three to four
    replicas. You can use Kustomize to pass those values into one template, so instead
    of having three Manifests that have different replica values, you have one template
    that you pass the values into.'
  prefs: []
  type: TYPE_NORMAL
- en: But how do you pass in the values?
  prefs: []
  type: TYPE_NORMAL
- en: Within a Kustomize directory, you typically have two directories – base and
    overlays. The base is where the template goes. The overlay directory is where
    each environment goes with specific values. For example, let’s say you have a
    `dev`, `staging`, and `prod` overlay.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Base configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.4 – Base configuration
  prefs: []
  type: TYPE_NORMAL
- en: The `dev` overlay, along with the others, would have a `kustomization.yaml`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Dev overlay configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.5 – Dev overlay configuration
  prefs: []
  type: TYPE_NORMAL
- en: Within the `kustomization.yaml` file is where you’d find the config for the
    replica count.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Kustomization file'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.6 – Kustomization file
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the `resources` map is pointing to the `base` directory, and the
    `replicas` map is specifying the deployment along with the replica count.
  prefs: []
  type: TYPE_NORMAL
- en: The primary difference between Helm and Kustomize is that Helm’s primary purpose
    is to package up a bunch of Kubernetes Manifests and deploy them like an app,
    whereas the primary purpose of Kustomize is to have a template that you push your
    values into (such as the replica count). Helm does this as well, but it’s not
    the primary purpose of Helm.
  prefs: []
  type: TYPE_NORMAL
- en: Using Kustomize configurations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that you know about Kustomize, let’s dive into it from a hands-on perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that you’ll need to do is install Kustomize. Because it varies
    based on the operating system, you can find a few different installation methods
    here: [https://kubectl.docs.kubernetes.io/installation/kustomize/](https://kubectl.docs.kubernetes.io/installation/kustomize/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once Kustomize is installed, find or create a new directory in which you want
    your Kustomize config to live. You can call it `kustomize`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create two new directories under the `kustomize` directory called `overlays`
    and `base`. Inside the `overlays` directory, create a new child directory called
    `dev`. It should look similar to the following screenshot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Dev overlay configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.7 – Dev overlay configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the `base` directory, create a new file called `deployment.yaml` and
    paste the following code into it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create a new file in the `base` directory called `kustomization.yaml`
    and paste the following configuration into it, which tells Kustomize which Kubernetes
    Manifest to utilize:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the last step, inside of `overlays` | `dev`, create a new file, call it
    `kustomization.yaml`, and paste the following Manifest into it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the directories and configurations are in place, `cd` into the `base`
    | `dev` directory and run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You’ll see an output similar to the following screenshot, which gives you a
    config with one replica, instead of two, which is what the template contains.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Kustomize output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.8 – Kustomize output
  prefs: []
  type: TYPE_NORMAL
- en: As with Helm charts, the topic of Kustomize could fill a small book itself,
    which means this section couldn’t cover it all. It should, however, get you started
    in the right direction.
  prefs: []
  type: TYPE_NORMAL
- en: Kustomize best practices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following is a list of best practices to follow in production when using
    Kustomize:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that you put overlays into their own directories. You don’t have to do
    this, but it makes for a much cleaner config.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that all code is stored in source control.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Follow a standard directory structure – `base` for the directory where the template
    goes and `overlays` for values that you wish to pass into the template.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, you’ll learn about the two primary deployment methods when
    it comes to containerized apps.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying with CI/CD and GitOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes deployments come in three stages:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Manifests on your local computer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Manifests with an automated solution such as CI/CD, which ultimately
    just runs `kubectl apply -f` commands, the same as your local computer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new and completely automated solution that’s (usually) 100% hands-off from
    a deployment perspective
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the first stage, it wasn’t scalable at all. A bunch of engineers were running
    commands on their localhost to deploy a containerized app, and they were all doing
    it in different ways with different code editors and different plugins. It was
    a mess and didn’t allow scalability for the deployment process. It also held engineers
    up from doing value-driven work and instead, they had to sit on their terminals
    and run commands all day.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you’ll learn about more common, automated, and new approaches
    to deploying apps, which will be around CI/CD and GitOps.
  prefs: []
  type: TYPE_NORMAL
- en: What is CI/CD?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to CI/CD, it’s assumed that if you’re reading this book, you’re
    already doing work in CI/CD and know what it is. Because of that, there won’t
    be an entire breakdown of CI/CD. Let’s do a brief overview.
  prefs: []
  type: TYPE_NORMAL
- en: By definition, CI/CD is a way to create an artifact of your application and
    deploy it to the desired destination in an automated fashion. As CI/CD increased
    in popularity, engineers started using it for other purposes – for example, packaging
    up Terraform code into an artifact and running it so infrastructure can be created
    automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the CI process, engineers are worried about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Testing code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packaging up code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring that all prerequisites and dependencies are met
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building container images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the CD process, engineers are worried about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring that they reached the correct destination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring that the app or services and infrastructure are up and running as expected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using CI/CD for Kubernetes deployments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As with everything else in tech, there are what feels like a million ways to
    do one thing. Because of that, we cannot specify every CI/CD, automation, and
    cloud scenario here. To make things simplistic, Terraform code for GKE and YAML
    pipelines for GitHub Actions will be shown. This is considered pseudocode, but
    it’ll actually work in the right environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s start with the Terraform code and break it down:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll start with the Google provider, specifying the region:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, `google_container_cluster` will be specified so you can implement the
    VPC you want to use, subnet, and worker node count:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last resource is for `google_container_node_pool`, which implements the
    needed Google APIs for GKE, the node count, node names, and node size or type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With the Terraform code, you’ll want a way to deploy it. The best way in today’s
    world is with CI/CD. When it comes to deploying infrastructure and services, CI/CD
    is a great and repeatable process.
  prefs: []
  type: TYPE_NORMAL
- en: To deploy the code, you can use any CI/CD platform of your choosing, but the
    code here is an example of how you can deploy the Terraform code via GitHub Actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Specifies `workflow_dispatch`, which means the code will only run if you click
    the `Deployment` button
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uses an Ubuntu container to run the pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checks out the code (clones it) to the Ubuntu container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configures Terraform in the Ubuntu container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configures the GCP SDK in the Ubuntu container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Runs `terraform init`, and formats, plans, and applies it to the directory
    where the GKE code lives:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: When using CI/CD, it makes the most sense to use it in this type of way for
    Kubernetes. You rarely ever want to use CI/CD to deploy a Kubernetes Manifest
    and instead, you’d want to use something such as a GitOps solution, as it’s far
    more efficient, manages the state, monitors the workloads, and a lot more.
  prefs: []
  type: TYPE_NORMAL
- en: What is GitOps?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By definition, GitOps is a set of tools that utilizes Git repositories as a
    source of truth to deliver Kubernetes resources as code. It’s an operational best
    practice used for app development, collaboration, compliance, and CI/CD, and applies
    the best practices to infrastructure automation. Now, let’s see a simpler explanation.
    It’s configuration management for Kubernetes; that’s it, plain and simple. Configuration
    management is all about ensuring that the desired state is the current state,
    which is what GitOps gives us.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know the definition of GitOps, let’s talk about what it actually
    does for Kubernetes. First things first, you have a source control repository.
    The repository contains your Kubernetes manifests that you wish to deploy to Kubernetes
    to run your applications. You also have a Kubernetes cluster, which is running
    on any environment you’d like. It could be on-premises, in a raw Kubernetes cluster,
    or even in a cloud-based service such as GKE or EKS. Now, you have the Kubernetes
    Manifests that you want to run in your production environment and the Kubernetes
    cluster that you want to run the Kubernetes manifests on, but how do you deploy
    them? The typical way is running something such as `kubectl apply -f` against
    the Kubernetes manifests, but that requires manual effort and leaves a lot to
    be desired. Instead, you can implement GitOps. To implement GitOps, there are
    a few solutions. You decide to implement a GitOps solution, and that GitOps solution
    needs access to both the Kubernetes cluster that you’re running and the source
    control system, such as GitHub or any other Git system where your source code
    is stored. To do that, you install the GitOps solution on the Kubernetes cluster
    and while doing that, you give the GitOps solution access to your source control
    system with some type of `kubectl apply -f` or `kubectl create -f` anymore. Instead,
    you’re using the CLI or whatever other solution comes with the GitOps platform
    to deploy the Kubernetes Manifests – and boom, just like that, you have an application
    deployed! Now, of course, we all wish it were that easy. A couple of sentences
    to explain and poof, you’re up and running in a production environment. However,
    it’s not that simple, which is why GitOps is in such high demand and isn’t the
    easiest thing to crack.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this, the most popular GitOps platforms are **ArgoCD**
    and Flux.
  prefs: []
  type: TYPE_NORMAL
- en: Using GitOps for automated deployments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Knowing the process to create the Kubernetes infrastructure, you can now deploy
    and manage a containerized app using GitOps. To follow this section, you’ll need
    a Kubernetes environment up and running, as ArgoCD will be deployed to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: This section is going to be more of a step-by-step guide because regardless
    of where you’re running Kubernetes, these are the steps to get ArgoCD up and running.
    Unlike with the CI/CD section, there aren’t tons of different platforms, cloud
    environments, or configuration code choices that can come into play, and because
    of that, the following solution can work in any environment.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring ArgoCD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, create a namespace for ArgoCD in your Kubernetes cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install ArgoCD using the preconfigured Kubernetes Manifest from ArgoCD that
    provides a highly available installation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 6.9 – ArgoCD creation output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B19116_06_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.9 – ArgoCD creation output
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the initial admin password to log in to ArgoCD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Open up ArgoCD’s UI via Kubernetes port forwarding. That way, you can access
    the frontend of ArgoCD without having to attach a load balancer to the service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that you know the UI works, log in to the server via the CLI. That way,
    you can deploy containerized apps with ArgoCD via the CLI to create a repeatable
    process instead of doing it through the UI, which is manual and repetitive.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.10 – ArgoCD portal'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.10 – ArgoCD portal
  prefs: []
  type: TYPE_NORMAL
- en: 'The port is what ArgoCD is hosting from the `kubectl port-forward` command
    that you ran in the previous step. Use the following command to log in to ArgoCD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 6.11 – Login output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.11 – Login output
  prefs: []
  type: TYPE_NORMAL
- en: In the Argo CD UI, go to **User Info** | **Update Password**. Change the password
    from the initial admin password to a password of your choosing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You now have officially deployed ArgoCD and have the ability to work with the
    GitOps platform on your terminal and locally on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying an app
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, you’re going to deploy an app. The app that you’ll use is
    a very popular demo-related app that a lot of folks use to showcase how an environment
    will work:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a namespace for your new app:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The Sock Shop is a popular microservice demo that you can find here: [https://microservices-demo.github.io/deployment/kubernetes-start.html](https://microservices-demo.github.io/deployment/kubernetes-start.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploy the Sock Shop in ArgoCD. To deploy the app, you will need to do the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new ArgoCD app.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Point to the repo where the app exists.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Point to the destination server, which is the server or service that you’re
    running Kubernetes on.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Specify the destination namespace:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that the app is deployed, you can check the status of the app:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 6.12 – Sock Shop resources'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.12 – Sock Shop resources
  prefs: []
  type: TYPE_NORMAL
- en: You can now check that the app was deployed in the ArgoCD UI.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Sock Shop app connection'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.13 – Sock Shop app connection
  prefs: []
  type: TYPE_NORMAL
- en: You’ll see the health of the app, whether it’s synced, and whether the status
    of the application is as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Production use cases for CI/CD and GitOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Two ways to think about CI/CD and GitOps in production are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD should be used to deploy the cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GitOps should be used to manage the Kubernetes resources inside the cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other words, CI/CD deploys the infrastructure and clusters and GitOps deploys
    and manages the apps. Use the best tool for the job, which is the infrastructure
    deployment type of workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of which GitOps and CI/CD solution you use, you always want to keep
    in mind that your goal is to automate and create repeatable workflows that work
    for you and your team. Regardless of what *hot* tool or platform is out right
    now, you want to use what’s best for your team, not whatever is the *new thing*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you’ll dive into multiple methods of troubleshooting containerized
    apps running in your Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting application deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Troubleshooting environments and applications typically always looks the same
    and follows a typical order:'
  prefs: []
  type: TYPE_NORMAL
- en: When was the last deployment?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What has changed?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Look at the logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who can access the app and who cannot, if anyone at all?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With Kubernetes, it’s pretty similar when it comes to application troubleshooting.
    The usual workflow is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Check the app itself running in the container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the overall health of the Pod(s).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the Service or route.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With these three steps, you can usually get to the bottom of what’s happening
    because, in reality, there can’t be any other problems. It’s either that the app
    itself isn’t working, the Pod itself has an issue, or the service or route isn’t
    working as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Although there could only be three potential problems at a high level, when
    you dive deeper into those problems, there could be various ways to troubleshoot
    the current issue you’re facing, which you’ll learn about in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with all troubleshooting techniques, you should think about it in the following
    order:'
  prefs: []
  type: TYPE_NORMAL
- en: What’s the problem?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What’s changed?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What could be the problem in the problem? As in, a Pod may be down, but it might
    not be because of the app. It could be because of a problem with the replication
    controller.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting Pods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The two commands that’ll help you debug Pods are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl describe`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl logs`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Take the following Kubernetes Manifest and deploy it. Notice how, for the container
    tag, it’s spelled as `lates`. That’s on purpose, as you want the container to
    fail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Retrieve the name of the Pod with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll see an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Error container image pull'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.14 – Error container image pull
  prefs: []
  type: TYPE_NORMAL
- en: Notice how, right off the bat, you can start the troubleshooting process. The
    status states that there was an error pulling the image. Now you know that there’s
    an issue with the image, let’s dive a bit deeper.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll see an output similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Pod description'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.15 – Pod description
  prefs: []
  type: TYPE_NORMAL
- en: The great thing about the `describe` command is that it gives you a log output
    underneath the `Events` section. You can now see that the issue is that it couldn’t
    pull the container image based on the name and tag that you gave.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step would be to run the `logs` command to see whether there’s any
    other data you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 6.16 – Pod logs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.16 – Pod logs
  prefs: []
  type: TYPE_NORMAL
- en: You can see from the screenshot here that there isn’t much more to go off of
    other than what was given in the `describe` command, so the troubleshooting has
    been successfully completed.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When troubleshooting Services, the first thing that you always want to confirm
    is whether the Service exists. If you don’t have a Service running in a Kubernetes
    cluster, you can use this example Manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Because Pod networks are separate from a host network, you’ll need a Pod to
    exec or SSH into so you can do the troubleshooting. The following is a command
    that you can use to configure a Pod for troubleshooting purposes based on a `busybox`
    container image, which is a popular image used for troubleshooting purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'First, see whether the service is running. You’ll do this outside of the `busybox`
    container image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: You should get the following output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Service configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.17 – Service configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'If the service is running, confirm that you can reach the service via DNS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll see an output similar to the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – nslookup of Pod'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.18 – nslookup of Pod
  prefs: []
  type: TYPE_NORMAL
- en: 'If the standard `nslookup` command doesn’t work, or if you want another type
    of confirmation, try an FQDN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: You’ll see an output similar to the following screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.19 – FQDN service lookup'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.19 – FQDN service lookup
  prefs: []
  type: TYPE_NORMAL
- en: 'Check to confirm that the service is defined correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: You’ll see an output similar to the following screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.20 – JSON output of service'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.20 – JSON output of service
  prefs: []
  type: TYPE_NORMAL
- en: 'Check that the service has endpoints – as in, confirm that there are Pods that
    the service is pointing to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll see an output similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.21 – Retrieving Pods based on label'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.21 – Retrieving Pods based on label
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, which should already be known, but just in case, check to confirm
    that the Pods that the service is pointing to are working:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: The final piece, which you’ll learn about in the next section, is implementing
    a service mesh for troubleshooting. A service mesh has several jobs, and one of
    the jobs is making it easier to troubleshoot latency issues between Services,
    along with ensuring that Services are working as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting Deployments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The primary command that’ll help you debug Deployments is similar to Pod debugging:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl` `describe deployment`'
  prefs: []
  type: TYPE_NORMAL
- en: Unless there’s something wrong with the Deployment controller itself, there
    usually isn’t a problem with the actual Deployment. It’s typically a problem with
    the Pods running inside of the Deployment. However, you still may want to check
    the Deployment itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, you would run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get an output similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.22 – Describing the nginx Deployment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.22 – Describing the nginx Deployment
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the `describe` command isn’t to tell you about logs or events or
    what’s happening – it’s to help you fully understand what’s deployed and how it’s
    deployed. That way, you can backtrack and see whether what’s deployed is actually
    supposed to be there.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll wrap up this chapter by talking about what a service
    mesh is, what an Ingress is, and how to think about implementing them.
  prefs: []
  type: TYPE_NORMAL
- en: Service meshes and Ingresses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Almost every containerized application needs to be routed in one way or another
    – whether it’s so outside users can use the application, applications can talk
    to each other, or one application needs to connect to another. Routes and Services
    are extremely important in Kubernetes, which is why service meshes and Ingresses
    play a huge part.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, you’ll need better visuals into what’s happening with services,
    how traffic is being routed, and what applications are routing to which load balancers
    and IP addresses. You’ll also eventually want a way to encrypt traffic between
    services, which Kubernetes doesn’t give you out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: Service meshes and Ingresess are typically more advanced-level topics, but in
    this book and possibly at this stage in your career, you’ll be ready to dive in
    and fully understand the pros and cons of using these two tools, plugins, and
    platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Why Ingress?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point in your Kubernetes journey, it’s almost certain that you’ve seen
    a Kubernetes Service. In fact, you’ve seen them throughout this book. A lot of
    the time, a Kubernetes Service comes with a frontend app that’s attached to it,
    in which you need a way for users to interact with the Kubernetes Service. It’s
    typically in front of a load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem with that is if you have a load balancer in front of your service,
    you have to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Pay extra for the cloud load balancer if you’re using a cloud Kubernetes service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up a virtual load balancer if you’re using an on-premises Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have a bunch of load balancers to manage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With an Ingress controller, you don’t have to worry about that.
  prefs: []
  type: TYPE_NORMAL
- en: You can have several different Kubernetes Services and have an Ingress controller
    point to all of them, and each of the services can be reached by a different path.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress controllers save time, money, management, and effort for engineers.
  prefs: []
  type: TYPE_NORMAL
- en: Using Ingresses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you know about Ingress controllers, let’s see how one can be configured
    using (at the time of writing) the most popular option, Nginx Ingress.
  prefs: []
  type: TYPE_NORMAL
- en: 'First things first – you’ll need a Kubernetes Deployment and Service to deploy.
    If you don’t already have them and would like to keep things simple, you can use
    the following Kubernetes Manifest, which is a sample Azure app. It doesn’t have
    to be running in Azure to work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the app itself is deployed, you can deploy the Ingress controller. The
    Ingress controller is part of the native Kubernetes API set from the named group,
    so you don’t have to worry about installing other CRDs or controllers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'The last step is to forward to the port for the app’s service so you can reach
    the app locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get an output similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.23 – Port-forwarding service communication'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.23 – Port-forwarding service communication
  prefs: []
  type: TYPE_NORMAL
- en: You should be able to reach out to the app over localhost.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.24 – AKS app'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_06_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.24 – AKS app
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know what an Ingress is from a theoretical and practical perspective,
    let’s move on to service meshes and look at how communication can occur more securely.
  prefs: []
  type: TYPE_NORMAL
- en: Why service meshes?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you deploy containerized applications into a Kubernetes cluster, there
    are two primary ways that those applications communicate:'
  prefs: []
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod-to-Pod communication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod-to-Pod communication isn’t recommended because Pods are ephemeral, which
    means they aren’t permanent. They are designed to go down at any time and only
    if they were part of a StatefulSet would they keep any type of unique identifier.
    However, Pods still need to be able to communicate with each other. Backends need
    to talk to frontends, middleware needs to talk to backends and frontends, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: The next communication method, which is the primary, is service-to-service.
    Service-to-service is the preferred method because a Service isn’t ephemeral and
    only gets deleted if manually deleted. Pods can connect to Services with Selectors
    or Tags. If a Pod goes down, but the Selector in the Kubernetes Manifest that
    deployed the Pod doesn’t change, the new Pod will be connected to the Service.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s the primary concern with everything described so far – all this traffic
    is unencrypted. Pod-to-Pod communication, or as some people like to call it, East-West
    traffic, is unencrypted. That means if for any reason a Pod is compromised or
    you have some segregation issues, there’s nothing out of the box that you can
    do.
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s where a service mesh comes into play. A service mesh has the ability
    to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Encrypt traffic between microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Help with network latency troubleshooting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Securely connect Kubernetes Services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform observability for tracing and alerting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a service mesh
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you know about a service mesh, let’s learn how to set one up. There
    are a ton of different service mesh platforms out there, all of which have their
    own method of being installed and configured.
  prefs: []
  type: TYPE_NORMAL
- en: Because it’s a complicated topic in itself, there’s no way to get through it
    all in this section. In fact, there are literal books for just service meshes.
    Let’s learn how to set up an Istio service mesh.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, download Istio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, export the path to the `$``PATH` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Output the `$PATH` variable int, `bashrc`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Install Istio on your Kubernetes cluster. Notice how Ingress is set to `false`.
    You can set it to `true` if you want to use the Istio Ingress. If you’re using
    another Ingress controller, such as Nginx Ingress, you can leave it as `false`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Istio is a great service mesh but doesn’t have a UI out of the box. One of
    the most popular ways to look at your service mesh graphically is by using Kiali,
    which is a simple install:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up port forwarding to Kiali so you can reach the UI locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'The last step is to take a Kubernetes Manifest, like the one you used in this
    chapter, and inject the sidecar (the service mesh container) into your Kubernetes
    Deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: At this point, you now have the theoretical grounding and a bit of hands-on
    knowledge for how to move forward on your service mesh journey.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overall, containerized application deployment, troubleshooting, and third-party
    tooling are going to be the core pieces of what your Kubernetes cluster looks
    like. Without proper troubleshooting, you won’t have successful deployments. Without
    third-party tooling such as Ingress controllers, you won’t be able to properly
    manage frontend apps. Out of the box, Kubernetes gives you a ton to use to make
    things work. However, there are more steps you need to take. For better or for
    worse, Kubernetes isn’t one of those platforms where you can just deploy it and
    walk away. It takes management and engineering skills to ensure it’s working as
    expected.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you’ll learn about how to monitor the workloads you’ve
    been deploying throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Learn Helm* by Andrew Block and Austin Dewey: [https://www.packtpub.com/product/learn-helm/9781839214295](https://www.packtpub.com/product/learn-helm/9781839214295%0D)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mastering Service Mesh* by Anjali Khatri and Vikram Khatri: [https://www.packtpub.com/product/mastering-service-mesh/9781789615791?_ga=2.161313023.37784508.1672298745-664251363.1663254593](https://www.packtpub.com/product/mastering-service-mesh/9781789615791?_ga=2.161313023.37784508.1672298745-664251363.1663254593)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
