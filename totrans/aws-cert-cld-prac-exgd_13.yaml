- en: '[*Chapter 11*](B17124_11_Final_SK_ePub.xhtml#_idTextAnchor276): Analytics on
    AWS'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this age of information, understanding your data has become extremely important.
    With current cutting-edge technologies, extensive amounts of data are generated
    every second – data that needs to be stored and analyzed. Companies perform data
    analytics to explain, predict, and ultimately gain a competitive advantage in
    business. Traditional analytics would include retail analytics, supply chain analytics,
    or stock rotation analytics. With **machine learning** (**ML**) and **artificial
    intelligence** taking a firm hold on the economy, new evolutions of analytics
    have come into play, such as cognitive analytics, fraud analytics, and speech
    analytics. The list is almost endless but suffice it to say that understanding
    your raw data has required considerable effort and a whole business unit dedicated
    to **data analytics** alone.
  prefs: []
  type: TYPE_NORMAL
- en: AWS offers a vast array of analytics tools that you can use to ingest, store,
    and effectively understand the data that's generated by your business. In this
    chapter, we will we look at some of those services in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning about data streaming with Amazon Kinesis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to query data stored in Amazon S3 with Amazon Athena
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Amazon Elasticsearch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of Amazon Glue and QuickSight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional analytics services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To complete the exercises in this chapter, you will need access to your AWS
    account and be logged in as the IAM user **Alice**.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about data streaming with Amazon Kinesis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To analyze your business data, you need to ingest that data into a service that
    can perform the required analysis on it. Businesses generate tons of data from
    a wide range of sources, including logs generated by applications, content such
    as videos, images, and documents, clickstream data from websites, IoT data, and
    more. Ingesting this data is the first step toward understanding it.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, rather than ingesting all the data first and then figuring out how
    you would go about understanding that data, **Amazon Kinesis** lets you process
    and analyze data as it arrives and respond to it instantly. Amazon Kinesis is
    a fully managed service that enables you to process streaming data at any scale
    in a cost-effective manner. Furthermore, it is **serverless**, meaning that you
    do not need to set up and manage expensive infrastructure to process your data.
    Amazon Kinesis is comprised of the following four key services:'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Kinesis Data Firehose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Kinesis Data Streams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Kinesis Data Analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Kinesis Video Streams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at each of these services in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Kinesis Data Firehose
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Modern business approaches and strategies to keep customers loyal and engaged
    have resulted in an insurmountable amount of data to collect, process, and make
    sense of. Whether you are trying to analyze what products your customers click
    on your website, make recommendations based on their product searches, or alert
    your security team about potentially fraudulent transactions, you need to collect
    and process data as it is being generated. Traditionally, you would have had to
    build the infrastructure to provide this kind of backend ingestion and processing
    of data, which can be cost-prohibitive for many businesses – not to mention the
    management overhead associated with maintaining hundreds of servers, storage,
    and network components.
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon Kinesis Firehose** is a fully managed service that can ingest and
    deliver streaming data to AWS data stores such as *Amazon S3*, *Redshift*, and
    *Amazon Elasticsearch* for near real-time analytics with existing **business intelligence**
    (**BI**) tools. The service workflow can be illustrated with the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Kinesis Firehose'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17124_11_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.1 – Kinesis Firehose
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Kinesis Firehose can also deliver data to third-party services such as
    *Datadog*, *New Relic*, *MongoDB*, and *Splunk*. Amazon Kinesis Firehose will
    also allow you to batch process, compress, transform, and even encrypt data before
    loading it into a service, which means you reduce overall storage costs and enhance
    security.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, incoming data streams can be automatically converted into open
    standard formats such as **Apache Parquet** and **Apache ORC**. Finally, with
    Amazon Kinesis Firehose, there are no infrastructure setup costs to worry about.
    You simply pay for the data you transfer through the service, any data conversion
    costs, delivery to Amazon VPCs, and data transfers.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at the Kinesis Data Streams service.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Kinesis Data Streams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whereas Kinesis Firehose is designed to load massive amounts of data into **data
    stores** such as Amazon S3 or Redshift for *near real-time*, **Amazon Kinesis
    Data Streams** is a fully managed *real-time* continuous data streaming service
    that allows you to capture gigabytes of data per second and stream it into custom
    applications for processing and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Kinesis Data Streams can make streaming data available to several analytical
    applications, such as Amazon S3 and AWS Lambda, within 70 milliseconds of the
    data being collected. It offers high levels of durability by replicating your
    streaming data across three Availability Zones.
  prefs: []
  type: TYPE_NORMAL
- en: Kinesis Firehose does not offer any data storage capabilities. However, Amazon
    Kinesis Data Streams will store and make your data accessible for up to 24 hours
    by default, but this can be raised to 7 days by enabling the extended data retention
    feature, or even up to 365 days by enabling the long-term data retention feature.
  prefs: []
  type: TYPE_NORMAL
- en: You ingest and store streaming data for processing to build real-time applications
    services such as **real-time dashboards**, **real-time anomaly detection**, **dynamic
    pricing**, and so on. Like Kinesis Firehose, you are charged on a pay-as-you-go
    basis with no upfront cost nor minimum fees. However, there is a fundamental difference
    in that Kinesis Data Streams uses the concept of *shards*, which uniquely identify
    the data records in a stream. A stream can be comprised of multiple shards that
    determine the overall capacity. Specifically, each shard represents up to five
    transactions per second for reads with a maximum data read rate of 2 MB per second.
    For writes, you can have up to 1,000 records per second and a total data write
    rate of 1 MB per second (including partition keys). The total capacity of the
    stream is the sum of the capacities of its shards. The important concept to appreciate
    here is that you are charged for each shard that's provisioned per hour, regardless
    of whether you use it or not.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 10*](B17124_10_Final_SK_ePub.xhtml#_idTextAnchor249), *Application
    Integration Services*, we discussed a service called **Amazon SQS**. Now, it may
    seem that Kinesis and SQS do the same thing, but they are very different. Amazon
    SQS is a message queueing service that helps store messages while they travel
    between the different components of your application. Amazon SQS helps you decouple
    your application stack so that individual messages can be tracked and managed
    independently, and so that the different components of your application can work
    independently.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at the Kinesis Data Analytics service.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Kinesis Data Analytics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kinesis Data Analytics** lets you query and analyze stream data in real time.
    Data can be streamed into the Kinesis Data Analytics application from various
    sources, including Amazon **Managed Streaming for Kafka** (**MSK**) and **Amazon
    Kinesis Data Streams** (discussed earlier). With Kinesis Data Analytics, you do
    not need to build complex streaming integrated applications with other AWS services.
    Instead, you can use standard programming and database query languages such as
    Java, Python, and SQL to query streaming data or build streaming applications.
    The following diagram illustrates these key features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Amazon Kinesis Data Analytics'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17124_11_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.2 – Amazon Kinesis Data Analytics
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Kinesis Data Analytics also enables you to analyze streaming data in
    real time and build streaming applications using open source libraries and connectors
    for **Apache Flink**. Apache Flink is a fully open source, unified stream processing
    and batch processing framework developed by the **Apache Software Foundation**.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you have access to the **Kinesis Data Analytics Studio**, which allows
    you to build sophisticated stream processing applications using SQL, Java, Python,
    and Scala.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of pricing, you are only charged for the resources that you use to
    run your streaming applications and there are no upfront commitments.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at the Amazon Kinesis Video Streams service.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Kinesis Video Streams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are looking to stream video devices to AWS for analytics, ML, playback,
    and other processing services, then the Amazon Kinesis Video Streams service is
    going to be the tool you use. Amazon Kinesis Video Streams can also ingest data
    from edge devices, smartphones, security cameras, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Kinesis Video Streams makes use of Amazon S3 as the underlying storage
    repository from your streaming videos, which, as you already know, offers high
    levels of **data durability**. In addition, you can search for and retrieve video
    fragments based on devices and timestamps. Your videos can be encrypted and indexed
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'With Amazon Kinesis Video Streams, you can play back videos for live or on-demand
    viewing. In addition, you can use Kinesis Video Streams to help you build applications
    that make use of computer vision video analytics technologies on AWS such as **Amazon
    Rekognition**. Incidentally, Amazon Rekognition is a fully managed image and video
    analysis service that can be used to identify objects, people, text, scenes, and
    activities in images and videos. Amazon Rekognition can also be used to detect
    any inappropriate content. The following diagram illustrates the Amazon Kinesis
    Video Streams service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – Amazon Kinesis Video Streams'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17124_11_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.3 – Amazon Kinesis Video Streams
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Kinesis Video Streams enables you to design applications for a wide range
    of use cases. One such use case includes the ability to stream video and audio
    for smart home devices such as doorbells. Amazon Kinesis Video Streams will ingest,
    index, and store the media streams and your application can use HTTP live streaming
    to play the stream to a smartphone app, allowing you to monitor and communicate
    with the person *knocking* on the door.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we looked at Amazon Kinesis and discussed its four key offerings.
    In the next section, we will introduce you to another AWS analytics service known
    as Amazon Athena, which is a fully managed, serverless interactive query service
    that enables you to analyze data in Amazon S3 using standard SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Learning how to query data stored in Amazon S3 with Amazon Athena
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Businesses store vast amounts of data in repositories such as Amazon S3\. A
    lot of this data is not necessarily being hosted on regular Amazon RDS or NoSQL
    databases. In many cases, this is because the dataset is not being regularly updated
    and queried. Previously, even if you wanted to perform ad hoc queries or analysis
    against some of that data, you would need to ingest it into a database and then
    run your queries against the database.
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon Athena** is a fully managed serverless solution that allows you to
    interactively query and analyze data directly in **Amazon S3** using standard
    SQL. There is no infrastructure to provision, and you only pay for the queries
    you run.'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Athena uses **Presto**, which is an open source SQL query engine that's
    designed to allow you to perform ad hoc analysis. You can use standard ANSI SQL,
    which provides full support for large joins, window functions, and arrays.
  prefs: []
  type: TYPE_NORMAL
- en: Data can be presented to Amazon Athena in a variety of formats, such as CSV,
    JSON, ORC, Avro, or Parquet. Furthermore, you can use Athena's JDBC driver to
    connect to a wide range of BI tools.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Athena allows you to present unstructured, semi-structured, and structured
    data to it, which you can then use to run queries to analyze that data. This process
    involves creating a database within the Athena service and one or more tables
    for each specific dataset that you want to query and analyze. These tables allow
    you to define metadata that tells Athena where the data is held in S3 and the
    structure of that data; for example, the column names and data types.
  prefs: []
  type: TYPE_NORMAL
- en: Tables need to be registered in Athena to perform queries and have the results
    returned. These tables can be created automatically or manually. Once your tables
    have been registered, you can use `SQL SELECT` statements to query them. Your
    query results can also be stored in Amazon S3 in a location you specify.
  prefs: []
  type: TYPE_NORMAL
- en: In an upcoming exercise, *Analyzing your sales report with Amazon Athena and
    AWS Glue*, we will upload some data to Amazon S3 and use Amazon Athena to query
    it. In this section, we introduced you to the Amazon Athena service and provided
    an overview of how you can use Athena to query data stored directly in Amazon
    S3.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at another AWS analytics tool known as Amazon
    Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Amazon Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Elasticsearch** is an open source text search and analytics engine that''s
    capable of storing, analyzing, and performing search functions against big volumes
    of data in near real time. You can use Elasticsearch to analyze all types of data
    such as textual, numerical, geospatial, structured, and unstructured data.'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon's offering of Elasticsearch as a service comes as a fully managed service
    with no need to set up and manage any infrastructure, allowing you to focus on
    your applications and their functionalities. Following the same pay-as-you-consume
    model, there are also no upfront costs, although you can reserve instances for
    a 1- or 3-year term for a significant discount over the on-demand pricing model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon Elasticsearch is designed to be highly scalable and can index all types
    of content to help you deliver applications for use cases such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Website search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging and log analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infrastructure metrics and monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raw data such as log files, messages, metrics, documents, and lists are ingested,
    normalized, and then indexed in Elasticsearch. You can then run complex queries
    against this data and use aggregations to review data summaries.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Elasticsearch also offers integration with **Kibana**, a data visualization
    tool that's used to analyze large datasets to help you produce visual representations
    of that data in the form of graphs, pie charts, heat maps, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: Another service that Amazon Elasticsearch integrates with is **Logstash**, which
    is an open source, server-side data processing pipeline that allows you to ingest
    data from a wide range of sources and transform it and send it to a **stash**
    such as Elasticsearch. Elasticsearch, Kibana, and Logstash are often referred
    to by the acronym **ELK**.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, Amazon Elasticsearch supports querying your cluster using standard
    SQL, making it easy for your developers to start using the service. You can also
    connect to your existing SQL-based BI and ETL tools using a JDBC driver.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we introduced you to the Amazon Elasticsearch service, which
    allows you to create highly scalable, secure, and available Elasticsearch clusters
    and offers full integration with Kibana and Logstash for a complete managed ELK
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at Amazon Glue and QuickSight.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of Amazon Glue and QuickSight
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Business data can often be stored in a wide range of services – databases, storage
    buckets, spreadsheets, and more. Being able to bring all the relevant data together
    for analysis can sometimes be a big project. Later, you may wish to extract and
    present that data in a manner that is easy to digest and understand using BI tools
    or seamlessly integrate insights from that data into your applications, dashboards,
    and reporting. Two services offered by AWS that can help with these types of requirements
    are **Amazon Glue** and **QuickSight**. We'll take a quick look at each of these
    services next.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of Amazon Glue
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon Glue is a serverless **Extract, Transform, and Load** (**ETL**) service.
    With Amazon Glue, you can discover, prepare, enrich, clean, and transform your
    data from various sources. You can then load the data into databases, data warehouses,
    and data lakes. Data from streaming sources can also be loaded for regular reporting
    and analysis. This data can then be used for analytics, as per your business requirements,
    and help with decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Glue comes with a **Data Catalog**, which is a central metadata repository
    that stores information about your data, such as table definitions. You use a
    **crawler** service to scan various repositories, classify data, and *infer* schema
    information such as its format and data types. The metadata is then stored as
    tables in the Data Catalog and used to generate ETL scripts to transform, flatten,
    and enrich your data. The data is then populated into your chosen data warehousing
    solution or data lakes, for example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon Glue also comes with the **AWS Glue console** service to help you define
    and orchestrate your ETL workflow. It lets you do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Define Glue objects such as jobs, crawlers, tables, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schedule **crawler** run operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define events or schedules for job triggers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Search for and filter lists of objects in Glue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edit transformation scripts directly or by using the visual tools provided.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Glue is a fully managed service and scales resources as needed to run
    your jobs. It handles errors and retries automatically. With Amazon Glue, you
    are charged an hourly rate, which is billing by the second. This pricing is based
    on running crawlers (for discovering data) and performing ETL jobs (for processing
    and loading data). In addition, you pay a monthly fee to store and access the
    metadata in the AWS Glue Catalog.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at the Amazon QuickSight service.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of Amazon QuickSight
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon QuickSight is a serverless and fully managed BI service in the cloud
    that can be used to create and publish interactive BI dashboards for your business
    data. This provides you with access to meaningful information for your business
    so that you can make decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon QuickSight can connect to your data wherever it is stored – whether it's
    stored in **AWS services**, **on-premises databases**, **spreadsheets**, **SaaS
    data**, or **B2B data**. This data can then be transformed into rich dashboards
    and reporting tools that can help your business understand operations, sales figures,
    profits, successes, and where there may be room for improvement. Amazon QuickSight
    can publish dashboards securely to enable collaborative efforts from your organization's
    workforce via mobile phones, email, or web applications.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon QuickSight also integrates with ML services, which allows it to build
    and deliver deeper insights from your data. With **ML Insights**, you can discover
    hidden insights across your datasets, such as any anomalies and variations, enabling
    you to quickly act to changes that occur. You can also schedule automatic anomaly
    detection jobs. With ML Insights, you can perform better forecasting, which can
    be used to perform accurate *what-if* analysis. Finally, you can summarize your
    data into easy-to-consume natural language narratives, which can help you deliver
    better contextual information in your dashboards and reporting services.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon QuickSight's pricing model is offered as a pay-as-you-use service and
    is determined by who is using the service; for example, admins, authors, and readers.
    Therefore, the pricing is based on the number of users, similar to a user-based
    license. Additional charges are incurred for services such as alerting and anomaly
    detection too. You can view the pricing overview at [https://aws.amazon.com/quicksight/pricing/](https://aws.amazon.com/quicksight/pricing/).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we reviewed two additional AWS services that fall within the
    analytics category. We introduced you to the Amazon Glue service, which is a fully
    managed and serverless ETL solution. We also provided an overview of the cloud-native
    BI tool, which uses ML to offer greater business insights from your data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover a couple of additional tools as part of the
    overall analytics offering.
  prefs: []
  type: TYPE_NORMAL
- en: Additional analytics services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will take a very quick look at some other AWS analytics
    services that you need to be aware of. Specifically, we will look at the **Elastic
    Map Reduce** (**EMR**) service, **CloudSearch**, and **Data Pipeline**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS EMR**: This provides a managed **Hadoop framework** to enable you to
    process vast amounts of big data. You can use open source tools such as Apache
    Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon
    EMR comes with an **integrated development environment** (**IDE**) called **EMR
    Studio** to help you develop, visualize, and debug data engineering and data science
    applications written in R, Python, Scala, and PySpark. You can run your EMR workloads
    on EC2 Instances, Amazon **Elastic Kubernetes Service** (**EKS**) clusters, and
    on-premises using the AWS Outpost service. In terms of pricing, you are charged
    at a per-instance rate for every second used, with a 1-minute minimum charge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Data Pipeline**: This is a web service that allows you to schedule and
    automate how your data is moved and transformed from various sources, including
    **on-premises servers**, into services such as Amazon S3, RDS, DynamoDB, and EMR.
    With AWS Data Pipeline, you can create workflows that transfer and transform data
    at scheduled intervals to ensure alignment with the application processes. For
    example, you can archive your web server logs to an Amazon S3 bucket daily, and
    then run weekly Amazon EMR jobs to analyze those logs and generate traffic reports
    that can be consumed by your application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS CloudSearch**: This is a fully managed service that enables you to deploy,
    manage, and scale a search solution for your web applications. Amazon CloudSearch
    supports 34 languages and adds rich search capabilities to your website, including
    free text, Boolean, and faceted search. It also offers features such as automated
    suggestions, highlighting, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we looked at some additional services offered by AWS that fall
    within the analytics category. In the next section, we will move on to this chapter's
    exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 11.1 – analyzing your sales report with Amazon Athena and AWS Glue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this exercise, you will need to download a sample CSV file, which is available
    in the Packt GitHub repository for this chapter [https://github.com/PacktPublishing/AWS-Certified-Cloud-Practitioner-Exam-Guide](https://github.com/PacktPublishing/AWS-Certified-Cloud-Practitioner-Exam-Guide).
    This is a simple CSV file that contains some sales data for the Vegan Studio,
    the fictitious company that you have been carrying out a series of exercises for
    in the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: You will need to store this CSV file in an Amazon S3 bucket and then use Amazon
    Athena to run queries against the data. Ensure that you have downloaded the CSV
    file and stored it on your computer before you start this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – Amazon S3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Log into your AWS account using the IAM user ID of our senior administrator,
    **Alice**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the Amazon S3 dashboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create two new buckets with appropriate names. For example, I have named my
    buckets `vegan-sales-report` (to store the CSV file) and `vegan-query-results`
    (to store the Athena query results). Since I have taken these names, you will
    not be able to use them since bucket names must be unique in the AWS ecosystem.
    Ensure that the buckets are created in the `us-east-1 (N.Virginia)` Region.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, upload your CSV file to the bucket that will be used to host the data.
    Recall the steps required from the previous chapters to complete the upload. (Tip:
    try to do this from memory as this will help you build your confidence.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Step 2 – Amazon Athena and Amazon Glue
  prefs:
  - PREF_IND
  - PREF_H2
  type: TYPE_NORMAL
- en: Navigate to the Amazon Athena dashboard. You can search for the Athena service
    from the top search bar in your AWS Management Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If this is the first time you are accessing Amazon Athena, you should see a
    splash screen. Click **Get Started**. If you do not see the **Get Started** option,
    this is because you are using the new user interface. AWS is notorious for making
    changes to the UI. If you do see the new console, then you will need to click
    on the **Explore the query editor** button. For this lab, we suggest that you
    use the old console for now. To access the old console, click on the ellipsis
    (three dashes) in the top far left of the console and switch the toggle to disable
    the **New Athena experience** option. This will take you back to the old console
    interface:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.4 – Disabling the New Athena experience toggle switch'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17124_11_04.png)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 11.4 – Disabling the New Athena experience toggle switch
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the top right-hand corner, click on the `s3://bucket-name`. You can also
    store your queries in a sub-folder and choose to encrypt your query results:![Figure
    11.5 – Amazon Athena – Settings
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17124_11_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 11.5 – Amazon Athena – Settings
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, click the **Save** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, from the left-hand menu, click on **Connect data source**, as per the
    following screenshot:![Figure 11.6 – Amazon Athena – Connect data source
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17124_11_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 11.6 – Amazon Athena – Connect data source
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Under **Choose where your data is located**, ensure that **Query data in Amazon
    S3** is selected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under **Choose a metadata catalog**, ensure that **AWS Glue Data Catalog** is
    selected. For this exercise, you will use AWS Glue to crawl your data and create
    a schema. There is a slight charge to this, but it is very minimum, and you will
    only need to do this once.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Next** button on the right-hand pane of the page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under **Connection details**, ensure that **AWS Glue Data Catalog in this account**
    is selected. Then, under **Choose a way to create a table**, select **Create a
    crawler in AWS Glue**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Connect to AWS Glue** button. This will launch **AWS Glue** in a
    new browser tab. Switch over to this tab to configure Amazon Glue. Do not close
    the Amazon Athena browser tab as we will return to this later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you see the splash screen, click the **Get started** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the left-hand menu, click **Crawlers**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the right-hand pane, click the `s3://bucket-name.` Note that at the end
    of the path defined for your S3 bucket, ensure that you add another slash (`/`).
    `s3://vegan-sales-report/`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click `VeganSalesRole` so that it is self-explanatory. Click `vegansalesdb`.
    Click **Create**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will be taken back to the **Configure the crawler's output** page, with
    your newly created database name shown in the **Database** text box. Click **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Review all steps** page, click **Finish**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will be redirected to the **Crawlers** page. Here, you will be able to see
    that your crawler has been created. Click on the checkbox next to your crawler
    and click the **Run crawler** button, as per the following screenshot:![Figure
    11.7 – Amazon Glue – Run crawler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '](img/B17124_11_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 11.7 – Amazon Glue – Run crawler
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After a minute or two, you should find that its **Status** is set to **Ready**
    and that the crawler has successfully run. You will see that a table has been
    added, as per the following screenshot:![Figure 11.8 – Amazon Glue – Crawl complete
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '](img/B17124_11_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 11.8 – Amazon Glue – Crawl complete
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that the crawl is complete, we can return to the Amazon Athena browser tab.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Step 3 – Amazon Athena
  prefs:
  - PREF_IND
  - PREF_H2
  type: TYPE_NORMAL
- en: Back in the Athena browser tab, go ahead and click on the **cancel** button
    at the bottom right-hand corner of the page. This will take you back to the main
    **Amazon Athena Data sources** page, where you will see that your recently created
    Glue Catalog is listed under **Data sources**, as per the following screenshot:![Figure
    11.9 – Amazon Athena – Data sources
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '](img/B17124_11_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 11.9 – Amazon Athena – Data sources
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, click on the **Query editor** tab.*   From the left-hand menu, select
    the database you created earlier in *Step 2*, which will also reveal the table
    that you created within the database, as per the following screenshot:![Figure
    11.10 – Amazon Athena – Query editor
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '](img/B17124_11_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 11.10 – Amazon Athena – Query editor
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, you can easily preview the data held in Amazon S3 by clicking on the ellipsis
    (the three dots next to your table name) and then clicking on **Preview table**
    from the context menu that appears. This will run a SQL query and retrieve the
    sample data from your table, as per the following screenshot:![Figure 11.11 –
    Amazon Athena – Sample query
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '](img/B17124_11_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 11.11 – Amazon Athena – Sample query
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can run additional queries. For example, you can replace the SQL statement
    in the top half of the pane with the following:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding statement will showcase all the cities where the sales that were
    achieved were equal to or above $100,000, as per the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.12 – Amazon Athena – Query to identify those cities where'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: sales were greater than or equal to $100,000
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17124_11_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.12 – Amazon Athena – Query to identify those cities where sales were
    greater than or equal to $100,000
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, Amazon Athena is extremely powerful in being able to access
    and query your raw data in Amazon S3\. You do not need to set up and deploy servers
    or run expensive databases for such ad hoc analysis of your data.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will perform a cleanup exercise to remove unwanted resources from our
    AWS account.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 11.2 – cleaning up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this exercise, you will delete the resources you created in the previous
    exercise to ensure that there are no unwanted costs:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the Amazon Glue console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the left-hand menu, click the **Crawlers** link. In the right-hand pane,
    select **vegan-sales-crawler**. From the **Actions** drop-down list, click the
    **Delete Crawler** option and then confirm the delete operation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, from the left-hand menu, click **Databases**. In the right-hand pane,
    select the **vegansalesdb** database. Then, from the **Actions** drop-down list,
    click the **Delete database** option.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Delete** button in the **Delete Database** confirmation dialog box
    that appears.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, you will need to delete the Amazon S3 buckets as they are no longer required:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the Amazon S3 console. From the left-hand menu, click on **Buckets**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the right-hand pane, select the `permanently delete` in the confirmation
    text box. Next, click the **Empty** button. You will get a confirmation message,
    stating that the bucket has been successfully emptied. Click the **Exit** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, with the **vegan-query-results** bucket still highlighted, click the **Delete**
    button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Confirm the delete operation by typing the bucket's name in the confirmation
    text box and then clicking on the **Delete bucket** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *Steps 1* to *4* for the **vegan-sales-report** bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that you have completed the cleanup exercise, we will summarize this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed several services from AWS that fall within the
    analytics category. Businesses today possess a vast array of data and being able
    to analyze and make sense of that data is extremely important. Information that's
    obtained from this data can help businesses respond to their customers' needs
    and demands, address potential issues, and even predict future growth. Ultimately,
    businesses can gain an advantage over competitors.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learned about services such as Amazon Kinesis, which allows
    customers to stream and respond in real time and near real time to data. You also
    learned about services that can be used to quickly query your data, such as Amazon
    Athena, as well as services to help you present that data using BI tools. Most
    of these analytical services are also offered as fully managed services on a pay-as-you-consume
    pricing model, making them very affordable for almost any business.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about various deployment and orchestration
    tools on AWS that can help you provision and deploy your applications in the cloud
    without extensive manual configuration. We will look at **Infrastructure as Code**
    (**IaC**), which has taken the IT world by storm as you can design and deploy
    end-to-end infrastructure solutions in a matter of minutes using predefined templates.
    We will also look at how to automate common IT tasks using serverless compute
    services such as AWS Lambda.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Answer the following questions to test your knowledge of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Which AWS service can help you ingest and deliver massive amounts of streaming
    data into Amazon Redshift for near real-time analytics?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon Athena
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon Kinesis Firehose
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon Kinesis Video Streams
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon RDS
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which AWS service can help you query streaming data using standard SQL queries
    in real time?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon Kinesis Data Streams
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon Kinesis Data Analytics
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon Glue
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon QuickSight
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: You are planning on building an application that will capture video streams
    from speed cameras on country roads for analysis. You need to be able to capture
    all the vehicles that break the speed limit and identify the offending drivers
    via the vehicles' license plates. Which two services on AWS can help you achieve
    these requirements? (Choose 2 answers.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon Athena
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon Kinesis Data Analytics
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon Kinesis Video Streams
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon Elasticsearch
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon Rekognition
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which AWS service enables you to index all types of content, offers integration
    with **Kibana**, and helps you build data visualization tools to analyze large
    datasets?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon Elasticsearch
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon Glue
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon Athena
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon Kinesis Firehose
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: You store several network log files (in CSV format) in an Amazon S3 bucket.
    You have been asked to analyze the contents of a specific file for possible malicious
    attacks. Which AWS service can help you analyze raw data in Amazon S3 and perform
    the necessary ad hoc analysis?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon Glue
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon QuickSight
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon Athena
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon Data Pipeline
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which AWS service can be used to perform serverless ETL functions to discover,
    prepare, enrich, clean, and transform your data from various sources for analysis?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AWS Glue
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: AWS Athena
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: AWS QuickSight
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: AWS Rekognition
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which AWS service enables you to create and publish interactive BI dashboards
    for your business data to provide access to meaningful information for your business
    to make decisions?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AWS Kinesis Data Analytics
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: AWS Glue
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: AWS QuickSight
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: AWS Kinesis Firehose
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
