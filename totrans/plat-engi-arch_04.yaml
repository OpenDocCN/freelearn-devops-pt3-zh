- en: <st c="0">4</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2">Architecting the Platform Core – Kubernetes as a Unified Layer</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="65">As a platform engineering team, you need to make a critical decision
    about the underlying technology stack of your core platform.</st> <st c="196">This
    decision will have a long-term impact on your organization as it will dictate
    the skills and resources you will need to build a platform that will support current
    and future self-service</st> <st c="388">use cases.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="398">Kubernetes</st>** <st c="409">– or</st> **<st c="415">K8s</st>**
    <st c="418">for short – is</st> <st c="433">not the solution to all problems,
    but when building platforms, Kubernetes can build</st> <st c="518">the foundation.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="533">In this chapter, you will gain insights into what makes Kubernetes
    the choice for many platform engineering teams.</st> <st c="649">We will explain
    the concept of</st> *<st c="680">promise theory</st>*<st c="694">, which Kubernetes
    is based on, and the benefits that come from the way it’s</st> <st c="771">been
    implemented.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="788">You will get a better understanding of how to navigate the</st>
    **<st c="848">Cloud Native Computing Foundation</st>** <st c="881">(</st>**<st
    c="883">CNCF</st>**<st c="887">) ecosystem</st> <st c="900">as it will be critical
    for you to pick the right projects to support you in your own</st> <st c="985">platform
    implementation.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1009">Once you are familiar with the benefits of Kubernetes and the ecosystem,
    you will learn about the considerations when defining the core layer of your platform,
    such as unifying infrastructure, application, and service capabilities.</st> <st
    c="1242">You will learn how to design for interoperability with your core corporate
    services that sit outside of your new platform and how to design for flexibility,
    reliability,</st> <st c="1412">and robustness.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1427">As such, we will cover the following main topics in</st> <st c="1480">the
    chapter:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1492">Why Kubernetes plays a vital role, and why it is (not)</st> <st
    c="1548">for everyone</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1560">Leveraging and managing Kubernetes</st> <st c="1596">infrastructure
    capabilities</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1623">Designing for flexibility, reliability,</st> <st c="1664">and robustness</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1678">Why Kubernetes plays a vital role, and why it is (not) for everyone</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '<st c="1746">For now, we will focus on Kubernetes, but there are</st> <st c="1799">other
    ways to provide a platform to run your workload.</st> <st c="1854">Besides many
    different flavors of Kubernetes, such as OpenShift, there are alternatives, such
    as Nomad, CloudFoundry, Mesos, and OpenNebula.</st> <st c="1995">They all have
    reasons for their existence, but only one has been adopted almost</st> <st c="2075">everywhere:
    Kubernetes!</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2098">Besides those platforms, you can use virtual machines or services
    from public cloud providers for serverless, app engines, and simple container
    services.</st> <st c="2253">In many cases, platforms utilize these services as
    well, when they are needed.</st> <st c="2332">An exclusive all-in Kubernetes strategy
    might take a few years longer, as it takes organizations a while to fully commit
    to it.</st> <st c="2460">However, there are two recent trends you</st> <st c="2501">can
    observe:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2513">Managing virtual machines</st> <st c="2540">from Kubernetes</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="2555">Migrating to virtual clusters and virtual machines managed by clusters
    to prevent cost explosions for</st> <st c="2658">hypervisor licenses</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="2677">Kubernetes comes with a vital ecosystem and community, a wide range
    of use cases implemented by other organizations, and highly motivated contributors
    to solve the next challenges coming up</st> <st c="2868">with Kubernete</st><st
    c="2882">s.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2885">Kubernetes – a place to start, but not the endgame!</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="2937">“</st>*<st c="2939">Kubernetes is a platform to build platforms.</st>
    <st c="2984">It’s a start but not the endgame</st>*<st c="3016">” is a quote from
    Kelsey Hightower, who worked at Google when, back in 2014, Kubernetes was released
    to the world.</st> <st c="3132">However, while Kubernetes plays a vital role in
    building modern</st> <st c="3195">cloud-native platforms, this doesn’t mean it’s
    the perfect fit for everyone.</st> <st c="3273">Remember the product-centric approach
    to platform engineering?</st> <st c="3336">It starts with understanding the pain
    points of your users.</st> <st c="3396">Once we know the pain points, we can work
    on how we would implement the use cases and which technology choices</st> <st
    c="3507">to make.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3515">Revisit the early section in</st> [*<st c="3545">Chapter 1</st>*](B31164_01.xhtml#_idTextAnchor014)
    <st c="3554">called</st> *<st c="3562">Do you really need a platform?</st>*<st
    c="3592">, where we provided a questionnaire that helps you decide what the core
    of the platform will be.</st> <st c="3689">The answer could be Kubernetes, but
    it doesn’t have to be.</st> <st c="3748">Let’s start by looking into our own example
    use case from Financial</st> <st c="3816">One AC</st><st c="3822">ME.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3826">Would Financial One ACME pick Kubernetes?</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="3868">If we think about the use case from</st> <st c="3905">Financial
    One ACME, “</st>*<st c="3926">Easier access to logs in production for problem
    triage</st>*<st c="3981">”, using the proposed solution doesn’t necessarily require
    Kubernetes as the</st> <st c="4059">underlying platform.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4079">If Kubernetes is not being used yet in our organization and the
    only thing we need is a new automation service that integrates into the different
    logging solutions, we may not want to propose Kubernetes as the underlying core
    platform.</st> <st c="4316">This is because it brings a new level of complexity
    into an organization that doesn’t yet have the required experience.</st> <st c="4436">We
    could implement the solution and operate it with all the existing tools and teams;
    maybe we could run it alongside other tools we already have, following the same
    operational processes for deployment, upgrades, monitoring, and</st> <st c="4666">so
    on.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4672">On the other hand, if there is pre-existing knowledge, or perhaps
    even Kubernetes is already available, then using Kubernetes as the core platform
    to orchestrate this new service would solve a lot of problems, such as providing</st>
    <st c="4901">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4915">New service containers</st> <st c="4939">as Pods</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="4946">Automated health checks for</st> <st c="4975">those services</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="4989">Resiliency and scalability through concepts such as ReplicaSets</st>
    <st c="5054">and Auto-Scaling</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="5070">External access through ingress controllers and</st> <st c="5119">Gateway
    API</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="5130">Basic observability of those services through Prometheus</st> <st
    c="5188">or OpenTelemetry</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="5204">However, do we really need to run our own Kubernetes cluster when
    we just need to deploy a simple service?</st> <st c="5312">The answer is no!</st>
    <st c="5330">There are alternatives, such as running the implementation using
    the capabilities of your preferred</st> <st c="5430">cloud provider:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="5445">Serverless</st>**<st c="5456">: The solution could be implemented
    as a set of serverless functions exposed via an</st> <st c="5540">API gateway.</st>
    <st c="5554">State or configuration can be stored in cloud storage services and
    can easily be accessed via</st> <st c="5648">an API.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="5655">Container</st>**<st c="5665">: If the solution is</st> <st c="5687">implemented
    in a container, that container can be lightweight and its endpoints can easily
    be exposed via an API gateway.</st> <st c="5809">There is no need for a full-fledged
    Kubernetes cluster that somebody needs</st> <st c="5884">to maintain.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="5896">This single use case for Financial One ACME</st> <st c="5940">may
    not lead us to choose Kubernetes as the core platform.</st> <st c="6000">However,
    when making this critical decision about what is to become the core of your future
    platform, we must also look beyond the first use case.</st> <st c="6147">Platform
    engineering will solve many more use cases by providing many self-service capabilities
    to the internal engineering teams in order to improve their</st> <st c="6303">day-to-day
    work.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6319">It’s a tricky and impactful decision to make, one that needs a
    good balance between looking forward and over-engineering.</st> <st c="6442">To
    make that decision easier, let’s look into the benefits of picking Kubernetes
    as the</st> <st c="6530">core pl</st><st c="6537">atform.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6545">Benefits of picking Kubernetes as the core platform</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="6597">To make the critical decision of</st> <st c="6631">picking the
    core of a future platform easier, let’s look at why other organizations are picking
    Kubernetes as the core building block.</st> <st c="6766">Understanding those reasons,
    the benefits, and also the challenges should make it easier for architects to
    make this</st> <st c="6883">important d</st><st c="6894">ecision.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6903">Declarative desired state – promise theory</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="6946">Traditional IT operations</st> <st c="6972">use the</st> **<st
    c="6981">obligation model</st>**<st c="6997">, which is</st> <st c="7008">when
    an external system instructs the target system to do certain things.</st> <st
    c="7082">This model requires a lot of logic to be put into the external system,
    such as an automated pipeline.</st> <st c="7184">A scripted pipeline, whether
    based on</st> <st c="7222">Jenkins, GitHub Actions, or other solutions, not only
    needs to apply changes to the target system.</st> <st c="7321">The pipeline also
    needs to deal with handling unpredicted outcomes and errors from outside the system
    it changes.</st> <st c="7435">For example, what do we do if deploying a new software
    version doesn’t work within a certain amount of time?</st> <st c="7544">Should
    we roll it back?</st> <st c="7568">How would the pipeline</st> <st c="7591">do
    that?</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7599">In the Kubernetes Documentary Part 1 (</st>[<st c="7638">https://www.youtube.com/watch?v=BE77h7dmoQU</st>](https://www.youtube.com/watch?v=BE77h7dmoQU)<st
    c="7682">), Kelsey Hightower explained the promise theory model that Kubernetes
    follows with a great analogy.</st> <st c="7784">It goes something</st> <st c="7802">like
    this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7812">If you write a letter, put it in an envelope, and put the destination
    address and the right stamps on it, then the post office promises to deliver that
    letter to the destination within a certain amount of time.</st> <st c="8024">Whether
    that delivery involves trucks, trains, planes or any other form of delivery doesn’t
    matter to the person who wrote that letter.</st> <st c="8160">The postal service
    will do whatever it takes to keep the promise of delivery.</st> <st c="8238">If
    a truck breaks down, some other truck will continue until the letter gets delivered
    to its final destination.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8350">The same principle is true for Kubernetes!</st> <st c="8394">In
    our analogy, the letter is a container image that we put into an envelope.</st>
    <st c="8472">The envelope in the Kubernetes world is a custom resource of a certain</st>
    **<st c="8543">Custom Resource Definition</st>** <st c="8569">(</st>**<st c="8571">CRD</st>**<st
    c="8574">).</st> <st c="8578">To deliver an image, this could be a definition
    of a</st> <st c="8631">Deployment, which includes the reference to the image,
    the number of replicas, the namespace this image should be deployed into, and
    the resource requirements (CPU and memory) for the image to run correctly.</st>
    <st c="8839">Kubernetes then does everything it can to fulfill the promise of
    deploying that image by finding the right Kubernetes node that meets all the requirements
    to run the container image with the specified amount of replicas and the required
    CPU</st> <st c="9080">and memory.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9091">Another example is an Ingress</st> <st c="9121">that exposes a
    deployed service to the outside world.</st> <st c="9176">Through annotations,
    it is possible to control the behavior of certain objects.</st> <st c="9256">For
    an Ingress, this could be the automatic creation of a TLS certificate for the
    domain that should be used to expose the matching services to be accessible via
    HTTPS.</st> <st c="9425">The following is an example of an Ingress object for</st>
    `<st c="9478">fund-transfer-service</st>` <st c="9499">to expose the object via
    a specific domain to the outside world using the Certificate Manager – a core
    Kubernetes ecosystem tool – to create a valid TLS certificate</st> <st c="9665">from</st>
    `<st c="9670">LetsEncrypt</st>`<st c="9681">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: <st c="10029">There is a bit more to a fully working Ingress object description
    than what’s shown in this manifest</st> *<st c="10131">[1]</st>* <st c="10134">example.</st>
    <st c="10144">However, this example does a good job of explaining how a definition
    will be translated by Kubernetes into the actual actions that one would expect
    – hence</st> <st c="10299">fulfilling</st> <st c="10311">the promise.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="10323">Now, the question is: “</st>*<st c="10347">How does all this
    magic work?</st>*<st c="10377">” To answer this, we will start by exploring the
    concepts of controllers</st> <st c="10450">and operators.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10465">Kubernetes controllers and operators</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '<st c="10502">Kubernetes</st> <st c="10514">controllers are essentially control
    loops that fulfill the promise theory of Kubernetes.</st> <st c="10603">In other
    words, controllers automate what IT admins often do manually: continuously observe
    a system’s current state, compare it with what we expect the system to look like,
    and execute remedial actions to keep the</st> <st c="10818">system running!</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10833">A core task of Kubernetes controllers is therefore</st> *<st c="10885">continuous
    reconciliation</st>*<st c="10910">. This continuous activity allows it to enforce
    the desired state, for example, making sure that the</st> *<st c="11011">desired
    state</st>* <st c="11024">expressed in the</st> *<st c="11042">Ingress definition</st>*
    <st c="11060">example from earlier matches the</st> *<st c="11094">current state</st>*<st
    c="11107">. If either the desired state or the current state changes, it means
    they are</st> *<st c="11185">out of sync</st>*<st c="11196">. The controller then
    tries to synchronize the two states by making changes to the managed object until
    the current state matches the desired</st> <st c="11338">state again!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11350">The following illustration shows how a controller watches the</st>
    *<st c="11413">desired state</st>* <st c="11426">(expressed through manifests
    and stored in etcd), compares it with the</st> *<st c="11498">current state</st>*
    <st c="11511">(the state persisted in etcd), and manages the</st> *<st c="11559">managed
    objects</st>* <st c="11574">(e.g., Ingress, Deployments, SSL certificates, and</st>
    <st c="11626">so on):</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1: Reconciliation and self-healing by design through Kubernetes
    controllers](img/Figure_4.01_B31164.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="11706">Figure 4.1: Reconciliation and self-healing by design through
    Kubernetes controllers</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="11790">This figure already shows the core</st> <st c="11826">concepts
    and power of controllers, highlighting how the automated reconciliation loop ensures
    automated self-healing by design.</st> <st c="11954">However, controllers fulfill
    other functions as well: observing cluster and node health, enforcing resource
    limits, running jobs on a schedule, processing life cycle events, and managing
    deployment rollouts</st> <st c="12161">and rollbacks.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12175">Now, let’s discuss Kubernetes</st> <st c="12205">operators.</st>
    <st c="12217">Operators are a subcategory of controllers and typically focus on
    a specific domain.</st> <st c="12302">A good example is the OpenTelemetry operator,
    which manages OpenTelemetry Collectors and the auto-instrumentation of workloads.</st>
    <st c="12430">This operator uses the same reconciliation loop to ensure that the
    desired configuration for OpenTelemetry is always applied.</st> <st c="12556">If
    the configuration is changed or if there is a problem with the current OpenTelemetry
    Collector or instrumentation, the operator will do its best to keep the promise
    of ensuring that the desired state is the actual state.</st> <st c="12780">To
    learn more, visit the OpenTelemetry website</st> <st c="12827">at</st> [<st c="12830">https://opentelemetry.io/docs/kubernetes/operator/</st>](https://opentelemetry.io/docs/kubernetes/operator/)<st
    c="12880">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12881">Other use cases for operators typically relate to managing and
    automating core services and applications such as databases, storage, service
    meshes, backup and restore, CI/CD, and</st> <st c="13062">messaging systems.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="13080">If you want to learn more about</st> <st c="13113">controllers</st>
    <st c="13124">and operators, have a look at the CNCF Operator Working Group and
    their white paper at</st> [<st c="13212">https://github.com/cncf/tag-app-delivery/tree/main/operator-wg</st>](https://github.com/cncf/tag-app-delivery/tree/main/operator-wg)<st
    c="13274">. Another excellent overview can be found on the</st> *<st c="13323">Kong
    Blog</st>* <st c="13332">post titled</st> *<st c="13345">What’s the Difference:
    Kubernetes Controllers vs Operators</st>*<st c="13403">. This blog also lists
    great examples of controllers and</st> <st c="13460">operators:</st> [<st c="13471">https://konghq.com/blog/learning-center/kubernetes-controllers-vs-operators</st>](https://konghq.com/blog/learning-center/kubernetes-controllers-vs-operators)'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13546">Now that we know more about controllers and operators, let’s have
    a look at how they also ensure built-in resiliency for all Kubernetes componen</st><st
    c="13691">ts</st> <st c="13695">and deployments!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13711">Built-in resilience driven by probes</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="13748">Controllers continuously</st> <st c="13774">validate that our
    system is in its desired state by observing the health of the Kubernetes cluster,
    along with its nodes and all deployed Pods.</st> <st c="13918">If one of the observed
    components is not healthy, the system tries to bring it back into a healthy state
    through certain automated actions.</st> <st c="14058">Take Pods, for example.</st>
    <st c="14082">If Pods are no longer healthy, they eventually get restarted to
    ensure the overall system’s resiliency.</st> <st c="14186">Restarting components
    is also often the default action an IT admin would execute following the “</st>*<st
    c="14282">Let’s try to turn it off and on again and see what</st>* *<st c="14334">happens!</st>*<st
    c="14342">” approach.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14354">Just like IT admins who probably won’t just turn things on and
    off at random, Kubernetes follows a more sophisticated approach to ensuring the
    resiliency of our Kubernetes clusters, nodes,</st> <st c="14544">and workloads.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="14558">Kubelet – a</st> <st c="14571">core component of Kubernetes –
    continuously observes the life cycle and the health state of Pods using several
    types of probes: startup, readiness, and liveness.</st> <st c="14733">The following
    illustration shows the different health states a pod can be in depending on the
    results of startup, readiness, and liveness</st> <st c="14871">probe checks:</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2: Kubelet determining the health status of Pods using probes](img/Figure_4.02_B31164.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="15031">Figure 4.2: Kubelet determining the health status of Pods using
    probes</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15101">Once Pods are no longer healthy, Kubernetes will try to restart
    Pods and bring the Pods back to a healthy state.</st> <st c="15215">There are
    a lot of different settings for both the evaluation of the probe results and the
    restart policies, which you must familiarize yourself with to fully take advantage
    of the built-in resiliency of Kubernetes.</st> <st c="15431">All those settings
    are declared on your Deployment and</st> <st c="15486">Pod definitions.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15502">If you want to learn</st> <st c="15524">more about how</st> <st
    c="15539">Kubelet manages the different probes and see some best practices, we
    can recommend checking out blog posts such as the one from Roman Belshevits on
    liveness</st> <st c="15696">probes:</st> [<st c="15704">https://dev.to/otomato_io/liveness-probes-feel-the-pulse-of-the-app-133e</st>](https://dev.to/otomato_io/liveness-probes-feel-the-pulse-of-the-app-133e
    )
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15776">Another great resource is the official Kubernetes documentation
    on configuring liveness, readiness, and startup</st> <st c="15889">probes:</st>
    [<st c="15897">https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/</st>](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15998">Health probes are only valid within Kubernetes</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16045">It’s important to understand that all these health checks are
    only done within the Kubernetes cluster and don’t tell us whether a service that
    is exposed via an Ingress to our end users is also considered healthy from the
    end users’ perspective.</st> <st c="16292">A best practice is to additionally
    check health and availability from an “outside-in” perspective as an external
    control.</st> <st c="16414">For example, you can use synthetic tests to validate
    that all exposed endpoints are reachable and return</st> <st c="16519">successful
    responses.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16540">Now that we have learned about built-in resiliency for Pods, how
    about more complex constructs, such as applications that are typically made up
    of several different Pods and other objects, such a</st><st c="16736">s Ingress</st>
    <st c="16747">and storage?</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16759">Workload and application life cycle orchestration</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '<st c="16809">As we have learned, Kubernetes provides</st> <st c="16849">built-in
    orchestration of the life cycle of Pods, as explained in the</st> <st c="16919">previous
    section.</st> <st c="16938">However, business applications that we deploy on Kubernetes
    typically have multiple dependent Pods and workloads that make up the application.</st>
    <st c="17081">Take our Financial One ACME as an example: the financial services
    applications deployed to support its customers contain multiple components, such
    as a frontend, a backend, caches, databases, and Ingress.</st> <st c="17286">Unfortunately,
    Kubernetes doesn’t have the concept of applications.</st> <st c="17354">While
    there are several initiatives and working groups to define an application, we
    currently have to rely on other approaches for managing applications, which are
    composites of</st> <st c="17532">multiple components.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17552">In the</st> *<st c="17560">Batching changes to combat dependencies</st>*
    <st c="17599">section in</st> [*<st c="17611">Chapter 5</st>*](B31164_05.xhtml#_idTextAnchor255)<st
    c="17620">, we will learn about tools such as Crossplane.</st> <st c="17668">Crossplane
    allows you to define so-called composites, which make it easy for application
    owners to define individual components of an application and then deploy individual
    instances, as shown in the</st> <st c="17868">following example:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: <st c="18240">Crossplane</st> <st c="18251">provides</st> <st c="18261">application</st>
    <st c="18273">and infrastructure orchestration and uses the operator pattern to
    continuously ensure that every application instance – as defined in the composite
    – is running</st> <st c="18434">as expected.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18446">Another tool we will learn more about</st> <st c="18484">in</st>
    [*<st c="18488">Chapter 5</st>*](B31164_05.xhtml#_idTextAnchor255) <st c="18497">is
    the</st> **<st c="18505">Keptn</st>** <st c="18510">CNCF project.</st> <st c="18525">Keptn
    provides automated application-aware life cycle orchestration and observability.</st>
    <st c="18612">It gives you the option to declaratively define pre- and post-deployment
    checks (validate dependencies, run tests, evaluate health, enforce SLO-based quality
    gates, and so on) without having to write your own Kubernetes operator to implement
    those actions.</st> <st c="18869">Keptn also provides automated deployment observability
    to better understand how many deployments happen, how many are successful, and
    where and why they fail by emitting OpenTelemetry traces and metrics for easier
    troubleshooting</st> <st c="19099">and reporting.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19113">Kubernetes provides a lot of the building blocks for building
    resilient systems.</st> <st c="19195">While you can write your own operators to
    expand this to your own problem domain, you can also use existing CNCF tools such</st>
    <st c="19318">as Crossplane</st> <st c="19332">or Keptn as they provide an easier
    declarative way to apply the concept of promise theory to</st> <st c="19426">more
    complex</st> <st c="19439">applications and</st> <st c="19456">infrastructure
    compositions.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19484">Restarting components is one way of ensuring resiliency, but there
    are more.</st> <st c="19562">Let’s have a look at auto-scaling, which solves another
    critical</st> <st c="19627">problem in</st> <st c="19638">dynamic environments!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19659">Auto-scaling clusters and workloads</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '<st c="19695">In most industries, the load</st> <st c="19725">expected on a
    system is not equally distributed across every day of the year.</st> <st c="19803">There
    is always some type of seasonality: retail gets spikes on Black Friday and Cyber
    Monday, tax services get spikes on tax day, and finance often spikes when paychecks
    are coming.</st> <st c="19986">The same is true for our own Financial One ACME
    customers.</st> <st c="20045">As a financial services organization, there is always
    some basic traffic from end users, but there will be spikes at the beginning and
    end of</st> <st c="20187">the month.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="20197">Kubernetes</st> <st c="20209">provides several ways to scale
    application</st> <st c="20251">workloads: manually (e.g., setting ReplicaSets)
    or automatically through tools such as</st> **<st c="20339">Horizontal Pod Autoscaler</st>**
    <st c="20364">(</st>**<st c="20366">HPA</st>**<st c="20369">),</st> **<st c="20373">Vertical
    Pod Autoscaler</st>** <st c="20396">(</st>**<st c="20398">VPA</st>**<st c="20401">),
    or</st> **<st c="20408">Kubernetes Event Driven Autoscaler</st>** <st c="20442">(</st>**<st
    c="20444">KEDA</st>**<st c="20448">).</st> <st c="20452">Those</st> <st c="20457">scaling
    options allow you to scale when your workloads run low on CPU or memory, when
    applications see a spike in incoming traffic, or when response time is starting</st>
    <st c="20624">to increase!</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20636">Besides workloads, you can and most likely have to also scale
    the size of your clusters and nodes through tools</st> <st c="20749">such</st>
    <st c="20753">as</st> **<st c="20757">Cluster Autoscaler</st>** <st c="20775">(</st>**<st
    c="20777">CA</st>**<st c="20779">) or</st> **<st c="20785">Karpenter</st>** *<st
    c="20794">[2]</st>*<st c="20798">, or through options available via your managed
    Kubernetes</st> <st c="20857">cloud vendor.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20870">As a platform engineering team, you need to make yourself familiar
    with all the different options but also be aware of all</st> <st c="20994">the
    considerations:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="21013">Setting limits</st>**<st c="21028">: Don’t allow applications
    to scale endlessly.</st> <st c="21076">You have options to enforce maximum limits
    per application, workload, namespaces,</st> <st c="21158">and more.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="21167">Cost control</st>**<st c="21180">: Auto-scaling is great but
    has a price tag.</st> <st c="21226">Make sure to report costs to the</st> <st
    c="21259">application owners.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="21278">Scale down</st>**<st c="21289">: Scaling up is easy!</st> <st
    c="21312">Make sure to also define indicators for when to scale down.</st> <st
    c="21372">This</st> <st c="21377">will keep costs</st> <st c="21393">under control.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="21407">To learn more about them, please review the</st> <st c="21452">documentation:</st>
    [<st c="21467">https://kubernetes.io/docs/concepts/workloads/autoscaling/</st>](https://kubernetes.io/docs/concepts/workloads/autoscaling/
    )
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21525">Now that we have learned about the options for scaling within
    a Kubernetes environment, how about sc</st><st c="21626">aling out to other</st>
    <st c="21646">Kubernetes clusters?</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21666">Declare once – run anywhere (in theory)</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="21706">The promise of Kubernetes as an open</st> <st c="21743">standard
    is that any declared state (Ingress, workloads, secrets, storage, network, etc.)
    will behave the same whether you run it on a single cluster or on multiple clusters
    to meet certain requirements, such as the separation of stages (dev, staging,
    and production) or the separation of regions (US, Europe,</st> <st c="22054">and
    Asia).</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22064">The same promise holds true in theory whether you operate your
    own Kubernetes cluster, use OpenShift, or use a managed Kubernetes service from
    one of the cloud vendors.</st> <st c="22234">What does</st> *<st c="22244">in
    theory</st>* <st c="22253">mean here?</st> <st c="22265">There are some specific
    technical differences between the different offerings you need to take into consideration.</st>
    <st c="22380">Depending on the offering, networking or storage may act slightly
    differently because the underlying implementation depends on the cloud vendor.</st>
    <st c="22525">Certain offerings will also come with specific versions of core
    Kubernetes services, services meshes, and operators that come with a managed installation.</st>
    <st c="22680">Some offerings require you to use vendor-specific annotations to
    configure the behavior of certain services.</st> <st c="22789">That’s why applying
    the same declarative state definition across different vendors will,</st> *<st
    c="22878">in theory</st>*<st c="22887">, work – in practice, you have to consider
    certain small differences that require some</st> <st c="22974">vendor-specific
    configuration!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23004">As the technical details and differences are constantly changing,
    it wouldn’t make sense to provide a current side-by-side comparison as part of
    this book.</st> <st c="23161">What you must understand is that while, in theory,
    you can take any Kubernetes object and deploy it on any flavor of Kubernetes,
    the outcome and behavior might be slightly different depending on where you deploy
    it.</st> <st c="23377">That’s why we suggest doing some technical research on
    the chosen target Kubernetes offering and how it differs from other offerings
    in case you want to go for multi-cloud/multi-Kubernetes, because the same Kubernetes
    objects might behave</st> <st c="23616">slightly differently!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23637">The good news is</st> <st c="23655">that the global community
    is working to solve this problem by providing better guidance and tools to make
    the</st> *<st c="23765">declare once – run anywhere</st>* <st c="23792">promise</st>
    <st c="23801">a reality.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="23811">We’ve looked at a lot of the benefits of picking Kubernetes as
    the core platform.</st> <st c="23894">However, there is another good reason why
    Kubernetes has seen such great adoption over the past 10 years since its first
    re</st><st c="24017">lease: the global community and</st> <st c="24050">the CNCF!</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24059">Global community and CNCF</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="24085">Kubernetes</st> <st c="24096">was announced by Google in June
    2014, and version 1.0 was released on July 21, 2015\.</st> <st c="24182">Google
    then worked with the Linux Foundation and formed the CNCF with Kubernetes as its
    initial project!</st> <st c="24287">Since then, the community and the projects
    have taken the world</st> <st c="24351">by storm!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24360">10 years later (at the time of writing this book), the CNCF has
    188 projects, 244,000 contributors, 16.6 million contributions, and members in
    193 countries worldwide.</st> <st c="24529">Many presentations that introduce
    Kubernetes and the</st> <st c="24582">CNCF often start by showing the CNCF</st>
    <st c="24619">landscape:</st> [<st c="24630">https://landscape.cncf.io/</st>](https://landscape.cncf.io/
    )
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24656">While the landscape is impressive, it has also been the source
    of many memes about how hard and complex it is to navigate the landscape of all
    projects this global community is working on.</st> <st c="24846">However, don’t
    be scared.</st> <st c="24872">The global CNCF community is part of the Linux Foundation
    and has the mission to provide support, oversight, and direction for fast-growing
    cloud-native projects, including Kubernetes, Envoy,</st> <st c="25064">and Prometheus.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25079">Here are a few things you should be aware of because they will
    help you navigate the ever-growing list of CNCF projects in the</st> <st c="25207">project
    landscape:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="25225">Project status</st>**<st c="25240">: CNCF</st> <st c="25248">actively
    tracks the status and activity of every project.</st> <st c="25306">The number
    of contributors and adopters, as well as how active development is for a project,
    are good indicators of whether you should look closer at a project.</st> <st c="25467">Projects
    that are stale, only have a single maintainer, or hardly have any adopters might
    not be of any use if you are deciding on tools that will help you for the long
    term in</st> <st c="25644">your platform.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="25658">Maturity level</st>**<st c="25673">: The CNCF also specifies
    a maturity level</st> <st c="25716">of sandbox, incubating, or graduated, which
    corresponds to the Innovators, Early Adopters, and Early Majority tiers of</st>
    <st c="25836">the</st> *<st c="25840">Crossing the Chasm</st>* <st c="25858">diagram
    (</st>[<st c="25868">https://en.wikipedia.org/wiki/Crossing_the_Chasm</st>](https://en.wikipedia.org/wiki/Crossing_the_Chasm)<st
    c="25917">).</st> <st c="25921">Graduated projects have been adopted widely across
    various industries and are a safe choice for the use cases they support.</st>
    <st c="26045">Incubating projects have crossed over from a technical playground
    to seeing good adoption with growing numbers of a diverse set of maintainers.</st>
    <st c="26189">To learn more about the criteria for CNCF maturity and to see who
    is at which level, check out the official site</st> <st c="26302">at</st> [<st
    c="26305">https://www.cncf.io/project-metrics/</st>](https://www.cncf.io/project-metrics/)<st
    c="26341">.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="26512">ADOPTERS.md</st>` <st c="26523">file, which every CNCF project
    typically has in its GitHub repository.</st> <st c="26595">If you decide to adopt
    one of those projects, we encourage you to also add your name to the list of adopters
    by opening up a pull request.</st> <st c="26734">This helps the project and will
    help other organizations decide whether this is a project</st> <st c="26824">worth
    pursuing!</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="26839">Kubernetes is vital because of its community</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26884">While Kubernetes has a strong technology base, it is really the
    community and the ecosystem that was built over the past 10+ years that makes
    Kubernetes a viable option for platform engineers to use as their</st> <st c="27093">core
    platform.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="27107">We have now learned more about what Kelsey Hightower meant when
    he said: “</st>*<st c="27182">Kubernetes is a platform to build platforms.</st>
    <st c="27228">It’s a start but not the endgame</st>*<st c="27260">.” There are
    many benefits of picking</st> <st c="27298">Kubernetes as the core platform, especially
    as it is built on the concept</st> <st c="27372">of</st> *<st c="27375">promise
    theory</st>*<st c="27389">. Kubernetes provides automated resiliency, scaling,
    and life cycle management of components.</st> <st c="27483">The ever-growing community
    provides solutions to many common problems through hundreds of open source CNCF
    projects that every organization</st> <st c="27623">can use.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27631">While we often focus on the benefit of Kubernetes for deploying
    and orchestrating applications, let’s have a look at how we can use Kubernetes
    to lift our infrastructure capabilities into our</st> <st c="27824">future platform!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27840">Leveraging and managing Kubernetes Infrastructure Capabilities</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="27903">Back in</st> [*<st c="27912">Chapter 2</st>*](B31164_02.xhtml#_idTextAnchor055)<st
    c="27921">, you were introduced to the Platform Reference Components model and
    the capability plane.</st> <st c="28012">When we are writing about lifting infrastructure
    capabilities to Kubernetes, the end user becomes aware of those capabilities when
    using the platform.</st> <st c="28163">We must differentiate between resources
    that need to be integrated with Kubernetes and those configured by specifications
    deployed to Kubernetes and manipulated or created new resources outside of the
    cluster.</st> <st c="28373">In the following figure, you can find examples in
    the resource integration and network section that require a solid integration;
    otherwise, they actively prevent a useful and</st> <st c="28548">functioning platform.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3: Capability plane with example tools](img/Figure_4.03_B31164.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="28825">Figure 4.3: Capability plane with example tools</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28872">Integrating infrastructure resources</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="28909">We will discuss the basic components and design decisions you
    must make for the platform’s underlying technologies.</st> <st c="29026">Firstly,
    due to Kubernetes’ power, you are more flexible in your tooling and can extend
    it as needed.</st> <st c="29128">This is especially helpful when you’re adjusting
    the</st> <st c="29181">platform’s capabilities for different</st> <st c="29219">use
    cases.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29229">Container storage interface</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="29257">The</st> **<st c="29262">Container Storage Interface</st>** <st
    c="29289">(</st>**<st c="29291">CSI</st>**<st c="29294">) provides</st> <st c="29306">access
    to the storage technology that is attached to the cluster or the nodes running
    the cluster.</st> <st c="29405">In the CSI developer documentation</st> *<st c="29440">[3]</st>*<st
    c="29443">, you can find a driver for almost every storage provider.</st> <st
    c="29502">The list contains</st> <st c="29520">cloud provider drivers such as
    AWS</st> **<st c="29555">Elastic Block Storage</st>** <st c="29576">(</st>**<st
    c="29578">EBS</st>**<st c="29581">), software-defined storage such as Ceph, or
    a connector for commercial solutions such as NetApp.</st> <st c="29680">In addition,
    the CSI driver also supports tool-specific drivers such as cert-manager and HashiCorp
    Vault.</st> <st c="29786">In short, the CSI is vital to any data that should live
    longer than the container it belongs to and is not stored in a database, or is
    needed for</st> <st c="29932">database storage.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29949">The installation of the driver depends on the infrastructure and
    storage technology.</st> <st c="30035">For a cloud provider, for example, you
    usually require</st> <st c="30090">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30104">A service account or</st> <st c="30126">permission policies</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="30145">Configuration for startup taints</st> <st c="30179">and tolerations</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="30194">Pre-installed</st> <st c="30209">external snapshotter</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="30229">The driver</st> <st c="30241">installation itself</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="30260">Due to their complexity, these components are deployed with Helm
    or other package management solutions.</st> <st c="30365">Sometimes, they require
    more privileges on the node, which can be a security concern when designing the
    platform.</st> <st c="30479">You will also need to consider how storage will</st>
    <st c="30527">be accessed:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="30539">ReadWriteOnce</st>** <st c="30553">(</st>**<st c="30555">RWO</st>**<st
    c="30558">): One</st> <st c="30566">Pod claims a portion of the available storage,
    which it can read from and write to, while other Pods cannot access it unless
    the original Pod releases</st> <st c="30717">the storage.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="30729">ReadWriteMany</st>** <st c="30743">(</st>**<st c="30745">RWM</st>**<st
    c="30748">): Multiple</st> <st c="30761">Pods can claim one portion of the available
    storage.</st> <st c="30814">They can read and write to it, and share that storage</st>
    <st c="30868">with others.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="30880">ReadOnlyMany</st>** <st c="30893">(</st>**<st c="30895">ROM</st>**<st
    c="30898">): Multiple</st> <st c="30910">Pods can claim one portion of the storage,
    but only to read</st> <st c="30971">from it.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="30979">ReadWriteOncePod</st>** <st c="30996">(</st>**<st c="30998">RWOP</st>**<st
    c="31002">): Can be</st> <st c="31012">claimed by only one Pod; no other Pod can
    take it, and it allows read and</st> <st c="31087">write operations.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="31104">The overview of the</st> <st c="31125">CSI driver provides further
    information on which access modes are supported.</st> <st c="31202">As there is
    no one-size-fits-all solution, you have to make your options transparent to the
    user and explain how to</st> <st c="31318">use them.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31327">The core elements to know and to understand for your users are</st>
    `<st c="31391">StorageClass</st>`<st c="31403">,</st> `<st c="31405">PersistentVolume</st>`<st
    c="31421">, and</st> `<st c="31427">PersistentVolumeClaim</st>`<st c="31448">.
    When a Pod/Deployment requires a volume,</st> `<st c="31491">StorageClass</st>`
    <st c="31503">will trigger the creation of a new volume.</st> <st c="31547">In
    case a Pod/Deployment has already claimed a volume once, and it didn’t get destroyed,
    the Kubernetes control plane will re-assign the volume to</st> <st c="31694">the
    Pod/Deployment.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4: Dynamically provisioning new persistent volumes](img/Figure_4.04_B31164.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="31893">Figure 4.4: Dynamically provisioning new persistent volumes</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31952">You define</st> `<st c="31964">StorageClass</st>` <st c="31976">as
    the platform</st> <st c="31992">team, ideally collaborating with your storage
    experts.</st> <st c="32048">The following example highlights the common definition
    of</st> `<st c="32106">StorageClass</st>`<st c="32118">. There is plenty of room
    to make a mistake in the configuration; for example, setting no</st> `<st c="32208">reclaimPolicy</st>`
    <st c="32221">will, by default, delete the later created and attached volume.</st>
    <st c="32286">However,</st> `<st c="32295">StorageClass</st>` <st c="32307">supports
    you in the dynamic creation of volumes for user-requested</st> `<st c="32375">PersistantVolumeClaim</st>`
    <st c="32396">and is therefore a strong enabler</st> <st c="32431">of self-service:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: <st c="32842">The downside of this is that you have to consider human error,
    which can take down your platf</st><st c="32936">orm by filling up the storage
    until the</st> <st c="32977">system freezes.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32992">CSI challenges</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="33007">The definition of a CSI is</st> <st c="33035">a fairly new approach,
    but the underlying technologies of storage management and software-defined storage
    are way older than Kubernetes.</st> <st c="33172">We can also discover this in
    many CSI drivers, which are nothing but a shim wrapping legacy code.</st> <st
    c="33270">For less flexible and scalable clusters, this might not be a problem,
    but in environments where you have a lot of action going on, you don’t want to
    have a CSI in your system that becomes the bottleneck.</st> <st c="33474">Some
    CSIs even have restrictions and limitations to prevent their failure at scale.</st>
    <st c="33558">To be fair, we usually see this with on-premises installations and
    some old storage technologies.</st> <st c="33656">With those, we can add the</st>
    **<st c="33683">Logical Unit Number</st>** <st c="33702">(</st>**<st c="33704">LUN</st>**<st
    c="33707">) presentation and</st> <st c="33727">connection limits on top of the
    things to consider.</st> <st c="33779">The LUN is for the Pods to make requests
    from storage space and retrieve data.</st> <st c="33858">There are limits on how
    many connections a physical server can have to the storage.</st> <st c="33942">Again,
    this is important when you manage your own storage</st> <st c="34000">and SANs.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="34009">Why are some CSIs so poor?</st> <st c="34037">The CSI does more
    than provide storage capacity.</st> <st c="34086">It communicates with the storage
    provider, promises the availability of the demanded capacity, and waits until
    the RAID controller, backup, and snapshot mechanisms are ready.</st> <st c="34261">In
    large-scale storage systems, we get even more: we can find storage capacity allocation
    and</st> <st c="34355">optimization procedures.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34379">To overcome such issues, you need to evaluate the storage, especially
    the storage drivers, of their cloud-native storage capabilities.</st> <st c="34515">This
    will require large-scale performance tests and a ridiculous number of created
    volumes.</st> <st c="34607">The CSI driver shouldn’t do any cut-downs or performance
    decrease and the created volume should be in a</st> <st c="34711">millisecond
    area.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34728">Furthermore, ensure that the</st> <st c="34758">driver allows
    cross-storage and cross-cloud/infrastructure migration; provides synchronous and
    asynchronous replication between those different infrastructures; provides feature
    parity across sites; and supports local storage types if needed, such as for</st>
    <st c="35013">edge scenarios.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35028">A good CSI will enable your platform to</st> <st c="35069">operate
    anywhere and to support a wide range of</st> <st c="35117">use cases.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35127">Container network interface</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="35155">The</st> **<st c="35160">Container Network Interface</st>** <st
    c="35187">(</st>**<st c="35189">CNI</st>**<st c="35192">) can</st> <st c="35199">become
    a platform’s most relevant component, but it is also its most underrated one.</st>
    <st c="35284">For many projects we have seen, some platform teams don’t care what
    they use as CNI, nor do they heavily utilize network policies, encryption, or
    fine-grained network configurations.</st> <st c="35467">Thanks to its simple abstraction
    of the network layer, it’s not overwhelming when getting started.</st> <st c="35566">Yet,
    on the other hand, there are many use cases where the most crucial component is
    the CNI.</st> <st c="35660">I have even seen projects fail because of the dynamic
    nature of a CNI that didn’t play along with the very traditional and legacy approach
    of</st> <st c="35802">implementing networks.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35824">A CNI always requires a dedicated implementation because it is
    a set of specifications and libraries for writing plugins to configure network
    interfaces.</st> <st c="35979">The CNI concerns itself only with the network connectivity
    of Linux containers and removing allocated resources when the container is deleted.</st>
    <st c="36122">Due to this focus, CNIs have a wide range of support, and the specifications
    are simple</st> <st c="36210">to implement.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="36223">Therefore, we architects should never treat the CNI as “</st>*<st
    c="36280">just another object in Kubernetes</st>*<st c="36314">.” We have to evaluate
    and introduce it.</st> <st c="36355">Some CNIs are made for easy maintenance and
    a solid but simple set of features.</st> <st c="36435">For example, if the primary
    focus is on layer 3, consider Flannel.</st> <st c="36502">Other CNIs, such as
    Calico, are a rock-solid choice with a rich feature set; Cilium introduced the
    usage of eBPF to provide even faster and more secure networking.</st> <st c="36666">If
    it’s still difficult to choose between those options because you may have additional
    requirements such as providing different levels of networks, then the community
    still has an answer for you: Multus.</st> <st c="36871">Take your time and discover
    your options.</st> <st c="36913">There are dozens</st> <st c="36930">of CNIs.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36938">The CNI can have serious effects on</st> <st c="36975">your platforms:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="36990">Security</st>**<st c="36999">: CNIs can provide different capabilities
    for network policies to achieve fine-grained control over your network.</st> <st
    c="37114">They can have additional encryption features, integrations into identity
    and access management systems, and</st> <st c="37222">detailed observability.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="37245">Scalability</st>**<st c="37257">: The larger the cluster gets,
    the more communication happens throughout the network.</st> <st c="37344">The
    CNI must support your growth target and stay efficient even with complex routing
    and chatting across</st> <st c="37449">the wires.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="37459">Performance</st>**<st c="37471">: How fast and direct can Pod-to-Pod
    communication be?</st> <st c="37527">How much complexity does the CNI introduce?</st>
    <st c="37571">How efficiently can it handle communication?</st> <st c="37616">Can
    it deal with many complex</st> <st c="37646">network policies?</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="37663">Operability</st>**<st c="37675">: High-level CNIs are not very
    inversive and simple to maintain.</st> <st c="37741">Powerful CNIs can, in theory,
    be replaced as long as they adhere to the CNI specification, but each comes with
    its own set of features, which are often</st> <st c="37893">not replaceable.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="37909">Be aware that not every</st> <st c="37934">CNI supports all of
    those features.</st> <st c="37970">Some, for example, do not even provide network
    policy support.</st> <st c="38033">Other CNIs are cloud-provider-specific integrating
    with just one cloud provider</st> <st c="38113">and they do enable some cloud</st>
    <st c="38143">provider-centric capabilities.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38173">Architectural challenges – CNI chaining and multiple CNIs</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="38231">Platforms are predestined for CNI chaining.</st> <st c="38276">CNI
    chaining</st> <st c="38288">introduces the sequential usage of</st> <st c="38323">multiple
    CNIs.</st> <st c="38339">The order in which CNIs are taken and for what purpose
    is defined in the</st> `<st c="38412">/etc/cni/net.d</st>` <st c="38426">directory
    and handled by the kubelet.</st> <st c="38465">This allows the platform team to
    handle one part of the network and provide a guarded approach, while platform
    users can freely configure their network at a higher level.</st> <st c="38636">For
    example, a platform user can access Antrea as a CNI to configure their networking
    to some extent.</st> <st c="38738">They can also apply network policies and egress
    configurations to prevent their application from chatting with everyone.</st>
    <st c="38859">On the other side of the platform, the platform engineering team
    will manage, via Cilium, the global cross-cluster communication, as well as the
    network encryption, to enforce security best practices.</st> <st c="39060">In
    addition, the networking data made visible by Cilium is made available to the
    operations and security teams.</st> <st c="39172">Where those use cases are most
    suitable is in the interaction with the cloud providers’ own CNIs.</st> <st c="39270">They
    often enable better integration between the platform and the cloud but lack many
    advanced features on the</st> <st c="39381">other side.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39392">Another approach to be evaluated would be to assign a Pod multiple
    network interfaces via Multus or CNI-Genie.</st> <st c="39504">Normally, a Pod
    has just one interface, but with Multus, for example, this could be multiple network
    interfaces.</st> <st c="39617">When does this become relevant?</st> <st c="39649">The
    following instances are examples when it</st> <st c="39694">becomes relevant:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39711">Separating control and operational data from</st> <st c="39757">application
    data</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="39773">Providing flexible network options for an extremely</st> <st c="39826">heterogeneous
    workload</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="39848">Taking multi-tenancy to another level by assigning completely
    different networks for</st> <st c="39934">each tenant</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="39945">Supporting unusual network protocols and connections, such as
    in edge scenarios or</st> <st c="40029">telco environments</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="40047">Network Function Virtualization</st>** <st c="40079">(</st>**<st
    c="40081">NFV</st>**<st c="40084">), which</st> <st c="40093">requires multiple
    networks due to</st> <st c="40128">its complexity</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="40142">The Multus CNI</st> <st c="40157">is a kind of meta-plugin on
    the node and sits between the actual CNIs and the Pod network interfaces, as shown
    in the following illustration.</st> <st c="40300">It attaches the different network
    interfaces to the Pod on one side and handles the connection to the anticipated
    CNIs for the right network interface on the</st> <st c="40458">other side.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5: Multus meta-CNI plugin](img/Figure_4.05_B31164.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="40544">Figure 4.5: Multus meta-CNI plugin</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40578">Both approaches must be evaluated well.</st> <st c="40619">They
    have a significan</st><st c="40641">t impact on your network complexity, performance,</st>
    <st c="40692">and security.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40705">Providing different CPU architectures</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '<st c="40743">Kubernetes supports</st> <st c="40764">multiple CPU architectures:
    AMD64, ARM64, 386, ARM, ppc64le, and even mainframes with s390x.</st> <st c="40857">Many
    clusters today run on AMD64, but at the time of writing, a strong interest in
    ARM64 is causing a shift.</st> <st c="40966">This discussion is primarily about
    saving costs and gaining a little bit of extra performance while reducing the
    total power consumption.</st> <st c="41104">At least on paper, it is a win-win-win
    situation.</st> <st c="41154">Not only is the ARM64 a possible change in the infrastructure,
    the open source architecture project RISC-V is gaining speed and is the first
    cloud provider to create RISC-V</st> <st c="41327">offerings</st> *<st c="41337">[4]</st>*<st
    c="41340">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41341">As platform engineers, we can enable those migrations and changes.</st>
    <st c="41409">A Kubernetes cluster can run multiple architectures simultaneously—not
    on the same node but with different groups of nodes.</st> <st c="41533">Remember
    that this change also requires an adjustment in the container build.</st> <st
    c="41611">With some software components, you can do a multi-architecture build;
    with others, it might require adjustments before the container for a different
    architecture can</st> <st c="41777">be created.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41788">To select the architecture on which a Deployment should be delivered,
    you just need to add a</st> `<st c="41882">nodeSelector</st>` <st c="41894">like
    this to the</st> `<st c="41912">Spec</st>` <st c="41916">section of the Deployment</st>
    <st c="41943">YAML file:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: <st c="41993">An upcoming alternative to provide different container image</st><st
    c="42054">s is to compile the software as a</st> **<st c="42089">WebAssembly</st>**
    <st c="42100">(</st>**<st c="42102">Wasm</st>**<st c="42106">) container.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42119">Wasm runtime</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="42132">The usage of Wasm as an</st> <st c="42157">alternative container
    format has increased drastically in the last year.</st> <st c="42230">Wasm is
    a binary instruction format for a stack-based virtual machine.</st> <st c="42301">Think
    of it as an intermediate layer between various programming languages and many
    different execution environments.</st> <st c="42419">You can take code written
    in over 30 different languages, compile it into a</st> `<st c="42495">*.wasm</st>`
    <st c="42501">file, and then execute that file on any</st> <st c="42542">Wasm
    runtime.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42555">The name</st> *<st c="42565">WebAssembly</st>*<st c="42576">,
    however, is misleading.</st> <st c="42602">Initially designed to make code run
    quickly on the web, today, it can run anywhere.</st> <st c="42686">Why should
    we use it?</st> <st c="42708">Wasm is secure and sandboxed by default.</st> <st
    c="42749">In a Wasm container, there is no operating system or anything else except
    the binary compiled code.</st> <st c="42849">This means there is nothing to break
    into or claim the context or service account from.</st> <st c="42937">Wasm also
    has an incredibly fast startup time where the limits are set by the runtime rather
    than the module.</st> <st c="43047">Some runtimes claim to be as fast as around
    50 ms.</st> <st c="43098">In comparison, your brain requires >110 ms to recognize
    whether something passes in front of your eyes.</st> <st c="43202">Furthermore,
    a Wasm container size is around 0.5 MB – 1.5 MB, whereas a very slim container
    can be around 5 MB.</st> <st c="43314">However, what we can see on the market
    is usually image sizes in the range of 300 MB – 600 MB, 1 GB – 3 GB, or even sometimes
    above 10 GB.</st> <st c="43453">With this reduced image size, a Wasm container
    also has a drastically reduced storage footprint.</st> <st c="43550">Wasm is also
    hardware-independent.</st> <st c="43585">The exact same image can run anywhere,
    as long as you have a Wasm</st> <st c="43651">runtime available.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43669">In the context of</st> <st c="43687">Kubernetes, the OCI and CRI
    runtimes support Wasm.</st> <st c="43739">This means you can run a Wasm container
    alongside a regular container.</st> <st c="43810">As you can see in the following
    figure, no further changes are required.</st> <st c="43883">The Wasm app image
    is stored at the node level and executed by the</st> <st c="43950">layers above.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6: Wasm on a Kubernetes node](img/Figure_4.06_B31164.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="44087">Figure 4.6: Wasm on a Kubernetes node</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44124">To make</st> <st c="44133">Wasm executable and available in your
    platform, you have to specify a runtime class and define its usage at the Deployment/Pod
    level.</st> <st c="44267">In the following example, you can see the specification
    for</st> `<st c="44327">crun</st>` <st c="44331">as</st> `<st c="44335">RuntimeClass</st>`
    <st c="44347">on the left side, and a</st> `<st c="44372">Pod</st>` <st c="44375">definition
    where, for</st> `<st c="44398">spec.runtimeClassName</st>`<st c="44419">, we assign</st>
    `<st c="44431">crun</st>` <st c="44435">on the right side.</st> <st c="44455">For</st>
    `<st c="44459">crun</st>`<st c="44463">, we also have to add an annotation to
    inform</st> `<st c="44509">crun</st>` <st c="44513">that this</st> `<st c="44524">Pod</st>`
    <st c="44527">has a</st> <st c="44534">Wasm image:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '| `<st c="44545">apiVersion: node.k8s.io/v1</st>``<st c="44572">kind: RuntimeClass</st>``<st
    c="44591">metadata:</st>``<st c="44601">name: crun</st>``<st c="44612">scheduling:</st>``<st
    c="44624">nodeSelector:</st>``<st c="44638">runtime: crun</st>``<st c="44652">handler:
    crun</st>` | `<st c="44666">apiVersion: v1</st>``<st c="44681">kind: Pod</st>``<st
    c="44691">metadata:</st>``**<st c="44701">name: wasm-demo-app</st>**` **`**<st
    c="44721">annotations:</st>**`'
  prefs: []
  type: TYPE_NORMAL
- en: '**`**<st c="44734">module.wasm.image/variant: compat</st>**`'
  prefs: []
  type: TYPE_NORMAL
- en: '**`<st c="44768">spec:</st>`'
  prefs: []
  type: TYPE_NORMAL
- en: '`**<st c="44774">runtimeClassName: crun</st>**`'
  prefs: []
  type: TYPE_NORMAL
- en: '**`**<st c="44797">containers:</st>**`'
  prefs: []
  type: TYPE_NORMAL
- en: '**`**<st c="44809">name: wasm-demo-app</st>**`'
  prefs: []
  type: TYPE_NORMAL
- en: '**`**<st c="44829">image:</st>**`'
  prefs: []
  type: TYPE_NORMAL
- en: '**`**<st c="44836">docker.io/cr7258/wasm-demo-app:v1</st>**`**************  |'
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="44870">Table 4.1: Definition of a RuntimeClass and a Pod that will be
    executed in the Wasm runtime</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44962">As an alternative to that, new developments such as SpinKube</st>
    <st c="45023">come with a whole set of tools to utilize Kubernetes resources in
    the best manner</st> *<st c="45106">[5]</st>*<st c="45109">. That approach allows
    the integration of the development experience into the deployment and the execution
    of the Wasm containerized app.</st> <st c="45247">The following image shows the
    workflow and how the different components work together.</st> <st c="45334">It
    makes the development process straightforward and brings a fast but robust new
    runtime environment to</st> <st c="45439">the platform.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7: SpinKube overview [6]](img/Figure_4.07_B31164.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="46231">Figure 4.7: SpinKube overview [6]</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46264">However, is Wasm</st> <st c="46281">really a technology that the
    industry has adopted?</st> <st c="46333">Here are just some examples and use cases
    showing</st> <st c="46383">its adoption:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="46396">Interoperability</st>**<st c="46413">: Figma runs as Wasm on</st>
    <st c="46438">any computer</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="46450">Plugin system</st>**<st c="46464">: You can write extensions
    in Wasm</st> <st c="46500">for Envoy</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="46509">Sandboxing</st>**<st c="46520">: A Firefox or Chrome browser
    can run Wasm in a sandboxed environment to protect</st> <st c="46602">your system</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="46613">Blockchain</st>**<st c="46624">: CosmWasm or the ICP uses versions
    of Wasm to</st> <st c="46672">run applications</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="46688">Container</st>**<st c="46698">: WasmCloud has a different concept
    to execute containers, or SpinKube, a Wasm runtime with a CLI for a simple</st>
    <st c="46810">development process</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="46829">Serverless platforms</st>**<st c="46850">: Cloudflare Workers
    or Fermyon Cloud run your</st> <st c="46898">Wasmized app</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="46910">Wasm is not a container replacement yet.</st> <st c="46952">It
    is an evolutionary step that suits a few cases very well, but it lacks adoption
    and has issues in the debugging process.</st> <st c="47076">However, it is a matter
    of time before these obstacles</st> <st c="47131">are solved.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47142">Enable platforms for GPU utilization</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="47179">Similar to the support for different CPU architectures or the
    extension of the container runtime to include Wasm, GPU enablement</st> <st c="47308">for
    users has become extremely relevant lately.</st> <st c="47357">A device plugin
    must be installed that is specific to the type of GPU, such as AMD, Intel, or
    Nvidia.</st> <st c="47459">This exposes custom schedulable resources such as</st>
    `<st c="47509">nvidia.com/gpu</st>` <st c="47523">to Kubernetes and its users.</st>
    <st c="47553">Also, we are not experts in GPUs and their capabilities; from a
    platform engineering perspective, the different providers have developed diverse
    feature sets and extensions for</st> <st c="47730">their plugins.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47744">I highly recommend developing or finding experts for this field
    if your user requires GPUs.</st> <st c="47837">The field of AI and LLMs is undergoing
    rapid expansion.</st> <st c="47893">Hardware and software providers come up with
    new tools, systems, and approaches monthly to take advantage of those technologies.</st>
    <st c="48022">Any scenario has its own very specific demands.</st> <st c="48070">Training
    a model requires a tremendous amount of data, GPUs, storage, and memory.</st>
    <st c="48152">Fine-tuning a model comes primarily down to how large a model you
    want to train.</st> <st c="48233">A seven-billion-parameter model can fit into
    a 14 GB VRAM, but increasing the precision can increase its size easily to 24
    GB or more.</st> <st c="48368">Lastly, providing an inference engine to serve
    an LLM for users requires a lot of</st> <st c="48450">network communication.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48472">Important note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48487">Inferencing means sending a prompt to an LLM.</st> <st c="48534">Most
    people believe that the LLM then creates the story like a human would.</st> <st
    c="48610">However, what is really happening is that after every word, the LLM
    has to send the whole prompt, including the new words, to the LLM again, so it
    can decide on the next word added to</st> <st c="48794">the sentence.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48807">As a rule of thumb, for the GPU VRAMs (Video RAM is the GPU’s
    memory) needed, you double the model’s size.</st> <st c="48915">Here are some</st>
    <st c="48929">more examples:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48943">Llama-2-70b requires 2 * 70 GB = 140</st> <st c="48981">GB VRAM</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="48988">Falcon-40b requires 2 * 40 GB = 80</st> <st c="49024">GB VRAM</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="49031">MPT-30b requires 2 * 30 GB = 60</st> <st c="49064">GB VRAM</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="49071">bigcode/starcoder requires 2 * 15.5 = 31</st> <st c="49113">GB
    VRAM</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="49120">Therefore, collaboration between the platform and the machine
    learning team is required.</st> <st c="49210">Depending on the models they want
    to use, your chosen hardware may no longer fit</st> <st c="49291">the requirements</st><st
    c="49307">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49308">Architectural challenges</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="49333">While the integration of GPUs is</st> <st c="49366">very straightforward,
    they have a few elements that need to be discussed; you should be aware of</st>
    <st c="49464">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="49478">GPU costs</st>**<st c="49488">: Cheap GPUs, with low VRAM and
    computational power, might not fit your use cases.</st> <st c="49572">Also, you
    might not use a GPU 24/7, but you should think about more dynamic and flexible
    possibilities.</st> <st c="49676">However, owning GPUs can be cheaper in the long
    run as compared to their regular CPU counterparts if your use case requires GPU</st>
    <st c="49804">computational power.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="49824">Privileged rights</st>**<st c="49842">: Many machine learning
    tools require further customization and tweaking, especially for the rights</st>
    <st c="49943">they demand.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="49955">End user requirements</st>**<st c="49977">: Besides the model
    sizes, what the data scientists and machine learning engineers want to do with
    the model depends very much on the actual use case they want to implement.</st>
    <st c="50152">Any minor change in the approach can make the platform unusable.</st>
    <st c="50217">This must be considered in the architecture for such a platform
    and to provide the most resources and greatest scaling</st> <st c="50336">flexibility
    possible.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="50357">External models pulled to the cluster</st>**<st c="50395">:
    As with containers, it is a common practice to pull models from pages such as
    HuggingFace.</st> <st c="50489">You might consider this in the network of a platform
    supporting machine</st> <st c="50561">learning activities.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '<st c="50581">Creating platforms that suit machine learning and LLM operations
    requires a new level of optimization.</st> <st c="50685">Non-running GPUs are
    a waste of money.</st> <st c="50724">Poorly used GPUs are a waste of money.</st>
    <st c="50763">However, there is more we have to ensure from an infrastructure
    perspective: data protection, platform security, and specialized observability
    for the models.</st> <st c="50922">In my opinion, machine learning and LLMs are
    an exciting use case and offer a playground for</st> <st c="51015">platform engineer</st><st
    c="51032">s.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51035">Solution space</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="51050">To optimize</st> <st c="51063">GPU usage, there are some</st>
    <st c="51089">approaches available:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="51110">Multi-Process</st>** **<st c="51125">Server</st>** <st c="51131">(</st>**<st
    c="51133">MPS</st>**<st c="51136">)</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="51138">Multi-Instance</st>** **<st c="51153">GPU</st>** <st c="51156">(</st>**<st
    c="51158">MIG</st>**<st c="51161">)</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="51163">Time-slicing/sharing</st>**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="51183">Depending on the</st> <st c="51201">GPU driver, you will find
    a full range of possibilities, as you do with Nvidia.</st> <st c="51281">Other
    GPU drivers might not be mature enough or feature-rich yet, but of</st> <st c="51354">course,
    you should evaluate</st> <st c="51382">this frequently.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="51398">Time-slicing is the</st> <st c="51419">worst option to take.</st>
    <st c="51441">Although it is better than nothing, MPS would be at least twice
    as efficient as the time-slice approach.</st> <st c="51546">However, MPS has one
    major drawback: processes are not strictly isolated, which leads to correlated
    failures between the slices.</st> <st c="51675">This is where MIG comes into the
    picture.</st> <st c="51717">It provides good process isolation and a static partitioning
    of the GPU.</st> <st c="51790">Static on a super dynamic, scalable, and anytime
    adjustable Kubernetes cluster?</st> <st c="51870">Yes, because machine learning
    training will not run just for a few seconds or</st> <st c="51948">minutes</st>
    *<st c="51956">[7]</st>*<st c="51959">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51960">The following figure shows the GPU’s memory partitions at a high
    level, to which different workloads can</st> <st c="52066">be assigned.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8: Example of splitting a GPU into three GPU instances](img/Figure_4.08_B31164.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="52479">Figure 4.8: Example of splitting a GPU into three GPU instances</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52542">Those GPU</st> <st c="52552">instances can be used either by a
    single pod, a pod with one container running multiple processes (not ideal), or
    by using something such as CUDA from Nvidia.</st> <st c="52712">CUDA</st> <st
    c="52716">is an MPS, so you can combine the different approaches, as shown in
    the</st> <st c="52789">following diagram:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9: Example of using three GPU instances in parallel](img/Figure_4.09_B31164.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="53128">Figure 4.9: Example of using three GPU instances in parallel</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53188">From here, we can take a look at the research and experiment corner
    where you can combine</st> <st c="53279">Kubernetes</st> `<st c="53484">v1.26</st>`
    <st c="53489">and is still in Alpha, therefore breaking API changes are likely
    with every release.</st> <st c="53575">There are some interesting articles and
    talks about it.</st> <st c="53631">Depending on when you are reading this, it
    might be out of</st> <st c="53690">dat</st><st c="53693">e</st> *<st c="53696">[8]</st>*<st
    c="53699">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53700">Enable cluster scalability</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="53727">As mentioned earlier in this</st> <st c="53756">chapter, the ability
    of a Kubernetes cluster to adjust its scale is beneficial for different demands,
    from resiliency to growing with the workload to fallback re-initiation, to providing
    the highest availability across data centers and availability zones.</st> <st
    c="54013">At its core, we differentiate between the horizontal and vertical autoscaler,
    which targets the Pods, and the horizontal CA, which adjusts the number</st> <st
    c="54163">of nodes.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54172">As with many capabilities, Kubernetes provides the specification
    but expects someone else to implement it.</st> <st c="54280">This at least applies
    to the VPA and the CA, which require at least a metrics server running at the</st>
    <st c="54380">cluster level.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54394">However, the HPA</st> <st c="54411">is feature-rich and allows
    metric-based scaling.</st> <st c="54461">Look at the following example of an HPA.</st>
    <st c="54502">With</st> `<st c="54507">stabilizationWindowsSeconds</st>`<st c="54534">,
    we can also tell Kubernetes to wait on previous actions to prevent flapping.</st>
    <st c="54613">Flapping is</st> <st c="54624">defined as follows according to the</st>
    <st c="54661">Kubernetes documentation:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54686">When managing the scale of a group of replicas using the HorizontalPodAutoscaler,
    it is possible that the number of replicas keeps fluctuating frequently due to
    the dynamic nature of the metrics evaluated.</st> <st c="54893">This is sometimes
    referred to as thrashing, or flapping.</st> <st c="54950">It’s similar to the
    concept of hysteresis in cybernetics.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55007">We can take the following configuration for an HPA.</st> <st c="55060">It
    looks simple; you can see that based on the policies, the behavior can change
    drastically.</st> <st c="55154">For example, when reducing 10% of the Pods while
    having a large deployment with hundreds of replicas, we want to be very careful.</st>
    <st c="55284">The shown scaling-down configuration will prevent deleting more
    than two Pods at the</st> <st c="55369">same time:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: <st c="56015">Issues with VPA, HPA, and CA</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="56044">There are some constraints that you have to consider for the</st>
    <st c="56106">autoscaler family:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="56124">HPA</st>**<st c="56128">:</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="56130">You</st> <st c="56134">have to set CPU and memory limits and requests
    on Pods correctly to prevent resource waste or frequently</st> <st c="56239">terminated
    Pods.</st>
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="56255">When HPA hits the limit of the available nodes, it can’t schedule
    more Pods.</st> <st c="56333">However, it might utilize all available resources,
    which could lead</st> <st c="56401">to issues.</st>
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="56411">VPA</st>**<st c="56415">:</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="56417">VPA and</st> <st c="56425">HPA shouldn’t be used for scaling based
    on the same metric.</st> <st c="56485">For example, while both can use CPU utilization
    to trigger a scale-up, HPA deploys more Pods, while VPA increases the CPU limits
    on existing Pods, which can lead to excessive scaling based on the same metric.</st>
    <st c="56694">Therefore, if using both VPA and HPA, one should use CPU for one
    and, for instance, memory for</st> <st c="56789">the other.</st>
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="56799">VPA might recommend using more resources than available within
    the cluster or the node.</st> <st c="56888">This causes the Pod to</st> <st c="56911">become
    unschedulable.</st>
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="56932">CA</st>**<st c="56935">:</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="56937">The</st> <st c="56941">CA scales are based on the requests and
    limits of the Pods.</st> <st c="57001">This can cause a lot of unused resources,
    poor utilization, and</st> <st c="57065">high costs.</st>
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="57076">When a CA triggers a scaling command to the cloud provider, this
    might take minutes to provide new nodes for the cluster.</st> <st c="57199">During
    this time, the application performance is degraded.</st> <st c="57258">In the
    worst case, it</st> <st c="57280">become</st><st c="57286">s unservable.</st>
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="57300">Solution space</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="57315">As platform engineers, we want to ensure that the user can define
    scaling behavior without putting the platform at risk.</st> <st c="57437">Utilizing
    HPA, VPA, and CA requires perfect configuration and control, guardrails provided
    by a policy engine, and close monitoring.</st> <st c="57570">It becomes mission-critical
    to control cluster scaling and descaling while enabling in-namespace autoscaling
    for</st> <st c="57683">your users.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57694">Managing scaling your Kubernetes cluster on cloud providers requires
    you to look into the CA and the different available cloud integrations for it.</st>
    <st c="57843">Besides, if you use the</st> **<st c="57867">Cluster API</st>**
    <st c="57878">(</st>**<st c="57880">CAPI</st>**<st c="57884">), you</st> <st c="57892">can
    also build on its capability for</st> <st c="57929">cluster</st> <st c="57936">autoscaling.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57949">Network capabilities and extensions</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="57985">Now, let’s look at the final resource integration of Kubernetes
    and the underlying infrastructure.</st> <st c="58085">To do this, we will start
    within the cluster networking mechanisms and work down to the DNS and load balancing.</st>
    <st c="58197">DNS and load balancing can happen within the cluster and in coordination
    with the infrastructure that Kuberne</st><st c="58306">tes</st> <st c="58311">runs
    on.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="58319">Ingress – the old way</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="58341">Ingress</st> <st c="58349">is the old definition of how an end
    user request from outside the cluster is routed into the system and toward the
    application that is exposed.</st> <st c="58494">For almost a decade, it was the
    way to go to define incoming network traffic.</st> <st c="58572">The ingress is
    usually represented by an ingress controller such as NGINX, HAProxy, or Envoy,
    to name a few.</st> <st c="58681">Those inherently take the routing rules defined
    as a standard resource from Kubernetes and manage the rest of it.</st> <st c="58795">As
    you can see in the following figure, from there on, the traffic is redirected
    to the right service, which forwards it to the Pod.</st> <st c="58928">Physically,
    the traffic will go from the network interface to the ingress controller to the
    Pod, but as good an orchestrator as Kubernetes is, there are some logical steps</st>
    <st c="59100">in between.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10: Ingress controller (source: Kubernetes docs)](img/Figure_4.10_B31164.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="59192">Figure 4.10: Ingress controller (source: Kubernetes docs)</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="59249">While it scales and is robust for some</st> <st c="59289">of the
    largest deployments out there, it has</st> <st c="59334">some downsides:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="59349">The Ingress API only supports TLS termination and simple content-based
    request routing of</st> <st c="59440">HTTP traffic</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="59452">It is limited in its available syntax and kept</st> <st c="59500">very
    simple</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="59511">It requires annotations</st> <st c="59536">for extensibility</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="59553">It reduces portability because every implementation has its own
    approach to</st> <st c="59630">do so</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="59635">Doing a multi-tenant cluster is more challenging because Ingress
    is usually at the cluster level and has a poor permission model.</st> <st c="59766">Also,
    it supports namespaced configurations, but it is not suitable for multi-teams
    and shared</st> <st c="59861">load-balancing infrastructure.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="59891">Some of the beauty of the Ingress approach is its wide support
    and integration with other tools, such as the cert-manager for certificate management
    and handling of the external DNS, which we will see soon.</st> <st c="60099">Also,
    the Kubernetes maintainer claims that there is no plan to deprecate Ingress as
    it perfectly supports simple web traffic in an</st> <st c="60231">uncomp</st><st
    c="60237">licated manner.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="60253">Gateway API – the new way</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="60279">In the autumn of 2023, the Gateway API</st> <st c="60318">became
    generally available.</st> <st c="60347">Instead of a single resource, the gateway
    API consists of multiple resource types following a pattern that was already used
    in other</st> <st c="60480">critical integrations:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="60502">Gateway</st>`<st c="60510">: Cluster entry point for</st> <st
    c="60537">incoming traffic</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="60553">GatewayClass</st>`<st c="60566">: Defines the gateway control
    type that will handle</st> <st c="60619">the gateway</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="60630">*Route</st>`<st c="60637">: Implements the traffic routing from
    the gateway to</st> <st c="60691">the service:</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="60703">HTTPRoute</st>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="60713">GRPCRoute</st>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="60723">TLSRoute</st>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="60732">TCPRoute</st>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="60741">UDPRoute</st>`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="60750">Comparing the following diagram</st> <st c="60783">with the Ingress
    approach, we can see the two steps for incoming traffic going through the gateway
    and being redirected</st> <st c="60903">by</st> `<st c="60906">*Route</st>`<st
    c="60912">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11: Gateway API](img/Figure_4.11_B31164.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="60972">Figure 4.11: Gateway API</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="60996">How do</st> <st c="61004">they compare?</st>
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **<st c="61017">Ingress</st>** | **<st c="61025">Gateway API</st>** |'
  prefs: []
  type: TYPE_TB
- en: '| <st c="61037">Protocols</st> | <st c="61047">HTTP/HTTPS only</st> | <st c="61063">L4
    and</st> <st c="61071">L7 protocols</st> |'
  prefs: []
  type: TYPE_TB
- en: '| <st c="61083">Multi-Tenancy</st> | <st c="61097">Difficult/Custom Extensions</st>
    | <st c="61125">Multi-Tenant</st> <st c="61139">by design</st> |'
  prefs: []
  type: TYPE_TB
- en: '| <st c="61148">Specifications</st> | <st c="61163">Annotation-based, controller-specific</st>
    | <st c="61201">Controller independent, own</st> <st c="61230">resource, standardized</st>
    |'
  prefs: []
  type: TYPE_TB
- en: '| <st c="61252">Definition/Resource</st> | <st c="61272">Ingress resource</st>
    | <st c="61289">Gateway, GatewayClass, *</st><st c="61314">Route resources</st>
    |'
  prefs: []
  type: TYPE_TB
- en: '| <st c="61330">Routing</st> | <st c="61338">host/path-based</st> | <st c="61354">Supports
    header</st> |'
  prefs: []
  type: TYPE_TB
- en: '| <st c="61370">Traffic Management</st> | <st c="61389">Limited to</st> <st
    c="61401">the vendor</st> | <st c="61411">Build-in/defined</st> |'
  prefs: []
  type: TYPE_TB
- en: '<st c="61428">Table 4.2: Comparing Ingress and the Gateway API</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="61477">This rich set of features is a game-changer for those who have
    to build platforms.</st> <st c="61561">Previously, most of these capabilities
    had to be bought commercially or brutally forced into the cluster by some hacky
    workaround and self-developed tools.</st> <st c="61717">However, with the Gateway
    API, a wide range of protocols is supported, and it comes with two</st> <st c="61809">interesting
    additional</st> <st c="61833">features loaded.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="61849">You can create cross-namespace routes with the Gateway API, either
    with</st> `<st c="61922">ReferenceGrant</st>` <st c="61936">or with</st> `<st
    c="61945">AllowedRoutes</st>`<st c="61958">. The allowed routes are implemented
    via reference bindings, which need to be defined by the gateway owner as from
    which namespaces traffic is expected.</st> <st c="62111">In practice, the Gateway
    configuration will be extended</st> <st c="62167">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: <st c="62305">This will allow traffic from the</st> `<st c="62339">alpha</st>`
    <st c="62344">and</st> `<st c="62349">omega</st>` <st c="62354">namespaces.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="62366">Alternatively, we can use a</st> `<st c="62395">ReferenceGrant</st>`<st
    c="62409">, which is described</st> <st c="62430">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="62441">ReferenceGrant can be used to enable cross namespace references
    within Gateway API.</st> <st c="62526">In particular, Routes may forward traffic
    to backends in other namespaces, or Gateways may refer to Secrets in another namespace.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="62655">They sound similar, but they aren’t.</st> <st c="62693">Let’s</st>
    <st c="62699">compare them:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="62712">ReferenceGrant</st>`<st c="62727">: A Gateway and Route in namespace
    A grant a service in namespace B to</st> <st c="62799">forward traffic</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="62814">AllowedRoutes</st>`<st c="62828">: A Route to namespace B is
    configured in a Gateway in</st> <st c="62884">namespace C</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="62895">Besides those cross-namespace additional security layers, the
    Gateway API also comes with its own extension capabilities.</st> <st c="63018">With
    them, you can define your own PolicyAttachment, such as BackendTLSPolicy, to validate
    the proper usage</st> <st c="63126">of TLS.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="63133">One last thing before we move on.</st> <st c="63168">The Gateway
    API comes with personas.</st> <st c="63205">Personas are pre-defined roles that
    allow fine-grained usage of gateway capabilities.</st> <st c="63291">Ingress just
    has one persona, a</st> <st c="63323">user, independent of whether it is an admin
    or a developer.</st> <st c="63383">The following table shows the write permissions
    of those personas in a</st> <st c="63454">four-tier model:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12: Write permissions for an advanced four-tier model](img/Figure_4.12_B31164.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="63693">Figure 4.12: Write permissions for an advanced four-tier model</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="63755">At this point, the</st> <st c="63775">Gateway API is the true
    savior of countless nights of getting inbound traffic right and building a multi-tenant
    platform with a clear and</st> <st c="63913">transparent approach</st><st c="63933">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="63934">ExternalDNS</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="63946">The</st> **<st c="63951">ExternalDNS</st>** <st c="63962">project,
    developed</st> <st c="63981">by the Kubernetes contributors, is a tool that is
    often used within cloud-provided Kubernetes clusters, but still not often highlighted
    as a relevant implementation.</st> <st c="64148">Yet it bridges the gap between
    some random IPs of Pods in the cluster, takes it toward a proper DNS that is publicly
    or privately reachable, and routes traffic to the application within the platform.</st>
    <st c="64348">ExternalDNS provides support for almost every cloud and cloud-like
    environment, as well as traffic- and content-focused services such</st> <st c="64482">as
    CloudFlare.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="64496">However, ExternalDNS is not a DNS.</st> <st c="64532">Surprise!</st>
    <st c="64542">It reads the resources from the Kubernetes API and configures external
    DNS to point to the cluster’s public endpoints.</st> <st c="64661">You could also
    say that ExternalDNS allows you to control DNS records dynamically via Kubernetes
    resources in a DNS</st> <st c="64777">provider-agnostic way.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="64799">Let’s have a look at how ExternalDNS works together with the CoreDNS
    of Kubernetes and the Cloud DNS Service.</st> <st c="64910">In the next diagram,
    you can see the managed Kubernetes on the left.</st> <st c="64979">In this case,
    AWS EKS and the CoreDNS are running on Kubernetes to resolve internal DNS calls.</st>
    <st c="65074">When ExternalDNS is deployed, it observes the gateway, ingress,
    and service resources.</st> <st c="65161">When changes apply or new services</st>
    <st c="65196">come up, ExternalDNS updates the DNS records on the cloud provider
    or DNS</st> <st c="65270">service provider.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13: ExternalDNS](img/Figure_4.13_B31164.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="65554">Figure 4.13: ExternalDNS</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="65578">However, when looking for an alternative approach, you usually
    have only two fallback options: create the DNS entries by yourself (manually)
    or have it automated in some way with a custom controller or function, or during
    the infrastructure</st> <st c="65820">creation process.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="65837">Therefore, it is double painful to see the limitations</st> <st
    c="65892">of ExternalDNS:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="65908">Missing fine-grained control; for example, ExternalDNS will create
    DNS records for all services and ingresses from</st> <st c="66024">any namespace</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="66037">ExternalDNS gives you only A records; if you need a TXT or CNAME
    record, you have to do</st> <st c="66126">it manually</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="66137">You will get default DNS configurations for the DNS name; otherwise,
    you have to manually</st> <st c="66227">define them</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="66239">Besides that, you can also find increased costs (due to its outrageous
    DNS record creation behavior) and added complexity or latency.</st> <st c="66374">I
    can’t fully agree with those types of issues as it is a question of how you</st>
    <st c="66452">handle them.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="66464">Consider using ExternalDNS only when you have mastered other parts
    of Kubernetes networking and are able to do fine-grained management of network
    policies and the gateway API so that you have strict control over which services
    are reachable, and how.</st> <st c="66716">In addition, consider enabling the
    DNSSEC feature and also establishing</st> <st c="66788">DNS monitori</st><st c="66800">ng.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="66804">Load balancing, EndpointSlices, and Topology-Aware Routing</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="66863">Lastly, to</st> <st c="66875">round off</st> <st c="66884">the
    network segment, we will briefly</st> <st c="66922">discuss</st> **<st c="66930">load
    balancing</st>**<st c="66944">,</st> **<st c="66946">EndpointSlices</st>**<st
    c="66960">, and</st> **<st c="66966">Topology-Aware Routing</st>**<st c="66988">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="66989">Load balancing is</st> <st c="67007">handled by the ingress controller
    or gateway API within the cluster.</st> <st c="67077">This can be outsourced to
    an external load balancer in combination with a cloud provider.</st> <st c="67167">All
    major cloud providers have their own approach, usually via their own controller,
    to manage the managed service load balancer.</st> <st c="67297">What is the difference
    between these options?</st> <st c="67343">Running your own load balancer within
    Kubernetes means that, first, all traffic gets routed to that entry point.</st>
    <st c="67456">With the correct setup, this can be multiple nodes with a very simple
    load distribution.</st> <st c="67545">The downside is that if one node is overloaded
    for some reason, it still gets the traffic to handle and distribute internally.</st>
    <st c="67672">A cloud load balancer will distribute the load across multiple nodes,
    and, depending on how the integration is done, it is aware of whether a node can
    handle more load or whether it should be redirected to another one.</st> <st c="67891">A</st>
    <st c="67893">downside of the public cloud load balancer is that you must also
    pay</st> <st c="67962">for it.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="67969">EndpointSlices</st> <st c="67984">become relevant for large-scale
    clusters.</st> <st c="68027">Endpoints are API objects that define a list of IP
    addresses and ports.</st> <st c="68099">These addresses belong to the Pods that
    are dynamically assigned to a service.</st> <st c="68178">When a service is created,
    Kubernetes automatically creates an associated Endpoint object.</st> <st c="68269">The
    Endpoint object maintains the Pods’ IP addresses and port numbers that match the
    service’s selector criteria.</st> <st c="68383">EndpointSlices were introduced
    in Kubernetes 1.16\.</st> <st c="68434">They provide a way to distribute the network
    endpoints across multiple resources, reducing the load on the Kubernetes API server
    and improving the performance of large clusters.</st> <st c="68612">While in the
    past, a single large Endpoint object for a service became slow, multiple small
    EndpointSlice objects are now created, each representing a portion of</st> <st
    c="68774">the endpoints.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="68788">The control plane creates and manages EndpointSlices to have no
    more than 100 endpoints per slice.</st> <st c="68888">This can be changed, up
    to a maximum of 1,000\.</st> <st c="68935">For the kube-proxy, an EndpointSlice
    is the source of truth for routing</st> <st c="69007">internal traffic.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="69024">EndpointSlices are also required for</st> <st c="69062">Topology-Aware
    Routing.</st> <st c="69086">With Topology-Aware Routing, you can keep the traffic
    within the cloud provider’s</st> **<st c="69168">Availability Zone</st>** <st
    c="69185">(</st>**<st c="69187">AZ</st>**<st c="69189">).</st> <st c="69193">This
    has two main advantages: it reduces the</st> <st c="69238">network costs and improves
    the performance.</st> <st c="69282">Instead of a Pod in AZ 1 communicating with
    another Pod in AZ 2 and sending a lot of data, the Pod in AZ 1 will now talk to
    a replica (if available) that is also in AZ 1\.</st> <st c="69453">To make this
    work best, the incoming traffic should be distributed evenly via an external load
    balancer and you should have at least three endpoints per zone.</st> <st c="69612">Otherwise,
    the controller will fail to assign that endpoint with a chance of</st> <st c="69688">around
    50%.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="69700">Kubernetes as part of the platform control plane</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '<st c="69749">Looking back to the second chapter, we can see in the reference
    architecture that a platform can be highly distributed, based on many services
    that, from a higher viewpoint, might not even belong together.</st> <st c="69956">We
    discussed that Kubernetes might often become a central part of your platform.</st>
    <st c="70037">However, how central can it become?</st> <st c="70073">As said,
    Kubernetes is not just there to run workload; it is a platform (based on promise
    theory and a standardized model and API) for building platforms.</st> <st c="70228">This
    shifts Kubernetes with one foot into the platform control plane and does this
    in two ways: as a resource controller and as a</st> <st c="70358">platform or</st><st
    c="70369">chestrator.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="70381">Steering resources from within Kubernetes</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="70423">The open source</st> <st c="70440">Crossplane project is the only
    provider-independent solution for managing cloud resources from within Kubernetes.</st>
    <st c="70554">Initially created to manage other Kubernetes clusters from within
    Kubernetes, it quickly became the universal solution for handling cloud resources.</st>
    <st c="70703">Cloud resources are available as CRDs and can be defined as Kubernetes-native
    resources through a specification file.</st> <st c="70821">This gives users the
    option to define what they need and leave the resource creation on the promise
    theory of Kubernetes.</st> <st c="70943">For the different clouds, so-called providers
    are available, which define the available resources.</st> <st c="71042">A user
    can create single resources or whole compositions, which are multiple resources
    that</st> <st c="71134">bel</st><st c="71137">ong together.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="71151">The problem of external versus internally defined resources</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="71211">What is the right approach to managing resources?</st> <st c="71262">Should
    they be defined by an infrastructure team or through input from demand forms?</st>
    <st c="71347">Can they be defined by the user via the platform?</st> <st c="71397">Anything
    is possible, but no simple</st> <st c="71433">answer exists.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="71447">Let’s have a look at the two approaches.</st> <st c="71489">First,
    let us take a look at a scenario at Financial One ACME, coming from a more traditional,
    conservative background: infrastructure teams have been fighting over the last
    few years for automation and a declarative approach.</st> <st c="71716">While
    they manage their on-premises environments through Ansible, they decided to use
    something simpler for the cloud providers: Terraform (or OpenTofu).</st> <st c="71870">We
    will not go through the whole stack, but until we hit the Kubernetes platform,
    everything is orchestrated through classic CI/CD push principles and IaC</st>
    <st c="72025">via Terraform.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="72039">Financial One ACME is starting a new project to develop custom
    software for their internal usage.</st> <st c="72138">The team will utilize the
    ACME platform and work on the system’s base architecture.</st> <st c="72222">They
    have defined that they will require certain file storage, a cache, a relational
    database, a notification service, and a message streaming service.</st> <st c="72374">As
    the platform provides some self-service, the team can copy the Terraform modules
    into their repository.</st> <st c="72481">From here, a predefined CI/CD pipeline
    will take over the configuration and deploy the resources in the defined environments.</st>
    <st c="72607">These self-defined but still externally managed resources are, in
    some ways, isolated from the rest of the system.</st> <st c="72722">They may live
    in the same repository or hierarchy and are managed by the team, but they are
    not</st> <st c="72818">strongly integrated.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="72838">On the other hand, they provide a certain level of stability.</st>
    <st c="72901">From within the platform user space, those resources are invisible,
    except that a discovery service exists.</st> <st c="73009">When an organization
    matures, the pipelines might be customized without notifying the owner or user.</st>
    <st c="73110">However, infrastructure and application are clearly separated, which
    is an advantage because of the totally different life cycles of</st> <st c="73243">the
    elements.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="73256">Fast forward and Financial One ACME has undergone a cloud-native
    transformation, leveraging platform engineering and IDPs to the maximum.</st>
    <st c="73395">Again, they plan to do a new internal project, which, of course,
    is completely different from the one before but somehow has exactly the same requirements.</st>
    <st c="73551">Some organizations’ behavior will never change.</st> <st c="73599">This
    time, the project team created a new project in their developer portal.</st> <st
    c="73676">Automatically, all base requirements will be pushed into a new Git repository.</st>
    <st c="73755">The chosen resources are selected and pushed to the platform, where
    a controller decides where it deploys those resources.</st> <st c="73878">Some
    end up as managed services on the cloud provider, others in a shared service account
    from a specialized team, and a few in the project’s namespace.</st> <st c="74031">After
    some time, the team understood that they had chosen the wrong configuration and,
    due to the adjustments, a migration to the new service took place.</st> <st c="74185">This
    scenario can be as true as the previous one but has</st> <st c="74242">different
    impacts.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="74260">The team needs to know in more detail which requirements they
    have; on the other hand, they have to trust in the predefined deployments and
    configurations.</st> <st c="74417">The platform engineering team has, in collaboration
    with the operational team, defined best practices with guardrails, ensuring operability
    but also matching almost all the requirements of the users.</st> <st c="74617">Within
    the cluster user space, the team can find all deployed resources and address them
    as a service within the platform, even though they are not running in it.</st>
    <st c="74780">However, sudden changes on the user side might cause resources to
    suddenly spike or get deleted in other places.</st> <st c="74893">Managed service
    teams have to handle such changes without warning.</st> <st c="74960">In total,
    all depending resources are acting extremely volatile and dynamic, hard to predict,
    and difficult to manage.</st> <st c="75079">On the other hand, the project can
    focus fully on the flow and progress as external resources are handled from within
    the cluster, rather than learning how to manage those with outer-cluster resource
    management solutions such</st> <st c="75305">as IaC.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="75312">Both approaches are fine.</st> <st c="75339">Both have pros and
    cons, and the one that is best for your organization often depends more on the
    human factor than any technological factor.</st> <st c="75481">However, what is
    clear is that internally defined cluster resources are more dynamic and shifted
    to the left, into the user’s responsibility, than in externally</st> <st c="75642">defined
    resources.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="75660">In the end, it becomes a philosophical discussion.</st> <st c="75712">Externally
    defined resources are more traditional, whereas the internally defined approach
    is progressive and future-oriented.</st> <st c="75839">However, we don’t have
    too many options to run cluster internal provisioning processes.</st> <st c="75927">Besides
    Crossplane, we have seen many meta-implementation controllers that read custom
    resources from the cluster and trigger CI/CD pipelines, for example.</st> <st
    c="76083">That’s a poor workaround, if</st> <st c="76112">someone asks.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="76125">In this section, we looked at the fundamental capabilities that
    are required for Kubernetes, the challenges around them, and how we can solve
    them.</st> <st c="76274">Also, they don’t feel like any of those crazy implementations
    you see at conferences.</st> <st c="76360">Getting these basics right will make
    the difference and decide whether everything else on top will be a pleasure or
    a pain.</st> <st c="76484">Up next, we will close this chapter by looking into
    the approach of finding the right node sizes and the ramifications for flexibility</st>
    <st c="76619">and</st> <st c="76622">reliability.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="76635">Designing for flexibility, reliability, and robustness</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '<st c="76690">In the previous part, we discussed cluster scalability and how
    the VPA, HPA, and CA play together.</st> <st c="76790">This helps to create a
    flexible, reliable, and robust system.</st> <st c="76852">A key part of this is
    also allowing customization as long as it doesn’t harm your system.</st> <st c="76942">Components
    of the cluster must play together seamlessly but must also be exchangeable where
    needed.</st> <st c="77042">This is sensitive: you have, on the one hand, a breathing
    cluster that grows and shrinks its demand over time; then, you have all the extensions
    on and around the cluster that allow you to serve the best possible feature set
    for your use case.</st> <st c="77286">You also have the continuously evolving
    open source community that frequently delivers updates and new developments, which
    should be integrated and made available for your users.</st> <st c="77465">As
    we told you earlier, this is why you must have a product mindset – to build the
    best possible platform for your users.</st> <st c="77587">Throw away things you
    don’t need or that are outdated, but keep the whole system as</st> <st c="77671">your
    core.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="77681">Now, for some reason, we still see discussions about whether
    you should put all your workload on Kubernetes, and whether it is reliable.</st>
    <st c="77819">We have to look at this discussion from two perspectives: bottom-up
    from Kubernetes’s infrastructure and core responsibilities, and top-down from
    what the user can see</st> <st c="77987">an</st><st c="77989">d experience.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="78003">Optimize consumption versus leaving enough head space</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="78057">As we learned in</st> [*<st c="78075">Chapter 2</st>*](B31164_02.xhtml#_idTextAnchor055)<st
    c="78084">, the Kubernetes cluster and the workload it manages have an interesting
    relationship and influence on each other.</st> <st c="78199">Some applications
    require more CPU power, others scale up instead, and the rest just run on demand.</st>
    <st c="78299">Finding the right match isn’t easy, but an ideal target is high
    resource consumption as it optimizes the usage of the available resources and
    therefor</st><st c="78449">e</st> <st c="78452">the costs.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="78462">How to make the clusters the right size</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="78502">Evaluating the right size</st> <st c="78528">for the cluster is
    always a challenge.</st> <st c="78568">Finding the right solution is a clear case
    of</st> *<st c="78614">it depends</st>*<st c="78624">. Let’s take Financial One
    ACME, which has to provide a new cluster.</st> <st c="78693">We don’t know a lot
    about the expected workload, just that it requires some memory and has a few moving
    parts that are not that resource-demanding.</st> <st c="78841">So, we can take
    one of the</st> <st c="78868">following options:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="78886">1x 16 vCPU, 64</st> <st c="78902">GB memory</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="78911">2x 8 vCPU, 32</st> <st c="78926">GB memory</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="78935">4x 4vCPU, 16</st> <st c="78949">GB memory</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="78958">8x 2vCPU, 8</st> <st c="78971">GB memory</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="78980">For availability reasons, the first option would be a bad idea.</st>
    <st c="79045">If you run an update in the cluster and the whole system goes down
    or you have to provision one new node, you need to shift all the apps over and
    shut down the old one.</st> <st c="79214">Option 4 comes with many nodes.</st>
    <st c="79246">Due to its small size, this can lead to a resource shortage as some
    base components that are required for the cluster will consume a part of the CPU
    and memory.</st> <st c="79407">In addition, depending on the cloud provider, it
    might be that you have other limitations, such as available IP addresses, bandwidth,
    and storage capacity.</st> <st c="79563">Also, if you have an app that needs 1
    vCPU, and might scale to 1.5 – 2.0 vCPU, it would kill an</st> <st c="79659">entire
    node.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="79671">However, how many resources are used per node by Kubernetes?</st>
    <st c="79733">By default, for the CPU, we can use the</st> <st c="79773">following
    rules:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="79789">6% of the</st> <st c="79800">first core</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="79810">1% of the</st> <st c="79821">second core</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="79832">0.5% of the next</st> <st c="79850">two cores</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="79859">0.25% from the 5th</st> <st c="79879">core onward</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="79890">We also have some rough rules for</st> <st c="79925">the memory:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="79936">25% of the first</st> <st c="79954">4 GB</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="79958">20% of the following</st> <st c="79980">4 GB</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="79984">10% of the next</st> <st c="80001">8 GB</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="80005">6% of the next 112 GB (up to</st> <st c="80035">128 GB)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="80042">2% of anything above</st> <st c="80064">128 GB</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="80070">In case the node has less than 1GB of memory, it is</st> <st c="80123">255
    MiB</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="80130">In addition, every</st> <st c="80150">node has an eviction threshold
    of 100 MB.</st> <st c="80192">If the resources are completely utilized and the
    threshold is crossed, Kubernetes starts cleaning up some Pods to prevent completely
    running out of memory.</st> <st c="80348">At learnk8s</st><st c="80359">, you
    can find a very detailed blog about</st> <st c="80401">it (</st>[<st c="80405">https://learnk8s.io/kubernetes-node-size</st>](https://learnk8s.io/kubernetes-node-size)<st
    c="80446">).</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="80449">Let’s visualize</st> <st c="80466">these numbers:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **<st c="80480">2 vCPU 8</st>** **<st c="80490">GB RAM</st>** | **<st
    c="80496">4 vCPU 16</st>** **<st c="80507">GB RAM</st>** | **<st c="80513">8 vCPU
    32</st>** **<st c="80524">GB RAM</st>** |'
  prefs: []
  type: TYPE_TB
- en: '| **<st c="80530">Kubelet +</st>** **<st c="80541">OS vCPU</st>** | <st c="80548">70
    m or</st> <st c="80557">0.07 vCPU</st> | <st c="80566">80 m or</st> <st c="80575">0.08
    vCPU</st> | <st c="80584">90 m or</st> <st c="80593">0.09 vCPU</st> |'
  prefs: []
  type: TYPE_TB
- en: '| **<st c="80602">Kubelet +</st>** **<st c="80613">OS memory</st>** | <st c="80622">1.8
    GB</st> | <st c="80629">2.6 GB</st> | <st c="80636">3.56 GB</st> |'
  prefs: []
  type: TYPE_TB
- en: '| **<st c="80644">Eviction threshold</st>** | <st c="80663">100 MB</st> | <st
    c="80670">100 MB</st> | <st c="80677">100 MB</st> |'
  prefs: []
  type: TYPE_TB
- en: '| **<st c="80684">Available vCPU</st>** | <st c="80699">1930 m or</st> <st
    c="80710">1.9 vCPU</st> | <st c="80718">3920 m or</st> <st c="80729">3.9 vCPU</st>
    | <st c="80737">7910n or</st> <st c="80747">7.9 vCPU</st> |'
  prefs: []
  type: TYPE_TB
- en: '| **<st c="80755">Available memory</st>** | <st c="80772">6.1 GB</st> | <st
    c="80779">13.3 GB</st> | <st c="80787">28.34 GB</st> |'
  prefs: []
  type: TYPE_TB
- en: '<st c="80796">Table 4.3: Resource consumption and available resources for different
    node sizes</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="80877">From those available</st> <st c="80899">resources, you also have
    to take away anything that is required to cluster, such as</st> `<st c="80983">DaemonSet</st>`
    <st c="80992">for logging and monitoring,</st> `<st c="81021">kube-proxy</st>`<st
    c="81031">, storage driver, and</st> <st c="81053">so on.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="81059">Doing the math, if you have many small nodes, the basic layer
    of used resources is larger than that of the one with big nodes.</st> <st c="81187">Let
    us look at an example.</st> <st c="81214">Two 2vCPU 8 GB RAM nodes require 140
    m of CPU and 3.6 GB of RAM in total, while one 4vCPU 16 GB RAM only requires 80
    m of CPU and 2.6 GB of RAM.</st> <st c="81359">Those are the costs for higher
    availability, but in a side-by-side comparison of just the available resources,
    a single node requires fewer resources to be assigned to Kubernetes.</st> <st
    c="81539">However, ideally, a node is utilized at its maximum capacity; we don’t
    run a virtual machine where 80% head space of non-utilized resources is standard!</st>
    <st c="81692">At least 80% utilization of a node would be the target to get the
    best performance/price ratio.</st> <st c="81788">This is also because of the energy
    proportionality of a server.</st> <st c="81852">The energy needed by a CPU doesn’t
    linearly scale with</st> <st c="81907">the workload.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="81920">Imagine that a server can consume up to 200 watts.</st> <st c="81972">Then,
    most servers would need between 150 and 180 watts for around 50% of CPU utilization.</st>
    <st c="82063">This means the more a server is utilized, the better the energy/CPU
    utilization ratio, which is also more cost-efficient</st> <st c="82184">and sustainable.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="82200">We have to consider other factors while choosing the node sizes
    for</st> <st c="82269">the cluster:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="82281">If you provide just a few large nodes, but the user defines an
    anti-affinity so that on each node only one Pod of a replica is running and you
    don’t have enough nodes because the expected replicas are too high, then some
    Pods</st> <st c="82508">become unschedulable.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="82529">Large nodes tend to be underutilized, so you spend more money
    on something you</st> <st c="82609">don’t use.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="82619">If you have too many small nodes and you continuously run into
    pending Pods for which each cluster has to scale, that might make the users unhappy
    as it always takes time to scale up new nodes.</st> <st c="82814">Also, it might
    affect the servability of an app if it runs continuously under</st> <st c="82892">resource
    limitations.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="82913">Many nodes cause a higher amount of network communication, container
    image pulls, and duplicate image storage on each node.</st> <st c="83038">Otherwise,
    in the worst case, you always pull an image from a registry again.</st> <st c="83116">With
    a small cluster, that isn’t a problem, but with a large number of nodes, this
    becomes</st> <st c="83207">quite chatty.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="83220">The more</st> <st c="83230">nodes there are, the more communication
    happens between them and the control plane.</st> <st c="83314">At some level of
    requests, this means increasing the node sizes of the</st> <st c="83385">control
    plane.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="83399">Finding the right node and cluster size is a science in itself.</st>
    <st c="83464">Neither end of the extreme is good.</st> <st c="83500">Start looking
    at the kind of workload you expect.</st> <st c="83550">If you’re not sure, start
    with something medium-sized and adjust the node sizes if needed.</st> <st c="83641">Also,
    consider always having a little bit more memory available.</st> <st c="83706">The
    core components of Kubernetes per node don’t require a lot of CPU, but do require
    at least something between 2 and 3 GB of memory plus all the other</st> <st c="83859">default
    components</st><st c="83877">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="83878">Solution space</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="83893">Most public cloud providers come with the ability to run multiple
    instance types for Kubernetes nodes.</st> <st c="83997">This is helpful for different
    use cases, from isolating workloads to optimizing the utilization or migrating
    from one CPU architecture to another.</st> <st c="84144">We also talked about
    GPU utilization, which you can combine in such scenarios to run non-GPU workloads
    on CPU nodes while doing model training on the</st> <st c="84294">GPU instances.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="84308">To do this, you have to manage and label the nodes correctly,
    and provide users with a transparent approach and support for defining their</st>
    <st c="84448">affinity settings.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="84466">To identify the right node sizes, learnk8s also provides you with
    an instance calculator (</st>[<st c="84557">https://learnk8s.io/kubernetes-instance-calculator</st>](https://learnk8s.io/kubernetes-instance-calculator)<st
    c="84608">), which you might consider before you start building your own Excel
    sheet for doing</st> <st c="84694">the math.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="84703">Also, when defining the cluster size and scale doesn’t look that
    relevant, with the right node sizes, you can have a direct impact on costs, utilization,
    application availability, user experience, and how many additional implementations
    you have to do to compensate for</st> <st c="84974">potential drawbacks</st><st
    c="84993">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="84994">Summary</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '<st c="85002">In this chapter, we got closer to some of the relevant components
    of Kubernetes as the cornerstone for your platform.</st> <st c="85121">We first
    examined whether Kubernetes is the right choice, and also looked at why it often
    is the right way to go.</st> <st c="85235">With the promise theory at its heart
    and many robust features for running and extending a platform, Kubernetes is an
    almost perfect foundation for a platform.</st> <st c="85394">From here, we looked
    into some of the very basic elements of Kubernetes: storage, networking, CPU architectures,
    and GPU support.</st> <st c="85524">In this context, we learned about some design
    considerations and problems we might face while implementing Kubernetes.</st>
    <st c="85643">While Kubernetes as a foundation might feel different in every environment,
    it is possible to create a unified experience.</st> <st c="85766">This will come
    with major drawbacks, such as losing the features of certain cloud providers,
    flexibility,</st> <st c="85872">and customizability.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="85892">Next, we discussed finding the balance between a very stiff and
    highly flexible system.</st> <st c="85981">Both can be seen as robust and reliable,
    but they come with very different challenges and problems.</st> <st c="86081">Therefore,
    we did a short thought experiment to find the right cluster sizes and node types
    before we closed this section by discussing approaches for implementing guardrails
    for the user space.</st> <st c="86276">This helps us provide flexibility within
    the user space but protects the platform from misbehavior and wrongly configured
    services by users.</st> <st c="86417">We learned about this in</st> <st c="86442">this
    chapter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="86455">In the next chapter, we will focus on the automation of platforms.</st>
    <st c="86523">Besides the infrastructure, automation is a critical component of
    a platform and, as you will see later on, can be a bottleneck and cost driver
    in the long run.</st> <st c="86684">You will learn how to design a proper release
    process, how to implement it in CI/CD and GitOps, and how to use this combination
    for the life cycle of the platform artifacts.</st> <st c="86858">We will also
    show you how to effectively observe</st> <st c="86907">this process</st><st c="86919">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="86920">Further Reading</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="86936">[1] Objects in</st> <st c="86952">Kubernetes:</st> [<st c="86964">https://kubernetes.io/docs/concepts/overview/working-with-objects/</st>](https://kubernetes.io/docs/concepts/overview/working-with-objects/
    )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="87030">[2]</st> <st c="87035">Karpenter:</st> [<st c="87046">https://karpenter.sh/</st>](https://karpenter.sh/
    )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="87067">[3] CSI drivers</st> <st c="87084">index:</st> [<st c="87091">https://kubernetes-csi.github.io/docs/drivers.html</st>](https://kubernetes-csi.github.io/docs/drivers.html
    )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="87141">[4] RISC-V Kubernetes nodes on</st> <st c="87173">Scaleway:</st>
    [<st c="87183">https://www.scaleway.com/en/docs/bare-metal/elastic-metal/how-to/kubernetes-on-riscv/</st>](https://www.scaleway.com/en/docs/bare-metal/elastic-metal/how-to/kubernetes-on-riscv/
    )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="87268">[5]</st> <st c="87273">SpinKube:</st> [<st c="87283">https://www.spinkube.dev/</st>](https://www.spinkube.dev/
    )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="87308">[6] SpinKube</st> <st c="87322">Overview:</st> [<st c="87332">https://www.spinkube.dev/docs/overview/</st>](https://www.spinkube.dev/docs/overview/
    )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="87371">[7] Nvidia – Improving GPU Utilization in</st> <st c="87414">Kubernetes:</st>
    [<st c="87426">https://developer.nvidia.com/blog/improving-gpu-utilization-in-kubernetes/</st>](https://developer.nvidia.com/blog/improving-gpu-utilization-in-kubernetes/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="87500">[8] DRA</st> <st c="87509">with GPU:</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[<st c="87518">https://docs.google.com/document/d/1BNWqgx_SmZDi-va_V31v3DnuVwYnF2EmN7D-O_fB6Oo/edit#heading=h.bxuci8gx6hna</st>](https://docs.google.com/document/d/1BNWqgx_SmZDi-va_V31v3DnuVwYnF2EmN7D-O_fB6Oo/edit#heading=h.bxuci8gx6hna)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[<st c="87626">https://static.sched.com/hosted_files/colocatedeventseu2024/83/Best%20Practices%20for%20LLM%20Serving%20with%20DRA.pdf</st>](https://static.sched.com/hosted_files/colocatedeventseu2024/83/Best%20Practices%20for%20LLM%20Serving%20with%20DRA.pdf)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[<st c="87745">https://github.com/NVIDIA/k8s-dra-driver?tab=re</st>](https://github.com/NVIDIA/k8s-dra-driver?tab=re)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
