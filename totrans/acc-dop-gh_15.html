<html><head></head><body>
		<div id="_idContainer159">
			<h1 id="_idParaDest-267"><em class="italic"><a id="_idTextAnchor267"/>Chapter 12</em>: Shift Left Testing for Increased Quality</h1>
			<p><strong class="bold">Testing</strong> and <strong class="bold">quality assurance</strong> (<strong class="bold">QA</strong>) is still one of the practices that holds back most companies. In this chapter, we'll take a closer look at the role that QA and testing play in terms of developer velocity and how to shift left test.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Shift left testing with test automation</li>
				<li>Eradicating flaky tests</li>
				<li>Code coverage</li>
				<li>Shift right – testing in production</li>
				<li>Fault injection and chaos engineering </li>
				<li>Testing and compliance</li>
				<li>Test management in GitHub</li>
			</ul>
			<h1 id="_idParaDest-268"><a id="_idTextAnchor268"/>Shift left testing with test automation</h1>
			<p>If you practice<a id="_idIndexMarker770"/> agile development and try to ship frequently, then manual testing isn't a scalable option. Even if you don't practice CI/CD and only ship on a sprint cadence, running all the necessary regression tests would take enormous manpower and a lot of time and money. But getting test automation right is not an easy task. Automated tests that have been created and maintained by a QA department or outsourced entity, for example, are <em class="italic">not</em> correlated with higher engineering velocity (<em class="italic">Forsgren N., Humble, J., &amp; Kim, G., 2018</em>, <em class="italic">Page 95</em>). To notice an impact on your velocity, you need reliable tests that have been created and maintained by the team. The theory behind this is that if developers maintain tests, they produce more testable code.</p>
			<p>Everybody knows what a good test portfolio should look like: you have a big base of automated unit tests (Level 0), fewer integration tests (Level 1), some integration tests that need test data (Level 2), and only a few functional tests (Level 3). This is called the test pyramid (<em class="italic">see Figure 12.1</em>):</p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/B17827_12_001.jpg" alt="Figure 12.1 – The test pyramid&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.1 – The test pyramid</p>
			<p>However, in <a id="_idIndexMarker771"/>most companies, the <a id="_idIndexMarker772"/>portfolio does not look like this. Sometimes, there are some unit tests, but most of the other tests are still at a very high level (<em class="italic">see Figure 12.2</em>):</p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/B17827_12_002.jpg" alt="Figure 12.2 – Example test portfolio&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.2 – Example test portfolio</p>
			<p>These high-level tests might be automated or manual. But still, it is not a test portfolio that will help <a id="_idIndexMarker773"/>you to release continuously with high quality. To achieve continuous quality, you must shift left your test portfolio (<em class="italic">see Figure 12.3</em>):</p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/B17827_12_003.jpg" alt="Figure 12.3 – Shift left testing&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.3 – Shift left testing</p>
			<p>This is not an easy task. Here<a id="_idIndexMarker774"/> are some principles that help with shift left testing:</p>
			<ul>
				<li><strong class="bold">Ownership</strong>: The team is responsible for QA and the tests are developed alongside the code – preferably with the test-first approach. QA engineers should be included in the team.</li>
				<li><strong class="bold">Shift left</strong>: Tests should always be written at the lowest level possible.</li>
				<li><strong class="bold">Write once – execute everywhere</strong>: Tests should be executed in all environments, even in production.</li>
				<li><strong class="bold">Test code is production code</strong>: The same quality standards that apply to normal code apply to test code. No shortcuts should be allowed here.</li>
				<li><strong class="bold">You code it – you test it</strong>: As a developer, you are responsible for the quality of your code, and you <a id="_idIndexMarker775"/>must make sure that all the tests are in place to ensure this quality.</li>
			</ul>
			<p>In 2013, a testing manifesto<a id="_idIndexMarker776"/> was created that describes the transformation of the QA role (<em class="italic">Sam Laing, 2015</em>):</p>
			<ul>
				<li>Testing throughout <em class="italic">over</em> testing at the end</li>
				<li>Preventing bugs <em class="italic">over</em> finding bugs</li>
				<li>Testing understanding <em class="italic">over</em> checking functionality</li>
				<li>Building the best system <em class="italic">over</em> breaking the system</li>
				<li>Team responsibility for quality <em class="italic">over</em> tester responsibility</li>
			</ul>
			<p>This sounds easy, but it isn't. Developers have to learn to think like testers and testers have to learn to think like engineers. Selling the vision and establishing the change's sustainability is not an easy task.</p>
			<h2 id="_idParaDest-269"><a id="_idTextAnchor269"/>Test-driven development</h2>
			<p>The <a id="_idIndexMarker777"/>key to test automation is having a testable software architecture. To get one, you must start as early as possible – that is, in the inner loop, when developers write their code.</p>
			<p><strong class="bold">Test-driven development</strong> (<strong class="bold">TDD</strong>) is a <a id="_idIndexMarker778"/>software development process where you write your automated test first and then the code that makes the test pass. It has been around for more than 20 years and the quality benefits have been proven in different studies (for example, <em class="italic">Müller, Matthias M.; Padberg, Frank, 2017</em> and <em class="italic">Erdogmus, Hakan; Morisio, Torchiano, 2014</em>). TDD not only has a big impact on the time that's spent on debugging and overall code quality; it also has a big influence on solid and testable software design. That's why it is also <a id="_idIndexMarker779"/>called <strong class="bold">test-driven design</strong>.</p>
			<p>TDD is simple. The<a id="_idIndexMarker780"/> steps are as follows:</p>
			<ol>
				<li><strong class="bold">Add or modify a test</strong>: Always start with a test. While writing the test, you <strong class="bold">design</strong> what your code will look like. There will be a time when your test will not compile because the classes and functions that you are calling do not exist yet. Most development environments support creating the necessary code right from within your test. This step is completed once your code compiles and the test can be executed. The test is supposed to fail. If the test passes, modify it or write a new test until it fails.</li>
				<li><strong class="bold">Run all tests</strong>: Run all the tests and verify that only the new test fails.</li>
				<li><strong class="bold">Write code</strong>: Write<a id="_idIndexMarker781"/> some simple code that makes the test pass. Always run all your tests to check if the test passes. The code does not need to be pretty in this stage and shortcuts are allowed. Just make the test pass. Bad code will give you an idea of what test you need next to ensure that the code gets better.</li>
				<li><strong class="bold">All tests pass</strong>: If all the tests pass, you have two options: write a new test or modify the existing one. Alternatively, you can refactor your code and tests.</li>
				<li><strong class="bold">Refactor</strong>: Refactor the code and the tests. Since you have a solid test harness, you can do more extreme refactoring than you normally would without TDD. Make sure that you run all the tests after each refactoring. If one test fails, undo the last step and <a id="_idIndexMarker782"/>retry until the tests keep passing after the refactoring step. After a successful refactoring, you can start a new iteration with a new failing test.</li>
			</ol>
			<p>Figure 12.4 shows an <a id="_idIndexMarker783"/>overview of the TDD cycle:</p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/B17827_12_004.jpg" alt="Figure 12.4 – The TDD cycle&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.4 – The TDD cycle</p>
			<p>A good test follows<a id="_idIndexMarker784"/> the following pattern:</p>
			<ul>
				<li><strong class="bold">Arrange</strong>: Set up <a id="_idIndexMarker785"/>the necessary objects for the test and <a id="_idIndexMarker786"/>the <strong class="bold">system under test</strong> (<strong class="bold">SUT</strong>) itself – normally, this is a class. You can use <strong class="bold">mocks</strong> and <strong class="bold">stubs</strong> to simulate<a id="_idIndexMarker787"/> system behavior (to learn more about mocks and <a id="_idIndexMarker788"/>stubs, see <em class="italic">Martin Fowler, 2007</em>).</li>
				<li><strong class="bold">Act</strong>: Execute <a id="_idIndexMarker789"/>the code that you want to test.</li>
				<li><strong class="bold">Assert</strong>: Verify <a id="_idIndexMarker790"/>the results, ensure that the state of the system is in the desired state, and ensure that the method has called the correct methods with the correct parameters.</li>
			</ul>
			<p>Each test<a id="_idIndexMarker791"/> should be completely autarkic – that is, it shouldn't depend on a system state that's been manipulated by previous tests, and it can be executed in isolation.</p>
			<p>TDD can also be used in pair programming. This is called <strong class="bold">Ping Pong Pair Programming</strong>. In<a id="_idIndexMarker792"/> this form of pair programming, one developer writes the test and the other writes the code that makes the test pass. This is a great pattern for pair programming and a good way to teach younger colleagues the benefits of TDD.</p>
			<p>TDD has been around for so long and the teams that practice it gain so much value – and yet I have met many teams that are not using it. Some don't use it because their code runs on embedded systems, while others don't use it because their code depends on SharePoint classes that are hard to mock. But these are just excuses. There might be some plumbing code that cannot be tested, but when you write logic, you can always test it first.</p>
			<h2 id="_idParaDest-270"><a id="_idTextAnchor270"/>Managing your test portfolio</h2>
			<p>With <a id="_idIndexMarker793"/>TDD, you <a id="_idIndexMarker794"/>should get a testable design in no time. And even in a brownfield environment, the number of automated tests will grow rapidly. The problem is that often, the quality of the tests is not optimal and with a growing test portfolio, you often get very long execution times and non-deterministic (flaky) tests. It is better to have fewer tests that are of higher quality. Long execution times hinder <a id="_idIndexMarker795"/>you from releasing quickly, and flaky tests produce unreliable quality signals and reduce the trust in your test suite (<em class="italic">see Figure 12.5</em>). With more QA maturity in the team, the quality of the test suite constantly rises – even if the amount of tests reduces after the first peak:</p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/B17827_12_005.jpg" alt="Figure 12.5 – Amount and quality of automated tests&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.5 – Amount and quality of automated tests</p>
			<p>To actively <a id="_idIndexMarker796"/>manage your test portfolio, you should define ground rules for your tests and constantly monitor the number of tests and their execution time. As an example, let's look at<a id="_idIndexMarker797"/> the <strong class="bold">taxonomy</strong> that's used by a team at Microsoft for their test portfolio.</p>
			<h3>Unit tests (Level 0)</h3>
			<p>Here, we<a id="_idIndexMarker798"/> have in-memory unit tests with no external dependencies and no deployment. They should be fast with an average execution time of fewer than 60 milliseconds. Unit tests are co-located with the code under test.</p>
			<p>With unit tests, you can't change to the system's state (such as the filesystem or its registry), queries to external data sources (web services and databases), or the mutexes, semaphores, stopwatches, and <strong class="source-inline">Thread.sleep</strong> operations.</p>
			<h3>Integration tests (Level 1)</h3>
			<p>This<a id="_idIndexMarker799"/> level involves tests with more complex requirements that may depend on a lightweight deployment and configuration. The tests should still be very fast, and each test must run under 2 seconds.</p>
			<p>With integration tests, you can't have dependencies on other tests and store large amounts of data. You also can't have too many tests in one assembly as this prevents the tests from being executed in parallel.</p>
			<h3>Functional tests with data (Level 2)</h3>
			<p>Functional tests<a id="_idIndexMarker800"/> run against a testable deployment with test data. Dependencies on systems such as the authentication provider can be stubbed out and allow dynamic identities to be used. This means that there's an isolated identity for every test so that the test can be executed in parallel against a deployment without them impacting each other.</p>
			<h3>Production tests (Level 3)</h3>
			<p>Production tests<a id="_idIndexMarker801"/> run against production and require a full product deployment. </p>
			<p>This is just an example, and your taxonomy may look different, depending on your programming language and product.</p>
			<p>If you have defined your taxonomy, you can set up reporting and start to transform your test portfolio. Make sure that you make it easy to write and execute high-quality unit and integration tests first. Then, start analyzing your legacy tests – manual or automated – and check which ones you can throw away. Convert the others into good functional tests (<em class="italic">Level 2</em>). The last step is to write your tests for production.</p>
			<p>The team at Microsoft started with 27,000 legacy tests (in orange) and reduced them to zero in 42 sprints (126 weeks). Most of the tests were replaced with unit tests; some were replaced with functional tests. Many were simply deleted, but there was a steady growth in <a id="_idIndexMarker802"/>unit tests, with there being over 40,000 in the end (<em class="italic">see Figure 12.6</em>):</p>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<img src="image/B17827_12_006.jpg" alt="Figure 12.6 – Test portfolio over time&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.6 – Test portfolio over time</p>
			<p>See <em class="italic">Shift left to make testing fast and reliable</em>, in the <em class="italic">Further reading</em> section, for more information on how the team at Microsoft shifted their test portfolio left.</p>
			<h1 id="_idParaDest-271"><a id="_idTextAnchor271"/>Eradicating flaky tests</h1>
			<p><strong class="bold">Non-deterministic</strong> or <strong class="bold">flaky tests</strong> are tests that <a id="_idIndexMarker803"/>sometimes pass and sometimes<a id="_idIndexMarker804"/> fail with the same code (<em class="italic">Martin Fowler, 2011</em>). Flaky tests can destroy the trust in your test suite. This can lead to teams just ignoring red test results, or developers deactivating tests, thereby reducing the test coverage and reliability of the suite.</p>
			<p>There are lots <a id="_idIndexMarker805"/>of reasons for flaky tests. Often, they are due to a lack of isolation. Many tests run in the same process on a machine – so each test must find and leave a clean state of the system. Another common reason is asynchronous behavior. Testing asynchronous code has its challenges as you never know which order the asynchronous tasks are executed in. Other reasons may include resource leaks or calls to remote resources.</p>
			<p>There are <a id="_idIndexMarker806"/>different ways to deal with flaky tests:</p>
			<ul>
				<li><strong class="bold">Retry failing tests</strong>: Some frameworks allow you to retry failing tests. Sometimes, you can even configure a higher level of isolation. If a test passes in a rerun, it is considered flaky, and you should file a reliability bug using <strong class="source-inline">git blame</strong>.</li>
				<li><strong class="bold">Reliability runs</strong>: You can execute workflows on code that has green builds. Tests that fail are flaky and you can file a reliability bug using <strong class="source-inline">git blame</strong>.</li>
			</ul>
			<p>Some companies quarantine flaky tests, but this also keeps you from collecting additional data as<a id="_idIndexMarker807"/> the test can't run. It's best practice to keep executing flaky tests but exclude them from the reporting.</p>
			<p>If you want to learn how GitHub or Google are dealing with flaky tests, read <em class="italic">Jordan Raine, 2020</em> or <em class="italic">John Micco, 2016</em>.</p>
			<h1 id="_idParaDest-272"><a id="_idTextAnchor272"/>Code coverage</h1>
			<p><strong class="bold">Code coverage</strong> is a<a id="_idIndexMarker808"/> metric (in percent) that calculates the number of code elements that get called by tests, divided by the total amount of code elements. Code elements can be anything, but lines of code, code blocks, or functions are common.</p>
			<p>Code coverage is an important metric as it shows you what parts of your code are not covered by your test suite. I like to watch the code coverage before I finish a code change as I often forget to write tests for edge cases such as exception handling, or more complex statements such as lambda expressions. It's no problem to add these tests the moment you are coding – it's much harder to add them later.</p>
			<p>But you should not focus on the absolute number as code coverage itself says nothing about the quality of the tests. It's better to have 70% code coverage with high-quality tests than 90% percent code coverage with low-quality tests. Depending on the programming language and frameworks you use, there might be some plumbing code that has a high effort in terms of testing but with very low value. Normally, you can exclude that code from code coverage calculations, but this is why the absolute value of code coverage is limited. However, measuring the value in each pipeline and focusing on new code helps improve the quality of your automated tests over time.</p>
			<h1 id="_idParaDest-273"><a id="_idTextAnchor273"/>Shift right – testing in production</h1>
			<p>If you start <a id="_idIndexMarker809"/>with automated testing, you rapidly see improved quality and a decline in the debugging effort of your engineers. But at some point, you must increase the effort tremendously to see a significant impact on quality. On the other hand, the time your tests need to execute slows down your release pipeline, especially if you <a id="_idIndexMarker810"/>add <strong class="bold">performance tests</strong> and <strong class="bold">load tests</strong> to the <a id="_idIndexMarker811"/>mix (<em class="italic">see Figure 12.7</em>):</p>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/B17827_12_007.jpg" alt="Figure 12.7 – The impact of testing effort on quality and velocity&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.7 – The impact of testing effort on quality and velocity</p>
			<p>It's impossible to release multiple times a day if your pipeline runs for more than 24 hours! The increased execution time of your pipeline also reduces your ability to roll forward quickly and deploy a fix if a bug occurs in production.</p>
			<p>The solution <a id="_idIndexMarker812"/>to this is simple: <strong class="bold">shift right</strong> some of the tests to production. All the tests that you run in production do not impact your ability to release fast, and you don't need performance or load tests as your code already has production load.</p>
			<p>However, there are<a id="_idIndexMarker813"/> some prerequisites to testing in production that increase the performance quality for your users instead of decreasing it. Let's take a look.</p>
			<h2 id="_idParaDest-274"><a id="_idTextAnchor274"/>Health data and monitoring</h2>
			<p>For<a id="_idIndexMarker814"/> testing in production, you must be constantly aware of the health of your application. This goes beyond normal logging. You need deep insights into how your application is operating. A good practice is to have test code that calls all dependent systems – such as the database, a Redis cache, or dependent REST services – and makes these tests available to your logging solutions. This way, you can have a constant <strong class="bold">heartbeat</strong> that indicates that all the systems are up and running and working together. If the test fails, you can have an alert that instantly notifies the team that something is wrong. You can also automate these alerts and have them trigger certain functions, such as activating a <strong class="bold">circuit breaker</strong>.</p>
			<p class="callout-heading">Circuit Breaker</p>
			<p class="callout">A <strong class="bold">circuit breaker</strong> is<a id="_idIndexMarker815"/> a pattern that prevents an application from repeatedly trying to execute an operation that is likely to fail, allowing the application to continue with altered functionality without having to wait for the failing operation to succeed (see <em class="italic">Michael Nygard, 2018</em>).</p>
			<h2 id="_idParaDest-275"><a id="_idTextAnchor275"/>Feature flags and canary releases</h2>
			<p>You don't <a id="_idIndexMarker816"/>want to test in <a id="_idIndexMarker817"/>production and cause a complete outage for all your customers. That's why you need feature flags, canary releases, a ring-based deployment, or a mix of those techniques (see <a href="B17827_09_Epub.xhtml#_idTextAnchor216"><em class="italic">Chapter 9</em></a> and <a href="B17827_10_Epub.xhtml#_idTextAnchor239"><em class="italic">Chapter 10</em></a>). It's important to gradually expose the changes so that if an outage occurs, you don't take down the complete production environment.</p>
			<h2 id="_idParaDest-276"><a id="_idTextAnchor276"/>Business continuity and disaster recovery</h2>
			<p>Another<a id="_idIndexMarker818"/> form of testing in production is <strong class="bold">business continuity and disaster recovery</strong> (<strong class="bold">BCDR</strong>) or<a id="_idIndexMarker819"/> failover testing. There should be a BCDR for every service or subsystem of your product, and you should execute a BCDR drill regularly. There is nothing worse than disaster recovery that is not working if your system is down. And you only know that it is working if you regularly test it.</p>
			<h2 id="_idParaDest-277"><a id="_idTextAnchor277"/>Exploratory testing and usability testing</h2>
			<p>Test automation <a id="_idIndexMarker820"/>does not imply<a id="_idIndexMarker821"/> that you should completely abandon manual testing. But the focus of manual tests shifts away from validating functionality and executing regressions tests manually on every release toward usability, fast and high-quality feedback, and bugs that are hard to find with structured test approaches.</p>
			<p><strong class="bold">Exploratory testing</strong> was<a id="_idIndexMarker822"/> introduced by Cem Kaner in 1999 (<em class="italic">Kaner C., Falk J., H. Q. Nguyen, 1999</em>). It is an approach to testing that focuses simultaneously on discovery, learning, test design, and execution. It relies on the individual tester to uncover defects that can't easily be discovered in other tests.</p>
			<p>There are many <a id="_idIndexMarker823"/>tools available that can facilitate exploratory testing. They help you record your sessions, take annotated screenshots, and often allow you to create a test case from the steps you have performed. Some extensions integrate with Jira, such as Zephyr and Capture, and there are browser extensions such as the Test and Feedback client for Azure Test Plans. The latter is free if you use it in standalone mode. These tools provide the high-quality feedback of stakeholders to developers – not only in terms of the defects that were discovered.</p>
			<p>Other ways to gather <a id="_idIndexMarker824"/>feedback include <a id="_idIndexMarker825"/>using <strong class="bold">usability testing</strong> techniques – such <a id="_idIndexMarker826"/>as <strong class="bold">hallway testing</strong> or <strong class="bold">guerrilla usability</strong> – to evaluate <a id="_idIndexMarker827"/>your solution by testing it on new, unbiased users. A special form of usability testing is A/B testing, which we'll cover in more detail in <em class="italic">Chapter 19</em>, <em class="italic">Experimentation and A/B Testing with GitHub</em>.</p>
			<p>The important part here is that all these tests can be executed in production. You should not have any manual tests in your CI/CD pipeline. Release fast and allow manual testing in production using feature flags and canary releases.</p>
			<h1 id="_idParaDest-278"><a id="_idTextAnchor278"/>Fault injection and chaos engineering</h1>
			<p>If you want to<a id="_idIndexMarker828"/> level up testing in production, you can practice <strong class="bold">fault injection</strong> – also known as <strong class="bold">chaos engineering</strong>. This <a id="_idIndexMarker829"/>means that you inject faults into your production system to see how it behaves under pressure and if your failover mechanisms and circuit breakers work. Possible faults could include high CPU load, high memory usage, disk I/O pressure, low disk space, or a service or entire machine being shut down or rebooted. Other possibilities include processes being killed, the system's time being changed, network traffic being dropped, latency being injected, and DNS servers being blocked.</p>
			<p>Practicing chaos engineering makes your system resilient. You cannot compare this to classical load or performance testing!</p>
			<p>Different tools can help you with chaos engineering. <strong class="bold">Gremlin</strong> (<a href="https://www.gremlin.com/">https://www.gremlin.com/</a>), for example, is an <a id="_idIndexMarker830"/>agent-based SaaS offering that supports most <a id="_idIndexMarker831"/>cloud providers (Azure, AWS, and Google Cloud) and all operating systems. It can also be used <a id="_idIndexMarker832"/>with Kubernetes. <strong class="bold">Chaos Mesh</strong> (<a href="https://chaos-mesh.org/">https://chaos-mesh.org/</a>) is <a id="_idIndexMarker833"/>an open <a id="_idIndexMarker834"/>source solution that's specialized for Kubernetes. <strong class="bold">Azure Chaos Studio</strong> (<a href="https://azure.microsoft.com/en-us/services/chaos-studio">https://azure.microsoft.com/en-us/services/chaos-studio</a>) is a<a id="_idIndexMarker835"/> solution that's specialized for Azure. What tool is best for you depends on the platforms that you support.</p>
			<p>Chaos engineering<a id="_idIndexMarker836"/> can be very effective and make your systems resilient, but it should be limited to canary environments that have little or no customer impact.</p>
			<h1 id="_idParaDest-279"><a id="_idTextAnchor279"/>Tests and compliance</h1>
			<p>Most <strong class="bold">compliance</strong> standards, such<a id="_idIndexMarker837"/> as <strong class="bold">ISO26262</strong> for <a id="_idIndexMarker838"/>automotive <a id="_idIndexMarker839"/>or <strong class="bold">GAMP</strong> for pharma, follow <a id="_idIndexMarker840"/>the <strong class="bold">V-Model</strong> as a <a id="_idIndexMarker841"/>development <a id="_idIndexMarker842"/>process. The V-Model requires the user and system requirements to be decomposed and specifications to be created at different levels of detail. This is the left-hand side of the <em class="italic">V</em>. It also requires all the levels to be validated to ensure that the system fulfills the requirements and specifications. This is the right-hand side of the <em class="italic">V</em>. Both sides can be seen in <em class="italic">Figure 12.8</em>:</p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/B17827_12_008.jpg" alt="Figure 12.8 – Validation in the V-Model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.8 – Validation in the V-Model</p>
			<p>This model must be combined with risk analysis, which is performed at every level of detail. Many <a id="_idIndexMarker843"/>documents must be signed during the release phase. This leads to a slow waterfall process with long specification, development, and release phases.</p>
			<p>But the <a id="_idIndexMarker844"/>standards are based on good practices – and if your <a id="_idIndexMarker845"/>practices are better than the ones in the standard, you can justify that in the audits. The standards don't require you to do validation manually, nor do they say anything about the time of the phases. The solution is to automate all the validation logic and add the approvals as code reviews in the pull requests when you're modifying the tests (shift left). Tests that you cannot automate must be moved to production (shift right). This way, you can automate the entire V and run through it multiple times a day:</p>
			<ol>
				<li value="1">Add or modify a requirement (for example, an issue).</li>
				<li>Create a pull request and link it to the issue.</li>
				<li>Modify your system design and architecture in your repository (for example, in markdown) or state that no modifications are needed in the pull request.</li>
				<li>Write your unit tests (this is, your software design) and the code to implement.</li>
				<li>Write or modify your functions, system, and integration tests.</li>
				<li>Have all the necessary roles approve the pull requests and make sure that the approvals are stale if new changes are pushed.</li>
				<li>Ship your changes to production and run your final tests there.</li>
			</ol>
			<p>You can also manage your risks as code. This way, you can integrate them into your automated process. If not, you can still attach the documents to the issue. This way, you have the end-to-end traceability for all your changes, all your necessary approvals, and all the validation steps completed. And you can still iterate fast and release to production regularly.</p>
			<h1 id="_idParaDest-280"><a id="_idTextAnchor280"/>Test management in GitHub</h1>
			<p>Unfortunately, GitHub doesn't have a great way to track your test runs and code coverage<a id="_idIndexMarker846"/> over time, nor can it help you detect or quarantine flaky tests. You<a id="_idIndexMarker847"/> can execute your tests as part of your workflow and can signal back the result – but for reporting, you have to rely on your test tooling.</p>
			<p>A good solution that integrates well<a id="_idIndexMarker848"/> with GitHub is <strong class="bold">Testspace</strong> (https://www.testspace.com/). It <a id="_idIndexMarker849"/>is a SaaS offering and is free for open source projects. It's easy to set up – just install the extension from the marketplace (https://github.com/marketplace/testspace-com), select the plan you want, and grant access to your repositories. Then, add the following step to your workflow:</p>
			<pre class="source-code">- uses: testspace-com/setup-testspace@v1</pre>
			<pre class="source-code">  with:</pre>
			<pre class="source-code">    domain: ${{github.repository_owner}}</pre>
			<p>If your repository is private, then you must create a token in <em class="italic">Testspace</em> and add it as a secret to that step as well: <strong class="source-inline">token: ${{ secrets.TESTSPACE_TOKEN }}</strong>.</p>
			<p>Then, you must add a step to push your test and code coverage results to <em class="italic">Testspace</em> after the step that executes the tests. You can use glob syntax to specify files in dynamic folders. Make sure that you execute the step, even if an error occurs (<strong class="source-inline">if: '!cancelled()'</strong>):</p>
			<pre class="source-code">- name: Push test results to Testspace</pre>
			<pre class="source-code">  run: |</pre>
			<pre class="source-code">    testspace **/TestResults.xml **/coverage.cobertura.xml</pre>
			<pre class="source-code">  if: '!cancelled()'</pre>
			<p><em class="italic">Testspace</em> provides reliable detection for flaky tests. It has a <em class="italic">Build Bot</em> that sends you a notification if new results arrive. You can comment on the results by answering the email (<em class="italic">see Figure 12.9</em>):</p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="image/B17827_12_009.jpg" alt="Figure 12.9 – Notification from Testspace about your build results&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.9 – Notification from Testspace about your build results</p>
			<p>It <a id="_idIndexMarker850"/>automatically integrates as a check into <a id="_idIndexMarker851"/>your pull request (<em class="italic">see Figure 12.10</em>):</p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/B17827_12_010.jpg" alt="Figure 12.10 – Testspace integrates into your pull request checks&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.10 – Testspace integrates into your pull request checks</p>
			<p>The UI of <em class="italic">Testpace</em> doesn't look very fancy, but it has really rich reports and a ton of functionality (<em class="italic">see Figure 12.11</em>):</p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/B17827_12_011.jpg" alt="Figure 12.11 – Rich reports of your test metrics&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.11 – Rich reports of your test metrics</p>
			<p>If you<a id="_idIndexMarker852"/> don't<a id="_idIndexMarker853"/> have a solution for test management yet, you can try <em class="italic">Testspace</em>. If you already have one, then it should be straightforward to integrate it into your workflow.</p>
			<h1 id="_idParaDest-281"><a id="_idTextAnchor281"/>Case study</h1>
			<p>The two<a id="_idIndexMarker854"/> pilot teams at <strong class="bold">Tailwind Gears</strong> have achieved a much higher <strong class="bold">delivery lead time</strong> and <strong class="bold">deployment frequency</strong> thanks to the DevOps practices that have been applied. The <strong class="bold">mean time to restore</strong> is also much better because the release pipelines help ship fixes faster. However, the <strong class="bold">change failure rate</strong> has dropped. Releasing more frequently also means that more deployments fail and finding bugs in the code is hard. The quality signals that come from the automated test suites are just not reliable enough and fixing one bug often introduces another bug in another module. There are still many parts of the application that need manual testing – but with one QA engineer in the team, this was not an option. So, some of these parts have been replaced with UI tests, while others have just been dropped.</p>
			<p>To evaluate the test portfolio, the teams must introduce a test taxonomy and include reporting in their pipelines. The QA engineers in the team are responsible for the taxonomy and the reports show that there are way too many functional and UI tests – and not enough unit tests. Many of the engineers are still not convinced that TDD would save them time and that it is possible to develop with TDD in certain cases, especially when the team is developing the embedded software. The teams decide to book a TDD training session together to learn about and practice TDD.</p>
			<p>After that, all<a id="_idIndexMarker855"/> the new code is written in TDD with a <strong class="bold">code coverage</strong> of 90% (at a minimum) for the new code. The teams also spend 30% of their time every sprint eradicating <strong class="bold">flaky tests</strong> and rewriting tests at a lower level.</p>
			<p>To discover flaky tests, the teams run reliability runs on pipelines with green tests. Flaky tests have the highest priority. After that, the team picks the tests with the longest execution time and decides what to do for each test. Most of the tests get converted into unit tests, though some get converted into integration tests. Some of the tests can be deleted as they bring no additional value.</p>
			<p>Structured manual tests get replaced completely by <strong class="bold">exploratory testing</strong>. If anything is found in these sessions, a unit test is created before they're fixed.</p>
			<p>The team that runs the web application also includes a new test category with tests that get executed in production. They implement <strong class="bold">application performance monitoring</strong> and collect many metrics so that they're aware of the health of the application in all environments. They also perform their first BCDR drills once per sprint to get started with <strong class="bold">testing in production</strong> and <strong class="bold">chaos engineering</strong>.</p>
			<h1 id="_idParaDest-282"><a id="_idTextAnchor282"/>Summary</h1>
			<p>In this chapter, you learned how to accelerate your software delivery by shifting testing to the left via test automation and then to the right with testing in production and chaos engineering. This way, you can release at a fast pace without making compromises in terms of quality. Finally, you learned how to manage your test portfolio, eradicate flaky tests, and make your application more resilient by injecting faults and chaos.</p>
			<p>In the next chapter, you will learn how to shift left security and implement DevSecOps practices into your development process.</p>
			<h1 id="_idParaDest-283"><a id="_idTextAnchor283"/>Further reading</h1>
			<p>The following references were used in this chapter to help you learn more about the topics that were discussed:</p>
			<ul>
				<li>Forsgren N., Humble, J., &amp; Kim, G. (2018). <em class="italic">Accelerate: The Science of Lean Software and DevOps: Building and Scaling High Performing Technology Organizations</em> (1st ed.) [E-book]. IT Revolution Press.</li>
				<li>Eran Kinsbruner (2018), <em class="italic">Continuous Testing for DevOps Professionals: A Practical Guide From Industry Experts</em> (Kindle Edition). CreateSpace Independent Publishing Platform.</li>
				<li>Sam Laing (2015), <em class="italic">The Testing Manifesto</em>, <a href="https://www.growingagile.co.za/2015/04/the-testing-manifesto/">https://www.growingagile.co.za/2015/04/the-testing-manifesto/</a>.</li>
				<li>Wolfgang Platz, Cynthia Dunlop (2019), <em class="italic">Enterprise Continuous Testing: Transforming Testing for Agile and DevOps</em> (Kindle Edition), Independently published.</li>
				<li>Tilo Linz (2014): <em class="italic">Testing in Scrum</em> (E-book), Rocky Nook.</li>
				<li>Kaner C., Falk J., H. Q. Nguyen (1999), <em class="italic">Testing Computer Software</em> (2nd Edition) Wiley.</li>
				<li>Roy Osherove (2009), <em class="italic">The Art of Unit Testing</em> (1st edition), Manning.</li>
				<li>Martin Fowler (2007), <em class="italic">Mocks Aren't Stubs</em> <a href="https://martinfowler.com/articles/mocksArentStubs.html">https://martinfowler.com/articles/mocksArentStubs.html</a>.</li>
				<li>Müller, Matthias M.; Padberg, Frank (2017). <em class="italic">About the Return on Investment of Test-Driven Development</em> (PDF). Universität Karlsruhe, Germany.</li>
				<li>Erdogmus, Hakan; Morisio, Torchiano (2014). <em class="italic">On the Effectiveness of Test-first Approach to Programming</em>. Proceedings of the IEEE Transactions on Software Engineering, 31(1). January 2005. (NRC 47445).</li>
				<li><em class="italic">Shift left to make testing fast and reliable</em>: https://docs.microsoft.com/en-us/devops/develop/shift-left-make-testing-fast-reliable.</li>
				<li>Martin Fowler (2011), <em class="italic">Eradicating Non-Determinism in Tests</em>, <a href="https://martinfowler.com/articles/nonDeterminism.html">https://martinfowler.com/articles/nonDeterminism.html</a>.</li>
				<li>Jordan Raine (2020). <em class="italic">Reducing flaky builds by 18x</em>. <a href="https://github.blog/2020-12-16-reducing-flaky-builds-by-18x/">https://github.blog/2020-12-16-reducing-flaky-builds-by-18x/</a>.</li>
				<li>John Micco (2016). <em class="italic">Flaky Tests at Google and How We Mitigate Them</em>. <a href="https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html">https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html</a>.</li>
				<li><em class="italic">Shift right to test in production</em>: <a href="https://docs.microsoft.com/en-us/devops/deliver/shift-right-test-production">https://docs.microsoft.com/en-us/devops/deliver/shift-right-test-production</a>.</li>
				<li>Michael Nygard (2018). <em class="italic">Release It! Design and Deploy Production-Ready Software</em> (2nd Edition). O'Reilly.</li>
			</ul>
		</div>
	</body></html>