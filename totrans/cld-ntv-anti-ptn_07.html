<html><head></head><body>
<div epub:type="chapter" id="_idContainer058">
<h1 class="chapter-number" id="_idParaDest-198"><a id="_idTextAnchor198"/><span class="koboSpan" id="kobo.1.1">7</span></h1>
<h1 id="_idParaDest-199"><a id="_idTextAnchor199"/><span class="koboSpan" id="kobo.2.1">Expressing Your Business Goals in Application Code</span></h1>
<p><span class="koboSpan" id="kobo.3.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">business logic</span></strong><span class="koboSpan" id="kobo.5.1"> that </span><a id="_idIndexMarker732"/><span class="koboSpan" id="kobo.6.1">makes our company technology unique and provides a competitive advantage is usually the business logic that we employ. </span><span class="koboSpan" id="kobo.6.2">Expressing our business rules </span><a id="_idIndexMarker733"/><span class="koboSpan" id="kobo.7.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">application code</span></strong><span class="koboSpan" id="kobo.9.1"> can drive forward automation, reduce cycle times, and increase productive output. </span><span class="koboSpan" id="kobo.9.2">However, when we move that logic to the cloud, we can be trapped by </span><a id="_idIndexMarker734"/><span class="koboSpan" id="kobo.10.1">some </span><strong class="bold"><span class="koboSpan" id="kobo.11.1">anti-patterns</span></strong><span class="koboSpan" id="kobo.12.1"> that we would normally get away with in the old monolithic, on-premises architectures we </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">are evolving.</span></span></p>
<p><span class="koboSpan" id="kobo.14.1">In this chapter, we’re going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">main topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.16.1">Lift </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">and shift</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.18.1">Stateful applications</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">Tight coupling, </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">low cohesion</span></span></li>
<li><span class="koboSpan" id="kobo.21.1">The comprehensive definition </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">of done</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.23.1">Other pitfalls</span></span></li>
</ul>
<h1 id="_idParaDest-200"><a id="_idTextAnchor200"/><span class="koboSpan" id="kobo.24.1">Lift and shift</span></h1>
<p><span class="koboSpan" id="kobo.25.1">When we move</span><a id="_idIndexMarker735"/><span class="koboSpan" id="kobo.26.1"> applications to the cloud, we need to shift our thinking from deploying an application as an independent unit to the application being the emergent behavior of the interaction of various services. </span><span class="koboSpan" id="kobo.26.2">In this section, we will explore the typical process of shifting an application to the cloud, the strategies we can use, and how to increase the maturity of our cloud </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">native solution.</span></span></p>
<h2 id="_idParaDest-201"><a id="_idTextAnchor201"/><span class="koboSpan" id="kobo.28.1">Building in the cloud versus building for the cloud</span></h2>
<p><span class="koboSpan" id="kobo.29.1">When we </span><a id="_idIndexMarker736"/><span class="koboSpan" id="kobo.30.1">migrate applications to the cloud, the simplest method is to package up the existing deployment in a VM, deploy it to the cloud, and call it cloud native. </span><span class="koboSpan" id="kobo.30.2">This thinking limits the actual usage of the cloud to simply reflect the existing topologies we had in our on-premises environment. </span><span class="koboSpan" id="kobo.30.3">But what have we achieved? </span><span class="koboSpan" id="kobo.30.4">We still have the same limitations of the system we just migrated from but without any of the advantages of the cloud. </span><span class="koboSpan" id="kobo.30.5">We’ve just moved our code from our server to someone else’s server. </span><span class="koboSpan" id="kobo.30.6">We may gain some efficiencies in maintainability, organizational complexity, and onboarding time. </span><span class="koboSpan" id="kobo.30.7">However, this is not unique to cloud hyperscalers, and we could achieve the same results with most other</span><a id="_idIndexMarker737"/><span class="koboSpan" id="kobo.31.1"> VM hosts. </span><span class="koboSpan" id="kobo.31.2">This </span><strong class="bold"><span class="koboSpan" id="kobo.32.1">lift-and-shift</span></strong><span class="koboSpan" id="kobo.33.1"> mindset gets us into the cloud but falls short of fully utilizing it. </span><span class="koboSpan" id="kobo.33.2">This mindset is the difference between building </span><em class="italic"><span class="koboSpan" id="kobo.34.1">in</span></em><span class="koboSpan" id="kobo.35.1"> the cloud versus building </span><em class="italic"><span class="koboSpan" id="kobo.36.1">for</span></em><span class="koboSpan" id="kobo.37.1"> the cloud. </span><span class="koboSpan" id="kobo.37.2">Once the application is in the cloud via this lift-and-shift methodology, we can make improvements and optimizations not only to the application itself but also to its surrounding infrastructure </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">and architecture.</span></span></p>
<p><span class="koboSpan" id="kobo.39.1">I previously worked for a company that had an existing on-premises solution. </span><span class="koboSpan" id="kobo.39.2">This on-premises solution was distributed to customers via remote deployment. </span><span class="koboSpan" id="kobo.39.3">The client provided a machine, and a specific onboarding team logged in to that machine and ran a playbook to set up the application. </span><span class="koboSpan" id="kobo.39.4">This lift-and-shift mindset persisted into the </span><em class="italic"><span class="koboSpan" id="kobo.40.1">cloud native</span></em><span class="koboSpan" id="kobo.41.1"> hosted offerings they provided. </span><span class="koboSpan" id="kobo.41.2">The onboarding team provisioned a new instance and database in the cloud, and then somebody installed the application, and the client accessed the cloud instance. </span><span class="koboSpan" id="kobo.41.3">This process was the company’s first iteration of providing services in the cloud. </span><span class="koboSpan" id="kobo.41.4">However, the manual processes have been persistent and difficult to shake. </span><span class="koboSpan" id="kobo.41.5">These processes are a classic example of building in the cloud versus building for the cloud. </span><span class="koboSpan" id="kobo.41.6">It can be challenging to relinquish control of these business procedures to automation. </span><span class="koboSpan" id="kobo.41.7">However, unless we utilize the advantages that the cloud provides, we fail to recognize the actual efficiencies of this shift. </span><span class="koboSpan" id="kobo.41.8">A good approach that would have allowed for much faster cycle times and reduced new customer entry barriers is shifting to self-serve, on-demand onboarding, using a cloud factory approach, as we will see in more detail later in the chapter. </span><span class="koboSpan" id="kobo.41.9">Similar techniques were adopted in their future cloud native applications, built from the ground up. </span><span class="koboSpan" id="kobo.41.10">However, this brings us to a new anti-pattern, having the attitude that “we’ll build it right </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">this time.”</span></span></p>
<h2 id="_idParaDest-202"><a id="_idTextAnchor202"/><span class="koboSpan" id="kobo.43.1">The myth of “We’ll build it right this time”</span></h2>
<p><span class="koboSpan" id="kobo.44.1">One of the anti-patterns we often see is software teams wanting to burn everything to the ground and start again to become cloud native. </span><span class="koboSpan" id="kobo.44.2">This all-or-nothing approach not only fragments your business into </span><strong class="bold"><span class="koboSpan" id="kobo.45.1">legacy</span></strong><span class="koboSpan" id="kobo.46.1"> (your original application) and </span><strong class="bold"><span class="koboSpan" id="kobo.47.1">greenfield</span></strong><span class="koboSpan" id="kobo.48.1"> (your brand-new application) development but also means that you neglect a product that customers are using to work on a product that will likely not have users until at least parity with your on-premises solution. </span><span class="koboSpan" id="kobo.48.2">The timelines of these projects are often wildly underestimated and require reskilling and re-resourcing to get the cloud native skills you need. </span><span class="koboSpan" id="kobo.48.3">The all-or-nothing approach frequently means that critical decisions around your application and its architecture are made upfront at the point in time when your organization likely has the least cloud experience </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">on hand!</span></span></p>
<p><span class="koboSpan" id="kobo.50.1">When shifting to the cloud, AWS has the 7Rs of migration strategies to use, which we went through in </span><a href="B22364_02.xhtml#_idTextAnchor055"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.51.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.52.1">. </span><span class="koboSpan" id="kobo.52.2">To refresh your memory, these are refactor, replatform, repurchase, rehost, relocate, retain, </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">and retire.</span></span></p>
<p><span class="koboSpan" id="kobo.54.1">You’ll notice that </span><em class="italic"><span class="koboSpan" id="kobo.55.1">rebuild</span></em><span class="koboSpan" id="kobo.56.1"> is not one of the options. </span><span class="koboSpan" id="kobo.56.2">To take full advantage of cloud native services in an existing application, we must choose an option that will eventually lead us down the refactor path. </span><span class="koboSpan" id="kobo.56.3">The easiest way to start is to build a cloud factory for our </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">existing application.</span></span></p>
<h2 id="_idParaDest-203"><a id="_idTextAnchor203"/><span class="koboSpan" id="kobo.58.1">Cloud factories</span></h2>
<p><span class="koboSpan" id="kobo.59.1">The lift-and-shift part of</span><a id="_idIndexMarker738"/><span class="koboSpan" id="kobo.60.1"> a cloud migration is unavoidable. </span><span class="koboSpan" id="kobo.60.2">Running the existing application in the cloud is the first step to migrating it to become cloud native. </span><span class="koboSpan" id="kobo.60.3">When deploying an</span><a id="_idIndexMarker739"/><span class="koboSpan" id="kobo.61.1"> on-premises application, there is a significant lead time, as it involves hardware provided by the customer, with customer-controlled access and rollouts with manual steps. </span><span class="koboSpan" id="kobo.61.2">As discussed in our earlier example, a common anti-pattern in this space reflects that process in the </span><a id="_idIndexMarker740"/><span class="koboSpan" id="kobo.62.1">cloud environment. </span><span class="koboSpan" id="kobo.62.2">Customers use </span><a id="_idIndexMarker741"/><span class="koboSpan" id="kobo.63.1">different </span><strong class="bold"><span class="koboSpan" id="kobo.64.1">firewalls</span></strong><span class="koboSpan" id="kobo.65.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.66.1">hypervisors</span></strong><span class="koboSpan" id="kobo.67.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.68.1">hardware</span></strong><span class="koboSpan" id="kobo.69.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.70.1">security</span></strong><span class="koboSpan" id="kobo.71.1"> in an on-premises environment. </span><span class="koboSpan" id="kobo.71.2">The rollout process </span><a id="_idIndexMarker742"/><span class="koboSpan" id="kobo.72.1">typically requires manual intervention to deal with the idiosyncrasies of the </span><a id="_idIndexMarker743"/><span class="No-Break"><span class="koboSpan" id="kobo.73.1">particular client.</span></span></p>
<p><span class="koboSpan" id="kobo.74.1">When deploying in a cloud environment, we get to specify these options ourselves. </span><span class="koboSpan" id="kobo.74.2">We say how big our VM is, how we configure our firewall and networking, or what operating system we use. </span><span class="koboSpan" id="kobo.74.3">Instead of multiple unique customer environments, we’re deploying the same cloud environment multiple times, meaning all the quirks are identical for each implementation case. </span><span class="koboSpan" id="kobo.74.4">We can now automate the provisioning workflow with certainty, reducing onboarding from a process that might take weeks with multiple client contacts to a process that can run in a pipeline and might take 30 minutes. </span><span class="koboSpan" id="kobo.74.5">Creating a cloud factory for your application is a crucial first step for migrating on-premises applications to the cloud without rearchitecting to a multitenant model. </span><span class="koboSpan" id="kobo.74.6">We will delve deeper into this in </span><a href="B22364_12.xhtml#_idTextAnchor320"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.75.1">Chapter 12</span></em></span></a><span class="koboSpan" id="kobo.76.1">. </span><span class="koboSpan" id="kobo.76.2">As we </span><a id="_idIndexMarker744"/><span class="koboSpan" id="kobo.77.1">start to transition our application to the cloud, the</span><a id="_idIndexMarker745"/><span class="koboSpan" id="kobo.78.1"> question still remains: how will we refactor this while retaining the end functionality? </span><span class="koboSpan" id="kobo.78.2">The answer is through the use of the strangler </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">fig pattern.</span></span></p>
<h2 id="_idParaDest-204"><a id="_idTextAnchor204"/><span class="koboSpan" id="kobo.80.1">Cloud native through the strangler fig pattern</span></h2>
<p><span class="koboSpan" id="kobo.81.1">A strangler fig is a plant that grows on a host tree. </span><span class="koboSpan" id="kobo.81.2">Sometimes, the host tree dies, leaving only the strangler fig. </span><span class="koboSpan" id="kobo.81.3">The </span><strong class="bold"><span class="koboSpan" id="kobo.82.1">strangler fig pattern</span></strong><span class="koboSpan" id="kobo.83.1">, coined by</span><a id="_idIndexMarker746"/><span class="koboSpan" id="kobo.84.1"> Martin Fowler, is similar. </span><span class="koboSpan" id="kobo.84.2">It </span><a id="_idIndexMarker747"/><span class="koboSpan" id="kobo.85.1">lets us take our existing applications and make them cloud native by slow degrees, eventually replacing our legacy solution altogether. </span><span class="koboSpan" id="kobo.85.2">Through this mechanism of action, we also allow for the deferral of system-wide architectural decisions until later in the process, once the cloud maturity of our organization has improved. </span><span class="koboSpan" id="kobo.85.3">The first stage of cloud migration is to take our existing application to the cloud – that is, rehost. </span><span class="koboSpan" id="kobo.85.4">You can also technically take this approach without the rehost phase and instead redirect traffic to our on-premises instance, although this requires additional networking and solid authorization strategies to be in place. </span><span class="koboSpan" id="kobo.85.5">This simple transition is depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.86.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.87.1">.1</span></em><span class="koboSpan" id="kobo.88.1">. </span><span class="koboSpan" id="kobo.88.2">We start with an on-premises instance and replace it with a cloud instance. </span><span class="koboSpan" id="kobo.88.3">The switch is transparent to the </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">end user.</span></span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer053">
<span class="koboSpan" id="kobo.90.1"><img alt="Figure 7.1 – Initial migration of an application from on-premises to the cloud" src="image/B22364_07_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.91.1">Figure 7.1 – Initial migration of an application from on-premises to the cloud</span></p>
<p><span class="koboSpan" id="kobo.92.1">By completing this stage, we have already achieved some efficiencies; provisioning is faster by removing the dependency on physical hardware and utilizing cloud factories, colocation costs have disappeared, and we mitigate the operational overhead of disparate systems. </span><span class="koboSpan" id="kobo.92.2">However, we’re not cloud native; the overheads around OS patching and database maintenance still exist, and we’re still operating in a manner that matches our infrastructure topology to our </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">customer base.</span></span></p>
<p><span class="koboSpan" id="kobo.94.1">The next stage of our</span><a id="_idIndexMarker748"/><span class="koboSpan" id="kobo.95.1"> migration is a simple but critical phase that supports the future iterations of our application. </span><span class="koboSpan" id="kobo.95.2">We need to add </span><a id="_idIndexMarker749"/><span class="koboSpan" id="kobo.96.1">an </span><strong class="bold"><span class="koboSpan" id="kobo.97.1">API proxy layer</span></strong><span class="koboSpan" id="kobo.98.1">. </span><span class="koboSpan" id="kobo.98.2">All hyperscalers </span><a id="_idIndexMarker750"/><span class="koboSpan" id="kobo.99.1">have a managed service that performs this function; in AWS, it is </span><strong class="bold"><span class="koboSpan" id="kobo.100.1">API Gateway</span></strong><span class="koboSpan" id="kobo.101.1">, Azure has </span><strong class="bold"><span class="koboSpan" id="kobo.102.1">API Management</span></strong><span class="koboSpan" id="kobo.103.1">, and </span><a id="_idIndexMarker751"/><span class="koboSpan" id="kobo.104.1">GCP has </span><strong class="bold"><span class="koboSpan" id="kobo.105.1">Apigee</span></strong><span class="koboSpan" id="kobo.106.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.107.1">API Gateway</span></strong><span class="koboSpan" id="kobo.108.1">. </span><span class="koboSpan" id="kobo.108.2">Some</span><a id="_idIndexMarker752"/><span class="koboSpan" id="kobo.109.1"> open source projects provide similar functionality for specific</span><a id="_idIndexMarker753"/><span class="koboSpan" id="kobo.110.1"> environments, such as </span><strong class="bold"><span class="koboSpan" id="kobo.111.1">Kubernetes</span></strong><span class="koboSpan" id="kobo.112.1">. </span><span class="koboSpan" id="kobo.112.2">The key </span><a id="_idIndexMarker754"/><span class="koboSpan" id="kobo.113.1">here is that we are introducing a layer between our end user and our application that can perform </span><strong class="bold"><span class="koboSpan" id="kobo.114.1">Layer 7 routing</span></strong><span class="koboSpan" id="kobo.115.1"> as defined in the OSI model. </span><span class="koboSpan" id="kobo.115.2">This model will allow us to inspect incoming traffic and decide actions based on HTTP request properties. </span><span class="koboSpan" id="kobo.115.3">In contrast to the architecture in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.116.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.117.1">.1</span></em><span class="koboSpan" id="kobo.118.1">, we now have an additional architectural element, the API proxy, which is once again transparent to the </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">end user.</span></span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer054">
<span class="koboSpan" id="kobo.120.1"><img alt="Figure 7.2 – Addition of an API proxy to the cloud instance" src="image/B22364_07_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.121.1">Figure 7.2 – Addition of an API proxy to the cloud instance</span></p>
<p><span class="koboSpan" id="kobo.122.1">Functionally, we have yet to start using the API layer’s capabilities to their full extent, but we have achieved some operational efficiencies as part of this change. </span><span class="koboSpan" id="kobo.122.2">If we were using </span><strong class="bold"><span class="koboSpan" id="kobo.123.1">Transport Layer Security</span></strong><span class="koboSpan" id="kobo.124.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.125.1">TLS</span></strong><span class="koboSpan" id="kobo.126.1">), we </span><a id="_idIndexMarker755"/><span class="koboSpan" id="kobo.127.1">would likely have a provisioned TLS certificate. </span><span class="koboSpan" id="kobo.127.2">Switching to a fully managed proxy allows the TLS termination to occur at the proxy layer, freeing us from the operational overhead of managing this with a manual or semi-automated process. </span><span class="koboSpan" id="kobo.127.3">The key is that our application is no longer bound to our deployed instance. </span><span class="koboSpan" id="kobo.127.4">Typically, we build on-premises applications using a monolithic architecture, as the deployment of these applications is tightly coupled to the topology of the hardware we deploy them on. </span><span class="koboSpan" id="kobo.127.5">In the cloud, these limitations no longer constrain us. </span><span class="koboSpan" id="kobo.127.6">It is detrimental to the ability of development teams to operate in this environment. </span><span class="koboSpan" id="kobo.127.7">Using the monolith architecture usually results in high internal coupling between components, making it difficult to predict the blast radius of a particular change without knowing the full scope of its use throughout </span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">the application.</span></span></p>
<p><span class="koboSpan" id="kobo.129.1">The solution is to use </span><a id="_idIndexMarker756"/><span class="koboSpan" id="kobo.130.1">the Layer 7 routing capabilities of the API proxy to decompose our application into new cloud native implementations. </span><span class="koboSpan" id="kobo.130.2">For example, many applications have a user management system so users can log in to the application. </span><span class="koboSpan" id="kobo.130.3">Traditionally, someone might achieve this by storing passwords in a database, ideally, hashed and salted. </span><span class="koboSpan" id="kobo.130.4">This approach is a definite source of </span><a id="_idIndexMarker757"/><span class="koboSpan" id="kobo.131.1">risk for most </span><a id="_idIndexMarker758"/><span class="koboSpan" id="kobo.132.1">companies. </span><strong class="bold"><span class="koboSpan" id="kobo.133.1">Insecure hash algorithms</span></strong><span class="koboSpan" id="kobo.134.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.135.1">timing leaks</span></strong><span class="koboSpan" id="kobo.136.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.137.1">password database security</span></strong><span class="koboSpan" id="kobo.138.1"> are all</span><a id="_idIndexMarker759"/><span class="koboSpan" id="kobo.139.1"> things your company is directly responsible for under this model. </span><span class="koboSpan" id="kobo.139.2">By migrating this to a managed service, we significantly de-risk our operations. </span><span class="koboSpan" id="kobo.139.3">We can also make changes at this stage to make our solution more cloud native through replatforming some of the easier migrations, such as databases, to a compatible managed database service. </span><span class="koboSpan" id="kobo.139.4">Continuing our architectural evolution, we break down the monolithic application into components in the </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer055">
<span class="koboSpan" id="kobo.141.1"><img alt="" role="presentation" src="image/B22364_07_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.142.1">Figure 7.3 – Beginning to decompose the monolith into domain-driven microservices</span></p>
<p><span class="koboSpan" id="kobo.143.1">Under this new architecture, we have separated concerns for our user management and our newly replatformed application monolith. </span><span class="koboSpan" id="kobo.143.2">Our user service provides an abstraction behind our API proxy for performing actions such as resetting passwords, updating email addresses, and other user-centric functions. </span><span class="koboSpan" id="kobo.143.3">At the same time, our original monolithic application still contains all the functionality external to users. </span><span class="koboSpan" id="kobo.143.4">We’ve managed to refactor one part of our application to be truly cloud native and use managed services. </span><span class="koboSpan" id="kobo.143.5">Most importantly, we don’t need to know how the entire application will be architected to achieve this. </span><span class="koboSpan" id="kobo.143.6">We need to understand this particular domain and the services available to accelerate it. </span><span class="koboSpan" id="kobo.143.7">We’ve also broken any coupling that may have existed between the user service and unrelated parts of the application. </span><span class="koboSpan" id="kobo.143.8">Under this new model, changes to the user service have a blast radius limited to the service itself without unforeseen side effects on the rest of </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">the application.</span></span></p>
<p><span class="koboSpan" id="kobo.145.1">In some simple cases, we may only have two targets for the API proxy: the new cloud native service and the old legacy service. </span><span class="koboSpan" id="kobo.145.2">However, as you perform this method of replacing or migrating functionality, it is also worth reevaluating your architecture and seeing whether you can reduce coupling within your application or increase cohesion within a specific domain by breaking out disparate services. </span><span class="koboSpan" id="kobo.145.3">Rarely, the perfect solution to a problem requiring refactoring to become cloud native is to build a cloud </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">native monolith.</span></span></p>
<p><span class="koboSpan" id="kobo.147.1">Slowly, we can </span><a id="_idIndexMarker760"/><span class="koboSpan" id="kobo.148.1">continue to break down the service into its emergent domains. </span><span class="koboSpan" id="kobo.148.2">We establish bounded contexts within our application, representing highly cohesive parts of our business context. </span><span class="koboSpan" id="kobo.148.3">For more information on bounded contexts and domain-driven design, I recommend reading </span><em class="italic"><span class="koboSpan" id="kobo.149.1">Domain Driven Design</span></em><span class="koboSpan" id="kobo.150.1"> by Eric Evans. </span><span class="koboSpan" id="kobo.150.2">We then decompose our architecture into these domains and look to utilize cloud native services wherever possible. </span><span class="koboSpan" id="kobo.150.3">As a part of this shift, if our application supports multiple customers, we can also build multitenancy into these services. </span><span class="koboSpan" id="kobo.150.4">Eventually, we will reach a point where we have integrated the entire application into a series of cloud native services backed by managed services that provide equivalent or improved functionality. </span><span class="koboSpan" id="kobo.150.5">As the final step in our architectural evolution, we have removed the monolith and left only the new application services. </span><span class="koboSpan" id="kobo.150.6">This is reflected in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.151.1">Figure 7</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.152.1">.4</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer056">
<span class="koboSpan" id="kobo.154.1"><img alt="" role="presentation" src="image/B22364_07_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.155.1">Figure 7.4 – The original monolith is deprecated and is truly cloud native</span></p>
<p><span class="koboSpan" id="kobo.156.1">By using the API proxy to slowly and methodically decompose the monolith, we have effectively accomplished the desired result: removing the legacy monolith and adopting cloud native services. </span><span class="koboSpan" id="kobo.156.2">At this point, it is possible to remove the API proxy; however, in most cases, the application proxy still provides benefits by acting as a central entry point to </span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">your application.</span></span></p>
<p><span class="koboSpan" id="kobo.158.1">We have examined typical anti-patterns in the initial cloud migration, including unproductive migration strategies such as one-and-done migration or retirement and rebuilding. </span><span class="koboSpan" id="kobo.158.2">We have also explored how the strangler fig pattern allows us to keep servicing our current clients while modernizing our application. </span><span class="koboSpan" id="kobo.158.3">Now, we have a path to becoming cloud native that does not require broad sweeping solutions all at once but can be part of a</span><a id="_idIndexMarker761"/><span class="koboSpan" id="kobo.159.1"> longer-term digital transformation focusing on client outcomes rather than </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">technological puritanism.</span></span></p>
<p><span class="koboSpan" id="kobo.161.1">Now that we have dived into the migrations of existing applications, we can start to look at how the applications themselves are constructed to be cloud native. </span><span class="koboSpan" id="kobo.161.2">The first stop on this journey is addressing where we store the state for </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">our applications.</span></span></p>
<h1 id="_idParaDest-205"><a id="_idTextAnchor205"/><span class="koboSpan" id="kobo.163.1">Stateful applications</span></h1>
<p><span class="koboSpan" id="kobo.164.1">Most applications are driven by a series of </span><strong class="bold"><span class="koboSpan" id="kobo.165.1">stateful processes</span></strong><span class="koboSpan" id="kobo.166.1"> at </span><a id="_idIndexMarker762"/><span class="koboSpan" id="kobo.167.1">their core. </span><span class="koboSpan" id="kobo.167.2">These states might be </span><strong class="bold"><span class="koboSpan" id="kobo.168.1">ephemeral</span></strong><span class="koboSpan" id="kobo.169.1"> – that is, they might not be data with long-term context, such as a user session that is only active while the user is on a website. </span><span class="koboSpan" id="kobo.169.2">In other scenarios, we might persist these states for longer-term storage. </span><span class="koboSpan" id="kobo.169.3">For example, an online store might require maintaining the state of a shopping cart, collecting payment, and shipping the items. </span><span class="koboSpan" id="kobo.169.4">These are all states that need to be persisted in our architecture somewhere. </span><span class="koboSpan" id="kobo.169.5">In a single server model, conflating the system’s local and external state is trivial. </span><span class="koboSpan" id="kobo.169.6">In this section, we will look into the scalability and robustness of these patterns to examine how we can manage the state </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">cloud natively.</span></span></p>
<h2 id="_idParaDest-206"><a id="_idTextAnchor206"/><span class="koboSpan" id="kobo.171.1">The stateless server</span></h2>
<p><span class="koboSpan" id="kobo.172.1">A common anti-pattern when building </span><a id="_idIndexMarker763"/><span class="koboSpan" id="kobo.173.1">cloud native applications is to store state locally to a server. </span><span class="koboSpan" id="kobo.173.2">Most cloud services have options, like </span><strong class="bold"><span class="koboSpan" id="kobo.174.1">session affinity</span></strong><span class="koboSpan" id="kobo.175.1">, to enable you</span><a id="_idIndexMarker764"/><span class="koboSpan" id="kobo.176.1"> to migrate applications to the cloud with a locally stored state. </span><span class="koboSpan" id="kobo.176.2">However, we should refrain from using these patterns in new or refactored cloud native applications. </span><span class="koboSpan" id="kobo.176.3">Two main patterns allow us to achieve this in </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">the cloud.</span></span></p>
<p><span class="koboSpan" id="kobo.178.1">In the </span><strong class="bold"><span class="koboSpan" id="kobo.179.1">state assertion pattern</span></strong><span class="koboSpan" id="kobo.180.1">, the</span><a id="_idIndexMarker765"/><span class="koboSpan" id="kobo.181.1"> client presents a </span><strong class="bold"><span class="koboSpan" id="kobo.182.1">verifiable state representation</span></strong><span class="koboSpan" id="kobo.183.1"> to the</span><a id="_idIndexMarker766"/><span class="koboSpan" id="kobo.184.1"> backend server. </span><span class="koboSpan" id="kobo.184.2">We typically use this pattern for transient state, the quintessential example of which is replacing user session tokens, which we can match to the ephemeral state stored on</span><a id="_idIndexMarker767"/><span class="koboSpan" id="kobo.185.1"> the machine, with a user session assertion like a </span><strong class="bold"><span class="koboSpan" id="kobo.186.1">JSON Web Token</span></strong><span class="koboSpan" id="kobo.187.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.188.1">JWT</span></strong><span class="koboSpan" id="kobo.189.1">) or </span><strong class="bold"><span class="koboSpan" id="kobo.190.1">Security Assertion Markup Language</span></strong><span class="koboSpan" id="kobo.191.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.192.1">SAML</span></strong><span class="koboSpan" id="kobo.193.1">) response. </span><span class="koboSpan" id="kobo.193.2">In both cases, the client stores their state, and we can verify that the client’s </span><a id="_idIndexMarker768"/><span class="koboSpan" id="kobo.194.1">state has not been altered through cryptographically secure signatures. </span><span class="koboSpan" id="kobo.194.2">This pattern comes with some caveats, for instance, the fact that these tokens (unless encrypted) are transparent to the end user, so we should never include secret information that we don’t want the user to see in the assertion. </span><span class="koboSpan" id="kobo.194.3">They are </span><a id="_idIndexMarker769"/><span class="koboSpan" id="kobo.195.1">also prime targets for token theft, so good practices around </span><strong class="bold"><span class="koboSpan" id="kobo.196.1">token lifetimes</span></strong><span class="koboSpan" id="kobo.197.1">, TLS, and </span><strong class="bold"><span class="koboSpan" id="kobo.198.1">storage of the tokens</span></strong><span class="koboSpan" id="kobo.199.1"> on the client’s device are all paramount with </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">this pattern.</span></span></p>
<p><span class="koboSpan" id="kobo.201.1">The second pattern is </span><a id="_idIndexMarker770"/><span class="koboSpan" id="kobo.202.1">using </span><strong class="bold"><span class="koboSpan" id="kobo.203.1">external state storage</span></strong><span class="koboSpan" id="kobo.204.1">. </span><span class="koboSpan" id="kobo.204.2">If the data we are handling is not transient and requires use by multiple parties, then we must persist the state to storage external to the server. </span><span class="koboSpan" id="kobo.204.3">The type of data being stored decides how we store it on the backend, too. </span><span class="koboSpan" id="kobo.204.4">The key here is to move the state out of our application, which provides numerous benefits in the world of </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">the cloud.</span></span></p>
<p><span class="koboSpan" id="kobo.206.1">We typically encounter three kinds of state</span><a id="_idIndexMarker771"/><span class="koboSpan" id="kobo.207.1"> data. </span><span class="koboSpan" id="kobo.207.2">Of course, there are always exceptions and edge cases, but as a general rule, we can choose external state storage suitable for our </span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">use case.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.209.1">Transient state data</span></strong><span class="koboSpan" id="kobo.210.1"> is </span><a id="_idIndexMarker772"/><span class="koboSpan" id="kobo.211.1">data that represents a substate of a system at a point in time, but it is inconsequential if the data gets deleted. </span><span class="koboSpan" id="kobo.211.2">This might be because the data itself is a cache of other data sources that can be reconstructed or because the nature of the data is transient anyway, for example, </span><strong class="bold"><span class="koboSpan" id="kobo.212.1">short-lived session tokens</span></strong><span class="koboSpan" id="kobo.213.1">. </span><span class="koboSpan" id="kobo.213.2">Typically, we store this </span><a id="_idIndexMarker773"/><span class="koboSpan" id="kobo.214.1">data because we require it at short notice. </span><span class="koboSpan" id="kobo.214.2">Think of it like your short-term memory. </span><span class="koboSpan" id="kobo.214.3">It holds values that you are currently actively working with but might be replaced at any point. </span><span class="koboSpan" id="kobo.214.4">Cloud services have solutions tailored toward high-performance workloads and can be leveraged for more cost-effective solutions. </span><span class="koboSpan" id="kobo.214.5">For high-performance workloads, we can use services like </span><a id="_idIndexMarker774"/> <strong class="bold"><span class="koboSpan" id="kobo.215.1">ElastiCache</span></strong><span class="koboSpan" id="kobo.216.1"> in AWS, </span><strong class="bold"><span class="koboSpan" id="kobo.217.1">Memorystore</span></strong><span class="koboSpan" id="kobo.218.1"> in GCP, or </span><strong class="bold"><span class="koboSpan" id="kobo.219.1">Azure Cache</span></strong><span class="koboSpan" id="kobo.220.1"> in </span><a id="_idIndexMarker775"/><span class="koboSpan" id="kobo.221.1">Azure; these all mirror the </span><a id="_idIndexMarker776"/><span class="koboSpan" id="kobo.222.1">concept of traditional deployed cache services. </span><span class="koboSpan" id="kobo.222.2">Other emerging solutions in the space, like </span><strong class="bold"><span class="koboSpan" id="kobo.223.1">Momento</span></strong><span class="koboSpan" id="kobo.224.1">, allow</span><a id="_idIndexMarker777"/><span class="koboSpan" id="kobo.225.1"> for cache as a service. </span><span class="koboSpan" id="kobo.225.2">If latency is not mission-critical, other proprietary solutions might be more cost-effective and scalable with only minimal impact on latency, for example, TTLs on DynamoDB (a NoSQL service from AWS) tables or even fully SaaS solutions such as Momento. </span><span class="koboSpan" id="kobo.225.3">The critical difference from the self-managed paradigm is that these services are managed, and all have options to be automatically scalable, allowing us to focus on those parts of our application that deliver value, our </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">domain logic.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.227.1">Persistent state data</span></strong><span class="koboSpan" id="kobo.228.1"> is data</span><a id="_idIndexMarker778"/><span class="koboSpan" id="kobo.229.1"> the system needs a persistent reference to with context in a semantic model. </span><span class="koboSpan" id="kobo.229.2">These might be items such as orders we want to keep a log of or bank accounts for which we want to maintain a balance. </span><span class="koboSpan" id="kobo.229.3">The way in which we</span><a id="_idIndexMarker779"/><span class="koboSpan" id="kobo.230.1"> store this data can have different modalities, such as </span><strong class="bold"><span class="koboSpan" id="kobo.231.1">relational</span></strong><span class="koboSpan" id="kobo.232.1"> versus </span><strong class="bold"><span class="koboSpan" id="kobo.233.1">non-relational</span></strong><span class="koboSpan" id="kobo.234.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.235.1">normalized</span></strong><span class="koboSpan" id="kobo.236.1"> versus </span><strong class="bold"><span class="koboSpan" id="kobo.237.1">denormalized</span></strong><span class="koboSpan" id="kobo.238.1">, or </span><strong class="bold"><span class="koboSpan" id="kobo.239.1">structured</span></strong><span class="koboSpan" id="kobo.240.1"> versus </span><strong class="bold"><span class="koboSpan" id="kobo.241.1">unstructured</span></strong><span class="koboSpan" id="kobo.242.1">. </span><span class="koboSpan" id="kobo.242.2">Typically, these representations of state can be thought of as records that might be akin to our long-term memory. </span><span class="koboSpan" id="kobo.242.3">At the time of writing, this is an exciting space, as there are leaps and bounds of progress being made in the serverless offerings for relational databases like </span><strong class="bold"><span class="koboSpan" id="kobo.243.1">Aurora Serverless</span></strong><span class="koboSpan" id="kobo.244.1"> on AWS or </span><strong class="bold"><span class="koboSpan" id="kobo.245.1">Cloud Spanner</span></strong><span class="koboSpan" id="kobo.246.1"> on </span><a id="_idIndexMarker780"/><span class="koboSpan" id="kobo.247.1">GCP. </span><span class="koboSpan" id="kobo.247.2">For non-relational databases, most cloud providers have well-established, truly serverless offerings (truly serverless in the way that they</span><a id="_idIndexMarker781"/><span class="koboSpan" id="kobo.248.1"> scale to zero). </span><span class="koboSpan" id="kobo.248.2">AWS </span><a id="_idIndexMarker782"/><span class="koboSpan" id="kobo.249.1">has </span><strong class="bold"><span class="koboSpan" id="kobo.250.1">DynamoDB</span></strong><span class="koboSpan" id="kobo.251.1">, Azure has </span><strong class="bold"><span class="koboSpan" id="kobo.252.1">Cosmos DB</span></strong><span class="koboSpan" id="kobo.253.1">, and GCP </span><a id="_idIndexMarker783"/><span class="koboSpan" id="kobo.254.1">has </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.255.1">Cloud Firestore</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.256.1">.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.257.1">Supporting data</span></strong><span class="koboSpan" id="kobo.258.1"> is </span><a id="_idIndexMarker784"/><span class="koboSpan" id="kobo.259.1">typically data that has little meaning without the context of persistent data. </span><span class="koboSpan" id="kobo.259.2">This might be data like photos, PDF documents, or other types of files that we want to store because it provides additional information. </span><span class="koboSpan" id="kobo.259.3">The difference between persistent and supporting data is that supporting data can be thought of as an object rather than a record. </span><span class="koboSpan" id="kobo.259.4">This distinction is also reflected in the way the services are named, usually referred to as blob or application stores. </span><span class="koboSpan" id="kobo.259.5">AWS</span><a id="_idIndexMarker785"/><span class="koboSpan" id="kobo.260.1"> has </span><strong class="bold"><span class="koboSpan" id="kobo.261.1">S3</span></strong><span class="koboSpan" id="kobo.262.1">, GCP </span><a id="_idIndexMarker786"/><span class="koboSpan" id="kobo.263.1">has </span><strong class="bold"><span class="koboSpan" id="kobo.264.1">Cloud Storage</span></strong><span class="koboSpan" id="kobo.265.1">, and Azure has </span><strong class="bold"><span class="koboSpan" id="kobo.266.1">Azure Blob Storage</span></strong><span class="koboSpan" id="kobo.267.1">. </span><span class="koboSpan" id="kobo.267.2">Once</span><a id="_idIndexMarker787"/><span class="koboSpan" id="kobo.268.1"> again, all of these are managed services, and their throughput and capacity will scale with </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">our requirements.</span></span></p>
<p><span class="koboSpan" id="kobo.270.1">The question is, when do we commit state to an external service? </span><span class="koboSpan" id="kobo.270.2">The general rule of thumb is that any state that requires persistence beyond one transaction should be committed to external state management. </span><span class="koboSpan" id="kobo.270.3">The local state is fine within the context of the transaction for processing purposes, but the external state is necessary for anything breaking this boundary. </span><span class="koboSpan" id="kobo.270.4">A parallel we can draw, which we have all likely suffered with in the past, is a multi-page web form, where every time you submit a value that is incorrect, it forgets the previous pages and takes you back to page one. </span><span class="koboSpan" id="kobo.270.5">That is the risk we run with local state that crosses </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">translation boundaries.</span></span></p>
<p><span class="koboSpan" id="kobo.272.1">These data types are the most common when serving Online Transaction Processing (OLTP) workloads. </span><span class="koboSpan" id="kobo.272.2">The storage and consumption patterns are different when serving analytical (OLAP) workloads. </span><span class="koboSpan" id="kobo.272.3">When analytical functionality is required, persisting data to an analytical store purpose-built for your use case is usually recommended, such as a data warehouse. </span><span class="koboSpan" id="kobo.272.4">Each of the hyperscalers has slightly different approaches in this space: GCP has the fully managed serverless </span><a id="_idIndexMarker788"/><span class="koboSpan" id="kobo.273.1">solution </span><strong class="bold"><span class="koboSpan" id="kobo.274.1">BigQuery</span></strong><span class="koboSpan" id="kobo.275.1">, AWS has </span><strong class="bold"><span class="koboSpan" id="kobo.276.1">Redshift</span></strong><span class="koboSpan" id="kobo.277.1">, and Azure </span><a id="_idIndexMarker789"/><span class="koboSpan" id="kobo.278.1">has </span><strong class="bold"><span class="koboSpan" id="kobo.279.1">Azure Synapse</span></strong><span class="koboSpan" id="kobo.280.1">. </span><span class="koboSpan" id="kobo.280.2">This area also has significant contenders</span><a id="_idIndexMarker790"/><span class="koboSpan" id="kobo.281.1"> outside</span><a id="_idIndexMarker791"/><span class="koboSpan" id="kobo.282.1"> of the hyperscalers, like </span><strong class="bold"><span class="koboSpan" id="kobo.283.1">Snowflake</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.284.1">and </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.285.1">Databricks</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.287.1">Now that we’ve discussed removing the state from the local server, let’s explore the new possibilities for resiliency and scalability this opens for us in a cloud </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">native environment.</span></span></p>
<h2 id="_idParaDest-207"><a id="_idTextAnchor207"/><span class="koboSpan" id="kobo.289.1">Resiliency and scalability in a stateless server paradigm</span></h2>
<p><span class="koboSpan" id="kobo.290.1">Werner Vogels, the CTO of AWS, once mentioned that “</span><em class="italic"><span class="koboSpan" id="kobo.291.1">Everything fails, all the time</span></em><span class="koboSpan" id="kobo.292.1">.” </span><span class="koboSpan" id="kobo.292.2">If we persist state locally to our server, then that state is only as durable as that single server. </span><span class="koboSpan" id="kobo.292.3">Large companies, such as hyperscalers, employ legions of engineers to ensure their applications are durable, available, and bug-free. </span><span class="koboSpan" id="kobo.292.4">Most people embarking on a cloud native transformation won’t have access to the same level of resourcing that these large companies do. </span><span class="koboSpan" id="kobo.292.5">This is where</span><a id="_idIndexMarker792"/><span class="koboSpan" id="kobo.293.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.294.1">stateless cloud paradigm</span></strong><span class="koboSpan" id="kobo.295.1"> allows us to trade on margin by using managed services to store our state. </span><span class="koboSpan" id="kobo.295.2">These managed services do have legions of engineers behind them. </span><span class="koboSpan" id="kobo.295.3">If we persist state external to our application, suddenly, the fault tolerance of our application becomes </span><span class="No-Break"><span class="koboSpan" id="kobo.296.1">less consequential.</span></span></p>
<p><span class="koboSpan" id="kobo.297.1">Server died? </span><span class="koboSpan" id="kobo.297.2">Start another one and investigate the cause. </span><span class="koboSpan" id="kobo.297.3">Our state was off the server, so it doesn’t matter whether the server went down. </span><span class="koboSpan" id="kobo.297.4">Our new server will pick right up where the old one left off. </span><span class="koboSpan" id="kobo.297.5">Even better, run multiple stateless instances of your server in a self-healing group. </span><span class="koboSpan" id="kobo.297.6">Cloud services also allow us to automate this part of our system. </span><span class="koboSpan" id="kobo.297.7">AWS</span><a id="_idIndexMarker793"/><span class="koboSpan" id="kobo.298.1"> uses </span><strong class="bold"><span class="koboSpan" id="kobo.299.1">Auto Scaling groups</span></strong><span class="koboSpan" id="kobo.300.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.301.1">Elastic Load Balancing</span></strong><span class="koboSpan" id="kobo.302.1">. </span><span class="koboSpan" id="kobo.302.2">GCP </span><a id="_idIndexMarker794"/><span class="koboSpan" id="kobo.303.1">has managed instance groups for VMs or </span><strong class="bold"><span class="koboSpan" id="kobo.304.1">Cloud Run/Google Kubernetes Engine</span></strong><span class="koboSpan" id="kobo.305.1"> for </span><a id="_idIndexMarker795"/><span class="koboSpan" id="kobo.306.1">containers, as well as </span><strong class="bold"><span class="koboSpan" id="kobo.307.1">load balancers</span></strong><span class="koboSpan" id="kobo.308.1"> to </span><a id="_idIndexMarker796"/><span class="koboSpan" id="kobo.309.1">distribute </span><a id="_idIndexMarker797"/><span class="koboSpan" id="kobo.310.1">traffic. </span><span class="koboSpan" id="kobo.310.2">Azure uses </span><strong class="bold"><span class="koboSpan" id="kobo.311.1">Virtual Machine Scale Sets</span></strong><span class="koboSpan" id="kobo.312.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.313.1">Azure App Service</span></strong><span class="koboSpan" id="kobo.314.1"> to a similar effect. </span><span class="koboSpan" id="kobo.314.2">All these</span><a id="_idIndexMarker798"/><span class="koboSpan" id="kobo.315.1"> services allow us</span><a id="_idIndexMarker799"/><span class="koboSpan" id="kobo.316.1"> to mitigate the risk of single-point failures in our system for the parts of the cloud that are our responsibility and typically contain the most bugs. </span><span class="koboSpan" id="kobo.316.2">It’s important to note that managing the state does not even need to be a process we entrust to our own code; we can go even further and use a fully managed state as </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">a service.</span></span></p>
<h2 id="_idParaDest-208"><a id="_idTextAnchor208"/><span class="koboSpan" id="kobo.318.1">State as a service</span></h2>
<p><span class="koboSpan" id="kobo.319.1">Typically, we build state </span><a id="_idIndexMarker800"/><span class="koboSpan" id="kobo.320.1">machines to replicate business processes. </span><span class="koboSpan" id="kobo.320.2">For example, we might onboard a new tenant in a distributed microservice architecture system. </span><span class="koboSpan" id="kobo.320.3">In the past, I have seen people build complex code in poorly documented and typically fragile ways. </span><span class="koboSpan" id="kobo.320.4">For example, a central tenant service called out to each of the microservices to orchestrate them, but this tenant service was touched by every team that needed onboarding actions to be performed. </span><span class="koboSpan" id="kobo.320.5">The result was unbound states and error-prone onboarding that resulted in a wide array of edge cases, with no one easily able to grasp the full complexity of </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">the system.</span></span></p>
<p><span class="koboSpan" id="kobo.322.1">We want a state machine that tells us if the requested action has been completed. </span><span class="koboSpan" id="kobo.322.2">Here is where </span><a id="_idIndexMarker801"/><span class="koboSpan" id="kobo.323.1">managed services </span><a id="_idIndexMarker802"/><span class="koboSpan" id="kobo.324.1">can also be of benefit. </span><span class="koboSpan" id="kobo.324.2">Solutions such as </span><strong class="bold"><span class="koboSpan" id="kobo.325.1">AWS Step Functions</span></strong><span class="koboSpan" id="kobo.326.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.327.1">Google Workflows</span></strong><span class="koboSpan" id="kobo.328.1">, or </span><strong class="bold"><span class="koboSpan" id="kobo.329.1">Azure Logic Apps</span></strong><span class="koboSpan" id="kobo.330.1"> allow </span><a id="_idIndexMarker803"/><span class="koboSpan" id="kobo.331.1">us to outsource the maintenance of the state to the cloud itself. </span><span class="koboSpan" id="kobo.331.2">This is an excellent solution for when centralized orchestration is required. </span><span class="koboSpan" id="kobo.331.3">In our previous example, we want to onboard a tenant, so we make a state machine that creates a new tenant in the tenant service, provisions a new user as the admin in the user service, and sends an email to that user to log in. </span><span class="koboSpan" id="kobo.331.4">Once the user has accepted the invitation, there may be more stages, such as provisioning new data for the tenant, prompting the admin to add other users, or setting up retention policies on </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">user files.</span></span></p>
<p><span class="koboSpan" id="kobo.333.1">We could do this in a distributed way with eventing and service-specific state, but typically, that results in unbound and undocumented behavior without appropriate oversight. </span><span class="koboSpan" id="kobo.333.2">The state machine as a service approach also allows us a single pane of glass to view our state machine structure and how various instances of state are progressing through it. </span><span class="koboSpan" id="kobo.333.3">When the tenant onboarding system breaks, we can immediately see where the error is by viewing our well-defined </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">state machine.</span></span></p>
<p><span class="koboSpan" id="kobo.335.1">The anti-pattern we typically see in this system is people using state machines for systems that do not cross bounded contexts (i.e., they don’t require orchestration). </span><span class="koboSpan" id="kobo.335.2">In these scenarios, we should instead rely on state representation internal to the bounded context, such as updating an order item from “ordered” to “packed” and then to “shipped.” </span><span class="koboSpan" id="kobo.335.3">The state transitions in this scenario are simple, linear, and within a bounded context. </span><span class="koboSpan" id="kobo.335.4">Hence, external state orchestration is not required. </span><span class="koboSpan" id="kobo.335.5">The final piece of the state puzzle is configuring </span><span class="No-Break"><span class="koboSpan" id="kobo.336.1">our applications.</span></span></p>
<h2 id="_idParaDest-209"><a id="_idTextAnchor209"/><span class="koboSpan" id="kobo.337.1">Application configuration as state</span></h2>
<p><span class="koboSpan" id="kobo.338.1">Fundamentally, our </span><a id="_idIndexMarker804"/><span class="koboSpan" id="kobo.339.1">application behavior is an emergent property of our application state filtered through our business logic. </span><span class="koboSpan" id="kobo.339.2">The anti-pattern here is defining application configuration in the same code we use to define our business logic. </span><span class="koboSpan" id="kobo.339.3">Application configuration is just another form of state, one that typically differs between deployed environments. </span><span class="koboSpan" id="kobo.339.4">Our code should be agnostic of the environment it is deployed in, instead, configuration should be managed through deployment itself. </span><span class="koboSpan" id="kobo.339.5">There are two places we typically store </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">application configuration:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.341.1">Externally in a key-value store or secret manager. </span><span class="koboSpan" id="kobo.341.2">We touched on this approach in </span><a href="B22364_05.xhtml#_idTextAnchor136"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.342.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.343.1"> for </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">feature flags.</span></span></li>
<li><span class="koboSpan" id="kobo.345.1">Internally in the template used to create new instances of our application, like through environment variables. </span><span class="koboSpan" id="kobo.345.2">This is typically for bootstrapping values, such as service discovery endpoints or database </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">connection strings.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.347.1">The difference between the local state in the configuration domain and the local state in the transaction domain is that the state in the configuration domain must satisfy two criteria to </span><span class="No-Break"><span class="koboSpan" id="kobo.348.1">be effective:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.349.1">It must be immutable; the configuration must not change due to external factors except for the </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">service’s redeployment</span></span></li>
<li><span class="koboSpan" id="kobo.351.1">It must be universal; all copies of the application must be provisioned with identical copies of </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">local state</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.353.1">These two paradigms ensure that our transactions are agnostic of the actual backend service completing the request. </span><span class="koboSpan" id="kobo.353.2">In the external case, we have a little more flexibility but need to be careful of the effects of rotation and </span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">cache invalidation.</span></span></p>
<p><span class="koboSpan" id="kobo.355.1">State allows our application to provide meaning through the lens of our business logic. </span><span class="koboSpan" id="kobo.355.2">However, improperly handled state can cause issues with resilience and scalability. </span><span class="koboSpan" id="kobo.355.3">Luckily, in the cloud landscape, there are many battle-tested tools that provide ways for us to store our application state. </span><span class="koboSpan" id="kobo.355.4">We can even shift our state machines entirely to the cloud with cloud native offerings while also reducing operational complexity to a minimum. </span><span class="koboSpan" id="kobo.355.5">While state is the lifeblood of our application, the health and malleability of our code are normally measured through two other properties; coupling </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">and cohesion.</span></span></p>
<h1 id="_idParaDest-210"><a id="_idTextAnchor210"/><span class="koboSpan" id="kobo.357.1">Tight coupling, low cohesion</span></h1>
<p><span class="koboSpan" id="kobo.358.1">In software design, two measures of interrelatedness are often used as a litmus test for sound system design. </span><span class="koboSpan" id="kobo.358.2">These are </span><strong class="bold"><span class="koboSpan" id="kobo.359.1">coupling</span></strong><span class="koboSpan" id="kobo.360.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.361.1">cohesion</span></strong><span class="koboSpan" id="kobo.362.1">. </span><span class="koboSpan" id="kobo.362.2">Coupling refers to disparate services calling each other to</span><a id="_idIndexMarker805"/><span class="koboSpan" id="kobo.363.1"> accomplish a </span><a id="_idIndexMarker806"/><span class="koboSpan" id="kobo.364.1">task. </span><span class="koboSpan" id="kobo.364.2">High coupling implies that the services are heavily interdependent and are challenging to operate in isolation without worrying about dependencies or side effects. </span><span class="koboSpan" id="kobo.364.3">Cohesion is the opposite. </span><span class="koboSpan" id="kobo.364.4">Coupling measures the relationships between services, and cohesion focuses on the relationships inside the service. </span><span class="koboSpan" id="kobo.364.5">If a service has low cohesion, it tries to do many disparate things simultaneously. </span><span class="koboSpan" id="kobo.364.6">We commonly see low cohesion and high coupling as an anti-pattern in cloud native software development. </span><span class="koboSpan" id="kobo.364.7">In this section, we will explore how these anti-patterns tend to be reflected in cloud environments and how to </span><span class="No-Break"><span class="koboSpan" id="kobo.365.1">avoid them.</span></span></p>
<h2 id="_idParaDest-211"><a id="_idTextAnchor211"/><span class="koboSpan" id="kobo.366.1">The Lambdalith versus single-purpose functions</span></h2>
<p><span class="koboSpan" id="kobo.367.1">A common anti-pattern we see is low cohesion in deployed infrastructure. </span><span class="koboSpan" id="kobo.367.2">Typically, this anti-pattern gets introduced through siloed infrastructure teams; for information on why this might be a lousy idea, see </span><a href="B22364_05.xhtml#_idTextAnchor136"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.368.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.369.1">. </span><span class="koboSpan" id="kobo.369.2">Let’s assume we have a serverless function on AWS, a </span><strong class="bold"><span class="koboSpan" id="kobo.370.1">Lambda function</span></strong><span class="koboSpan" id="kobo.371.1">, and</span><a id="_idIndexMarker807"/><span class="koboSpan" id="kobo.372.1"> every time we want a new one, we need a sign-off from the infrastructure team to create a new function for us rather than being empowered to create a new Lambda function ourselves. </span><span class="koboSpan" id="kobo.372.2">Then, we get a feature that should only take a day to implement but should really be a serverless function. </span><span class="koboSpan" id="kobo.372.3">Rather than wait for the infrastructure team to deal with their backlog of tickets and provide us with our function, we see a tantalizing preexisting Lambda function that, if we just added some extra routing, could also handle this other functionality. </span><span class="koboSpan" id="kobo.372.4">Compound this effect over many features, and suddenly, we end up with a significant monolithic serverless function. </span><span class="koboSpan" id="kobo.372.5">Hence the moniker, the </span><strong class="bold"><span class="koboSpan" id="kobo.373.1">Lambdalith</span></strong><span class="koboSpan" id="kobo.374.1">. </span><span class="koboSpan" id="kobo.374.2">The</span><a id="_idIndexMarker808"/><span class="koboSpan" id="kobo.375.1"> problem is that these serverless functions have low cohesion. </span><span class="koboSpan" id="kobo.375.2">This means that by modifying our function, we have a large blast radius that could impact utterly unrelated functionality simply due to process inefficiencies and </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">siloed ownership.</span></span></p>
<p><span class="koboSpan" id="kobo.377.1">I previously worked with an organization that had an architecture team separate from the infrastructure and development teams. </span><span class="koboSpan" id="kobo.377.2">Creating a service required the interaction of three teams and was aligned to a monthly cadence. </span><span class="koboSpan" id="kobo.377.3">This particular organization had teams aligned to business domains; each business domain typically had a few services they managed. </span><span class="koboSpan" id="kobo.377.4">While feature development was rapid, the event of a new service being added to support those features was exceedingly rare. </span><span class="koboSpan" id="kobo.377.5">These containers grew to significant complexity with low cohesion between application parts. </span><strong class="bold"><span class="koboSpan" id="kobo.378.1">Conway’s law</span></strong><span class="koboSpan" id="kobo.379.1"> was alive and well, and the </span><a id="_idIndexMarker809"/><span class="koboSpan" id="kobo.380.1">architecture closely followed the team topologies to </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">a fault.</span></span></p>
<p><span class="koboSpan" id="kobo.382.1">In any process, be it online sales or provisioning new infrastructure, the more difficult this process is, the less likely it will be completed. </span><span class="koboSpan" id="kobo.382.2">Typically, people ask how much friction is suitable to ensure we still produce secure, deployable artifacts. </span><span class="koboSpan" id="kobo.382.3">The answer almost always is as little as humanly possible. </span><span class="koboSpan" id="kobo.382.4">We should enable teams to take ownership of their own output by providing them with a safe and secure platform in which they can achieve their goals. </span><span class="koboSpan" id="kobo.382.5">Infrastructure and architectural resources should be available to support them at all points. </span><span class="koboSpan" id="kobo.382.6">However, if the development team cannot drive the process, you will find that the process will be </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">woefully underutilized.</span></span></p>
<p><span class="koboSpan" id="kobo.384.1">The truly cloud native antithesis of the Lambdalith is the single-purpose serverless function. </span><span class="koboSpan" id="kobo.384.2">In this pattern, each function does exactly one thing and does it well. </span><span class="koboSpan" id="kobo.384.3">For example, a </span><strong class="bold"><span class="koboSpan" id="kobo.385.1">single-purpose function</span></strong><span class="koboSpan" id="kobo.386.1"> might </span><a id="_idIndexMarker810"/><span class="koboSpan" id="kobo.387.1">handle the HTTP </span><strong class="source-inline"><span class="koboSpan" id="kobo.388.1">POST</span></strong><span class="koboSpan" id="kobo.389.1"> method on a specific API endpoint. </span><span class="koboSpan" id="kobo.389.2">This does not mean it cannot share code with other single-purpose functions. </span><span class="koboSpan" id="kobo.389.3">Typically, grouping these functions into pseudoservices with high internal cohesion makes sense. </span><span class="koboSpan" id="kobo.389.4">However, each deployed function should be completely agnostic of its peers in the pseudoservice group. </span><span class="koboSpan" id="kobo.389.5">This grouping might be performed by having several single-purpose functions deployed from the same repo (or parent folder if using a monorepo). </span><span class="koboSpan" id="kobo.389.6">This pattern provides us with high cohesion in our deployed units. </span><span class="koboSpan" id="kobo.389.7">Each unit is only concerned with satisfying the requirements for a single type of request. </span><span class="koboSpan" id="kobo.389.8">There is a limit to the level of atomicity to which we should break these units down. </span><span class="koboSpan" id="kobo.389.9">Namely, they should never be so atomic that we must chain </span><span class="No-Break"><span class="koboSpan" id="kobo.390.1">multiple together.</span></span></p>
<h2 id="_idParaDest-212"><a id="_idTextAnchor212"/><span class="koboSpan" id="kobo.391.1">Chaining serverless functions</span></h2>
<p><span class="koboSpan" id="kobo.392.1">Another anti-pattern we commonly see is the chaining of serverless functions</span><a id="_idIndexMarker811"/><span class="koboSpan" id="kobo.393.1"> in the call stack. </span><span class="koboSpan" id="kobo.393.2">This form of coupling can have an extremely negative effect on your solution’s performance and cost-effectiveness. </span><span class="koboSpan" id="kobo.393.3">For example, consider a serverless function that uses a typical </span><a id="_idIndexMarker812"/><span class="koboSpan" id="kobo.394.1">synchronous </span><strong class="bold"><span class="koboSpan" id="kobo.395.1">backend for frontend</span></strong><span class="koboSpan" id="kobo.396.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.397.1">BFF</span></strong><span class="koboSpan" id="kobo.398.1">) approach to call some business logic in another serverless function that queries a database. </span><span class="koboSpan" id="kobo.398.2">This situation is illustrated in the </span><span class="No-Break"><span class="koboSpan" id="kobo.399.1">following figure.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer057">
<span class="koboSpan" id="kobo.400.1"><img alt="" role="presentation" src="image/B22364_07_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.401.1"> Figure 7.5 – Chained invocations of serverless functions</span></p>
<p><span class="koboSpan" id="kobo.402.1">As we can see in the figure, each </span><a id="_idIndexMarker813"/><span class="koboSpan" id="kobo.403.1">preceding call runs whilst waiting for the subsequent call to complete. </span><span class="koboSpan" id="kobo.403.2">With this invocation pattern, we are doubling our running compute. </span><span class="koboSpan" id="kobo.403.3">In a containerized or VM-level environment, this is not an issue, as our compute resource can serve other requests while we wait for the chained call to finish. </span><span class="koboSpan" id="kobo.403.4">However, in a serverless function environment, our function can only serve one invocation at a time. </span><span class="koboSpan" id="kobo.403.5">This means that while we wait for the second serverless function in the chain to complete, our first lambda function cannot serve any other requests. </span><span class="koboSpan" id="kobo.403.6">Therefore, we are doubling our computing costs and resource consumption without any tangible benefit. </span><span class="koboSpan" id="kobo.403.7">Some cloud providers, such as GCP, are building platforms that allow this unused computing power to be better utilized. </span><span class="koboSpan" id="kobo.403.8">However, most default implementations are limited to completing a single request at a time. </span><span class="koboSpan" id="kobo.403.9">Chained functions are a prime example of coupling that can be converted to high cohesion internally in a single function. </span><span class="koboSpan" id="kobo.403.10">We more often need to perform the reverse operation and decouple </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">coupled services.</span></span></p>
<h2 id="_idParaDest-213"><a id="_idTextAnchor213"/><span class="koboSpan" id="kobo.405.1">Decoupling coupled services</span></h2>
<p><span class="koboSpan" id="kobo.406.1">When we call services as </span><a id="_idIndexMarker814"/><span class="koboSpan" id="kobo.407.1">dependencies from another service, we increase the blast radius of changes to the service being depended on to include our dependent service. </span><span class="koboSpan" id="kobo.407.2">This is a form of tight coupling that can be very detrimental to the performance of our application. </span><span class="koboSpan" id="kobo.407.3">The more services we chain together, the less reliable our service becomes, as we are now dealing with the product of the reliabilities of each service in the chain. </span><span class="koboSpan" id="kobo.407.4">Let’s say each service has a 95% reliability rate. </span><span class="koboSpan" id="kobo.407.5">If we combine 4 services in a single call, our reliability decreases to 81.4% (0.95^4). </span><span class="koboSpan" id="kobo.407.6">Typically, this problem arises as it fits our mental model of services very well. </span><span class="koboSpan" id="kobo.407.7">As programmers, when we need to perform some work internal to our application, we call a function and await the results. </span><span class="koboSpan" id="kobo.407.8">Extending this model to a multiservice architecture, we call another service and await </span><span class="No-Break"><span class="koboSpan" id="kobo.408.1">the results.</span></span></p>
<p><span class="koboSpan" id="kobo.409.1">Luckily, cloud providers have a cloud native way to solve this tight coupling problem. </span><span class="koboSpan" id="kobo.409.2">It requires two changes in thinking to </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">implement correctly:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.411.1">We need to break the idea of synchronous feedback. </span><span class="koboSpan" id="kobo.411.2">Sending an HTTP </span><strong class="source-inline"><span class="koboSpan" id="kobo.412.1">202</span></strong><span class="koboSpan" id="kobo.413.1"> return code and performing work asynchronously is just as, if not more, valid than a synchronous response with an HTTP </span><strong class="source-inline"><span class="koboSpan" id="kobo.414.1">200</span></strong><span class="koboSpan" id="kobo.415.1"> response all in </span><span class="No-Break"><span class="koboSpan" id="kobo.416.1">one call.</span></span></li>
<li><span class="koboSpan" id="kobo.417.1">We need to stop thinking of each of the services that need to work as dependents and start thinking of them as isolated units of work that need to </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">be completed.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.419.1">The </span><a id="_idIndexMarker815"/><span class="koboSpan" id="kobo.420.1">key to implementing these solutions in a cloud native environment is to</span><a id="_idIndexMarker816"/><span class="koboSpan" id="kobo.421.1"> decouple these </span><a id="_idIndexMarker817"/><span class="koboSpan" id="kobo.422.1">services by</span><a id="_idIndexMarker818"/><span class="koboSpan" id="kobo.423.1"> putting a managed service in the middle. </span><span class="koboSpan" id="kobo.423.2">AWS has </span><strong class="bold"><span class="koboSpan" id="kobo.424.1">Simple Queue Service</span></strong><span class="koboSpan" id="kobo.425.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.426.1">EventBridge</span></strong><span class="koboSpan" id="kobo.427.1">, GCP has </span><strong class="bold"><span class="koboSpan" id="kobo.428.1">Google Pub/Sub</span></strong><span class="koboSpan" id="kobo.429.1">, and </span><a id="_idIndexMarker819"/><span class="koboSpan" id="kobo.430.1">Azure has </span><strong class="bold"><span class="koboSpan" id="kobo.431.1">Azure Event Grid</span></strong><span class="koboSpan" id="kobo.432.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.433.1">Azure </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.434.1">Service Bus</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.435.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.436.1">These managed services all provide similar functionality. </span><span class="koboSpan" id="kobo.436.2">They act as a message broker between our services so that our services do not need to talk to one another synchronously to pass information between them. </span><span class="koboSpan" id="kobo.436.3">They differ slightly in how they operate. </span><span class="koboSpan" id="kobo.436.4">Some are </span><strong class="bold"><span class="koboSpan" id="kobo.437.1">simple message queues</span></strong><span class="koboSpan" id="kobo.438.1">, and others are </span><strong class="bold"><span class="koboSpan" id="kobo.439.1">complete event bus implementations</span></strong><span class="koboSpan" id="kobo.440.1"> with publish and </span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">subscribe functionality.</span></span></p>
<p><span class="koboSpan" id="kobo.442.1">The result of using any of these services is similar. </span><span class="koboSpan" id="kobo.442.2">Instead of our reliability now being the result of a series product, we have decoupled the services to concern themselves with the reliability of the managed service. </span><span class="koboSpan" id="kobo.442.3">Let’s take our four unreliable services and attach them to our managed service, allowing for asynchronous execution. </span><span class="koboSpan" id="kobo.442.4">Assuming our managed service has four 9s of uptime (99.99% uptime), our result is four services, each with 95.98% reliability. </span><span class="koboSpan" id="kobo.442.5">If any of our services goes down, the other services will </span><span class="No-Break"><span class="koboSpan" id="kobo.443.1">still operate.</span></span></p>
<p><span class="koboSpan" id="kobo.444.1">Implementing </span><strong class="bold"><span class="koboSpan" id="kobo.445.1">dead letter queues</span></strong><span class="koboSpan" id="kobo.446.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.447.1">DLQs</span></strong><span class="koboSpan" id="kobo.448.1">) can </span><a id="_idIndexMarker820"/><span class="koboSpan" id="kobo.449.1">further improve the reliability of these services. </span><span class="koboSpan" id="kobo.449.2">If one of our services cannot process messages, we can send the backlog of messages to be processed to the DLQ. </span><span class="koboSpan" id="kobo.449.3">Once we have fixed our service and everything is operational, we can automatically replay the events from our DLQ and</span><a id="_idIndexMarker821"/><span class="koboSpan" id="kobo.450.1"> complete the outstanding work. </span><span class="koboSpan" id="kobo.450.2">This means that instead of a single service failure impacting all systems, the blast radius of a single system is limited to the system itself. </span><span class="koboSpan" id="kobo.450.3">The system will eventually be consistent once all unprocessed messages have been replayed. </span><span class="koboSpan" id="kobo.450.4">When we need to eventually trace these events through our system, perhaps to troubleshoot why they ended up in our DLQ, we need to correlate their path, which brings us to an essential part of distributed systems: telemetry and </span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">event correlation.</span></span></p>
<h2 id="_idParaDest-214"><a id="_idTextAnchor214"/><span class="koboSpan" id="kobo.452.1">Telemetry and event correlation</span></h2>
<p><span class="koboSpan" id="kobo.453.1">You can’t improve what you cannot measure. </span><span class="koboSpan" id="kobo.453.2">Understanding precisely the degree of coupling within a deployed application can be challenging. </span><span class="koboSpan" id="kobo.453.3">Typically, we come across an anti-pattern using traditional logging systems with distributed systems. </span><span class="koboSpan" id="kobo.453.4">Traditional logging systems do not provide the granularity (level of detail) and traceability (correlation with other messages) required to debug and improve distributed systems. </span><span class="koboSpan" id="kobo.453.5">Typically, when we debug a distributed system, we are trying to piece together the result of an action across multiple deployed units. </span><span class="koboSpan" id="kobo.453.6">This is where</span><a id="_idIndexMarker822"/><span class="koboSpan" id="kobo.454.1"> robust </span><strong class="bold"><span class="koboSpan" id="kobo.455.1">telemetry</span></strong><span class="koboSpan" id="kobo.456.1"> comes into play. </span><span class="koboSpan" id="kobo.456.2">We can tag all of our requests, messages, and invocations with a </span><strong class="bold"><span class="koboSpan" id="kobo.457.1">correlation ID</span></strong><span class="koboSpan" id="kobo.458.1"> on</span><a id="_idIndexMarker823"/><span class="koboSpan" id="kobo.459.1"> entry into our distributed system, and then use this correlation ID to trace the effect of that action across all of our deployed units and managed services. </span><span class="koboSpan" id="kobo.459.2">We will go into more detail on telemetry systems in </span><a href="B22364_10.xhtml#_idTextAnchor270"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.460.1">Chapter 10</span></em></span></a><span class="koboSpan" id="kobo.461.1">. </span><span class="koboSpan" id="kobo.461.2">However, we can utilize the correlation aspect of modern telemetry systems to assist us in decoupling applications. </span><span class="koboSpan" id="kobo.461.3">By following our traces, we can reveal dependencies between systems that previously would have required us to look into the source code or environmental configuration to find. </span><span class="koboSpan" id="kobo.461.4">Once we identify the dependencies within our application, we can slowly move from tightly coupled dependencies (one service calling another) to loosely coupled dependencies (two or more services joined by a shared, managed message bus </span><span class="No-Break"><span class="koboSpan" id="kobo.462.1">or queue).</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.463.1">Tight coupling</span></strong><span class="koboSpan" id="kobo.464.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.465.1">low cohesion</span></strong><span class="koboSpan" id="kobo.466.1"> are</span><a id="_idIndexMarker824"/><span class="koboSpan" id="kobo.467.1"> anti-patterns we are typically shielded</span><a id="_idIndexMarker825"/><span class="koboSpan" id="kobo.468.1"> from in an on-premises environment. </span><span class="koboSpan" id="kobo.468.2">In the cloud, these patterns become dysfunctional, leading to poorly performing applications and unexpected side effects. </span><span class="koboSpan" id="kobo.468.3">The key to rectifying these anti-patterns is, firstly, to be able to measure the coupling and cohesion, and, secondly, to work to decouple tightly coupled services while increasing internal cohesion. </span><span class="koboSpan" id="kobo.468.4">Typically, modeling cohesion and coupling should be part of the architectural planning for a feature and form part of the definition of done. </span><span class="koboSpan" id="kobo.468.5">Let’s explore some common pitfalls and address the comprehensive definition </span><span class="No-Break"><span class="koboSpan" id="kobo.469.1">of done.</span></span></p>
<h1 id="_idParaDest-215"><a id="_idTextAnchor215"/><span class="koboSpan" id="kobo.470.1">The comprehensive definition of done</span></h1>
<p><span class="koboSpan" id="kobo.471.1">When creating software</span><a id="_idIndexMarker826"/><span class="koboSpan" id="kobo.472.1"> in </span><strong class="bold"><span class="koboSpan" id="kobo.473.1">siloed release models</span></strong><span class="koboSpan" id="kobo.474.1">, as discussed in </span><a href="B22364_05.xhtml#_idTextAnchor136"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.475.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.476.1">, we looked at empowering teams to own the delivery of their outputs from conception to deployment and beyond into operations. </span><span class="koboSpan" id="kobo.476.2">However, this requires the development team to also take ownership (with support from other teams) of the functionality and responsibilities that the siloed release pipeline previously hid from the team on the path to production. </span><span class="koboSpan" id="kobo.476.3">Hence, we need to revisit the definition of </span><em class="italic"><span class="koboSpan" id="kobo.477.1">done</span></em><span class="koboSpan" id="kobo.478.1"> (and, in some cases, the definition of </span><em class="italic"><span class="koboSpan" id="kobo.479.1">ready</span></em><span class="koboSpan" id="kobo.480.1">) for our software teams. </span><span class="koboSpan" id="kobo.480.2">Previously we have visited the cultural and business shift required to make this happen, but in this section, we will discuss building these requirements intrinsically into your definition </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">of done.</span></span></p>
<h2 id="_idParaDest-216"><a id="_idTextAnchor216"/><span class="koboSpan" id="kobo.482.1">Ignoring security</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.483.1">Security</span></strong><span class="koboSpan" id="kobo.484.1"> is a </span><a id="_idIndexMarker827"/><span class="koboSpan" id="kobo.485.1">critical factor in the </span><a id="_idIndexMarker828"/><span class="koboSpan" id="kobo.486.1">delivery pipeline. </span><span class="koboSpan" id="kobo.486.2">Neglecting sound security practices can lead to a gradual accumulation of risk for the company, often unnoticed until a breach occurs. </span><span class="koboSpan" id="kobo.486.3">This omission can result in a blame game and severe consequences. </span><span class="koboSpan" id="kobo.486.4">To develop secure applications, it’s crucial to integrate several security practices into</span><a id="_idIndexMarker829"/><span class="koboSpan" id="kobo.487.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.488.1">software delivery life cycle</span></strong><span class="koboSpan" id="kobo.489.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.490.1">SDLC</span></strong><span class="koboSpan" id="kobo.491.1">). </span><span class="koboSpan" id="kobo.491.2">These practices should be part of the definition of done for any work, and their review should be as rigorous as code review </span><span class="No-Break"><span class="koboSpan" id="kobo.492.1">before deployment.</span></span></p>
<p><span class="koboSpan" id="kobo.493.1">Ignoring </span><strong class="bold"><span class="koboSpan" id="kobo.494.1">open source</span></strong><span class="koboSpan" id="kobo.495.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.496.1">external dependencies</span></strong><span class="koboSpan" id="kobo.497.1"> is an anti-pattern. </span><span class="koboSpan" id="kobo.497.2">In the software world, many open source packages provide base functionality on which we build our business logic. </span><span class="koboSpan" id="kobo.497.3">However, each package we pull from an external source represents a possible vector for malicious code to be added to our application. </span><span class="koboSpan" id="kobo.497.4">Maintaining and alerting on</span><a id="_idIndexMarker830"/><span class="koboSpan" id="kobo.498.1"> a </span><strong class="bold"><span class="koboSpan" id="kobo.499.1">software bill of materials</span></strong><span class="koboSpan" id="kobo.500.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.501.1">SBoM</span></strong><span class="koboSpan" id="kobo.502.1">) gives you an indication of the health of your project. </span><span class="koboSpan" id="kobo.502.2">Many tools exist to help you manage the versions of software packages used. </span><span class="koboSpan" id="kobo.502.3">A typical pattern for managing dependencies at a language level is to use a read-through private artifact repository for your language, populating this artifact registry with internal packages to be used and allowing it to pull and cache upstream packages. </span><span class="koboSpan" id="kobo.502.4">This repository will enable you to have a single pane of glass containing all dependencies and versions of your application, GCP, AWS, Azure, and many niche players, all of which can export and monitor SBoMs from their respective artifact repository services. </span><span class="koboSpan" id="kobo.502.5">Pull requests should be instrumented to ensure that the packages they add do not add any new vulnerabilities, and maintenance should be done regularly, informed by the SBoM, to address any new vulnerabilities that have </span><span class="No-Break"><span class="koboSpan" id="kobo.503.1">been found.</span></span></p>
<p><span class="koboSpan" id="kobo.504.1">Not having a </span><strong class="bold"><span class="koboSpan" id="kobo.505.1">threat model</span></strong><span class="koboSpan" id="kobo.506.1"> for your application or building one and ignoring it is an anti-pattern. </span><span class="koboSpan" id="kobo.506.2">Typically, when we</span><a id="_idIndexMarker831"/><span class="koboSpan" id="kobo.507.1"> see the shift from a dedicated security team to a supported and empowered development team, the development team uses the security team to produce a threat model but fails to address it throughout the SDLC. </span><span class="koboSpan" id="kobo.507.2">The</span><a id="_idIndexMarker832"/><span class="koboSpan" id="kobo.508.1"> preliminary threat model should form part of the definition of ready for the team. </span><span class="koboSpan" id="kobo.508.2">The threat model should be fundamental in deciding how to tackle a problem and must be verified to ensure the built solution correctly mitigates the identified risks. </span><span class="koboSpan" id="kobo.508.3">Thus, the threat model should be a living document as a change is implemented, providing details on how risks are mitigated so that the changes can be merged confidently. </span><span class="koboSpan" id="kobo.508.4">Once in production, the application should be monitored through a </span><strong class="bold"><span class="koboSpan" id="kobo.509.1">cloud native application protection platform</span></strong><span class="koboSpan" id="kobo.510.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.511.1">CNAPP</span></strong><span class="koboSpan" id="kobo.512.1">) to </span><a id="_idIndexMarker833"/><span class="koboSpan" id="kobo.513.1">catch any risks or misconfigurations that might not be addressed by the threat model. </span><span class="koboSpan" id="kobo.513.2">The key to effective threat modeling is to choose the correct level of granularity. </span><span class="koboSpan" id="kobo.513.3">If you are a stock market making changes to the settlement engine, then the proper level of granularity might be every merge. </span><span class="koboSpan" id="kobo.513.4">Other lower-risk environments might only require threat modeling on a less granular level. </span><span class="koboSpan" id="kobo.513.5">The idea is to find the correct amount of friction that mitigates the risk to the proper level without compromising on the necessary level of security for </span><span class="No-Break"><span class="koboSpan" id="kobo.514.1">your application.</span></span></p>
<p><span class="koboSpan" id="kobo.515.1">The final </span><em class="italic"><span class="koboSpan" id="kobo.516.1">ignoring</span></em><span class="koboSpan" id="kobo.517.1"> security anti-pattern to address is born out of the increased flexibility the cloud gives us, and that is the failure to address defense in depth. </span><span class="koboSpan" id="kobo.517.2">In an on-premises environment, the delineation between what is inside the network and what is outside the network is evident. </span><span class="koboSpan" id="kobo.517.3">You have a physical cable going to a firewall that serves as the ingress point for all your traffic. </span><span class="koboSpan" id="kobo.517.4">Your solution might have some software-defined networking downstream, but there is a clear separation. </span><span class="koboSpan" id="kobo.517.5">In the cloud environment, all of a sudden, different services run</span><a id="_idIndexMarker834"/><span class="koboSpan" id="kobo.518.1"> in </span><strong class="bold"><span class="koboSpan" id="kobo.519.1">virtual private clouds</span></strong><span class="koboSpan" id="kobo.520.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.521.1">VPCs</span></strong><span class="koboSpan" id="kobo.522.1">) and outside VPCs. </span><span class="koboSpan" id="kobo.522.2">Endpoints can be addressable over the internet or through endpoint projections into your network. </span><span class="koboSpan" id="kobo.522.3">Some services exist in </span><strong class="bold"><span class="koboSpan" id="kobo.523.1">cloud provider-managed networks</span></strong><span class="koboSpan" id="kobo.524.1"> and require</span><a id="_idIndexMarker835"/><span class="koboSpan" id="kobo.525.1"> additional networking. </span><span class="koboSpan" id="kobo.525.2">All of this means that it is less clear where traffic is flowing. </span><span class="koboSpan" id="kobo.525.3">There is tooling to help with this but, fundamentally, the highly configurable nature of cloud environments means that misconfigurations can present a larger risk surface. </span><span class="koboSpan" id="kobo.525.4">Managed cloud services already have </span><a id="_idIndexMarker836"/><span class="koboSpan" id="kobo.526.1">strong </span><strong class="bold"><span class="koboSpan" id="kobo.527.1">identity and access management</span></strong><span class="koboSpan" id="kobo.528.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.529.1">IAM</span></strong><span class="koboSpan" id="kobo.530.1">) tooling. </span><span class="koboSpan" id="kobo.530.2">This should be complemented with robust, </span><strong class="bold"><span class="koboSpan" id="kobo.531.1">zero-trust authentication and authorization</span></strong><span class="koboSpan" id="kobo.532.1"> tooling </span><a id="_idIndexMarker837"/><span class="koboSpan" id="kobo.533.1">in your code that is validated at every application level. </span><span class="koboSpan" id="kobo.533.2">Many organizations are still early in their journey of implementing zero-trust architecture. </span><span class="koboSpan" id="kobo.533.3">Hence, it should be considered a North Star principle rather than an absolute requirement. </span><span class="koboSpan" id="kobo.533.4">The key is asking yourself, “</span><em class="italic"><span class="koboSpan" id="kobo.534.1">What happens if we accidentally expose this service to the internet directly?</span></em><span class="koboSpan" id="kobo.535.1">” This limits the blast radius of cloud misconfigurations and ensures that if an internal service is accidentally exposed to the public, it still authorizes incoming traffic. </span><span class="koboSpan" id="kobo.535.2">This blast radius consideration also needs to be considered from a </span><a id="_idIndexMarker838"/><span class="koboSpan" id="kobo.536.1">CI/CD perspective. </span><span class="koboSpan" id="kobo.536.2">One client I worked with had a single repository and project for all infrastructure. </span><span class="koboSpan" id="kobo.536.3">This resulted in highly privileged CI/CD accounts with enormous blast radii spanning multiple disparate systems. </span><span class="koboSpan" id="kobo.536.4">Having a robust defense-in-depth strategy means that as application architecture shifts to more of a self-serve model, the platform that our developers are building on top of is secure enough to tolerate failures at each level. </span><span class="koboSpan" id="kobo.536.5">Just as we must ensure our developers are building secure platforms, we must also ensure we are building </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">observable ones.</span></span></p>
<h2 id="_idParaDest-217"><a id="_idTextAnchor217"/><span class="koboSpan" id="kobo.538.1">Ignoring observability</span></h2>
<p><span class="koboSpan" id="kobo.539.1">In the monolith, logging </span><a id="_idIndexMarker839"/><span class="koboSpan" id="kobo.540.1">stages to the console was enough to debug our application. </span><span class="koboSpan" id="kobo.540.2">This worked because the application was a simple arrangement (infrastructure) of complex objects (our code). </span><span class="koboSpan" id="kobo.540.3">In the cloud native world, we shift much of that complexity into the infrastructure, giving us a complex arrangement (infrastructure) of simple objects (our code). </span><span class="koboSpan" id="kobo.540.4">This requires much more robust </span><strong class="bold"><span class="koboSpan" id="kobo.541.1">logging</span></strong><span class="koboSpan" id="kobo.542.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.543.1">telemetry practices</span></strong><span class="koboSpan" id="kobo.544.1"> than logging into a console. </span><span class="koboSpan" id="kobo.544.2">We will dive into this topic in significantly more detail in </span><a href="B22364_10.xhtml#_idTextAnchor270"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.545.1">Chapter 10</span></em></span></a><span class="koboSpan" id="kobo.546.1">. </span><span class="koboSpan" id="kobo.546.2">However, we will go through some aspects in this section that should form the basis of the definition </span><span class="No-Break"><span class="koboSpan" id="kobo.547.1">of done.</span></span></p>
<p><span class="koboSpan" id="kobo.548.1">The first anti-pattern is ignoring spans and only using logging. </span><span class="koboSpan" id="kobo.548.2">Logging provides us with point-in-time information about the state of our application. </span><span class="koboSpan" id="kobo.548.3">Spans are different. </span><span class="koboSpan" id="kobo.548.4">They provide us with context for a period of execution in our application. </span><span class="koboSpan" id="kobo.548.5">As part of our definition of done, we should include the addition of spans that provide meaningful information about executing subsections of our code. </span><span class="koboSpan" id="kobo.548.6">Throughout the execution of the span, we should also ensure that we are adding enough enriching data to make the diagnosis of issues easier through our observability platform. </span><span class="koboSpan" id="kobo.548.7">For any deployment that exceeds the scope of a single instance, we must also consider correlation to allow us to group spans together and trace their path through our distributed application. </span><span class="koboSpan" id="kobo.548.8">Trying to piece together the execution context of a request from a series of log entries across multiple services is significantly more difficult than reading a correlated span </span><span class="No-Break"><span class="koboSpan" id="kobo.549.1">flame graph.</span></span></p>
<p><span class="koboSpan" id="kobo.550.1">The second anti-pattern is </span><a id="_idIndexMarker840"/><span class="koboSpan" id="kobo.551.1">collecting metrics with no functional output. </span><span class="koboSpan" id="kobo.551.2">We quite often see a company collecting many metrics but no alerting or deviation monitoring. </span><span class="koboSpan" id="kobo.551.3">We have the data to check whether our application is performing as intended. </span><span class="koboSpan" id="kobo.551.4">However, we are missing that crucial step that actually tells us when it isn’t. </span><span class="koboSpan" id="kobo.551.5">With comprehensive monitoring, alerting, and rectification procedures, we can ensure that our system’s non-functional requirements, such as latency and error percentage, do not fall outside of acceptable margins. </span><span class="koboSpan" id="kobo.551.6">Therefore, as part of our definition of done, we should ensure </span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">two things:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.553.1">Firstly, we must ensure that the changes being made have monitoring set up. </span><span class="koboSpan" id="kobo.553.2">This might be through </span><a id="_idIndexMarker841"/><span class="koboSpan" id="kobo.554.1">synthetic traffic, </span><strong class="bold"><span class="koboSpan" id="kobo.555.1">application performance monitoring</span></strong><span class="koboSpan" id="kobo.556.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.557.1">APM</span></strong><span class="koboSpan" id="kobo.558.1">), observability tooling, and </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">traditional logging.</span></span></li>
<li><span class="koboSpan" id="kobo.560.1">Secondly, we must ensure that the right people are notified when this tooling detects a problem. </span><span class="koboSpan" id="kobo.560.2">This might be done by automatically creating a ticket in your ticketing system, notifying users on a messaging channel, or other means. </span><span class="koboSpan" id="kobo.560.3">The important thing is that regressions are identified, and people know rectification </span><span class="No-Break"><span class="koboSpan" id="kobo.561.1">must occur.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.562.1">By including these two items in our definition of done, we can provide certainty that as we add new functionality or modify existing functionality, we don’t breach the non-functional requirements of the system. </span><span class="koboSpan" id="kobo.562.2">This level of observability also gives us insight into which parts of our applications are candidates for optimization as part of our continuous improvement process. </span><span class="koboSpan" id="kobo.562.3">Previously, for clients where users complained about the slowness of the application, we filtered our metrics to rank requests by two factors: how often they were called and how long the typical transaction took. </span><span class="koboSpan" id="kobo.562.4">We found that three endpoints were consistently called and consistently slow. </span><span class="koboSpan" id="kobo.562.5">With some query optimization, we reduced the response time by two orders of magnitude. </span><span class="koboSpan" id="kobo.562.6">The change took about three days in total, and the end users were significantly happier. </span><span class="koboSpan" id="kobo.562.7">Without collecting these metrics and utilizing their outputs, we would have needed significant testing in a production environment to get the same level of insight. </span><span class="koboSpan" id="kobo.562.8">Observability is great for finding the cause of </span><a id="_idIndexMarker842"/><span class="koboSpan" id="kobo.563.1">an incident (i.e. </span><span class="koboSpan" id="kobo.563.2">when something goes wrong) but what about stopping incidents from occurring in the </span><span class="No-Break"><span class="koboSpan" id="kobo.564.1">first place?</span></span></p>
<h2 id="_idParaDest-218"><a id="_idTextAnchor218"/><span class="koboSpan" id="kobo.565.1">Ignoring reliability</span></h2>
<p><span class="koboSpan" id="kobo.566.1">The final part of this section discusses</span><a id="_idIndexMarker843"/><span class="koboSpan" id="kobo.567.1"> ignoring </span><strong class="bold"><span class="koboSpan" id="kobo.568.1">reliability</span></strong><span class="koboSpan" id="kobo.569.1">. </span><span class="koboSpan" id="kobo.569.2">This is an anti-pattern that we see all too often in cloud migrations. </span><span class="koboSpan" id="kobo.569.3">Teams care about having their features work without considering their continued operation. </span><span class="koboSpan" id="kobo.569.4">This is where the mentality of </span><em class="italic"><span class="koboSpan" id="kobo.570.1">You build it, you run it</span></em><span class="koboSpan" id="kobo.571.1"> can be beneficial. </span><span class="koboSpan" id="kobo.571.2">Development teams that also own the operation of their output are more likely to consider reliability because they are invested and want to avoid call-outs at nighttime or during weekends. </span><span class="koboSpan" id="kobo.571.3">Cloud native services provide significant tooling to ensure reliability and continuity of service. </span><span class="koboSpan" id="kobo.571.4">However, utilizing these services can mean the difference between an outage of seconds and an outage of days. </span><span class="koboSpan" id="kobo.571.5">Any company that wishes to conform to internal or external </span><strong class="bold"><span class="koboSpan" id="kobo.572.1">service-level objectives</span></strong><span class="koboSpan" id="kobo.573.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.574.1">SLOs</span></strong><span class="koboSpan" id="kobo.575.1">) or has </span><a id="_idIndexMarker844"/><span class="koboSpan" id="kobo.576.1">a contractual </span><strong class="bold"><span class="koboSpan" id="kobo.577.1">service-level agreement</span></strong><span class="koboSpan" id="kobo.578.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.579.1">SLA</span></strong><span class="koboSpan" id="kobo.580.1">) must </span><a id="_idIndexMarker845"/><span class="koboSpan" id="kobo.581.1">ensure that they treat reliability as a critical aspect of their definition </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">of done.</span></span></p>
<p><span class="koboSpan" id="kobo.583.1">The first anti-pattern we will address is an aspect of the deployment process. </span><span class="koboSpan" id="kobo.583.2">As we discussed in </span><a href="B22364_05.xhtml#_idTextAnchor136"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.584.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.585.1">, development teams should own the deployment and operation of their changes. </span><span class="koboSpan" id="kobo.585.2">The anti-pattern we often see in this space utilizes the same deployment strategy across our environments. </span><span class="koboSpan" id="kobo.585.3">In a development or test environment, it is typical for us to</span><a id="_idIndexMarker846"/><span class="koboSpan" id="kobo.586.1"> use </span><strong class="bold"><span class="koboSpan" id="kobo.587.1">all-or-nothing</span></strong><span class="koboSpan" id="kobo.588.1"> deployment strategies. </span><span class="koboSpan" id="kobo.588.2">This strategy is sound when we want to guarantee that the version of the code we are calling is the latest version and maintain fast feedback loops between the deployment and testing cycles. </span><span class="koboSpan" id="kobo.588.3">Applying this same methodology to a production environment means that if our change breaks functionality, the change either breaks everything or nothing. </span><span class="koboSpan" id="kobo.588.4">We might even have avoidable downtime on a successful deployment as the new services might take time to come online. </span><span class="koboSpan" id="kobo.588.5">For production systems, we care about two things: </span><strong class="bold"><span class="koboSpan" id="kobo.589.1">early feedback</span></strong><span class="koboSpan" id="kobo.590.1"> on a problem and </span><strong class="bold"><span class="koboSpan" id="kobo.591.1">quick rectification</span></strong><span class="koboSpan" id="kobo.592.1"> of a problem. </span><span class="koboSpan" id="kobo.592.2">Many cloud native deployment approaches will allow us to make incremental or quickly revertable changes to preserve our system’s operation, especially when using highly managed services such as API gateways or functions as a service. </span><span class="koboSpan" id="kobo.592.3">These strategies usually come at the cost of additional time to deploy or additional resources provision. </span><span class="koboSpan" id="kobo.592.4">They also normally require external state management, as any internal state </span><a id="_idIndexMarker847"/><span class="koboSpan" id="kobo.593.1">will be lost on deployment. </span><span class="koboSpan" id="kobo.593.2">Some of the methods we can use are </span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.595.1">Rolling deployments</span></strong><span class="koboSpan" id="kobo.596.1">: These</span><a id="_idIndexMarker848"/><span class="koboSpan" id="kobo.597.1"> deployments take a set of resources running the same application (say, a set of three containers that might all be running our user service) and then incrementally update each one in series until all services are running the new version, waiting for each service to become healthy before starting to deploy the next. </span><span class="koboSpan" id="kobo.597.2">This allows us to mitigate the avoidable downtime that comes with waiting for services to become ready-to-serve traffic in an all-or-nothing approach but does not provide us robust options for returning the application to a good state in the event of a failure that is only present </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">at runtime.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.599.1">Blue-green deployment</span></strong><span class="koboSpan" id="kobo.600.1">: In this</span><a id="_idIndexMarker849"/><span class="koboSpan" id="kobo.601.1"> strategy, you have two separate groups of resources. </span><span class="koboSpan" id="kobo.601.2">One set of resources serves your production traffic using the latest known production stable version of the application (blue) while the other is deployed (green). </span><span class="koboSpan" id="kobo.601.3">After you have ensured that the newly deployed system is working, you cut across to the new system, which might be</span><a id="_idIndexMarker850"/><span class="koboSpan" id="kobo.602.1"> through </span><strong class="bold"><span class="koboSpan" id="kobo.603.1">aliases</span></strong><span class="koboSpan" id="kobo.604.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.605.1">internal DNS</span></strong><span class="koboSpan" id="kobo.606.1">. </span><span class="koboSpan" id="kobo.606.2">You can then decommission the old blue target resources. </span><span class="koboSpan" id="kobo.606.3">In </span><a id="_idIndexMarker851"/><span class="koboSpan" id="kobo.607.1">the event of a failure, it is trivial to point the references, DNS or otherwise, back to the old deployment of </span><span class="No-Break"><span class="koboSpan" id="kobo.608.1">the application.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.609.1">Canary deployment</span></strong><span class="koboSpan" id="kobo.610.1">: In this </span><a id="_idIndexMarker852"/><span class="koboSpan" id="kobo.611.1">strategy, you follow the same deployment methodology as the blue-green deployment strategy. </span><span class="koboSpan" id="kobo.611.2">The critical difference is cutting over from the blue to green resources. </span><span class="koboSpan" id="kobo.611.3">Instead of an instantaneous cutover, we slowly redirect some traffic to our new instances. </span><span class="koboSpan" id="kobo.611.4">This becomes our</span><a id="_idIndexMarker853"/><span class="koboSpan" id="kobo.612.1"> canary in the coal mine; we can test the services with a subset of production data, and if something goes wrong, we will only impact a small subset of requests instead of all requests. </span><span class="koboSpan" id="kobo.612.2">Otherwise, if all is well, we progress to all traffic heading to the new resources, and the old resources can </span><span class="No-Break"><span class="koboSpan" id="kobo.613.1">be decommissioned.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.614.1">These methodologies do not need to be applied only to your application code; this pattern can be used anywhere you have an expensive rollout process. </span><span class="koboSpan" id="kobo.614.2">One client I worked with had a database that had to be updated once a month. </span><span class="koboSpan" id="kobo.614.3">Each month, the data used to build the database was either modified or appended. </span><span class="koboSpan" id="kobo.614.4">Ingestion of the new data and verifying that it was correct took 15 minutes, and the client could not tolerate 15 minutes of downtime. </span><span class="koboSpan" id="kobo.614.5">Hence, we created two tables: one for the most recent data and one for last month’s data. </span><span class="koboSpan" id="kobo.614.6">Each time new data needed to be ingested, we would populate whichever table contained the oldest data with the latest data. </span><span class="koboSpan" id="kobo.614.7">We would then check this table against the current table in use. </span><span class="koboSpan" id="kobo.614.8">If all was well, we would update the view consumed by the end users to point to the table containing the new data. </span><span class="koboSpan" id="kobo.614.9">This allowed a seamless transition between datasets without taking the system offline and allowed quick fallbacks to the last known good configuration if there was an issue. </span><span class="koboSpan" id="kobo.614.10">Understanding which deployment strategy suits your purposes is essential, and selecting an appropriate deployment strategy needs to form part of the definition </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">of done.</span></span></p>
<p><span class="koboSpan" id="kobo.616.1">The second reliability </span><a id="_idIndexMarker854"/><span class="koboSpan" id="kobo.617.1">anti-pattern we will examine is the failure to address </span><strong class="bold"><span class="koboSpan" id="kobo.618.1">disaster recovery</span></strong><span class="koboSpan" id="kobo.619.1"> correctly. </span><span class="koboSpan" id="kobo.619.2">Cloud services have sensible defaults to prevent data loss events, such </span><a id="_idIndexMarker855"/><span class="koboSpan" id="kobo.620.1">as storing objects in multiple regions or automating database backup processes. </span><span class="koboSpan" id="kobo.620.2">This process is usually tunable to meet your </span><strong class="bold"><span class="koboSpan" id="kobo.621.1">recovery point objective</span></strong><span class="koboSpan" id="kobo.622.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.623.1">RPO</span></strong><span class="koboSpan" id="kobo.624.1">) – that is, how much data we can tolerate the loss of. </span><span class="koboSpan" id="kobo.624.2">Despite </span><a id="_idIndexMarker856"/><span class="koboSpan" id="kobo.625.1">how protective cloud services are against data loss events, protection against service loss events is usually heavily dependent on your architecture. </span><span class="koboSpan" id="kobo.625.2">The critical metric data loss prevention does not address is</span><a id="_idIndexMarker857"/><span class="koboSpan" id="kobo.626.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.627.1">recovery time objective</span></strong><span class="koboSpan" id="kobo.628.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.629.1">RTO</span></strong><span class="koboSpan" id="kobo.630.1">). </span><span class="koboSpan" id="kobo.630.2">Restoring a database from a backup may take a significant amount of time. </span><span class="koboSpan" id="kobo.630.3">Likewise, standing up a new instance of your infrastructure may not be a short process. </span><span class="koboSpan" id="kobo.630.4">If your application catastrophically fails, then having a plan in place to restore service to your end users is extremely valuable. </span><span class="koboSpan" id="kobo.630.5">The first mistake teams generally make in this space is creating one copy of their infrastructure, calling it a day, and then moving on with new features. </span><span class="koboSpan" id="kobo.630.6">In this scenario, disaster recovery has been completely ignored. </span><span class="koboSpan" id="kobo.630.7">In the event of a catastrophic failure, not only will the team be scrambling to recreate their service but there’s no defined process to do so. </span><span class="koboSpan" id="kobo.630.8">The second scenario we commonly see is people having a theoretical disaster recovery strategy. </span><span class="koboSpan" id="kobo.630.9">They have a list of steps to take in case of a failure, but if the strategy is theoretical, so are the chances of it actually working. </span><span class="koboSpan" id="kobo.630.10">An untested strategy is a waste of keystrokes. </span><span class="koboSpan" id="kobo.630.11">Any disaster recovery strategy needs to be simulated regularly. </span><span class="koboSpan" id="kobo.630.12">The time to test it for the first time (and likely the first time much of the team sees the strategy) should not be when there is a critical outage. </span><span class="koboSpan" id="kobo.630.13">Typically, disaster recovery has a few options; the key is that all options must be tested. </span><span class="koboSpan" id="kobo.630.14">The possibilities we typically look</span><a id="_idIndexMarker858"/><span class="koboSpan" id="kobo.631.1"> at for recovery are </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.633.1">Cold recovery</span></strong><span class="koboSpan" id="kobo.634.1">: This strategy is for</span><a id="_idIndexMarker859"/><span class="koboSpan" id="kobo.635.1"> non-critical services. </span><span class="koboSpan" id="kobo.635.2">A cold recovery assumes you are starting from nothing, provisioning a new instance of your application, and restoring from backups to restore service. </span><span class="koboSpan" id="kobo.635.3">It is important to note that not having a disaster recovery plan is not the same as having a cold recovery plan. </span><span class="koboSpan" id="kobo.635.4">Like all plans, cold recovery must be documented and tested regularly to ensure the process meets your RPOs </span><span class="No-Break"><span class="koboSpan" id="kobo.636.1">and RTOs.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.637.1">Warm recovery</span></strong><span class="koboSpan" id="kobo.638.1">: This</span><a id="_idIndexMarker860"/><span class="koboSpan" id="kobo.639.1"> strategy involves having a second (or more) minimal copy of your application running in a different location that can be quickly scaled up to take over from the service if it fails. </span><span class="koboSpan" id="kobo.639.2">Ideally, this </span><strong class="bold"><span class="koboSpan" id="kobo.640.1">failover</span></strong><span class="koboSpan" id="kobo.641.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.642.1">scale-up</span></strong><span class="koboSpan" id="kobo.643.1"> would be automated, but while automation is being built, it is perfectly acceptable to manually fail-over. </span><span class="koboSpan" id="kobo.643.2">An alternative architecture to warm standby that uses the same principles involves keeping the supporting structures of your application running, however, only starting your application when failover is required. </span><span class="koboSpan" id="kobo.643.3">This</span><a id="_idIndexMarker861"/><span class="koboSpan" id="kobo.644.1"> variation on the strategy is commonly referred to as the </span><strong class="bold"><span class="koboSpan" id="kobo.645.1">pilot </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.646.1">light strategy</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.647.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.648.1">Hot recovery</span></strong><span class="koboSpan" id="kobo.649.1">: This </span><a id="_idIndexMarker862"/><span class="koboSpan" id="kobo.650.1">strategy involves running your application in a multi-active architecture. </span><span class="koboSpan" id="kobo.650.2">Much like we can run multiple servers to ensure that we can tolerate the failure of any single server, this pattern takes the same approach but with your entire architecture. </span><span class="koboSpan" id="kobo.650.3">The failure of any active deployment means that traffic can be redirected to the </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">healthy region.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.652.1">The concept of </span><strong class="bold"><span class="koboSpan" id="kobo.653.1">chaos engineering</span></strong><span class="koboSpan" id="kobo.654.1"> is </span><a id="_idIndexMarker863"/><span class="koboSpan" id="kobo.655.1">important to illustrate here. </span><span class="koboSpan" id="kobo.655.2">Remember the quote by Werner Vogels, “</span><em class="italic"><span class="koboSpan" id="kobo.656.1">Everything fails, all the time</span></em><span class="koboSpan" id="kobo.657.1">.” </span><span class="koboSpan" id="kobo.657.2">Chaos engineering reinforces this by purposely introducing failures into your system, ensuring that your system is fault-tolerant. </span><span class="koboSpan" id="kobo.657.3">Another good strategy to use is the concept of game days, especially for manual processes. </span><span class="koboSpan" id="kobo.657.4">These simulated events run through the disaster recovery strategy with the responsible team to ensure that everyone is familiar with the process. </span><span class="koboSpan" id="kobo.657.5">Therefore, as each feature or service is completed, the disaster recovery strategy must be updated to include the requirements of the new changes and needs to form part of the definition </span><span class="No-Break"><span class="koboSpan" id="kobo.658.1">of done.</span></span></p>
<p><span class="koboSpan" id="kobo.659.1">Security, observability, and </span><a id="_idIndexMarker864"/><span class="koboSpan" id="kobo.660.1">reliability are intrinsic parts of changes to our system that are often ignored. </span><span class="koboSpan" id="kobo.660.2">By addressing these intrinsics as part of our definition of done, we ensure that our development teams are not just building applications that are built to exhibit the features they are creating but also providing a platform that our end users can trust. </span><span class="koboSpan" id="kobo.660.3">These parts of our system form a fundamental baseline of cloud native operability, but there are many other pitfalls we can fall </span><span class="No-Break"><span class="koboSpan" id="kobo.661.1">victim to.</span></span></p>
<h1 id="_idParaDest-219"><a id="_idTextAnchor219"/><span class="koboSpan" id="kobo.662.1">Other pitfalls</span></h1>
<p><span class="koboSpan" id="kobo.663.1">There are several anti-patterns that commonly manifest in cloud native application development. </span><span class="koboSpan" id="kobo.663.2">This section will dissect some of these anti-patterns, their lineage from traditional software development, how to identify them, and the proactive mindset shifts required to evade them. </span><span class="koboSpan" id="kobo.663.3">In our scenario, cloud native applications have the capability to scale to any size we choose, sparking fascinating interactions between our software and the potential solutions to our problems. </span><span class="koboSpan" id="kobo.663.4">By understanding these anti-patterns and adopting a proactive mindset, we can empower ourselves to make informed decisions and avoid </span><span class="No-Break"><span class="koboSpan" id="kobo.664.1">potential pitfalls.</span></span></p>
<h2 id="_idParaDest-220"><a id="_idTextAnchor220"/><span class="koboSpan" id="kobo.665.1">Solving cloud native problems without cloud native experience</span></h2>
<p><span class="koboSpan" id="kobo.666.1">I was working with a </span><a id="_idIndexMarker865"/><span class="koboSpan" id="kobo.667.1">customer trying </span><a id="_idIndexMarker866"/><span class="koboSpan" id="kobo.668.1">to migrate their existing data structures into an </span><strong class="bold"><span class="koboSpan" id="kobo.669.1">OpenSearch</span></strong><span class="koboSpan" id="kobo.670.1"> cluster. </span><span class="koboSpan" id="kobo.670.2">We had well-defined schemas into which the data had to be marshaled. </span><span class="koboSpan" id="kobo.670.3">The problem, however, was that the client attempted to copy their relational data structures directly across to OpenSearch with no denormalization in between. </span><span class="koboSpan" id="kobo.670.4">This meant that to marshal the data, we needed to perform multiple lookups to fetch related data structures. </span><span class="koboSpan" id="kobo.670.5">These lookups created a situation in which a single request for a model could balloon out to thousands of downstream requests for all of its associated data. </span><span class="koboSpan" id="kobo.670.6">Despite our continued protests that the data structures needed to be denormalized or migrated to a high-performance, read-only copy of the relational database, the client wanted to preserve the system’s </span><em class="italic"><span class="koboSpan" id="kobo.671.1">flexibility</span></em><span class="koboSpan" id="kobo.672.1"> by retaining the original relational shape in a non-relational datastore. </span><span class="koboSpan" id="kobo.672.2">We implemented many improvements to push the model as far as possible, including batching requests and local caching for repeated values. </span><span class="koboSpan" id="kobo.672.3">However, some requests were simply too deeply nested to optimize. </span><span class="koboSpan" id="kobo.672.4">The solution initially proposed by the client was to scale the cluster, so the client scaled the cluster until more performance bottlenecks were hit, and then the client scaled the cluster again. </span><span class="koboSpan" id="kobo.672.5">We had an interesting call with the cloud provider. </span><span class="koboSpan" id="kobo.672.6">They informed the client that they were provisioning more infrastructure than the cloud provider had provisioned for some subsidiary services. </span><span class="koboSpan" id="kobo.672.7">This is the first anti-pattern we would like to address. </span><span class="koboSpan" id="kobo.672.8">The easy access to virtually unlimited cloud resources comes with the temptation to solve performance problems by throwing more resources at it, and the resulting cloud bill will scale equally as quickly. </span><span class="koboSpan" id="kobo.672.9">We should often look inward at our application instead of outwardly at the infrastructure it is running on to solve problems around application performance. </span><span class="koboSpan" id="kobo.672.10">Scaling our infrastructure vertically to solve performance issues will only take us so far. </span><span class="koboSpan" id="kobo.672.11">This indicates that an alternative specialized solution may be required, your service has low cohesion, or your application is </span><span class="No-Break"><span class="koboSpan" id="kobo.673.1">poorly optimized.</span></span></p>
<p><span class="koboSpan" id="kobo.674.1">This brings us to the second </span><a id="_idIndexMarker867"/><span class="koboSpan" id="kobo.675.1">anti-pattern, which can also result in the first anti-pattern. </span><span class="koboSpan" id="kobo.675.2">This pattern typically starts with someone responsible for a cloud native service coming across a staged architecture online with many pretty icons and boxes and then trying to shoehorn that architecture into their use case. </span><span class="koboSpan" id="kobo.675.3">Our architecture should be informed by the requirements of the application code we need to write rather than the code we write conforming to some architecture. </span><span class="koboSpan" id="kobo.675.4">The cause of this can be multifaceted. </span><span class="koboSpan" id="kobo.675.5">A common driver for this anti-pattern is what we typically refer to as resume-driven development. </span><span class="koboSpan" id="kobo.675.6">This occurs when someone is more concerned about getting experience with a particular technology than about that technology’s potential to solve the problem. </span><span class="koboSpan" id="kobo.675.7">Staged architectures can form a good starting point for potential solutions and often illustrate best practices. </span><span class="koboSpan" id="kobo.675.8">However, we must temper these architectures, considering their suitability across various factors. </span><span class="koboSpan" id="kobo.675.9">Typically, before adopting a staged architecture verbatim, we should ask ourselves some questions like </span><span class="No-Break"><span class="koboSpan" id="kobo.676.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.677.1">Do we operate at the scale for which this staged architecture solves </span><span class="No-Break"><span class="koboSpan" id="kobo.678.1">a problem?</span></span></li>
<li><span class="koboSpan" id="kobo.679.1">Do we have the internal skill set to implement and </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">maintain it?</span></span></li>
<li><span class="koboSpan" id="kobo.681.1">Does this model follow our standard architecture practices or will it </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">be unique?</span></span></li>
<li><span class="koboSpan" id="kobo.683.1">Can we make any changes to ensure this architecture more accurately solves </span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">our problem?</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.685.1">The third anti-pattern we will address is manually changing deployed infrastructure or code bases outside our CI/CD pipeline. </span><span class="koboSpan" id="kobo.685.2">A typical example might be that our application runs a query that takes a little while to complete in production. </span><span class="koboSpan" id="kobo.685.3">So, the developer logs into production and quickly adds an index to the lookup column, and the problem is solved. </span><span class="koboSpan" id="kobo.685.4">Despite the compounding of errors that need to occur to allow the developer to make this change, fundamentally, we are introducing instability into our application. </span><span class="koboSpan" id="kobo.685.5">This concept is known as </span><strong class="bold"><span class="koboSpan" id="kobo.686.1">environmental drift</span></strong><span class="koboSpan" id="kobo.687.1">. </span><span class="koboSpan" id="kobo.687.2">Our code and deployment pipelines define a model that does not</span><a id="_idIndexMarker868"/><span class="koboSpan" id="kobo.688.1"> correlate with what is deployed. </span><span class="koboSpan" id="kobo.688.2">In our example, we looked at the developer making changes to production, which means the first time that all of our subsequent changes are tested with this environmental drift is when those changes hit our production environment. </span><span class="koboSpan" id="kobo.688.3">It also causes an issue when we need to recreate our infrastructure; by </span><a id="_idIndexMarker869"/><span class="koboSpan" id="kobo.689.1">circumventing our source model, we will create the same issue whenever we try to create a new instance of our infrastructure. </span><span class="koboSpan" id="kobo.689.2">The solution to this problem is relatively simple; development teams should not be able to change a non-ephemeral environment without following their CI/CD process. </span><span class="koboSpan" id="kobo.689.3">If they want to prototype a fix or conduct a technical spike that would be accelerated by having write access to the environment, then create a sandbox that can be destroyed once the work is done. </span><span class="koboSpan" id="kobo.689.4">This way, you prevent the accumulation of tests and quick fixes in any environments on the path to production. </span><span class="koboSpan" id="kobo.689.5">Ideally, these lower environments should be as close to the production environment as possible. </span><span class="koboSpan" id="kobo.689.6">On the topic of production environments, we must be careful about how we scale our code in reaction to </span><span class="No-Break"><span class="koboSpan" id="kobo.690.1">real-world events.</span></span></p>
<h2 id="_idParaDest-221"><a id="_idTextAnchor221"/><span class="koboSpan" id="kobo.691.1">Suffering from success – the downside of limitless scale</span></h2>
<p><span class="koboSpan" id="kobo.692.1">We have an upper bound for our </span><a id="_idIndexMarker870"/><span class="koboSpan" id="kobo.693.1">application’s throughput when working with on-premises infrastructure. </span><span class="koboSpan" id="kobo.693.2">Eventually, we will run out of system resources to serve requests. </span><span class="koboSpan" id="kobo.693.3">In a cloud environment, we often see the same thinking come into play – an anti-pattern where rate limits and service limits are ignored. </span><span class="koboSpan" id="kobo.693.4">The consequences of neglecting rate limits, service limits, or throttling are significantly higher in the cloud. </span><span class="koboSpan" id="kobo.693.5">Rather than being capped by our infrastructure, we have a virtually unlimited pool of resources to scale into. </span><span class="koboSpan" id="kobo.693.6">Suppose we combine this lack of physical limits with stateless servers that can interchangeably serve any request, irrespective of any service-level partitioning that we might have. </span><span class="koboSpan" id="kobo.693.7">In that case, we can scale to meet our customer’s needs very rapidly and virtually limitlessly. </span><span class="koboSpan" id="kobo.693.8">In this scenario, we must set artificial caps on using our service. </span><span class="koboSpan" id="kobo.693.9">How these limits are partitioned (i.e., by user, tenant, customer, etc.) is up to the implementer. </span><span class="koboSpan" id="kobo.693.10">We set rational limits for using our theoretically limitless service to control runaway costs and ensure that we don’t impact services for any other clients. </span><span class="koboSpan" id="kobo.693.11">Many cloud native managed services already have built-in functionality that we can use to perform rate-limiting, usage monitoring, and licensing applications. </span><span class="koboSpan" id="kobo.693.12">Commonly, this is</span><a id="_idIndexMarker871"/><span class="koboSpan" id="kobo.694.1"> applied at </span><a id="_idIndexMarker872"/><span class="koboSpan" id="kobo.695.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.696.1">API aggregation layer</span></strong><span class="koboSpan" id="kobo.697.1">, such as in AWS </span><strong class="bold"><span class="koboSpan" id="kobo.698.1">API Gateway</span></strong><span class="koboSpan" id="kobo.699.1">, Azure </span><strong class="bold"><span class="koboSpan" id="kobo.700.1">APIM</span></strong><span class="koboSpan" id="kobo.701.1">, or GCP </span><strong class="bold"><span class="koboSpan" id="kobo.702.1">API Gateway</span></strong><span class="koboSpan" id="kobo.703.1">. </span><span class="koboSpan" id="kobo.703.2">Luckily, these </span><a id="_idIndexMarker873"/><span class="koboSpan" id="kobo.704.1">same API keys can be used as part of our</span><a id="_idIndexMarker874"/><span class="koboSpan" id="kobo.705.1"> authentication strategy, for example, tying a request to a client to enable defense in depth checking that our API key matches the tenant we’re calling. </span><span class="koboSpan" id="kobo.705.2">As the complexity of our application grows, we might require custom authorization and rate-limiting logic on our APIs. </span><span class="koboSpan" id="kobo.705.3">For example, AWS allows you to add custom authorization to API Gateway through Lambda functions. </span><span class="koboSpan" id="kobo.705.4">Other niche API proxy players like </span><strong class="bold"><span class="koboSpan" id="kobo.706.1">Apigee</span></strong><span class="koboSpan" id="kobo.707.1"> (now acquired by Google) and </span><strong class="bold"><span class="koboSpan" id="kobo.708.1">Kong</span></strong><span class="koboSpan" id="kobo.709.1"> allow for complex logic through a comprehensive </span><span class="No-Break"><span class="koboSpan" id="kobo.710.1">policy </span></span><span class="No-Break"><a id="_idIndexMarker875"/></span><span class="No-Break"><span class="koboSpan" id="kobo.711.1">language.</span></span></p>
<p><span class="koboSpan" id="kobo.712.1">In the on-premises monolith, things tended to fail together. </span><span class="koboSpan" id="kobo.712.2">Was our server overloaded or not? </span><span class="koboSpan" id="kobo.712.3">It’s a question with a relatively simple answer. </span><span class="koboSpan" id="kobo.712.4">In the cloud native world, where we have services built up of many components, things tend to fail piecemeal. </span><span class="koboSpan" id="kobo.712.5">We need to be tolerant of these faults, but we also need to be aware that the scales the cloud lets us operate at can lead to some interesting behaviors. </span><span class="koboSpan" id="kobo.712.6">The next anti-pattern we will address is using bad timeout and</span><a id="_idIndexMarker876"/><span class="koboSpan" id="kobo.713.1"> retry practices, especially in the context of concurrent executions. </span><span class="koboSpan" id="kobo.713.2">Let’s assume we have a process that needs to load CSV files into a database and a service that processes a single file from these buckets as they arrive. </span><span class="koboSpan" id="kobo.713.3">Let’s assume our clients upstream, who deliver our files into the S3 bucket for us to consume, realize that they had an error in their system and haven’t uploaded files for the last three days. </span><span class="koboSpan" id="kobo.713.4">That’s fine; they have added all the files. </span><span class="koboSpan" id="kobo.713.5">Let’s assume we have a naive architecture that sends a request to an HTTP endpoint to pull the file for processing using S3 events and SNS. </span><span class="koboSpan" id="kobo.713.6">If we’ve ignored the consequences of concurrent execution, we could suddenly begin ingesting a large amount of data simultaneously. </span><span class="koboSpan" id="kobo.713.7">This puts an enormous load on the database we are loading the files into. </span><span class="koboSpan" id="kobo.713.8">If we don’t have timeouts configured for these processes, we could end up completely overloading our database. </span><span class="koboSpan" id="kobo.713.9">Therefore, all calls in our application code must have a timeout, and the expiration of those timeouts must be handled gracefully, cleaning up any work in progress that they </span><span class="No-Break"><span class="koboSpan" id="kobo.714.1">have requested.</span></span></p>
<p><span class="koboSpan" id="kobo.715.1">So, if a timeout fails, then what next? </span><span class="koboSpan" id="kobo.715.2">A naive response might be that we simply need to retry the request. </span><span class="koboSpan" id="kobo.715.3">If the failure results from factors other than an overloaded system, and these errors are rare, then we can probably get away with this approach. </span><span class="koboSpan" id="kobo.715.4">However, it’s important to note that retries are compounding the issue; we are requesting more server time to solve our problem. </span><span class="koboSpan" id="kobo.715.5">If the system is already overloaded, then this just compounds the effect as old requests being retried are now also competing with new requests. </span><span class="koboSpan" id="kobo.715.6">A common tactic here is an exponential backoff algorithm, although it is advisable to cap your maximum retry period and the total number of retries. </span><span class="koboSpan" id="kobo.715.7">This can work; however, once your server gets overloaded, a whole bunch of calls are going to fail, and if all these calls are retried using the same algorithm, then all we’ve done is kick the can down the road, and we will overload the server on the </span><span class="No-Break"><span class="koboSpan" id="kobo.716.1">next retry.</span></span></p>
<p><span class="koboSpan" id="kobo.717.1">Another important aspect of </span><a id="_idIndexMarker877"/><span class="koboSpan" id="kobo.718.1">retry behavior is the concept of </span><strong class="bold"><span class="koboSpan" id="kobo.719.1">jitter</span></strong><span class="koboSpan" id="kobo.720.1">. </span><span class="koboSpan" id="kobo.720.2">We introduce </span><strong class="bold"><span class="koboSpan" id="kobo.721.1">randomness</span></strong><span class="koboSpan" id="kobo.722.1"> into </span><a id="_idIndexMarker878"/><span class="koboSpan" id="kobo.723.1">our retry behavior to prevent a stampeding herd </span><a id="_idIndexMarker879"/><span class="koboSpan" id="kobo.724.1">situation. </span><span class="koboSpan" id="kobo.724.2">We also need to be aware of the multiplicative effect of retries. </span><span class="koboSpan" id="kobo.724.3">Suppose our service makes calls that go three layers deep, and each service retries five times. </span><span class="koboSpan" id="kobo.724.4">In that case, the downstream system will receive 53 retries or 125 requests, which is the opposite of the behavior we want when downstream services are overloaded. </span><span class="koboSpan" id="kobo.724.5">Luckily, there are three effortless ways to avoid </span><span class="No-Break"><span class="koboSpan" id="kobo.725.1">this situation:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.726.1">Decouple spiky and expensive traffic with message queues, then scale your downstream services based on </span><span class="No-Break"><span class="koboSpan" id="kobo.727.1">queue depth</span></span></li>
<li><span class="koboSpan" id="kobo.728.1">Use cloud provider SDKs where possible, as these will already have retry behaviors built </span><span class="No-Break"><span class="koboSpan" id="kobo.729.1">into them</span></span></li>
<li><span class="koboSpan" id="kobo.730.1">Use managed services, as these typically scale easier and have built-in retry and rate-limiting functionality you don’t need to </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">build yourself</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.732.1">This brings us to our last anti-pattern, using implicit properties of ephemeral resources for hardcoded </span><span class="No-Break"><span class="koboSpan" id="kobo.733.1">dependency mapping.</span></span></p>
<h2 id="_idParaDest-222"><a id="_idTextAnchor222"/><span class="koboSpan" id="kobo.734.1">Avoiding implicit ephemeral specification</span></h2>
<p><span class="koboSpan" id="kobo.735.1">When </span><a id="_idIndexMarker880"/><span class="koboSpan" id="kobo.736.1">writing code, especially </span><strong class="bold"><span class="koboSpan" id="kobo.737.1">infrastructure as code</span></strong><span class="koboSpan" id="kobo.738.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.739.1">IaC</span></strong><span class="koboSpan" id="kobo.740.1">), we can easily fall into the anti-pattern of using</span><a id="_idIndexMarker881"/><span class="koboSpan" id="kobo.741.1"> direct specifications for partially ephemeral resources. </span><span class="koboSpan" id="kobo.741.2">An ephemeral specification, for example, would be applying an IaC configuration that outputs the IP address of an instance, then referring to the first configuration instance by directly using that IP address in another IaC configuration. </span><span class="koboSpan" id="kobo.741.3">If we change the first configuration, the IP address might change, but our ephemeral specification has created a hard dependency between them. </span><span class="koboSpan" id="kobo.741.4">Instead, we should use resources that aren’t ephemeral, such as DNS entries that can be updated. </span><span class="koboSpan" id="kobo.741.5">This is the simplest form of service discovery. </span><span class="koboSpan" id="kobo.741.6">There are robust, full-featured service discovery platforms that extend this functionality for various cloud providers and deployment configurations. </span><span class="koboSpan" id="kobo.741.7">Ideally, any dependencies between our </span><a id="_idIndexMarker882"/><span class="koboSpan" id="kobo.742.1">infrastructure should be explicit rather than implicit through hardcoded values to make our deployments truly agnostic of the state of the </span><span class="No-Break"><span class="koboSpan" id="kobo.743.1">deployed environment.</span></span></p>
<h1 id="_idParaDest-223"><a id="_idTextAnchor223"/><span class="koboSpan" id="kobo.744.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.745.1">We have now explored some common anti-patterns we see when shifting our application logic to the cloud. </span><span class="koboSpan" id="kobo.745.2">Our application code is typically the value differentiator or competitive advantage in our business, so we can move it to the cloud and, by doing so, increase its availability, resilience, and performance. </span><span class="koboSpan" id="kobo.745.3">Now that we understand the implications of running our application code in the cloud, how can we store all our data? </span><span class="koboSpan" id="kobo.745.4">This is what we will dive into in the </span><span class="No-Break"><span class="koboSpan" id="kobo.746.1">next chapter.</span></span></p>
</div>
</body></html>