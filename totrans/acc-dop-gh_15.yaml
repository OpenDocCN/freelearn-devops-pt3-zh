- en: '*Chapter 12*: Shift Left Testing for Increased Quality'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Testing** and **quality assurance** (**QA**) is still one of the practices
    that holds back most companies. In this chapter, we''ll take a closer look at
    the role that QA and testing play in terms of developer velocity and how to shift
    left test.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Shift left testing with test automation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eradicating flaky tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shift right – testing in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fault injection and chaos engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing and compliance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test management in GitHub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shift left testing with test automation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you practice agile development and try to ship frequently, then manual testing
    isn't a scalable option. Even if you don't practice CI/CD and only ship on a sprint
    cadence, running all the necessary regression tests would take enormous manpower
    and a lot of time and money. But getting test automation right is not an easy
    task. Automated tests that have been created and maintained by a QA department
    or outsourced entity, for example, are *not* correlated with higher engineering
    velocity (*Forsgren N., Humble, J., & Kim, G., 2018*, *Page 95*). To notice an
    impact on your velocity, you need reliable tests that have been created and maintained
    by the team. The theory behind this is that if developers maintain tests, they
    produce more testable code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Everybody knows what a good test portfolio should look like: you have a big
    base of automated unit tests (Level 0), fewer integration tests (Level 1), some
    integration tests that need test data (Level 2), and only a few functional tests
    (Level 3). This is called the test pyramid (*see Figure 12.1*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – The test pyramid'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17827_12_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.1 – The test pyramid
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in most companies, the portfolio does not look like this. Sometimes,
    there are some unit tests, but most of the other tests are still at a very high
    level (*see Figure 12.2*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Example test portfolio'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17827_12_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.2 – Example test portfolio
  prefs: []
  type: TYPE_NORMAL
- en: 'These high-level tests might be automated or manual. But still, it is not a
    test portfolio that will help you to release continuously with high quality. To
    achieve continuous quality, you must shift left your test portfolio (*see Figure
    12.3*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – Shift left testing'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17827_12_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.3 – Shift left testing
  prefs: []
  type: TYPE_NORMAL
- en: 'This is not an easy task. Here are some principles that help with shift left
    testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ownership**: The team is responsible for QA and the tests are developed alongside
    the code – preferably with the test-first approach. QA engineers should be included
    in the team.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shift left**: Tests should always be written at the lowest level possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Write once – execute everywhere**: Tests should be executed in all environments,
    even in production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test code is production code**: The same quality standards that apply to
    normal code apply to test code. No shortcuts should be allowed here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**You code it – you test it**: As a developer, you are responsible for the
    quality of your code, and you must make sure that all the tests are in place to
    ensure this quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In 2013, a testing manifesto was created that describes the transformation
    of the QA role (*Sam Laing, 2015*):'
  prefs: []
  type: TYPE_NORMAL
- en: Testing throughout *over* testing at the end
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preventing bugs *over* finding bugs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing understanding *over* checking functionality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the best system *over* breaking the system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Team responsibility for quality *over* tester responsibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This sounds easy, but it isn't. Developers have to learn to think like testers
    and testers have to learn to think like engineers. Selling the vision and establishing
    the change's sustainability is not an easy task.
  prefs: []
  type: TYPE_NORMAL
- en: Test-driven development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The key to test automation is having a testable software architecture. To get
    one, you must start as early as possible – that is, in the inner loop, when developers
    write their code.
  prefs: []
  type: TYPE_NORMAL
- en: '**Test-driven development** (**TDD**) is a software development process where
    you write your automated test first and then the code that makes the test pass.
    It has been around for more than 20 years and the quality benefits have been proven
    in different studies (for example, *Müller, Matthias M.; Padberg, Frank, 2017*
    and *Erdogmus, Hakan; Morisio, Torchiano, 2014*). TDD not only has a big impact
    on the time that''s spent on debugging and overall code quality; it also has a
    big influence on solid and testable software design. That''s why it is also called
    **test-driven design**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TDD is simple. The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Add or modify a test**: Always start with a test. While writing the test,
    you **design** what your code will look like. There will be a time when your test
    will not compile because the classes and functions that you are calling do not
    exist yet. Most development environments support creating the necessary code right
    from within your test. This step is completed once your code compiles and the
    test can be executed. The test is supposed to fail. If the test passes, modify
    it or write a new test until it fails.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Run all tests**: Run all the tests and verify that only the new test fails.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Write code**: Write some simple code that makes the test pass. Always run
    all your tests to check if the test passes. The code does not need to be pretty
    in this stage and shortcuts are allowed. Just make the test pass. Bad code will
    give you an idea of what test you need next to ensure that the code gets better.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**All tests pass**: If all the tests pass, you have two options: write a new
    test or modify the existing one. Alternatively, you can refactor your code and
    tests.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Refactor**: Refactor the code and the tests. Since you have a solid test
    harness, you can do more extreme refactoring than you normally would without TDD.
    Make sure that you run all the tests after each refactoring. If one test fails,
    undo the last step and retry until the tests keep passing after the refactoring
    step. After a successful refactoring, you can start a new iteration with a new
    failing test.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 12.4 shows an overview of the TDD cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – The TDD cycle'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17827_12_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.4 – The TDD cycle
  prefs: []
  type: TYPE_NORMAL
- en: 'A good test follows the following pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Arrange**: Set up the necessary objects for the test and the **system under
    test** (**SUT**) itself – normally, this is a class. You can use **mocks** and
    **stubs** to simulate system behavior (to learn more about mocks and stubs, see
    *Martin Fowler, 2007*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Act**: Execute the code that you want to test.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Assert**: Verify the results, ensure that the state of the system is in the
    desired state, and ensure that the method has called the correct methods with
    the correct parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each test should be completely autarkic – that is, it shouldn't depend on a
    system state that's been manipulated by previous tests, and it can be executed
    in isolation.
  prefs: []
  type: TYPE_NORMAL
- en: TDD can also be used in pair programming. This is called **Ping Pong Pair Programming**.
    In this form of pair programming, one developer writes the test and the other
    writes the code that makes the test pass. This is a great pattern for pair programming
    and a good way to teach younger colleagues the benefits of TDD.
  prefs: []
  type: TYPE_NORMAL
- en: TDD has been around for so long and the teams that practice it gain so much
    value – and yet I have met many teams that are not using it. Some don't use it
    because their code runs on embedded systems, while others don't use it because
    their code depends on SharePoint classes that are hard to mock. But these are
    just excuses. There might be some plumbing code that cannot be tested, but when
    you write logic, you can always test it first.
  prefs: []
  type: TYPE_NORMAL
- en: Managing your test portfolio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With TDD, you should get a testable design in no time. And even in a brownfield
    environment, the number of automated tests will grow rapidly. The problem is that
    often, the quality of the tests is not optimal and with a growing test portfolio,
    you often get very long execution times and non-deterministic (flaky) tests. It
    is better to have fewer tests that are of higher quality. Long execution times
    hinder you from releasing quickly, and flaky tests produce unreliable quality
    signals and reduce the trust in your test suite (*see Figure 12.5*). With more
    QA maturity in the team, the quality of the test suite constantly rises – even
    if the amount of tests reduces after the first peak:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – Amount and quality of automated tests'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17827_12_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.5 – Amount and quality of automated tests
  prefs: []
  type: TYPE_NORMAL
- en: To actively manage your test portfolio, you should define ground rules for your
    tests and constantly monitor the number of tests and their execution time. As
    an example, let's look at the **taxonomy** that's used by a team at Microsoft
    for their test portfolio.
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests (Level 0)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we have in-memory unit tests with no external dependencies and no deployment.
    They should be fast with an average execution time of fewer than 60 milliseconds.
    Unit tests are co-located with the code under test.
  prefs: []
  type: TYPE_NORMAL
- en: With unit tests, you can't change to the system's state (such as the filesystem
    or its registry), queries to external data sources (web services and databases),
    or the mutexes, semaphores, stopwatches, and `Thread.sleep` operations.
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests (Level 1)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This level involves tests with more complex requirements that may depend on
    a lightweight deployment and configuration. The tests should still be very fast,
    and each test must run under 2 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: With integration tests, you can't have dependencies on other tests and store
    large amounts of data. You also can't have too many tests in one assembly as this
    prevents the tests from being executed in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Functional tests with data (Level 2)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Functional tests run against a testable deployment with test data. Dependencies
    on systems such as the authentication provider can be stubbed out and allow dynamic
    identities to be used. This means that there's an isolated identity for every
    test so that the test can be executed in parallel against a deployment without
    them impacting each other.
  prefs: []
  type: TYPE_NORMAL
- en: Production tests (Level 3)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Production tests run against production and require a full product deployment.
  prefs: []
  type: TYPE_NORMAL
- en: This is just an example, and your taxonomy may look different, depending on
    your programming language and product.
  prefs: []
  type: TYPE_NORMAL
- en: If you have defined your taxonomy, you can set up reporting and start to transform
    your test portfolio. Make sure that you make it easy to write and execute high-quality
    unit and integration tests first. Then, start analyzing your legacy tests – manual
    or automated – and check which ones you can throw away. Convert the others into
    good functional tests (*Level 2*). The last step is to write your tests for production.
  prefs: []
  type: TYPE_NORMAL
- en: 'The team at Microsoft started with 27,000 legacy tests (in orange) and reduced
    them to zero in 42 sprints (126 weeks). Most of the tests were replaced with unit
    tests; some were replaced with functional tests. Many were simply deleted, but
    there was a steady growth in unit tests, with there being over 40,000 in the end
    (*see Figure 12.6*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.6 – Test portfolio over time'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17827_12_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.6 – Test portfolio over time
  prefs: []
  type: TYPE_NORMAL
- en: See *Shift left to make testing fast and reliable*, in the *Further reading*
    section, for more information on how the team at Microsoft shifted their test
    portfolio left.
  prefs: []
  type: TYPE_NORMAL
- en: Eradicating flaky tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Non-deterministic** or **flaky tests** are tests that sometimes pass and
    sometimes fail with the same code (*Martin Fowler, 2011*). Flaky tests can destroy
    the trust in your test suite. This can lead to teams just ignoring red test results,
    or developers deactivating tests, thereby reducing the test coverage and reliability
    of the suite.'
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of reasons for flaky tests. Often, they are due to a lack of
    isolation. Many tests run in the same process on a machine – so each test must
    find and leave a clean state of the system. Another common reason is asynchronous
    behavior. Testing asynchronous code has its challenges as you never know which
    order the asynchronous tasks are executed in. Other reasons may include resource
    leaks or calls to remote resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different ways to deal with flaky tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '`git blame`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`git blame`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some companies quarantine flaky tests, but this also keeps you from collecting
    additional data as the test can't run. It's best practice to keep executing flaky
    tests but exclude them from the reporting.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn how GitHub or Google are dealing with flaky tests, read
    *Jordan Raine, 2020* or *John Micco, 2016*.
  prefs: []
  type: TYPE_NORMAL
- en: Code coverage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Code coverage** is a metric (in percent) that calculates the number of code
    elements that get called by tests, divided by the total amount of code elements.
    Code elements can be anything, but lines of code, code blocks, or functions are
    common.'
  prefs: []
  type: TYPE_NORMAL
- en: Code coverage is an important metric as it shows you what parts of your code
    are not covered by your test suite. I like to watch the code coverage before I
    finish a code change as I often forget to write tests for edge cases such as exception
    handling, or more complex statements such as lambda expressions. It's no problem
    to add these tests the moment you are coding – it's much harder to add them later.
  prefs: []
  type: TYPE_NORMAL
- en: But you should not focus on the absolute number as code coverage itself says
    nothing about the quality of the tests. It's better to have 70% code coverage
    with high-quality tests than 90% percent code coverage with low-quality tests.
    Depending on the programming language and frameworks you use, there might be some
    plumbing code that has a high effort in terms of testing but with very low value.
    Normally, you can exclude that code from code coverage calculations, but this
    is why the absolute value of code coverage is limited. However, measuring the
    value in each pipeline and focusing on new code helps improve the quality of your
    automated tests over time.
  prefs: []
  type: TYPE_NORMAL
- en: Shift right – testing in production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you start with automated testing, you rapidly see improved quality and a
    decline in the debugging effort of your engineers. But at some point, you must
    increase the effort tremendously to see a significant impact on quality. On the
    other hand, the time your tests need to execute slows down your release pipeline,
    especially if you add **performance tests** and **load tests** to the mix (*see
    Figure 12.7*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.7 – The impact of testing effort on quality and velocity'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17827_12_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.7 – The impact of testing effort on quality and velocity
  prefs: []
  type: TYPE_NORMAL
- en: It's impossible to release multiple times a day if your pipeline runs for more
    than 24 hours! The increased execution time of your pipeline also reduces your
    ability to roll forward quickly and deploy a fix if a bug occurs in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution to this is simple: **shift right** some of the tests to production.
    All the tests that you run in production do not impact your ability to release
    fast, and you don''t need performance or load tests as your code already has production
    load.'
  prefs: []
  type: TYPE_NORMAL
- en: However, there are some prerequisites to testing in production that increase
    the performance quality for your users instead of decreasing it. Let's take a
    look.
  prefs: []
  type: TYPE_NORMAL
- en: Health data and monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For testing in production, you must be constantly aware of the health of your
    application. This goes beyond normal logging. You need deep insights into how
    your application is operating. A good practice is to have test code that calls
    all dependent systems – such as the database, a Redis cache, or dependent REST
    services – and makes these tests available to your logging solutions. This way,
    you can have a constant **heartbeat** that indicates that all the systems are
    up and running and working together. If the test fails, you can have an alert
    that instantly notifies the team that something is wrong. You can also automate
    these alerts and have them trigger certain functions, such as activating a **circuit
    breaker**.
  prefs: []
  type: TYPE_NORMAL
- en: Circuit Breaker
  prefs: []
  type: TYPE_NORMAL
- en: A **circuit breaker** is a pattern that prevents an application from repeatedly
    trying to execute an operation that is likely to fail, allowing the application
    to continue with altered functionality without having to wait for the failing
    operation to succeed (see *Michael Nygard, 2018*).
  prefs: []
  type: TYPE_NORMAL
- en: Feature flags and canary releases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You don't want to test in production and cause a complete outage for all your
    customers. That's why you need feature flags, canary releases, a ring-based deployment,
    or a mix of those techniques (see [*Chapter 9*](B17827_09_Epub.xhtml#_idTextAnchor216)
    and [*Chapter 10*](B17827_10_Epub.xhtml#_idTextAnchor239)). It's important to
    gradually expose the changes so that if an outage occurs, you don't take down
    the complete production environment.
  prefs: []
  type: TYPE_NORMAL
- en: Business continuity and disaster recovery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another form of testing in production is **business continuity and disaster
    recovery** (**BCDR**) or failover testing. There should be a BCDR for every service
    or subsystem of your product, and you should execute a BCDR drill regularly. There
    is nothing worse than disaster recovery that is not working if your system is
    down. And you only know that it is working if you regularly test it.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory testing and usability testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Test automation does not imply that you should completely abandon manual testing.
    But the focus of manual tests shifts away from validating functionality and executing
    regressions tests manually on every release toward usability, fast and high-quality
    feedback, and bugs that are hard to find with structured test approaches.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploratory testing** was introduced by Cem Kaner in 1999 (*Kaner C., Falk
    J., H. Q. Nguyen, 1999*). It is an approach to testing that focuses simultaneously
    on discovery, learning, test design, and execution. It relies on the individual
    tester to uncover defects that can''t easily be discovered in other tests.'
  prefs: []
  type: TYPE_NORMAL
- en: There are many tools available that can facilitate exploratory testing. They
    help you record your sessions, take annotated screenshots, and often allow you
    to create a test case from the steps you have performed. Some extensions integrate
    with Jira, such as Zephyr and Capture, and there are browser extensions such as
    the Test and Feedback client for Azure Test Plans. The latter is free if you use
    it in standalone mode. These tools provide the high-quality feedback of stakeholders
    to developers – not only in terms of the defects that were discovered.
  prefs: []
  type: TYPE_NORMAL
- en: Other ways to gather feedback include using **usability testing** techniques
    – such as **hallway testing** or **guerrilla usability** – to evaluate your solution
    by testing it on new, unbiased users. A special form of usability testing is A/B
    testing, which we'll cover in more detail in *Chapter 19*, *Experimentation and
    A/B Testing with GitHub*.
  prefs: []
  type: TYPE_NORMAL
- en: The important part here is that all these tests can be executed in production.
    You should not have any manual tests in your CI/CD pipeline. Release fast and
    allow manual testing in production using feature flags and canary releases.
  prefs: []
  type: TYPE_NORMAL
- en: Fault injection and chaos engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you want to level up testing in production, you can practice **fault injection**
    – also known as **chaos engineering**. This means that you inject faults into
    your production system to see how it behaves under pressure and if your failover
    mechanisms and circuit breakers work. Possible faults could include high CPU load,
    high memory usage, disk I/O pressure, low disk space, or a service or entire machine
    being shut down or rebooted. Other possibilities include processes being killed,
    the system's time being changed, network traffic being dropped, latency being
    injected, and DNS servers being blocked.
  prefs: []
  type: TYPE_NORMAL
- en: Practicing chaos engineering makes your system resilient. You cannot compare
    this to classical load or performance testing!
  prefs: []
  type: TYPE_NORMAL
- en: Different tools can help you with chaos engineering. **Gremlin** ([https://www.gremlin.com/](https://www.gremlin.com/)),
    for example, is an agent-based SaaS offering that supports most cloud providers
    (Azure, AWS, and Google Cloud) and all operating systems. It can also be used
    with Kubernetes. **Chaos Mesh** ([https://chaos-mesh.org/](https://chaos-mesh.org/))
    is an open source solution that's specialized for Kubernetes. **Azure Chaos Studio**
    ([https://azure.microsoft.com/en-us/services/chaos-studio](https://azure.microsoft.com/en-us/services/chaos-studio))
    is a solution that's specialized for Azure. What tool is best for you depends
    on the platforms that you support.
  prefs: []
  type: TYPE_NORMAL
- en: Chaos engineering can be very effective and make your systems resilient, but
    it should be limited to canary environments that have little or no customer impact.
  prefs: []
  type: TYPE_NORMAL
- en: Tests and compliance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most **compliance** standards, such as **ISO26262** for automotive or **GAMP**
    for pharma, follow the **V-Model** as a development process. The V-Model requires
    the user and system requirements to be decomposed and specifications to be created
    at different levels of detail. This is the left-hand side of the *V*. It also
    requires all the levels to be validated to ensure that the system fulfills the
    requirements and specifications. This is the right-hand side of the *V*. Both
    sides can be seen in *Figure 12.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.8 – Validation in the V-Model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17827_12_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.8 – Validation in the V-Model
  prefs: []
  type: TYPE_NORMAL
- en: This model must be combined with risk analysis, which is performed at every
    level of detail. Many documents must be signed during the release phase. This
    leads to a slow waterfall process with long specification, development, and release
    phases.
  prefs: []
  type: TYPE_NORMAL
- en: 'But the standards are based on good practices – and if your practices are better
    than the ones in the standard, you can justify that in the audits. The standards
    don''t require you to do validation manually, nor do they say anything about the
    time of the phases. The solution is to automate all the validation logic and add
    the approvals as code reviews in the pull requests when you''re modifying the
    tests (shift left). Tests that you cannot automate must be moved to production
    (shift right). This way, you can automate the entire V and run through it multiple
    times a day:'
  prefs: []
  type: TYPE_NORMAL
- en: Add or modify a requirement (for example, an issue).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a pull request and link it to the issue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify your system design and architecture in your repository (for example,
    in markdown) or state that no modifications are needed in the pull request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write your unit tests (this is, your software design) and the code to implement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write or modify your functions, system, and integration tests.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Have all the necessary roles approve the pull requests and make sure that the
    approvals are stale if new changes are pushed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ship your changes to production and run your final tests there.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can also manage your risks as code. This way, you can integrate them into
    your automated process. If not, you can still attach the documents to the issue.
    This way, you have the end-to-end traceability for all your changes, all your
    necessary approvals, and all the validation steps completed. And you can still
    iterate fast and release to production regularly.
  prefs: []
  type: TYPE_NORMAL
- en: Test management in GitHub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unfortunately, GitHub doesn't have a great way to track your test runs and code
    coverage over time, nor can it help you detect or quarantine flaky tests. You
    can execute your tests as part of your workflow and can signal back the result
    – but for reporting, you have to rely on your test tooling.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good solution that integrates well with GitHub is **Testspace** (https://www.testspace.com/).
    It is a SaaS offering and is free for open source projects. It''s easy to set
    up – just install the extension from the marketplace (https://github.com/marketplace/testspace-com),
    select the plan you want, and grant access to your repositories. Then, add the
    following step to your workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If your repository is private, then you must create a token in *Testspace*
    and add it as a secret to that step as well: `token: ${{ secrets.TESTSPACE_TOKEN
    }}`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, you must add a step to push your test and code coverage results to *Testspace*
    after the step that executes the tests. You can use glob syntax to specify files
    in dynamic folders. Make sure that you execute the step, even if an error occurs
    (`if: ''!cancelled()''`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*Testspace* provides reliable detection for flaky tests. It has a *Build Bot*
    that sends you a notification if new results arrive. You can comment on the results
    by answering the email (*see Figure 12.9*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.9 – Notification from Testspace about your build results'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17827_12_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.9 – Notification from Testspace about your build results
  prefs: []
  type: TYPE_NORMAL
- en: 'It automatically integrates as a check into your pull request (*see Figure
    12.10*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.10 – Testspace integrates into your pull request checks'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17827_12_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.10 – Testspace integrates into your pull request checks
  prefs: []
  type: TYPE_NORMAL
- en: 'The UI of *Testpace* doesn''t look very fancy, but it has really rich reports
    and a ton of functionality (*see Figure 12.11*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.11 – Rich reports of your test metrics'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17827_12_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.11 – Rich reports of your test metrics
  prefs: []
  type: TYPE_NORMAL
- en: If you don't have a solution for test management yet, you can try *Testspace*.
    If you already have one, then it should be straightforward to integrate it into
    your workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The two pilot teams at **Tailwind Gears** have achieved a much higher **delivery
    lead time** and **deployment frequency** thanks to the DevOps practices that have
    been applied. The **mean time to restore** is also much better because the release
    pipelines help ship fixes faster. However, the **change failure rate** has dropped.
    Releasing more frequently also means that more deployments fail and finding bugs
    in the code is hard. The quality signals that come from the automated test suites
    are just not reliable enough and fixing one bug often introduces another bug in
    another module. There are still many parts of the application that need manual
    testing – but with one QA engineer in the team, this was not an option. So, some
    of these parts have been replaced with UI tests, while others have just been dropped.
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate the test portfolio, the teams must introduce a test taxonomy and
    include reporting in their pipelines. The QA engineers in the team are responsible
    for the taxonomy and the reports show that there are way too many functional and
    UI tests – and not enough unit tests. Many of the engineers are still not convinced
    that TDD would save them time and that it is possible to develop with TDD in certain
    cases, especially when the team is developing the embedded software. The teams
    decide to book a TDD training session together to learn about and practice TDD.
  prefs: []
  type: TYPE_NORMAL
- en: After that, all the new code is written in TDD with a **code coverage** of 90%
    (at a minimum) for the new code. The teams also spend 30% of their time every
    sprint eradicating **flaky tests** and rewriting tests at a lower level.
  prefs: []
  type: TYPE_NORMAL
- en: To discover flaky tests, the teams run reliability runs on pipelines with green
    tests. Flaky tests have the highest priority. After that, the team picks the tests
    with the longest execution time and decides what to do for each test. Most of
    the tests get converted into unit tests, though some get converted into integration
    tests. Some of the tests can be deleted as they bring no additional value.
  prefs: []
  type: TYPE_NORMAL
- en: Structured manual tests get replaced completely by **exploratory testing**.
    If anything is found in these sessions, a unit test is created before they're
    fixed.
  prefs: []
  type: TYPE_NORMAL
- en: The team that runs the web application also includes a new test category with
    tests that get executed in production. They implement **application performance
    monitoring** and collect many metrics so that they're aware of the health of the
    application in all environments. They also perform their first BCDR drills once
    per sprint to get started with **testing in production** and **chaos engineering**.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to accelerate your software delivery by shifting
    testing to the left via test automation and then to the right with testing in
    production and chaos engineering. This way, you can release at a fast pace without
    making compromises in terms of quality. Finally, you learned how to manage your
    test portfolio, eradicate flaky tests, and make your application more resilient
    by injecting faults and chaos.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to shift left security and implement
    DevSecOps practices into your development process.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following references were used in this chapter to help you learn more about
    the topics that were discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Forsgren N., Humble, J., & Kim, G. (2018). *Accelerate: The Science of Lean
    Software and DevOps: Building and Scaling High Performing Technology Organizations*
    (1st ed.) [E-book]. IT Revolution Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eran Kinsbruner (2018), *Continuous Testing for DevOps Professionals: A Practical
    Guide From Industry Experts* (Kindle Edition). CreateSpace Independent Publishing
    Platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sam Laing (2015), *The Testing Manifesto*, [https://www.growingagile.co.za/2015/04/the-testing-manifesto/](https://www.growingagile.co.za/2015/04/the-testing-manifesto/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolfgang Platz, Cynthia Dunlop (2019), *Enterprise Continuous Testing: Transforming
    Testing for Agile and DevOps* (Kindle Edition), Independently published.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tilo Linz (2014): *Testing in Scrum* (E-book), Rocky Nook.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaner C., Falk J., H. Q. Nguyen (1999), *Testing Computer Software* (2nd Edition)
    Wiley.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roy Osherove (2009), *The Art of Unit Testing* (1st edition), Manning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Martin Fowler (2007), *Mocks Aren't Stubs* [https://martinfowler.com/articles/mocksArentStubs.html](https://martinfowler.com/articles/mocksArentStubs.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Müller, Matthias M.; Padberg, Frank (2017). *About the Return on Investment
    of Test-Driven Development* (PDF). Universität Karlsruhe, Germany.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Erdogmus, Hakan; Morisio, Torchiano (2014). *On the Effectiveness of Test-first
    Approach to Programming*. Proceedings of the IEEE Transactions on Software Engineering,
    31(1). January 2005\. (NRC 47445).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Shift left to make testing fast and reliable*: https://docs.microsoft.com/en-us/devops/develop/shift-left-make-testing-fast-reliable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Martin Fowler (2011), *Eradicating Non-Determinism in Tests*, [https://martinfowler.com/articles/nonDeterminism.html](https://martinfowler.com/articles/nonDeterminism.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jordan Raine (2020). *Reducing flaky builds by 18x*. [https://github.blog/2020-12-16-reducing-flaky-builds-by-18x/](https://github.blog/2020-12-16-reducing-flaky-builds-by-18x/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: John Micco (2016). *Flaky Tests at Google and How We Mitigate Them*. [https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html](https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Shift right to test in production*: [https://docs.microsoft.com/en-us/devops/deliver/shift-right-test-production](https://docs.microsoft.com/en-us/devops/deliver/shift-right-test-production).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Michael Nygard (2018). *Release It! Design and Deploy Production-Ready Software*
    (2nd Edition). O'Reilly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
