<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer160">&#13;
			<h1 id="_idParaDest-180" class="chapter-number"><a id="_idTextAnchor181"/>8</h1>&#13;
			<h1 id="_idParaDest-181"><a id="_idTextAnchor182"/>Security Reality Check</h1>&#13;
			<p>Security in general, and especially in Kubernetes, is an ironic thing. Everyone knows it’s important, yet it’s not held to the same necessity as, for example, developers. In fact, if you look at the ratio, there’s probably 1 security engineer to 100 developers. Environments aren’t secure out of the box, especially when it comes to access control, yet security is arguably one of the most overlooked pieces of Kubernetes. Because of the lack of security awareness around Kubernetes, this chapter is going to focus on a little bit of everything that you should be thinking about when securing a <span class="No-Break">Kubernetes environment.</span></p>&#13;
			<p>From a theoretical perspective, you’ll be learning how to think about security in Kubernetes. From a hands-on perspective, you’ll be learning not only how to implement security practices, but which tools and platforms <span class="No-Break">to use.</span></p>&#13;
			<p>When thinking about production, this chapter may very well be the most important one in this entire book. You must walk before you run, and therefore, you must learn how to use Kubernetes in production before you can secure it. The focus of <em class="italic">Chapters 1-7</em> was to get you to that point. This chapter, however, is all about taking things to the next level, and as with most areas of <strong class="bold">Information Technology</strong> (<strong class="bold">IT</strong>), that <em class="italic">next level</em> <span class="No-Break">is security.</span></p>&#13;
			<p>By the end of this chapter, you’ll know which practices to utilize when securing a Kubernetes environment from the cluster itself to the containerized applications running inside of the cluster. You’ll also know which tools and platforms to use to get the <span class="No-Break">job done.</span></p>&#13;
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>&#13;
			<ul>&#13;
				<li>Out-of-the-box <span class="No-Break">Kubernetes security</span></li>&#13;
				<li>Investigating <span class="No-Break">cluster security</span></li>&#13;
				<li>Understanding <strong class="bold">role-based access </strong><span class="No-Break"><strong class="bold">control</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">RBAC</strong></span><span class="No-Break">)</span></li>&#13;
				<li>Kubernetes resource (<span class="No-Break">object) security</span></li>&#13;
				<li><span class="No-Break">Kubernetes Secrets</span></li>&#13;
			</ul>&#13;
			<p class="callout-heading">Important note</p>&#13;
			<p class="callout">Much as with all other engineering-related books and research analyst analyses, the figures/percentages used within this chapter are based on various experiences in the field. In this book, if there are figures that do not have specific associated data sources, the data is collated from the production experiences of the author, <span class="No-Break">Michael Levan.</span></p>&#13;
			<h1 id="_idParaDest-182"><a id="_idTextAnchor183"/>Technical requirements</h1>&#13;
			<p>For this chapter, as with most of the chapters in this book, you will need a Kubernetes cluster running. Although you can run these tests on something such as Minikube, it’s highly recommended to create a Kubeadm cluster or a Kubernetes managed service cluster in the cloud with something such as <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>), <strong class="bold">Amazon Elastic Kubernetes Service</strong> (<strong class="bold">Amazon EKS</strong>), or <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>). The reason why is that you should see what it’s truly like from a production perspective to run Kubernetes security tests, which will open your eyes to see how secure (or insecure) it is out of the box and what you can do to mitigate <span class="No-Break">those risks.</span></p>&#13;
			<p>If you want to deploy a Kubeadm cluster, check out this Git repo <span class="No-Break">for help:</span></p>&#13;
			<p><a href="https://github.com/AdminTurnedDevOps/Kubernetes-Quickstart-Environments/tree/main/Bare-Metal/kubeadm%0D"><span class="No-Break">https://github.com/AdminTurnedDevOps/Kubernetes-Quickstart-Environments/tree/main/Bare-Metal/kubeadm</span></a></p>&#13;
			<p>For the overall code used in this chapter, you can find <span class="No-Break">it here:</span></p>&#13;
			<p><a href="https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch8"><span class="No-Break">https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch8</span></a><a href="https://github.com/AdminTurnedDevOps/Packt/tree/main/50-Kubernetes-concepts-every-DevOps-Engineer-should-know/Ch8%0D"/></p>&#13;
			<h1 id="_idParaDest-183"><a id="_idTextAnchor184"/>Out-of-the-box Kubernetes security</h1>&#13;
			<p>At this point in time, there are<a id="_idIndexMarker501"/> two typical groups of people—those who are so incredibly new to Kubernetes and those who are as close to an <em class="italic">expert</em> <span class="No-Break">as possible.</span></p>&#13;
			<p>With the group that’s new to Kubernetes, they’re just trying to understand the breakdown of the environment. They aren’t even at the stage of thinking about <span class="No-Break">security yet.</span></p>&#13;
			<p>With the group that’s advanced—yes, they’re implementing security practices. The problem is that the advanced group is extremely small compared to the group that’s new <span class="No-Break">to Kubernetes.</span></p>&#13;
			<p>Then, there are the engineers that are somewhat in between. They aren’t super new, but they aren’t ridiculously advanced either. This is the group that a lot of engineers fall into, and quite frankly, the group that’s somewhat in between is just starting to think <span class="No-Break">about security.</span></p>&#13;
			<p>As with most platforms, nothing<a id="_idIndexMarker502"/> is 100% secure out of the box. In fact, regardless of how much time you spend to secure an environment, it will never be 100%. The whole goal of security is to mitigate as much risk as possible, but you’ll never be able to mitigate 100% of <span class="No-Break">the risk.</span></p>&#13;
			<p>From a theoretical perspective, let’s talk about a few things around overall security and <span class="No-Break">Kubernetes security.</span></p>&#13;
			<h2 id="_idParaDest-184"><a id="_idTextAnchor185"/>Security breakdown</h2>&#13;
			<p>Cybersecurity by definition is the protection of systems and networks from system disclosure. This means the <a id="_idIndexMarker503"/>protection of anything from the physical server/computer itself to the operating system to any data and metadata on the server/computer or network. If you think about it, that’s a lot of information. How many emails do you think get sent through Gmail per day? The specific number for Gmail isn’t certain, but for all email providers, the number is collectively 319.6 billion (with a B). Thinking about it from a theoretical but most likely accurate perspective, it’s safe to guess that at least 25% of that <span class="No-Break">is Gmail.</span></p>&#13;
			<p><span class="No-Break">The point?</span></p>&#13;
			<p>Emails alone contain a massive amount of information, but what about everything else? Information getting sent through networks from one country to another; data on hard drives: there’s a lot that falls into the <em class="italic">protection of systems </em><span class="No-Break"><em class="italic">and networks</em></span><span class="No-Break">.</span></p>&#13;
			<p>Norton states in a recent blog (<a href="https://us.norton.com/blog/emerging-threats/cybersecurity-statistics#">https://us.norton.com/blog/emerging-threats/cybersecurity-statistics#</a>) that there are roughly 2,200 cybersecurity attacks per day. To be honest, that seems a bit low. However, even if that number is accurate, that’s 800,000 cyberattacks per year. It’s certainly no <span class="No-Break">small number.</span></p>&#13;
			<p>With that knowledge, as engineers, we must prepare our systems and networks for such types of behavior. As the cloud continues to grow and Kubernetes becomes more mainstream, there will be more attacks directly related <span class="No-Break">to Kubernetes.</span></p>&#13;
			<p>As discussed in the opening of this section, the idea of security isn’t to stop all risks. The truth is, you’ll never be able to stop everything. The security tools, platforms, and engineers that focus <a id="_idIndexMarker504"/>on security implementations have one goal in mind—to stop as many security threats as possible. If a system is secure, the operating system may not be. If the operating system is secure, the network may not be. If the network is secure, the applications may not be… and around and around we go. Security is something that can never be 100%, but engineers can take precautions to get as close to 100% <span class="No-Break">as possible.</span></p>&#13;
			<p>Thinking about everything in this section, the question comes back to this: <em class="italic">What is security?</em> In short, it’s a method of <span class="No-Break">protecting data.</span></p>&#13;
			<h2 id="_idParaDest-185"><a id="_idTextAnchor186"/>Kubernetes security</h2>&#13;
			<p>The <em class="italic">State Of Kubernetes</em> security report from Red Hat (<a href="https://www.redhat.com/en/resources/state-kubernetes-security-report">https://www.redhat.com/en/resources/state-kubernetes-security-report</a>) highlights security issues <a id="_idIndexMarker505"/>directly related to the Kubernetes <span class="No-Break">security landscape:</span></p>&#13;
			<ul>&#13;
				<li>93% of respondents experienced at least 1 security incident in their Kubernetes environments in the last <span class="No-Break">12 months.</span></li>&#13;
				<li>More than half of the respondents (55%) have had to delay an application rollout because of <span class="No-Break">security concerns.</span></li>&#13;
				<li>Around 70% of security issues in Kubernetes are due to misconfigurations (according to Gartner, <span class="No-Break">it’s 99%).</span></li>&#13;
			</ul>&#13;
			<p>When you look at these statistics from a security report coming right from Red Hat, there’s a trend that everyone can easily see—security is a huge issue in the <span class="No-Break">Kubernetes space.</span></p>&#13;
			<p>The truth is, as many engineers and executives will attest, security is an absolute mess in the Kubernetes space right now. There’s no specific reason why, but there’s an educated guess as to why. If you look at the preceding statistics from Red Hat stating that 70% of security issues are due to misconfigurations, that means the primary reason is that engineers are still trying to figure <span class="No-Break">out Kubernetes.</span></p>&#13;
			<p>As you’ve learned about in this book, and as I’m sure you’ve seen online, almost everyone is still trying to figure out Kubernetes. There’s no expert in Kubernetes because the landscape changes every day. There’s no end goal to all things Kubernetes because it constantly changes. It’s not like a math equation where once you solve it, it’s complete. Once you <em class="italic">solve</em> Kubernetes, 10 more things around Kubernetes would emerge. Because of that, how could a configuration not be misconfigured most of the time? This goes <a id="_idIndexMarker506"/>especially for engineers that aren’t just focused on Kubernetes, but focused on many areas as well. How can engineers be as close to <em class="italic">experts</em> as possible within Kubernetes if it’s always changing? Misconfigurations are constantly bound <span class="No-Break">to occur.</span></p>&#13;
			<p>Because of that, the landscape of Kubernetes security is a mess. In fact, it most likely will be for a long time. It’s tough to secure something that’s <span class="No-Break">constantly changing.</span></p>&#13;
			<p>There’s some light at the end of the tunnel, though. As with all platforms and environments, there are best practices that you can follow. Again, thinking about security, what’s the goal? To not fix all problems, but to mitigate as many as possible. The purpose of this chapter is to do exactly that: to mitigate as many security risks inside of your Kubernetes environment <span class="No-Break">as possible.</span></p>&#13;
			<p>Let’s <span class="No-Break">jump in!</span></p>&#13;
			<h1 id="_idParaDest-186"><a id="_idTextAnchor187"/>Investigating cluster security</h1>&#13;
			<p>Taking Kubernetes out of the equation, let’s think about overall infrastructure and/or cloud security. At a <a id="_idIndexMarker507"/>high level, you have the network, the servers, the connections to the servers, user access, and ensuring that the applications installed on the servers are secure. In the world of cloud computing, you don’t have to worry about the physical security aspect. But if your clusters are in a data center, you do have to think about physical security. Locks on the data center rack cages ensure that no one can plug in any old USB key and that no one can literally take a server out of the rack and walk away <span class="No-Break">with it.</span></p>&#13;
			<p>Server security is a combination of what’s running inside and on the server—the applications running, programs that are being executed, and the overall operating system itself. Let’s say, for example, you’re running an older version of Ubuntu. Chances are you should absolutely check and confirm that there are no security holes. That’s still very important for any Kubernetes cluster running on Ubuntu. However, Kubernetes has its own set <span class="No-Break">of standards.</span></p>&#13;
			<p>From a networking perspective, security still holds true in Kubernetes as well as in any other environment. If you have a frontend or backend Kubernetes service that’s accepting traffic from <a id="_idIndexMarker508"/>anywhere, that essentially means you have a blanket open firewall. If you’re not encrypting Pod-to-Pod and/or service-to-service communication <a id="_idIndexMarker509"/>with something such as a service mesh or a security-centric <strong class="bold">Container Network Interface</strong> (<strong class="bold">CNI</strong>), you could open yourself up to <span class="No-Break">more risks.</span></p>&#13;
			<p>For example, Kubernetes by definition is an API. As with all APIs, there can be security risks. That means one of the biggest security focus points is to ensure that the Kubernetes API version that you’re currently on doesn’t have a major security risk as that could literally take down your <span class="No-Break">entire environment.</span></p>&#13;
			<p>A big portion of Kubernetes security is benchmarks and other automated testing, which you’ll learn about in <span class="No-Break">this section.</span></p>&#13;
			<h2 id="_idParaDest-187"><a id="_idTextAnchor188"/>Cluster hardening and benchmarks</h2>&#13;
			<p>The <strong class="bold">Center for Internet Security</strong> (<strong class="bold">CIS</strong>) has been the de facto standard of hardening systems for years. CIS <a id="_idIndexMarker510"/>benchmarks are a set of globally<a id="_idIndexMarker511"/> identified standards and best practices when it comes to helping engineers set up their security defenses. Whether it’s in the cloud, on-prem, or a specific application/tool, there’s a best practice for it, and that’s exactly what CIS helps you <span class="No-Break">figure out.</span></p>&#13;
			<p>Because CIS is essentially a list of best practices, you have to imagine that there are thousands of different best practices spread across platforms and environments. If you think about a Linux distro, such as Ubuntu, there are specific best practices for that distro alone. If you think about across<a id="_idIndexMarker512"/> an entire platform such as <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>), there are even more <span class="No-Break">best practices.</span></p>&#13;
			<p>As you look at CIS in general, you’ll see that there are a ton of prepopulated CIS environments. For example, in AWS, there <a id="_idIndexMarker513"/>are CIS-hardened <strong class="bold">Amazon Machine </strong><span class="No-Break"><strong class="bold">Images</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">AMIs</strong></span><span class="No-Break">):</span></p>&#13;
			<div>&#13;
				<div id="_idContainer132" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_01.jpg" alt="Figure 8.1 – Hardened AMI&#13;&#10;" width="796" height="657"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Hardened AMI</p>&#13;
			<p>In other clouds, such as <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>) or Azure, there’s the same thing. Even on phones <a id="_idIndexMarker514"/>such as an iPhone, there are <span class="No-Break">CIS benchmarks:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer133" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_02.jpg" alt="Figure 8.2 – iOS hardening&#13;&#10;" width="696" height="402"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – iOS hardening</p>&#13;
			<p>CIS can literally be an entire book in itself, so here’s the takeaway—CIS benchmarks are a list of best practices and standards to follow from a security perspective across systems, platforms, apps, <span class="No-Break">and environments.</span></p>&#13;
			<p>Because of the popularity of Kubernetes, in 2017, CIS worked with the community to create a benchmark specifically <span class="No-Break">for Kubernetes:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer134" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_03.jpg" alt="Figure 8.3 – Securing Kubernetes&#13;&#10;" width="860" height="453"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Securing Kubernetes</p>&#13;
			<p>There are even CIS benchmarks for specific Kubernetes environments, such <span class="No-Break">as GKE.</span></p>&#13;
			<p>As you go through this chapter, and as you go through your Kubernetes security journey in general, a lot of tools and platforms you’ll see that do things such as container image scanning and cluster scanning use CIS benchmarks. Platforms such as Checkov, <strong class="source-inline">kube-bench</strong>, Kubescape, and a few <a id="_idIndexMarker515"/>of the other popular tools in the security space all scan against CIS and the <strong class="bold">National Vulnerability </strong><span class="No-Break"><strong class="bold">Database</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">NVD</strong></span><span class="No-Break">).</span></p>&#13;
			<p>You can download <a id="_idIndexMarker516"/>the latest Kubernetes CIS benchmark for free. You just need to put in your name and email <span class="No-Break">at </span><span class="No-Break">https://www.cisecurity.org/benchmark/kubernetes</span><span class="No-Break">.</span></p>&#13;
			<h3>Going over the Kubernetes CIS benchmark</h3>&#13;
			<p>The CIS benchmarks in<a id="_idIndexMarker517"/> Kubernetes is a huge PDF that you can download and go through to ensure that how you’re implementing a Kubernetes environment is up to the best standards and best practices possible for the Kubernetes API version that <span class="No-Break">you’re running.</span></p>&#13;
			<p>Let’s learn how<a id="_idIndexMarker518"/> to download the PDF for the Kubernetes CIS benchmark. Follow <span class="No-Break">these steps:</span></p>&#13;
			<ol>&#13;
				<li>Go to this link and fill in your <span class="No-Break">information: </span><a href="https://www.cisecurity.org/benchmark/kubernetes"><span class="No-Break">https://www.cisecurity.org/benchmark/kubernetes</span></a><span class="No-Break">.</span></li>&#13;
				<li>After the information is filled in, you should get an email to download the PDFs. There are going to be a lot, so search for <strong class="source-inline">Kubernetes</strong>. You should then see all the <span class="No-Break">Kubernetes benchmarks.</span></li>&#13;
				<li>Choose the first one, which at the time of writing this, is for Kubernetes API version 1.23, and click the orange <strong class="bold">Download </strong><span class="No-Break"><strong class="bold">PDF</strong></span><span class="No-Break"> button:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer135" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_04.jpg" alt="Figure 8.4 – Kubernetes CIS information&#13;&#10;" width="874" height="685"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Kubernetes CIS information</p>&#13;
			<p>There are 302 pages, so the reality is you probably don’t want to read through it all, especially after reading this chapter (or maybe you do!). Skim through it and search for things that you find interesting. I like the part about Kubernetes Secrets where it explicitly says that you should think about an external <span class="No-Break">Secrets store.</span></p>&#13;
			<h3>A note about general server hardening</h3>&#13;
			<p>Server hardening should be an absolute priority across any environment. Whether you’re running Windows <a id="_idIndexMarker519"/>servers, Linux servers, or a mixture of both, hardening your systems is the key to mitigating as much security vulnerability at the system level <span class="No-Break">as possible.</span></p>&#13;
			<p>Because CIS has been around for such a long time, there’s a benchmark for almost everything. For example, here is a screenshot that showcases just a few <span class="No-Break">benchmarks available:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer136" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_05.jpg" alt="Figure 8.5 – Benchmark options&#13;&#10;" width="1198" height="615"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Benchmark options</p>&#13;
			<p>Even from a desktop perspective, you can run CIS benchmarks against certain applications and tools such as Google Chrome or <span class="No-Break">Microsoft Office:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer137" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_06.jpg" alt="Figure 8.6 – Desktop benchmark options&#13;&#10;" width="1174" height="601"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – Desktop benchmark options</p>&#13;
			<p>To see a full list, check <span class="No-Break">out </span><a href="https://www.cisecurity.org/cis-benchmarks/"><span class="No-Break">https://www.cisecurity.org/cis-benchmarks/</span></a><span class="No-Break">.</span></p>&#13;
			<h2 id="_idParaDest-188"><a id="_idTextAnchor189"/>System scanning</h2>&#13;
			<p>Although not Kubernetes-specific, or Kubernetes-scanning-specific, the truth is that if you’re running any<a id="_idIndexMarker520"/> type of system that is in your Kubernetes <a id="_idIndexMarker521"/>environment as a Control Plane, worker node, or both, you should run a system scan to ensure that the environment is properly configured. To do this, follow <span class="No-Break">these steps:</span></p>&#13;
			<ol>&#13;
				<li value="1">Download the CIS-CAT® Lite tool (it’s the free one) <span class="No-Break">from </span><a href="https://learn.cisecurity.org/cis-cat-lite"><span class="No-Break">https://learn.cisecurity.org/cis-cat-lite</span></a><span class="No-Break">.</span></li>&#13;
				<li>Next, extract it and open up the <span class="No-Break"><strong class="source-inline">Assessor-GUI</strong></span><span class="No-Break"> binary:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer138" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_07.jpg" alt="Figure 8.7 – GUI binary&#13;&#10;" width="958" height="478"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – GUI binary</p>&#13;
			<ol>&#13;
				<li value="3">Within the GUI tool, choose the <strong class="bold">Advanced</strong> option so that you can specify a <span class="No-Break">remote host:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer139" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_08.jpg" alt="Figure 8.8 – Advanced option&#13;&#10;" width="1195" height="528"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – Advanced option</p>&#13;
			<ol>&#13;
				<li value="4">Choose an option that gives you the ability to add a <span class="No-Break">remote system:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer140" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_09.jpg" alt="Figure 8.9 – Adding target system&#13;&#10;" width="1190" height="593"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – Adding target system</p>&#13;
			<ol>&#13;
				<li value="5">Type in the<a id="_idIndexMarker522"/> information of the host that you wish to scan, such as the IP address, name, system type, and username/password (or <span class="No-Break">SSH key):</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer141" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_10.jpg" alt="Figure 8.10 – Target system information&#13;&#10;" width="602" height="529"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – Target system information</p>&#13;
			<ol>&#13;
				<li value="6">As you can see in the next screenshot, there’s no specific scan for Kubernetes. Hopefully, this will be something that’s added in the future, although you’ll see later in this chapter<a id="_idIndexMarker523"/> that there are tools that specifically scan Kubernetes against CIS. In this case, you can choose the Ubuntu <span class="No-Break">Linux option:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer142" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_11.jpg" alt="Figure 8.11 – Available benchmarks&#13;&#10;" width="876" height="385"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11 – Available benchmarks</p>&#13;
			<ol>&#13;
				<li value="7">Click the <span class="No-Break"><strong class="bold">Save</strong></span><span class="No-Break"> button:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer143" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_12.jpg" alt="Figure 8.12 – Adding target system&#13;&#10;" width="1061" height="705"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12 – Adding target system</p>&#13;
			<ol>&#13;
				<li value="8">To ensure that you<a id="_idIndexMarker524"/> can properly scan the server, test <span class="No-Break">the connection:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer144" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_13.jpg" alt="Figure 8.13 – Specifying the Control Plane&#13;&#10;" width="1185" height="609"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.13 – Specifying the Control Plane</p>&#13;
			<ol>&#13;
				<li value="9">Click <strong class="bold">Next</strong>, and the<a id="_idIndexMarker525"/> testing <span class="No-Break">should begin:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer145" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_14.jpg" alt="Figure 8.14 – Running the installation&#13;&#10;" width="858" height="483"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.14 – Running the installation</p>&#13;
			<ol>&#13;
				<li value="10">You’ll then see a screen that asks you to pick a location to save the report. Leave this at its default settings and then start <span class="No-Break">the assessment:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer146" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_15.jpg" alt="Figure 8.15 – Assessment results&#13;&#10;" width="1046" height="875"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.15 – Assessment results</p>&#13;
			<p>Once the assessment is <a id="_idIndexMarker526"/>complete, you’ll see the report output in the default report location that you saw in the <span class="No-Break">prior step:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer147" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_16.jpg" alt="Figure 8.16 – Benchmark report&#13;&#10;" width="654" height="555"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.16 – Benchmark report</p>&#13;
			<h2 id="_idParaDest-189"><a id="_idTextAnchor190"/>Cluster network security</h2>&#13;
			<p>In Kubernetes, there are going to be two different types of network security—internal security and host <a id="_idIndexMarker527"/>security. Host security, of course, can be anything from your cloud VPC and security groups to on-prem firewalls running in your environment. Internal security is Pod security, service security, and, overall, how Kubernetes resources communicate with <span class="No-Break">each other.</span></p>&#13;
			<p>To keep things<a id="_idIndexMarker528"/> Kubernetes-centric, you’ll be learning about internal security and not host security. If you’d like to learn about host security, it’s highly recommended to take a look at how networking works as a whole and different security-related topics such as firewalls, firewall rules, port mappings, and how network routes <span class="No-Break">are configured.</span></p>&#13;
			<p>For the rest of this section, you’ll be <span class="No-Break">learning about:</span></p>&#13;
			<ul>&#13;
				<li>CNI <span class="No-Break">security methods</span></li>&#13;
				<li><strong class="bold">extended Berkeley Packet </strong><span class="No-Break"><strong class="bold">Filter</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">eBPF</strong></span><span class="No-Break">)</span></li>&#13;
			</ul>&#13;
			<h3>CNI security</h3>&#13;
			<p>Throughout this book, you’ve learned about service mesh, and in the next section, you’ll be learning about eBPF. There is, however, one other security approach you can take from a CNI perspective. As you look through different CNIs, you’ll see multiple different types of plugins. Some, such as Flannel, are for the beginner-level engineer that just needs to get something up and running. It doesn’t have any fancy features. It’s watered down and pretty <a id="_idIndexMarker529"/>basic, and that’s the purpose <span class="No-Break">of it.</span></p>&#13;
			<p>Then, you see other plugins, such as Calico, which is more of an advanced-level CNI and has a strong emphasis on security. In fact, you can actually encrypt Pod-to-Pod communication using Calico and WireGuard without even having to implement a service mesh, and that’s one of the main reasons that engineers implement a <span class="No-Break">service mesh.</span></p>&#13;
			<p>When you’re starting down your internal network security journey, one of the primary questions you should ask yourself concerns how you want to implement a CNI and why you want to implement it. Do you want a CNI that’s simply <em class="italic">ready to go</em> out of the box? Or do you want a CNI that may require a bit more configuration and time, but has the proper security components in place to make your life easier in the <span class="No-Break">long run?</span></p>&#13;
			<p>You can learn more about Calico and WireGuard <span class="No-Break">at </span><a href="https://projectcalico.docs.tigera.io/security/encrypt-cluster-pod-traffic"><span class="No-Break">https://projectcalico.docs.tigera.io/security/encrypt-cluster-pod-traffic</span></a><span class="No-Break">.</span></p>&#13;
			<h3>eBPF</h3>&#13;
			<p>eBPF can be an entire book in itself, but<a id="_idIndexMarker530"/> in short, it’s a way to remove the need to update Linux<a id="_idIndexMarker531"/> kernel code for certain programs to run. From a Kubernetes perspective, it can also remove the need for <span class="No-Break"><strong class="source-inline">kube-proxy</strong></span><span class="No-Break">’s responsibilities.</span></p>&#13;
			<p>Let’s focus on a few key parts when it comes to Kubernetes <span class="No-Break">and eBPF:</span></p>&#13;
			<ul>&#13;
				<li>Removal <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">kube-proxy</strong></span></li>&#13;
				<li><span class="No-Break">Easier scaling</span></li>&#13;
				<li><span class="No-Break">Security</span></li>&#13;
			</ul>&#13;
			<p><strong class="source-inline">kube-proxy</strong> has helped make Kubernetes usable. Without it, Kubernetes wouldn’t have worked. However, there’s a concern. <strong class="source-inline">kube-proxy</strong> uses iptables. Although iptables have been in Linux for a long time, it doesn’t scale very well. iptables rules are stored in a list, and when Pods establish a new connection to a Kubernetes Service, they go through every single iptable rule until the specific rule that’s being looked for is reached. Although that may not seem like a lot for a few rules, if you have thousands (which you most likely will), it’s a <span class="No-Break">performance concern.</span></p>&#13;
			<p>From a scalability perspective, as<a id="_idIndexMarker532"/> the number of Kubernetes Services (any type of Kubernetes Service) grows inside your cluster, the connection performance degrades. One of the reasons is that iptable rules are not incremental when you create them, which means that <strong class="source-inline">kube-proxy</strong> writes the whole table for every single update. It’s a huge <span class="No-Break">performance impact.</span></p>&#13;
			<p>Now that you<a id="_idIndexMarker533"/> know some theory behind why eBPF matters, which again, can be an entire book in itself, let’s dive into the hands-on implementation <span class="No-Break">of eBPF:</span></p>&#13;
			<ol>&#13;
				<li value="1">First, it all depends on the cluster you’re using. As with every other Kubernetes environment, if you’re using a managed Kubernetes Service in the cloud, using eBPF will vary based on the CNI you specify for the Kubernetes Managed <span class="No-Break">Service deployment.</span></li>&#13;
			</ol>&#13;
			<p>If you’re planning to run Kubeadm, for example, the following command is what you should use to remove <strong class="source-inline">kube-proxy</strong>. Even if you don’t use all the flags, ensure that you use the <strong class="source-inline">--skip-phases=addon/kube-proxy</strong> flag as this is needed so that <strong class="source-inline">kube-proxy</strong> doesn’t <span class="No-Break">get installed:</span></p>&#13;
			<pre class="console">&#13;
<strong class="bold">sudo kubeadm init --skip-phases=addon/kube-proxy --control-plane-endpoint $publicIP --apiserver-advertise-address $ip_address --pod-network-cidr=$cidr --upload-certs</strong></pre>&#13;
			<ol>&#13;
				<li value="2">Next, install Helm if you don’t already <span class="No-Break">have it:</span><pre class="console">&#13;
<strong class="bold">curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3</strong></pre><pre class="console">&#13;
<strong class="bold">chmod 700 get_helm.sh</strong></pre><pre class="console">&#13;
<strong class="bold">./get_helm.sh</strong></pre></li>&#13;
				<li>Add the Cilium <span class="No-Break">Helm repo:</span><pre class="console">&#13;
<strong class="bold">helm repo add cilium https://helm.cilium.io/</strong></pre></li>&#13;
				<li>Once the repo is added, you can install Cilium with Helm. Notice the flag to set the <span class="No-Break"><strong class="source-inline">kube-proxy</strong></span><span class="No-Break"> replacement:</span><pre class="console">&#13;
<strong class="bold">helm install cilium cilium/cilium \\</strong></pre><pre class="console">&#13;
<strong class="bold">--namespace kube-system \\</strong></pre><pre class="console">&#13;
<strong class="bold">--set kubeProxyReplacement=strict \\</strong></pre><pre class="console">&#13;
<strong class="bold">--set k8sServiceHost=ip_address_of_control_plane \\</strong></pre><pre class="console">&#13;
<strong class="bold">--set k8sServicePort=6443</strong></pre></li>&#13;
				<li>After a few <a id="_idIndexMarker534"/>minutes, check to see that the Cilium Pods are<a id="_idIndexMarker535"/> running successfully by running the <span class="No-Break">following command:</span><pre class="console">&#13;
<strong class="bold">kube get pods -n kube-system</strong></pre></li>&#13;
			</ol>&#13;
			<p>The output should look similar to the <span class="No-Break">following screenshot:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer148" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_17.jpg" alt="Figure 8.17 – Cilium Pods&#13;&#10;" width="517" height="248"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.17 – Cilium Pods</p>&#13;
			<p>Utilizing eBPF is still an extremely new topic and you may not see it through all environments. However, I can assure you that you’ll begin to see it more and more as eBPF becomes more popular and the benefits are <span class="No-Break">seen more.</span></p>&#13;
			<h2 id="_idParaDest-190"><a id="_idTextAnchor191"/>Upgrading the Kubernetes API</h2>&#13;
			<p>In every<a id="_idIndexMarker536"/> Kubernetes environment, you must keep track of the Kubernetes API. The last thing you want to do is have an insanely out-of-date API for any software/platform, but definitely for Kubernetes as well. All APIs, even Kubernetes, can eventually have a security hole that needs to be patched. You must ensure that your environment is ready <span class="No-Break">for it.</span></p>&#13;
			<p>When you keep track of a Kubernetes API, the inevitable will happen: you’ll have to upgrade the API. This isn’t just for features and to keep the system up to date, but from a security perspective, you don’t want to be too far behind as every old version of every piece of software stops getting patched and security <span class="No-Break">holes open.</span></p>&#13;
			<p>For the rest of this section, you’ll learn how to do a Kubernetes upgrade on a cluster running Kubeadm. If you don’t have Kubeadm, that’s fine—still follow along. Eventually, you’ll have to do an upgrade on a raw Kubernetes cluster, so it’s still good <span class="No-Break">to know.</span></p>&#13;
			<p class="callout-heading">Important note</p>&#13;
			<p class="callout">For any type of upgrade, especially in production, you should not only vigorously test the upgrade path, but you should back up your <span class="No-Break">environment components.</span></p>&#13;
			<h3>Upgrading Control Planes</h3>&#13;
			<p>Let’s begin by upgrading a Kubeadm<a id="_idIndexMarker537"/> Control Plane. Follow along with <span class="No-Break">these steps:</span></p>&#13;
			<ol>&#13;
				<li value="1">Run the <strong class="source-inline">upgrade</strong> command, which will show which upgrade path <span class="No-Break">is available:</span><pre class="console">&#13;
<strong class="bold">kubeadm upgrade plan</strong></pre></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer149" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_18.jpg" alt="Figure 8.18 – Kubernetes upgrade&#13;&#10;" width="1085" height="265"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.18 – Kubernetes upgrade</p>&#13;
			<ol>&#13;
				<li value="2">In the following output, you’ll see the target versions for every upgrade available, along with the command to run. The output will also show what the current Kubernetes API version is and which Control Plane components will <span class="No-Break">be upgraded:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer150" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_19.jpg" alt="Figure 8.19 – Upgrade path&#13;&#10;" width="749" height="300"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.19 – Upgrade path</p>&#13;
			<ol>&#13;
				<li value="3">Before running the<a id="_idIndexMarker538"/> upgrade, you’ll want to download the latest version of the API and confirm that Kubeadm gets put on hold to not upgrade all Control Plane components at once. Note that running the following command may result in you having to restart <span class="No-Break">the server:</span><pre class="console">&#13;
<strong class="bold">apt-mark unhold kubeadm &amp;&amp; apt-get update &amp;&amp; apt-get install -y kubeadm=1.25.x-00 &amp;&amp; apt-mark hold kubeadm</strong></pre></li>&#13;
				<li>Once complete, run the upgrade, <span class="No-Break">like so:</span><pre class="console">&#13;
<strong class="bold">kubeadm upgrade apply v1.25.x</strong></pre></li>&#13;
			</ol>&#13;
			<p>You’ll see an output similar to the <span class="No-Break">following screenshot:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer151" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_20.jpg" alt="Figure 8.20 – Upgrade output&#13;&#10;" width="1078" height="619"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.20 – Upgrade output</p>&#13;
			<p>Here is the second part <a id="_idIndexMarker539"/>of the output from the <span class="No-Break">preceding screenshot:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer152" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_21.jpg" alt="Figure 8.21 – Upgrade output continued&#13;&#10;" width="1079" height="306"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.21 – Upgrade output continued</p>&#13;
			<p>Now that you’ve upgraded the Control Plane, let’s learn how to upgrade <span class="No-Break">worker nodes.</span></p>&#13;
			<h3>Upgrading worker nodes</h3>&#13;
			<ol>&#13;
				<li value="1">Before running the <a id="_idIndexMarker540"/>upgrade, you’ll want to download the latest version and confirm that Kubeadm gets put on hold to not upgrade all Control Plane components at once. Note that running the following command may result in you having to restart <span class="No-Break">the server:</span><pre class="console">&#13;
<strong class="bold">apt-mark unhold kubeadm &amp;&amp; apt-get update &amp;&amp; apt-get install -y kubeadm=1.25.x-00 &amp;&amp; apt-mark hold kubeadm</strong></pre></li>&#13;
				<li>Next, upgrade the worker node, <span class="No-Break">like so:</span><pre class="console">&#13;
<strong class="bold">sudo kubeadm upgrade node</strong></pre></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer153" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_22.jpg" alt="Figure 8.22 – Node upgrade&#13;&#10;" width="1084" height="254"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.22 – Node upgrade</p>&#13;
			<p>That’s it! This process<a id="_idIndexMarker541"/> is a bit more straightforward compared to the <span class="No-Break">Control Plane.</span></p>&#13;
			<h3>Upgrading the kubelet</h3>&#13;
			<p>The last step is to<a id="_idIndexMarker542"/> upgrade the kubelet on both the Control Planes and worker nodes. <span class="No-Break">Follow along:</span></p>&#13;
			<ol>&#13;
				<li value="1">Run the following for the <span class="No-Break">kubelet upgrade:</span><pre class="console">&#13;
<strong class="bold">apt-mark unhold kubelet kubectl &amp;&amp; apt-get update &amp;&amp; apt-get install -y kubelet=1.25.x-00 kubectl=1.25.x-00 &amp;&amp; apt-mark hold kubelet kubectl</strong></pre></li>&#13;
				<li>Next, reload the kubelet, <span class="No-Break">like so:</span><pre class="console">&#13;
<strong class="bold">sudo systemctl daemon-reload</strong></pre><pre class="console">&#13;
<strong class="bold">sudo systemctl restart kubelet</strong></pre></li>&#13;
				<li>Run the following command, and you should now see that the Kubernetes cluster <span class="No-Break">is upgraded:</span><pre class="console">&#13;
<strong class="bold">kubectl get nodes</strong></pre></li>&#13;
			</ol>&#13;
			<p>Although this may not seem like something purely security related, and maybe it’s not, it’s still extremely important for security. You can’t have old versions of software lying around, just as you<a id="_idIndexMarker543"/> can’t have old versions of APIs lying around. For platform engineering teams, it’s <span class="No-Break">no different.</span></p>&#13;
			<h2 id="_idParaDest-191"><a id="_idTextAnchor192"/>Audit logging and troubleshooting</h2>&#13;
			<p>Kubernetes generates several logs. In fact, most Kubernetes resources have the metrics endpoint enabled. That means, everything and anything that’s generated with that Kubernetes resource—such as authentication, access, Pods going down, containers coming up, end users accessing it, and everything in <span class="No-Break">between—is recorded.</span></p>&#13;
			<p>The problem is that<a id="_idIndexMarker544"/> audit logging—and, sometimes, even the metrics server—isn’t enabled or even installed by default. You have the ability to install and configure audit logging in Kubernetes, but it’s not prepared out of <span class="No-Break">the box.</span></p>&#13;
			<p>What’s meant by that is the Kubernetes API for audit logging is available and <em class="italic">turned on</em> out of the box. It just won’t start to generate any logs that you can see because you first need to set up a policy via the <strong class="source-inline">audit.k8s.io/v1</strong> API, but policies don’t exist by default—it’s up to the engineer to create those policies. The policy can be anything from <em class="italic">show me everything</em> to <em class="italic">show me particular read actions on these particular Kubernetes resources</em>. It can be as high-level or as granular as <span class="No-Break">you’d like.</span></p>&#13;
			<p>There are a lot of policies, including audit logging, that can be turned on. In fact, it could most likely be a topic that spans an entire cluster itself. Because of that, we’ll stick with audit logging in this section. However, the following <a id="_idIndexMarker545"/>screenshot showcases the <strong class="bold">Open Web Application Security Project</strong> (<strong class="bold">OWASP</strong>) Top 10 for Kubernetes, and one of the top 10 is <span class="No-Break">proper logging:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer154" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_23.jpg" alt="Figure 8.23 – OWASP Top 10&#13;&#10;" width="1037" height="779"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.23 – OWASP Top 10</p>&#13;
			<p>You can see more information about it <span class="No-Break">here: </span><a href="https://github.com/OWASP/www-project-kubernetes-top-ten/blob/main/2022/en/src/K05-inadequate-logging.md"><span class="No-Break">https://github.com/OWASP/www-project-kubernetes-top-ten/blob/main/2022/en/src/K05-inadequate-logging.md</span></a><span class="No-Break">.</span></p>&#13;
			<p>Before jumping into the<a id="_idIndexMarker546"/> hands-on part, let’s talk about what audit logging is. Audit logging is recorded by the Kubernetes API server. With those <em class="italic">recordings</em>, which are just logs, it documents a chronological set in an order that shows the sequence of actions on a <span class="No-Break">Kubernetes cluster.</span></p>&#13;
			<p><span class="No-Break">It generates:</span></p>&#13;
			<ul>&#13;
				<li>Actions taken <span class="No-Break">by users</span></li>&#13;
				<li>Actions taken by <span class="No-Break">Kubernetes resources</span></li>&#13;
				<li>The Control <span class="No-Break">Plane itself</span></li>&#13;
			</ul>&#13;
			<p>Essentially, audit logs allow you to ask yourself questions such as the following: 1) <em class="italic">What happened?</em> 2) <em class="italic">When did it happen?</em> 3) <em class="italic">How did it happen?</em> No question should be left unanswered as you can retrieve everything and anything about a Kubernetes cluster via the <span class="No-Break">audit logs.</span></p>&#13;
			<p>With that, let’s learn how to<a id="_idIndexMarker547"/> set them up. Follow <span class="No-Break">these steps:</span></p>&#13;
			<ol>&#13;
				<li value="1">Create a network policy such as the one shown next. For the purposes of this section, you can store it <span class="No-Break">under </span><span class="No-Break"><strong class="source-inline">/etc/kubernetes/simple-policy.yaml</strong></span><span class="No-Break">:</span><pre class="console">&#13;
apiVersion: audit.k8s.io/v1</pre><pre class="console">&#13;
kind: Policy</pre><pre class="console">&#13;
rules:</pre><pre class="console">&#13;
- level: Metadata</pre></li>&#13;
			</ol>&#13;
			<p class="callout-heading">Important note</p>&#13;
			<p class="callout">If you’re on a managed Kubernetes Service, such as AKS or EKS, you’ll have to turn on audit logging in a different way, and it all depends on the server you’re using. However, you should still read through this section as you’ll end up coming across audit logs on bare-metal/VM environments (especially during this time when hybrid cloud is becoming far more popular) at some point on your <span class="No-Break">Kubernetes journey.</span></p>&#13;
			<ol>&#13;
				<li value="2">Next, open up the following location via Vim or an editor of your <span class="No-Break">choosing: </span><span class="No-Break"><strong class="source-inline">/etc/kubernetes/manifests/kube-apiserver.yaml</strong></span><span class="No-Break">.</span></li>&#13;
				<li>Add in the following code, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.24</em>. This will give you the ability to set audit log consumption and set how long the logs are kept, which path the output of the audit logs should go to, and where your audit <span class="No-Break">policy exists:</span><pre class="console">&#13;
- --audit-log-maxage=7</pre><pre class="console">&#13;
- --audit-log-maxbackup=2</pre><pre class="console">&#13;
- --audit-log-maxsize=50</pre><pre class="console">&#13;
- --audit-log-path=/var/log/audit.log</pre><pre class="console">&#13;
- --audit-policy-file=/etc/kubernetes/simple-policy.yaml</pre></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer155" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_24.jpg" alt="Figure 8.24 – Audit policy path&#13;&#10;" width="737" height="571"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.24 – Audit policy path</p>&#13;
			<ol>&#13;
				<li value="4">Under <strong class="source-inline">volumeMounts</strong>, add the following code, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.25</em>. For Kubernetes, the<a id="_idIndexMarker548"/> policy and the path for the audit logs need to be mounted in <span class="No-Break">the cluster:</span><pre class="console">&#13;
- mountPath: /etc/kubernetes/simple-policy.yaml</pre><pre class="console">&#13;
  name: audit</pre><pre class="console">&#13;
  readOnly: true</pre><pre class="console">&#13;
- mountPath: /var/log/audit.log</pre><pre class="console">&#13;
  name: audit-log</pre><pre class="console">&#13;
  readOnly: false</pre></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer156" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_25.jpg" alt="Figure 8.25 – Policy mount&#13;&#10;" width="465" height="116"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.25 – Policy mount</p>&#13;
			<ol>&#13;
				<li value="5">Under <strong class="source-inline">hostPath</strong>, add <a id="_idIndexMarker549"/><span class="No-Break">the following:</span><pre class="console">&#13;
- hostPath:</pre><pre class="console">&#13;
    path: /etc/kubernetes/simple-policy.yaml</pre><pre class="console">&#13;
    type: File</pre><pre class="console">&#13;
  name: audit</pre><pre class="console">&#13;
- hostPath:</pre><pre class="console">&#13;
    path: /var/log/audit.log</pre><pre class="console">&#13;
    type: FileOrCreate</pre><pre class="console">&#13;
  name: audit-log</pre></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer157" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_26.jpg" alt="Figure 8.26 – Policy host path&#13;&#10;" width="487" height="175"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.26 – Policy host path</p>&#13;
			<ol>&#13;
				<li value="6">Restart the<a id="_idIndexMarker550"/> kubelet by running the <span class="No-Break">following command:</span><pre class="console">&#13;
<strong class="bold">sudo systemctl restart kubelet</strong></pre></li>&#13;
				<li>Confirm that the kubelet is still running, <span class="No-Break">like so:</span><pre class="console">&#13;
<strong class="bold">kubectl get nodes</strong></pre></li>&#13;
			</ol>&#13;
			<p>You can now view the audit logs on the Control Plane at the path/location where you stored the <strong class="source-inline">audit.log</strong> file by executing the <span class="No-Break">following command:</span></p>&#13;
			<pre class="console">&#13;
tail -f /var/log/audit.log</pre>&#13;
			<p>You should see a bunch of log output. For security purposes, I haven’t included a screenshot showcasing <span class="No-Break">the output.</span></p>&#13;
			<p>As mentioned earlier, this type of configuration would be for a Kubeadm cluster or something on-prem. For the cloud, it’s going to be a bit different. However, it’s still important to understand this process. Remember—the cloud abstracts a lot away from engineers, but engineers must still understand the underlying components of a system to properly work <span class="No-Break">with it.</span></p>&#13;
			<h1 id="_idParaDest-192"><a id="_idTextAnchor193"/>Understanding RBAC</h1>&#13;
			<p>When it comes to users, groups, and<a id="_idIndexMarker551"/> service accounts, there are two questions you must ask yourself. The first is: <em class="italic">Who can access your cluster?</em> Which users, service accounts, and groups have the ability to run <strong class="source-inline">kubectl</strong> commands on the clusters in development, staging, and production? Which of those users have a Kubeconfig that gives them access to particular clusters? Which environments can they <span class="No-Break">connect to?</span></p>&#13;
			<p>The second question is: <em class="italic">What can they do once they’re inside the cluster?</em> Can they list Pods? Create Pods? See Ingress Controllers? Create Ingress Controllers? What types of Kubernetes resources can they interact with throughout <span class="No-Break">each environment?</span></p>&#13;
			<p>When you’re <a id="_idIndexMarker552"/>setting up a Kubernetes environment, you must also think about authentication and authorization. Who can access your cluster and what can they do? Further, you must think about what the users can do throughout each environment. For example, thinking about the single tenancy model that you learned about in a previous chapter, one engineer may have full admin access on one cluster and read-only access on another. With that, you must also think about multiple authorization methods in terms of which permissions you’re <span class="No-Break">giving people.</span></p>&#13;
			<p>In this section, you’re going to learn how to manage from a permissions perspective users, groups, and teams in Kubernetes <span class="No-Break">using RBAC.</span></p>&#13;
			<p>Please note that although this section is not huge, it should point you in the right direction in terms of how to think about RBAC and how to start <span class="No-Break">implementing it.</span></p>&#13;
			<h2 id="_idParaDest-193"><a id="_idTextAnchor194"/>What is RBAC?</h2>&#13;
			<p>RBAC, as with many other topics in this book (I’m a broken record at this point), can be an entire book in itself. Because of that, let’s do a brief theoretical explanation and then dive into the <span class="No-Break">hands-on piece.</span></p>&#13;
			<p>RBAC, by definition, is a way to ensure that users, groups, and service accounts only have the permissions that they need from an authorization perspective. RBAC does not do authentication—it does authorization. The authentication piece comes before RBAC. Once there’s a user, group, or service account created, then RBAC can jump into action and start <span class="No-Break">creating permissions.</span></p>&#13;
			<p>Within RBAC for Kubernetes, you have four primary resources that you want <span class="No-Break">to utilize:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break"><strong class="source-inline">Roles</strong></span></li>&#13;
				<li><span class="No-Break"><strong class="source-inline">ClusterRoles</strong></span></li>&#13;
				<li><span class="No-Break"><strong class="source-inline">RoleBindings</strong></span></li>&#13;
				<li><span class="No-Break"><strong class="source-inline">ClusterRoleBindings</strong></span></li>&#13;
			</ul>&#13;
			<p>You’ll learn more about them in the <span class="No-Break">upcoming sections.</span></p>&#13;
			<p>When you’re thinking about RBAC, think: <em class="italic">What am I allowing this person to do </em><span class="No-Break"><em class="italic">inside Kubernetes?</em></span></p>&#13;
			<p>One thing to keep in mind is that RBAC is typically the bane of every security engineer’s existence. It’s one of<a id="_idIndexMarker553"/> those topics in Kubernetes that makes everyone bang their head against a wall because it can start to become insanely complex, and there’s no central way to manage hundreds of RBAC roles and permissions. There are tools and platforms out there that are trying to mitigate this, such as Kubescape’s <span class="No-Break">RBAC Visualizer.</span></p>&#13;
			<p>To continue along with this chapter, you’ll need a user, group, or service account. Because Kubernetes doesn’t have an out-of-the-box method for creating users and groups, let’s use a <span class="No-Break">service account.</span></p>&#13;
			<p>Run the following command to create a new service account <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">miketest</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="console">&#13;
kubectl create sa miketest</pre>&#13;
			<p>Once the service account is created, it can be used for the <span class="No-Break">following sections.</span></p>&#13;
			<h2 id="_idParaDest-194"><a id="_idTextAnchor195"/>Roles and ClusterRoles</h2>&#13;
			<p><strong class="source-inline">Roles</strong> are permissions that you can give users, groups, and service accounts, and they are namespace<a id="_idIndexMarker554"/> scoped. Meaning, let’s say you create a role called <strong class="source-inline">readpods</strong>. That role would be tied to a namespace—for example, a namespace called <strong class="source-inline">ingress</strong>. That means the <strong class="source-inline">readpods</strong> role only works on the <strong class="source-inline">ingress</strong> namespace, and it’s not tied to any <span class="No-Break">other namespace.</span></p>&#13;
			<p>How about if you want a role/permissions for a user/group/service account that’s used across all namespaces through the cluster? That’s where <strong class="source-inline">ClusterRoles</strong> come into play. A <strong class="source-inline">ClusterRole</strong> is the same thing as a <strong class="source-inline">Role</strong>. The only difference is that it’s not <span class="No-Break">namespace scoped.</span></p>&#13;
			<p>Let’s dive in to learn how you can create <strong class="source-inline">Roles</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">ClusterRoles</strong></span><span class="No-Break">.</span></p>&#13;
			<h3>Roles</h3>&#13;
			<p>The following code snippet is an example of a <strong class="source-inline">Role</strong> that you can create. It’s scoped to the <strong class="source-inline">ingress</strong> namespace and sets <a id="_idIndexMarker555"/>read-only permissions for the Pod Kubernetes resource. Notice in the<a id="_idIndexMarker556"/> verbs that it’s all read permissions—<strong class="source-inline">get</strong>, <strong class="source-inline">watch</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">list</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
kind: Role&#13;
apiVersion: rbac.authorization.k8s.io/v1&#13;
metadata:&#13;
  namespace: ingress&#13;
  name: reader&#13;
rules:&#13;
- apiGroups: [""]&#13;
  resources: ["pods"]&#13;
  verbs: ["get", "watch", "list"]</pre>&#13;
			<p>Implementing the preceding <strong class="source-inline">Role</strong> will ensure that you have a proper role created to give a user/group/service account read-only permissions for Pods in the <span class="No-Break"><strong class="source-inline">ingress</strong></span><span class="No-Break"> namespace.</span></p>&#13;
			<h3>ClusterRoles</h3>&#13;
			<p>As with the preceding <strong class="source-inline">Role</strong>, the following <strong class="source-inline">ClusterRole</strong> creates a <strong class="source-inline">ClusterRole</strong> called <strong class="source-inline">reader</strong> for <a id="_idIndexMarker557"/>read-only permissions on Pods. The key<a id="_idIndexMarker558"/> difference is that it’s not scoped to a <span class="No-Break">particular namespace:</span></p>&#13;
			<pre class="source-code">&#13;
kind: ClusterRole&#13;
apiVersion: rbac.authorization.k8s.io/v1&#13;
metadata:&#13;
  name: reader-cluster&#13;
rules:&#13;
- apiGroups: [""]&#13;
  resources: ["pods"]&#13;
  verbs: ["get", "watch", "list"]</pre>&#13;
			<p>Implementing the preceding <strong class="source-inline">ClusterRole</strong> will ensure that you have a proper role created to give a user/group/service account read-only permissions for Pods across all namespaces in <span class="No-Break">the cluster.</span></p>&#13;
			<p>Next, let’s learn how to<a id="_idIndexMarker559"/> bind <strong class="source-inline">Roles</strong> to a <a id="_idIndexMarker560"/>particular <span class="No-Break">service account.</span></p>&#13;
			<h2 id="_idParaDest-195"><a id="_idTextAnchor196"/>RoleBindings and ClusterRoleBindings</h2>&#13;
			<p>A <strong class="source-inline">RoleBinding</strong> is a way <a id="_idIndexMarker561"/>that you tie/attach a <strong class="source-inline">Role</strong> to a user/group/service account. For example, let’s say you have a <strong class="source-inline">Role</strong> called <strong class="source-inline">podreaders</strong> and you <a id="_idIndexMarker562"/>want to tie/attach that role to the <strong class="source-inline">miketest</strong> service account. You would use a <strong class="source-inline">RoleBinding</strong> to perform <span class="No-Break">that action.</span></p>&#13;
			<p>Just as with <strong class="source-inline">Roles</strong> and <strong class="source-inline">ClusterRoles</strong>, the only difference is that <strong class="source-inline">RoleBindings</strong> are namespace scoped and <strong class="source-inline">ClusterRoleBindings</strong> are not and can be used throughout <span class="No-Break">the cluster.</span></p>&#13;
			<p>Let’s learn how to implement <strong class="source-inline">RoleBindings</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">ClusterRoleBindings</strong></span><span class="No-Break">.</span></p>&#13;
			<h3>RoleBinding</h3>&#13;
			<p>The following <strong class="source-inline">RoleBinding</strong> takes the <strong class="source-inline">Role</strong> that you created in the previous section and attaches<a id="_idIndexMarker563"/> it to the <strong class="source-inline">miketest</strong> service account. See how there’s a <strong class="source-inline">kind</strong> and the service account kind is specified? This is where you can <a id="_idIndexMarker564"/>specify a <strong class="source-inline">group</strong> or a <strong class="source-inline">user</strong>. It’s also scoped to the <strong class="source-inline">ingress</strong> namespace as this is not <span class="No-Break">a </span><span class="No-Break"><strong class="source-inline">ClusterRoleBinding</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: rbac.authorization.k8s.io/v1&#13;
kind: RoleBinding&#13;
metadata:&#13;
  name: reader-pod&#13;
  namespace: ingress&#13;
subjects:&#13;
- kind: ServiceAccount&#13;
  name: miketest&#13;
  apiGroup: ""&#13;
roleRef:&#13;
  kind: Role&#13;
  name: reader&#13;
  apiGroup: rbac.authorization.k8s.io</pre>&#13;
			<h3>ClusterRoleBinding</h3>&#13;
			<p>Much as with <a id="_idIndexMarker565"/>the preceding <strong class="source-inline">RoleBinding</strong>, the<a id="_idIndexMarker566"/> following <strong class="source-inline">ClusterRoleBinding</strong> will attach the <strong class="source-inline">miketest</strong> service account to the <strong class="source-inline">ClusterRole</strong> and reference the <span class="No-Break">following </span><span class="No-Break"><strong class="source-inline">ClusterRole</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: rbac.authorization.k8s.io/v1&#13;
kind: ClusterRoleBinding&#13;
metadata:&#13;
  name: read-pod-global&#13;
subjects:&#13;
- kind: ServiceAccount&#13;
  name: miketest&#13;
  apiGroup: ""&#13;
roleRef:&#13;
  kind: ClusterRole&#13;
  name: reader-cluster&#13;
  apiGroup: rbac.authorization.k8s.io</pre>&#13;
			<p>Now that you know about overall authentication and authorization permissions, it’s time to learn about overall Kubernetes resource security and the approaches that you can take out of the<a id="_idIndexMarker567"/> gate to ensure a successful <a id="_idIndexMarker568"/><span class="No-Break">security-centric deployment.</span></p>&#13;
			<h1 id="_idParaDest-196"><a id="_idTextAnchor197"/>Kubernetes resource (object) security</h1>&#13;
			<p>Throughout this chapter, you learned a little bit about Kubernetes resource security. Remember, Kubernetes resources can be anything from Pods to Ingress Controllers to Services. Essentially, anything <a id="_idIndexMarker569"/>running inside of the Kubernetes cluster that you’re reaching via the API is a <span class="No-Break">Kubernetes resource.</span></p>&#13;
			<p>In this section, you’re going to learn the top methods of today to secure Kubernetes resources within Kubernetes and by using <span class="No-Break">third-party tools.</span></p>&#13;
			<h2 id="_idParaDest-197"><a id="_idTextAnchor198"/>Pod security</h2>&#13;
			<p>When it comes to <a id="_idIndexMarker570"/>network security in a Kubernetes environment, there are two parts—the host network and the internal network. For the purposes of this section, we can’t go into host networking because every environment is going to be different. Whether it’s different physical hardware or virtual hardware setups, there’s no <em class="italic">one-size-fits-all</em> <span class="No-Break">network environment.</span></p>&#13;
			<p>However, there are a few helpful tips that work across <span class="No-Break">every environment:</span></p>&#13;
			<ol>&#13;
				<li value="1">Ensure that you have proper <span class="No-Break">firewall rules.</span></li>&#13;
				<li>Ensure that you’re implementing proper routing protocols and not just opening up the <span class="No-Break">entire network.</span></li>&#13;
				<li>Ensure that you have the proper port setup <span class="No-Break">in place.</span></li>&#13;
				<li>Ensure that you’re logging and observing <span class="No-Break">network traffic.</span></li>&#13;
			</ol>&#13;
			<p>For Kubernetes network security, there are <span class="No-Break">network policies.</span></p>&#13;
			<p>Network Policies are built into Kubernetes via the <strong class="source-inline">networking.k8s.io/v1</strong> API. Network Policies act like firewall rules for both Ingress and Egress traffic. However, network policies aren’t just about whitelisting or blacklisting IP addresses and ports. You can do much more with a policy. For example, you can block traffic from a specific network to a specific namespace, from a specific namespace, or to/from a specific application. Because of the vast number of options that come with Network Policies, you have plenty of options, but you’ll want to ensure that you’re setting up the right policies. One wrong accidental <strong class="source-inline">162.x.x.x</strong> instead of <strong class="source-inline">172.x.x.x</strong> can completely throw off the entire network workflow in a network policy and completely halt <span class="No-Break">application workloads.</span></p>&#13;
			<p>Let’s dive into what a network policy <span class="No-Break">looks like.</span></p>&#13;
			<p>To test this out, run the following Pods in your <span class="No-Break">Kubernetes environment:</span></p>&#13;
			<pre class="console">&#13;
kubectl run busybox1 --image=busybox --labels app=busybox1 -- sleep 3600&#13;
kubectl run busybox2 --image=busybox --labels app=busybox2 -- sleep 3600</pre>&#13;
			<p>The preceding new <a id="_idIndexMarker571"/>Pods will run a container image called <strong class="source-inline">busybox</strong>, which is a small form factor that’s usually used <span class="No-Break">for testing.</span></p>&#13;
			<p>Next, obtain the IP address of the Pods, <span class="No-Break">like so:</span></p>&#13;
			<pre class="console">&#13;
kubectl get pods -o wide</pre>&#13;
			<div>&#13;
				<div id="_idContainer158" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_27.jpg" alt="Figure 8.27 – Pod output&#13;&#10;" width="571" height="126"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.27 – Pod output</p>&#13;
			<p>Run a ping against the <span class="No-Break"><strong class="source-inline">busybox1</strong></span><span class="No-Break"> Pod:</span></p>&#13;
			<pre class="console">&#13;
kubectl exec -ti busybox2 -- ping -c3 ip_of_busybox_one</pre>&#13;
			<div>&#13;
				<div id="_idContainer159" class="IMG---Figure">&#13;
					<img src="Images/B19116_08_28.jpg" alt="Figure 8.28 – Ping output&#13;&#10;" width="563" height="115"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.28 – Ping output</p>&#13;
			<p>Now that you <a id="_idIndexMarker572"/>know there’s proper <strong class="bold">Internet Control Message Protocol</strong> (<strong class="bold">ICMP</strong>) communication, create<a id="_idIndexMarker573"/> a network policy that blocks all Ingress traffic to the <span class="No-Break"><strong class="source-inline">busybox1</strong></span><span class="No-Break"> Pod:</span></p>&#13;
			<pre class="source-code">&#13;
<strong class="bold">kubectl create -f - &lt;&lt;EOF</strong>&#13;
<strong class="bold">kind: NetworkPolicy</strong>&#13;
<strong class="bold">apiVersion: networking.k8s.io/v1</strong>&#13;
<strong class="bold">metadata:</strong>&#13;
<strong class="bold">  name: web-deny-all</strong>&#13;
<strong class="bold">spec:</strong>&#13;
<strong class="bold">  podSelector:</strong>&#13;
<strong class="bold">    matchLabels:</strong>&#13;
<strong class="bold">      app: busybox1</strong>&#13;
<strong class="bold">  ingress: []</strong>&#13;
<strong class="bold">EOF</strong></pre>&#13;
			<p>Run the ping against <span class="No-Break"><strong class="source-inline">busybox1</strong></span><span class="No-Break"> again:</span></p>&#13;
			<pre class="console">&#13;
kubectl exec -ti busybox2 -- ping -c3 ip_of_busybox_one</pre>&#13;
			<p>There should now be 100% <span class="No-Break">packet loss.</span></p>&#13;
			<p>If you’re in an environment where this didn’t work, such as a standard Minikube environment, the reason why it’s most likely not working is that you’re using a CNI that doesn’t have network policies enabled or doesn’t support <span class="No-Break">network policies.</span></p>&#13;
			<p>To find out how to enable network policies, you’ll need to do a quick search on how to implement network<a id="_idIndexMarker574"/> policies for your <span class="No-Break">specific CNI.</span></p>&#13;
			<h2 id="_idParaDest-198"><a id="_idTextAnchor199"/>Policy enforcement</h2>&#13;
			<p>In the previous section, you<a id="_idIndexMarker575"/> learned about security at the network layer, which is of course needed. After (or before) the network layer is the application layer, which is where policy enforcement around Pods and containers comes <span class="No-Break">into play.</span></p>&#13;
			<p>The whole idea behind policy enforcement is to give you the ability to protect your Pods, ensure best security practices, and set standards for <span class="No-Break">your organization.</span></p>&#13;
			<p>For example, one of the biggest best practices in production is to ensure that you’re not using the latest container image version in production. Instead, you always want to use a container image version that’s properly versioned and battle-tested for protection. With policy enforcement in Kubernetes, you can <span class="No-Break">accomplish that.</span></p>&#13;
			<p>Right now, the two biggest ways to implement policy enforcement are using <strong class="bold">Open Policy Agent</strong> (<strong class="bold">OPA</strong>) and Kyverno. They’re <a id="_idIndexMarker576"/>both the same from a policy enforcement perspective, but the biggest difference is that Kyverno only works inside of Kubernetes. Because of that, a lot of engineers are going toward using OPA so that they can use it throughout their environment and not just <span class="No-Break">in Kubernetes.</span></p>&#13;
			<p>Because of that, the hands-on section will be <span class="No-Break">using OPA.</span></p>&#13;
			<p class="callout-heading">What about Pod Security Policies?</p>&#13;
			<p class="callout">If you’ve heard of Pod Security Policies, they’re <a id="_idIndexMarker577"/>essentially the same thing as OPA. However, they were deprecated in v1.21 of Kubernetes and completely removed <span class="No-Break">in v1.25.</span></p>&#13;
			<h3>OPA</h3>&#13;
			<p>When you want to configure a specific policy, you can use a policy agent such as OPA. OPA allows you to write<a id="_idIndexMarker578"/> policies in an OPA-specific language called Rego (which you’ll see later in this section). When you write a policy, any request or event that comes in from another Kubernetes resource or an outside entity will be queried. OPA’s decision agent will give it a <em class="italic">pass</em> <span class="No-Break">or </span><span class="No-Break"><em class="italic">fail</em></span><span class="No-Break">.</span></p>&#13;
			<p>But how does OPA know how to <span class="No-Break">implement policies?</span></p>&#13;
			<p>That’s where OPA Gatekeeper comes into play. Gatekeeper is a <em class="italic">middle ground</em> of sorts that allows Kubernetes <a id="_idIndexMarker579"/>to interact with OPA. Gatekeeper is installed on Kubernetes and enables the use of <span class="No-Break">OPA policies.</span></p>&#13;
			<p>Let’s dive in from a<a id="_idIndexMarker580"/> hands-on perspective to set up OPA. The first part will be deploying OPA Gatekeeper and the second part will be implementing a policy. Proceed <span class="No-Break">as follows:</span></p>&#13;
			<ol>&#13;
				<li value="1">Add Helm chart for Gatekeeper, <span class="No-Break">like so:</span><pre class="console">&#13;
<strong class="bold">helm repo add gatekeeper https://open-policy-agent.github.io/gatekeeper/charts</strong></pre></li>&#13;
				<li>Install the Helm chart by running the <span class="No-Break">following command:</span><pre class="console">&#13;
<strong class="bold">helm install gatekeeper/gatekeeper --name-template=gatekeeper --namespace gatekeeper-system --create-namespace</strong></pre></li>&#13;
				<li>Confirm that all Kubernetes resources for Gatekeeper <span class="No-Break">were deployed:</span><pre class="console">&#13;
<strong class="bold">kubectl get all -n gatekeeper-system</strong></pre></li>&#13;
			</ol>&#13;
			<p>Now that Gatekeeper is installed, let’s start implementing the <span class="No-Break">OPA policies.</span></p>&#13;
			<p>The configuration is the definition/output of what OPA Gatekeeper is allowed to create policies for. In the following <strong class="source-inline">config.yaml</strong> file, because of the way that it’s written, Gatekeeper knows that it can only specify policies for Pods and no other Kubernetes resources. Run the <span class="No-Break">following code:</span></p>&#13;
			<pre class="source-code">&#13;
<strong class="bold">kubectl create -f - &lt;&lt;EOF</strong>&#13;
<strong class="bold">apiVersion: config.gatekeeper.sh/v1alpha1</strong>&#13;
<strong class="bold">kind: Config</strong>&#13;
<strong class="bold">metadata:</strong>&#13;
<strong class="bold">  name: config</strong>&#13;
<strong class="bold">  namespace: "gatekeeper-system"</strong>&#13;
<strong class="bold">spec:</strong>&#13;
<strong class="bold">  sync:</strong>&#13;
<strong class="bold">    syncOnly:</strong>&#13;
<strong class="bold">      - group: ""</strong>&#13;
<strong class="bold">        version: "v1"</strong>&#13;
<strong class="bold">        kind: "Pod"</strong>&#13;
<strong class="bold">EOF</strong></pre>&#13;
			<p>A constraint template is a policy that you configure for an environment. It’s a template, so you can use it across <span class="No-Break">multiple places.</span></p>&#13;
			<p>The Rego code/policy in the<a id="_idIndexMarker581"/> following constraint template <a id="_idIndexMarker582"/>ensures no one can utilize the latest tag of a container image. Run the <span class="No-Break">following code:</span></p>&#13;
			<pre class="source-code">&#13;
<strong class="bold">kubectl create -f - &lt;&lt;EOF</strong>&#13;
<strong class="bold">apiVersion: templates.gatekeeper.sh/v1beta1</strong>&#13;
<strong class="bold">kind: ConstraintTemplate</strong>&#13;
<strong class="bold">metadata:</strong>&#13;
<strong class="bold">  name: blocklatesttag</strong>&#13;
<strong class="bold">  annotations:</strong>&#13;
<strong class="bold">    description: Blocks container images from using the latest tag</strong>&#13;
<strong class="bold">spec:</strong>&#13;
<strong class="bold">  crd:</strong>&#13;
<strong class="bold">    spec:</strong>&#13;
<strong class="bold">      names:</strong>&#13;
<strong class="bold">        kind: blocklatesttag # this must be the same name as the name on metadata.name (line 4)</strong>&#13;
<strong class="bold">  targets:</strong>&#13;
<strong class="bold">    - target: admission.k8s.gatekeeper.sh</strong>&#13;
<strong class="bold">      rego: |</strong>&#13;
<strong class="bold">        package blocklatesttag</strong>&#13;
<strong class="bold">        violation[{"msg": msg, "details": {}}]{</strong>&#13;
<strong class="bold">        input.review.object.kind == "Pod"</strong>&#13;
<strong class="bold">        imagename := input.review.object.spec.containers[_].image</strong>&#13;
<strong class="bold">        endswith(imagename,"latest")</strong>&#13;
<strong class="bold">        msg := "Images with tag the tag \"latest\" is not allowed"</strong>&#13;
<strong class="bold">        }</strong>&#13;
<strong class="bold">EOF</strong></pre>&#13;
			<p>Next, you have<a id="_idIndexMarker583"/> the constraint. The constraint takes the<a id="_idIndexMarker584"/> template that you created earlier and allows you to use the template to create a policy inside of a <span class="No-Break">Kubernetes cluster:</span></p>&#13;
			<pre class="source-code">&#13;
<strong class="bold">kubectl create -f - &lt;&lt;EOF</strong>&#13;
<strong class="bold">apiVersion: constraints.gatekeeper.sh/v1beta1</strong>&#13;
<strong class="bold">kind: blocklatesttag</strong>&#13;
<strong class="bold">metadata:</strong>&#13;
<strong class="bold">  name: nolatestcontainerimage</strong>&#13;
<strong class="bold">spec:</strong>&#13;
<strong class="bold">  match:</strong>&#13;
<strong class="bold">    kinds:</strong>&#13;
<strong class="bold">      - apiGroups: [""]</strong>&#13;
<strong class="bold">        kinds: ["Pod"]</strong>&#13;
<strong class="bold">  parameters:</strong>&#13;
<strong class="bold">    annotation: "no-latest-tag-used"</strong>&#13;
<strong class="bold">EOF</strong></pre>&#13;
			<p>The OPA policy is <span class="No-Break">now created.</span></p>&#13;
			<p>To confirm that the policy is working as expected, you can test it out with the two following <span class="No-Break">Kubernetes manifests:</span></p>&#13;
			<ul>&#13;
				<li>The following manifest with the container image’s latest tag shouldn’t work because of the policy that<a id="_idIndexMarker585"/> you created earlier. The deployment <a id="_idIndexMarker586"/>itself will deploy, but the Pods won’t be scheduled and won’t <span class="No-Break">come online.</span></li>&#13;
			</ul>&#13;
			<p>Try running the <span class="No-Break">following code:</span></p>&#13;
			<pre class="console">&#13;
<strong class="bold">kubectl create -f - &lt;&lt;EOF</strong>&#13;
<strong class="bold">apiVersion: apps/v1</strong>&#13;
<strong class="bold">kind: Deployment</strong>&#13;
<strong class="bold">metadata:</strong>&#13;
<strong class="bold">  name: nginx-deployment</strong>&#13;
<strong class="bold">spec:</strong>&#13;
<strong class="bold">  selector:</strong>&#13;
<strong class="bold">    matchLabels:</strong>&#13;
<strong class="bold">      app: nginxdeployment</strong>&#13;
<strong class="bold">  replicas: 2</strong>&#13;
<strong class="bold">  template:</strong>&#13;
<strong class="bold">    metadata:</strong>&#13;
<strong class="bold">      labels:</strong>&#13;
<strong class="bold">        app: nginxdeployment</strong>&#13;
<strong class="bold">    spec:</strong>&#13;
<strong class="bold">      containers:</strong>&#13;
<strong class="bold">      - name: nginxdeployment</strong>&#13;
<strong class="bold">        image: nginx:latest</strong>&#13;
<strong class="bold">        ports:</strong>&#13;
<strong class="bold">        - containerPort: 80</strong>&#13;
<strong class="bold">EOF</strong></pre>&#13;
			<p>Wait a few minutes and when you see that it doesn’t come online, delete it, <span class="No-Break">as follows:</span></p>&#13;
			<pre class="console">&#13;
<strong class="bold">kubectl delete deployment nginx-deployment</strong></pre>&#13;
			<ul>&#13;
				<li>Next, try the following <a id="_idIndexMarker587"/>manifest. It will work, and the Pods<a id="_idIndexMarker588"/> will come online because the container image version <span class="No-Break">is specified:</span><pre class="console">&#13;
<strong class="bold">kubectl create -f - &lt;&lt;EOF</strong></pre><pre class="console">&#13;
<strong class="bold">apiVersion: apps/v1</strong></pre><pre class="console">&#13;
<strong class="bold">kind: Deployment</strong></pre><pre class="console">&#13;
<strong class="bold">metadata:</strong></pre><pre class="console">&#13;
<strong class="bold">  name: nginx-deployment</strong></pre><pre class="console">&#13;
<strong class="bold">spec:</strong></pre><pre class="console">&#13;
<strong class="bold">  selector:</strong></pre><pre class="console">&#13;
<strong class="bold">    matchLabels:</strong></pre><pre class="console">&#13;
<strong class="bold">      app: nginxdeployment</strong></pre><pre class="console">&#13;
<strong class="bold">  replicas: 2</strong></pre><pre class="console">&#13;
<strong class="bold">  template:</strong></pre><pre class="console">&#13;
<strong class="bold">    metadata:</strong></pre><pre class="console">&#13;
<strong class="bold">      labels:</strong></pre><pre class="console">&#13;
<strong class="bold">        app: nginxdeployment</strong></pre><pre class="console">&#13;
<strong class="bold">    spec:</strong></pre><pre class="console">&#13;
<strong class="bold">      containers:</strong></pre><pre class="console">&#13;
<strong class="bold">      - name: nginxdeployment</strong></pre><pre class="console">&#13;
<strong class="bold">        image: nginx:1.23.1</strong></pre><pre class="console">&#13;
<strong class="bold">        ports:</strong></pre><pre class="console">&#13;
<strong class="bold">        - containerPort: 80</strong></pre><pre class="console">&#13;
<strong class="bold">EOF</strong></pre></li>&#13;
			</ul>&#13;
			<p>OPA is a huge topic in itself. I highly recommend diving into it more. We only had a few pages together in<a id="_idIndexMarker589"/> this book to dive into it, but it goes<a id="_idIndexMarker590"/> far <span class="No-Break">more in-depth.</span></p>&#13;
			<h2 id="_idParaDest-199"><a id="_idTextAnchor200"/>Scanning container images</h2>&#13;
			<p>One popular security-style <em class="italic">entry point</em> for many engineers to start their security journey is by scanning container images. Scanning a container image means that you’re using a tool/platform to look inside the<a id="_idIndexMarker591"/> container image and see if there are any vulnerabilities. The vulnerability list typically comes <a id="_idIndexMarker592"/>from the NVD and the CIS benchmarks for Kubernetes. Both are a curated list of best practices from a security perspective and also contain <span class="No-Break">known vulnerabilities.</span></p>&#13;
			<p>There are a lot of tools in this space. In this section, let’s stick to one that’s as <em class="italic">built in</em> as <span class="No-Break">possible: Snyk.</span></p>&#13;
			<p>Snyk is used to scan containers for vulnerabilities from a list that’s pre-defined (as stated earlier) of best practices. A while back, Docker and Snyk partnered to ensure that security is embedded natively into any containerized workload. With that partnership, when you run the <strong class="source-inline">docker scan</strong> command, it’s actually using Snyk on <span class="No-Break">the backend.</span></p>&#13;
			<p>To use Snyk, ensure that you have the Docker CLI installed and run the <span class="No-Break">following command:</span></p>&#13;
			<pre class="console">&#13;
docker scan containerimage:containerversion</pre>&#13;
			<p>For example, let’s say you want to scan the <strong class="source-inline">ubuntu:latest</strong> container image, as <span class="No-Break">shown here:</span></p>&#13;
			<pre class="console">&#13;
docker scan ubuntu:latest</pre>&#13;
			<p>Once you run the <strong class="source-inline">docker scan</strong> command, you can scroll through all of the vulnerabilities. You’ll see a summary of the vulnerabilities that were found, what was tested, and which platform <span class="No-Break">was used.</span></p>&#13;
			<p>Vulnerabilities can range from being super basic, in that it just ends up being a best practice to fix, or something that’s incredibly crucial and leaves your environment open <span class="No-Break">for attack.</span></p>&#13;
			<h1 id="_idParaDest-200"><a id="_idTextAnchor201"/>Kubernetes Secrets</h1>&#13;
			<p>Wrapping up this chapter, and<a id="_idIndexMarker593"/> the overall book, you’ll learn about <span class="No-Break">Kubernetes Secrets.</span></p>&#13;
			<p>Secrets, in short, are anything that you don’t want to be in plain text. Typically, they are things such as passwords and API keys. However, they could even be usernames. Any type of data that you don’t want to be in plain text, at rest, or in transit can be considered <span class="No-Break">a Secret.</span></p>&#13;
			<p>At this point in your<a id="_idIndexMarker594"/> engineering journey, it’s assumed that you don’t need to be taught about Secrets, so we’re going to skip that part and dive right into the <span class="No-Break">hands-on part.</span></p>&#13;
			<h2 id="_idParaDest-201"><a id="_idTextAnchor202"/>Creating Kubernetes Secrets</h2>&#13;
			<p>To create a<a id="_idIndexMarker595"/> Kubernetes Secret, you’ll use the <strong class="source-inline">secret</strong> resource from the <strong class="source-inline">v1</strong> core <span class="No-Break">API group.</span></p>&#13;
			<p>For example, the following is a Secret called <strong class="source-inline">testsecret</strong> with a username <span class="No-Break">and password:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: v1&#13;
kind: Secret&#13;
metadata:&#13;
  name: testsecret&#13;
type: Opaque&#13;
data:&#13;
  username: YWRtaW4=&#13;
  password: MWYyZDFlMmU2N2Rm</pre>&#13;
			<p>Confirm that the Secret was created by running the <span class="No-Break">following command:</span></p>&#13;
			<pre class="console">&#13;
kubectl get secrets</pre>&#13;
			<p>Next, use the <strong class="source-inline">secret</strong> by putting it inside a Pod, <span class="No-Break">like so:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: nginxpod&#13;
spec:&#13;
  containers:&#13;
  - name: mypod&#13;
    image: nginx:latest&#13;
  volumes:&#13;
  - name: foo&#13;
    secret:&#13;
      secretName: testsecret</pre>&#13;
			<h2 id="_idParaDest-202"><a id="_idTextAnchor203"/>Don’t use Kubernetes Secrets</h2>&#13;
			<p>Although you literally just created a new Kubernetes Secrets a few seconds ago, here’s the thing—it’s not a <span class="No-Break">recommended practice.</span></p>&#13;
			<p>The biggest reason<a id="_idIndexMarker596"/> is that the default opaque standard for Kubernetes Secrets stores secrets in plain text. Yes—that’s right. The secrets will be stored in plain text in the <strong class="source-inline">etcd</strong> database. Thinking about it from another perspective, think about Kubernetes Manifests. Even if the secret wasn’t in plain text in Etcd, it would still be in plain text in the Kubernetes Manifest that’s creating the secret, and if it’s in plain text, where would you store it? You can’t push the manifest up to GitHub because then your secret would be compromised. Because of this, many engineers—and, quite frankly, even the Kubernetes documentation—highly recommend using a third-party secret provider. The most popular at this time for Kubernetes is <span class="No-Break">HashiCorp Vault.</span></p>&#13;
			<h1 id="_idParaDest-203"><a id="_idTextAnchor204"/>Summary</h1>&#13;
			<p>As you went through this chapter, there may have been some thoughts in your head of pure confusion. That’s okay—we’re all trying to <em class="italic">get it</em> when it comes to security in general, especially <span class="No-Break">in Kubernetes.</span></p>&#13;
			<p>Kubernetes security is an advanced topic, which is why the goal was to leave this topic for the last chapter of the book. Without Kubernetes security, environments will continue to be targets for attackers. However, before understanding Kubernetes security, you must fully understand how to utilize Kubernetes in production. The goal of <em class="italic">Chapters 1-7</em> was to help with understanding Kubernetes <span class="No-Break">in production.</span></p>&#13;
			<p>The next goal, once you close this book, is to take what you’ve learned in this chapter along with the various methodologies highlighted and implement them in your production environment for <span class="No-Break">optimal results.</span></p>&#13;
			<h1 id="_idParaDest-204"><a id="_idTextAnchor205"/>Further reading</h1>&#13;
			<ul>&#13;
				<li><em class="italic">Learn Kubernetes Security</em> by <em class="italic">Kaizhe Huang</em> and <em class="italic">Pranjal </em><span class="No-Break"><em class="italic">Jumde</em></span><span class="No-Break">: </span><a href="https://www.packtpub.com/product/learn-kubernetes-security/9781839216503"><span class="No-Break">https://www.packtpub.com/product/learn-kubernetes-security/9781839216503</span></a></li>&#13;
			</ul>&#13;
		</div>&#13;
	</div></body></html>