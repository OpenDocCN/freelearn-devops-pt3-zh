- en: '*Chapter 19*: Experimentation and A|B Testing'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will discuss how you can evolve and continuously improve
    your products by conducting experiments to validate hypotheses through evidence-based
    DevOps practices, such as **A|B testing**. This is sometimes called **hypothesis-driven
    development** or just **experimentation**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Conducting experiments with the scientific method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effective A|B testing with GrowthBook and Flagger
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experimentation and OKR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conducting experiments with the scientific method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditionally, requirements management was more guesswork than science. The
    closest that came to a scientific approach were interviews or market research
    in general. The problem with this approach is that you cannot ask people what
    they do not yet know. You can ask them what they want but not what they need,
    as they probably won’t know that yet, especially in a market segment that gets
    disrupted.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of hypothesis-driven development is to apply the **scientific method**
    to product management, an empirical method of acquiring evidence-based knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scientific method is a process of experimentation used to explore observations
    and answer questions that aim to discover cause-and-effect relationships. It follows
    certain process steps (see *Figure 19.1*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.1 – The scientific method'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17827_19_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 19.1 – The scientific method
  prefs: []
  type: TYPE_NORMAL
- en: 'We will look at the various steps in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Observation**: Observing reality using all five senses: smell, sight, sound,
    touch, and taste.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question**: Formulating a question based on observation and existing research
    or previous experiments.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Hypothesis**: Stating a hypothesis based on knowledge obtained while formulating
    the question. The hypothesis is a prediction of what you think will occur based
    on observation and research. Hypotheses are often written in the *if … then …*
    form, for example: “*if* we modify this variable, *then* we expect this change
    to be observable.”'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Experiment**: The experiment proves or disproves the hypothesis. In the experiment,
    you have different variables. **Independent variables** are the ones you change
    to trigger a result. **Dependent variables** are the things you measure and expect
    to change. In the experiment, you collect **qualitative data** through observations
    and **quantitative data** by measuring and collecting metrics.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiments also use control groups to prove that the variance is more than
    just chance. To test treatment with a drug, you must design an experiment in which
    a portion of the population – the **control group** – is left untreated and given
    a placebo, while the **experimental group** is treated with the potential drug
    (see *Figure 19.2*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.2 – Conducting a scientific experiment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17827_19_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 19.2 – Conducting a scientific experiment
  prefs: []
  type: TYPE_NORMAL
- en: To have a good experiment, you should only change *one variable at a time* while
    keeping all other variables the same. You should also *avoid bias*. No matter
    how hard you try, bias can sneak so easily into your observations and conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**: After the experiment, you analyze the results and compare the
    actual results to the expected ones. What have you learned from the experiment?
    Can you verify or refute your hypothesis? Is there a new hypothesis or new question
    to formulate? Or do you need more experiments to be sure?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Results**: The final step is to share your results. Even if your hypothesis
    was refuted, it is still valuable learning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The scientific method is an iterative, empirical method, but the steps do not
    necessarily occur in that order. At any point, you can modify your question and
    change your hypothesis – and observation is going on all the time. Instead of
    a clear cycle, the process diagram looks more like *Figure 19.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.3 – There is no strict order of the steps in the process'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17827_19_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 19.3 – There is no strict order of the steps in the process
  prefs: []
  type: TYPE_NORMAL
- en: 'The scientific approach is very important in our industry – not only for building
    the right things. You should also use the approach when hunting down bugs or production
    issues: formulate a hypothesis based on facts observed. Conduct an experiment
    by changing only one thing at a time, normally a configuration value. Perform
    a cross-check to ensure that there is no other system or variable interfering
    with your experiment. Make a conclusion and document your results before starting
    with the next hypothesis.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a look at how you can use the scientific method to evolve and continuously
    improve your software.
  prefs: []
  type: TYPE_NORMAL
- en: Observation – gathering and analyzing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can watch people using your application for observation. We’ve talked in
    [*Chapter 12*](B17827_12_Epub.xhtml#_idTextAnchor267), *Shift Left Testing for
    Increased Quality*, about **usability testing** techniques, such as **h****allway
    testing** or **guerrilla usability**. However, normally users are scattered across
    the world and it’s easier to look at the data they produce than to interview them.
  prefs: []
  type: TYPE_NORMAL
- en: Data is the most important ingredient for **hypothesis-driven development**!
    The more you experiment, the more data you will collect over time.
  prefs: []
  type: TYPE_NORMAL
- en: When observing data, you should not only focus on the data points at hand. Ask
    yourself what the data does not tell you. If your goal was to increase the number
    of active users every month, you should not only focus your observation on the
    data about the current users. Check the data for failed login attempts. How many
    users would like to use your applications but are locked out and can’t recover
    their password or second authentication factor? How many people do not come back
    after they need to verify their mail or mobile number? How many cancel the registration
    process and how long do they wait before doing so?
  prefs: []
  type: TYPE_NORMAL
- en: 'To answer these kinds of questions, you cannot simply look into your usage
    data. You have to combine data from all sources available (see *Figure 19.4*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.4 – Logging sources for gathering data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17827_19_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 19.4 – Logging sources for gathering data
  prefs: []
  type: TYPE_NORMAL
- en: 'This quantitative data can then be combined with qualitative data, such as
    customer surveys, data from your customer service center, or any kind of analytics
    data. *Figure 19.5* shows the different data sources that you can use to gain
    insights and formulate the question:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.5 – Data sources for observation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17827_19_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 19.5 – Data sources for observation
  prefs: []
  type: TYPE_NORMAL
- en: With these questions in mind, you can then start to formulate a hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: Formulating the hypothesis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The hypothesis is the prediction of what you think will occur based on your
    observation and research. Hypotheses can be written in a simple *if … then …*
    form: *If* *<we modify this variable>*, *then* *<we expect this change to be observable>*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*If* we shorten our registration form by deleting fields such as phone number
    and mailing address, *then* the number of people canceling the registration process
    (*abandonment rate*) will decrease.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since you will have many hypotheses on your backlog, it is common to have a
    fix form, similar to user stories, which includes the customer segment and the
    feature name. This makes your hypotheses more discoverable on a backlog:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We believe* *{customer segment}*'
  prefs: []
  type: TYPE_NORMAL
- en: '*want* *{feature}*'
  prefs: []
  type: TYPE_NORMAL
- en: '*because* *{value proposition}*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This form also forces you to bring three aspects into your hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Who**: for whom are we changing the application?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What**: what are we changing?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How**: how will this change impact the users?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These ingredients make up a good hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We believe* new users'
  prefs: []
  type: TYPE_NORMAL
- en: '*want* a shorter registration form with fewer input fields'
  prefs: []
  type: TYPE_NORMAL
- en: '*because* this allows them to test the application and gain confidence before
    revealing their personal data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that focusing on the value proposition leads to a more abstract description
    of the **how** with a greater focus on the **why**. In marketing, you often find
    details like this in your hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the impact?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By how much/how great it is?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After what period of time?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This results in a one-to-one relationship between hypotheses and experiments.
    Especially when starting with experimentation, I think separating the experiment
    from the underlaying hypothesis helps. You will probably need multiple experiments
    before you finally can say with certainty whether the hypothesis was true or false.
  prefs: []
  type: TYPE_NORMAL
- en: Building the experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When defining your experiment, you should try to keep as many variables fixed
    as possible. The best thing is to look at your baseline data. How will weekends
    and vacations impact your data? How will political and macroeconomic trends impact
    your experiment?
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, make sure that both your control group and your experimental group are
    big enough. If you only experiment on a small group, your results might not be
    representative. If your control group is too small, you might not have enough
    data to compare your results to, especially if there are other external factors
    that you did not foresee. A good experiment should contain the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: What’s the change?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What’s the expected impact?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who is the audience or the customer segment?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much change are we expecting?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How long do we run the experiment?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the baseline we compare the data to (a control group or historical data)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here is an example.
  prefs: []
  type: TYPE_NORMAL
- en: The new, *shorter registration form* (**what’s the change**) will *reduce the
    abandonment rate* of the registration form (**impact**) for *50%* of our *new
    users* (**for whom**) by more than *15%* (**how much**) after *14 days* (**after
    how long**) compared to our control group (**baseline**).
  prefs: []
  type: TYPE_NORMAL
- en: With the experiment defined, you can start implementing and running it. If you
    develop with feature flags (see [*Chapter 10*](B17827_10_Epub.xhtml#_idTextAnchor239),
    *Feature Flags and the Feature Lifecycle*), this is as simple as writing a new
    feature. The only difference is that you do not turn the feature on for all users
    but for your experimentation group instead.
  prefs: []
  type: TYPE_NORMAL
- en: Validating the results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After the experiment, you analyze the results and compare the actual results
    to the expected ones. What have you learned from the experiment? Can you verify
    or falsify your hypothesis or do you need more experiments to be sure? Is there
    a new hypothesis or new question to formulate?
  prefs: []
  type: TYPE_NORMAL
- en: The retrospective study of the results is an important part. Do not skip it
    and just assume that the hypothesis is true or false because your metrics exceed
    a threshold. Analyze the data and check for unexpected influences, strays, and
    statistical outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning from your hypotheses and experiments should lead to new ideas and
    complete the build-measure-learn loop (see *Figure 19.6*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.6 – Hypothesis-driven experimentation with the build-measure-learn
    loop'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17827_19_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 19.6 – Hypothesis-driven experimentation with the build-measure-learn
    loop
  prefs: []
  type: TYPE_NORMAL
- en: There are many tools available that can help you with effective A|B testing
    and experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: Effective A|B testing with GrowthBook and Flagger
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GitHub does not have the tooling to help you with A|B testing but there are
    many tools available on the market. The problem is that many of these tools have
    completely different scopes. Some are more like web experience tools, which you
    can use to build your website using a **content management system** (**CMS**)
    or to build A|B tests using a visual editor to create and test your variations
    (for example, **Optimizely** – see https://www.optimizely.com/). Some are more
    focused on marketing, landing pages, and campaign management, such as **HubSpot**
    (https://www.hubspot.com/). These tools are great but probably not the right choice
    for an engineering team.
  prefs: []
  type: TYPE_NORMAL
- en: A better solution is provided by the tools for doing feature flags, such as
    **LaunchDarkly**, **VWO**, or **Unleash**. I have covered these tools in[*Chapter
    10*](B17827_10_Epub.xhtml#_idTextAnchor239), *Feature Flags and the Feature Lifecycle*,
    so I will not cover them again here. If you are using one of these solutions for
    feature flags, this is the first place you should look for a solution for A|B
    testing.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, I will focus on **GrowthBook** and **Flagger**, two open source
    projects with a strong focus on experimentation, but with a completely different
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: GrowthBook
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**GrowthBook** ([https://github.com/growthbook/growthbook](https://github.com/growthbook/growthbook))
    is a solution with a free and open core. It is also available as a SaaS and Enterprise
    plan. It provides an SDK for **React**, **JavaScript**, **PHP**, **Ruby**, **Python**,
    **Go**, and **Kotlin**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution design of GrowthBook is completely containerized. If you want
    to try it out, you just have to clone the repository and then run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once up, you can access the Growthbook on [http://localhost:3000](http://localhost:3000).
  prefs: []
  type: TYPE_NORMAL
- en: Running GrowthBook in GitHub Codespaces
  prefs: []
  type: TYPE_NORMAL
- en: If you want to try out GrowthBook, you can run it in GitHub Codespaces. For
    this to work, you have to configure `docker-compose.yml` to use the correct DNS
    names, since GrowthBook uses localhost to connect to its MongoDB. Set `APP_ORIGIN`
    under `environment` to your local address of port `3000` and `API_HOST` to your
    local address of port `3001` and make port `3001` visible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once connected, you can use it to serve feature flags or build experiments.
    To build experiments, you have to connect a data source to GrowthBook – for example,
    **BigQuery**, **Snowflake**, **Redshift**, or **Google Analytics**, among many
    others. There are predefined data schemas and you can also build your own. You
    then create metrics based on your data source. Metrics can be any of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Account Created`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Page Visits`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Time on Site`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Revenue per User`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To run an experiment, you would normally use your feature flags. You could
    also run an inline experiment directly with one of the SDKs. This is what an experiment
    would look like in JavaScript:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The experiment runs based on your defined metrics and the results look like
    they do in *Figure 19.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.7 – The results of an experiment in GrowthBook'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17827_19_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 19.7 – The results of an experiment in GrowthBook
  prefs: []
  type: TYPE_NORMAL
- en: You can add and remove metrics to the experiment and also export it as a Jupyter
    notebook.
  prefs: []
  type: TYPE_NORMAL
- en: GrowthBook also comes with the Google Chrome extension **GrowthBook DevTools**
    for JavaScript and the React SDK, which allows you to directly interact with your
    feature flags in the browser. A visual editor is currently in beta.
  prefs: []
  type: TYPE_NORMAL
- en: GrowthBook is straightforward and also based on feature flags like the solutions
    introduced in [*Chapter 10*](B17827_10_Epub.xhtml#_idTextAnchor239).
  prefs: []
  type: TYPE_NORMAL
- en: Flagger
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A completely different approach uses **Flagger** ([https://flagger.app/](https://flagger.app/)).
    It’s a delivery operator for **Kubernetes** and can be used with the **service
    mesh** **Istio**. Flagger is more often used for **canary releases** to Kubernetes
    clusters but it can also route traffic on HTTP match conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'You could create an experiment for all users with an `insider` cookie for 20
    minutes, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You can combine Flagger with metrics from **Prometheus**, **Datadog**, **Dynatrace,**
    among many others. I’m not going to go into more detail here. See the Flagger
    documentation ([https://docs.flagger.app/](https://docs.flagger.app/)) for more
    information. There is also a good tutorial from Stefan Prodan: *GitOps recipe
    for Progressive Delivery with Flux v2, Flagger and Istio* (see [https://github.com/stefanprodan/gitops-istio](https://github.com/stefanprodan/gitops-istio)).'
  prefs: []
  type: TYPE_NORMAL
- en: A solution with Flagger and Istio brings great flexibility but it is also quite
    complex and not suited to beginners. If you are already on Kubernetes and Istio
    and perform canary releases, then Flagger might be a powerful framework for you.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there are many solutions out there that can help you run experiments
    and A|B tests. From CMS- and campaign-focused tools to Kubernetes operators, there
    is a wide range of solutions that have completely different approaches. The best
    solution for you depends on a lot of things – mostly your existing toolchain,
    pricing, and support. I think it is more important to focus on the process and
    data analytics. Serving two versions of your application should not be the challenge
    – making sense of your data probably is.
  prefs: []
  type: TYPE_NORMAL
- en: Experimentation and OKR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 1*, *Metrics That Matter*, I introduced you to **Objectives and
    Key Results** (**OKRs**) as a framework to define and track objectives and their
    outcomes in a transparent way. OKRs help organizations achieve high alignment
    on strategic goals while keeping a maximum level of autonomy for the individual
    teams.
  prefs: []
  type: TYPE_NORMAL
- en: 'Engineering teams are an expensive resource and a lot of stakeholders are requesting
    things from them all the time: testers submitting bugs, customers requesting new
    features, and management wanting to catch up with the competition and make promises
    to important customers. How should a team ever find the freedom to conduct experiments?
    And what experiments would be the best to start with?'
  prefs: []
  type: TYPE_NORMAL
- en: OKRs can give you the ability to have a strong alignment with higher-level goals
    by simultaneously preserving the autonomy to decide *what* to build and *how*
    to build it.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume your company wants to be the market leader with a market share
    of 75% and it will need a constant growth rate of newly registered users to achieve
    that. The **key result** for your team is a growth rate of 20% each month. This
    will then set the priority for your team. Of course, there will be other things
    to do, but the priority will be the OKR. The team probably first investigates
    how many people come to the registration page in the first place and from what
    referral. How many people click on the **Register Now** button? How many finish
    the dialog? At what point do they not come back? And at that point, they are automatically
    starting to formulate hypotheses and can run experiments to prove them.
  prefs: []
  type: TYPE_NORMAL
- en: OKRs are also good for cross-team collaboration, as teams probably have OKRs
    with high synergy effects, as they are aligned to higher-level goals. In this
    example, the team probably wants to talk with marketing, as they will have similar
    OKRs. They might have their own ideas for experiments to help drive the engagement
    rate for the landing pages that lead to your registration site.
  prefs: []
  type: TYPE_NORMAL
- en: OKRs are a great tool to grant people the freedom to experiment by ensuring
    alignment with other teams and higher-level goals.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Experimentation, A|B testing, and hypothesis-driven development are difficult
    topics as they require a high level of maturity in many areas:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Management**: Your teams need the autonomy to decide on their own *what*
    to build and *how* to build it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Culture**: You need a culture of trust where people are not afraid to fail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-team collaboration**: Your teams must be able to work interdisciplinarily,
    as experimentation often requires the collaboration of different departments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Technical capabilities**: You must be able to release changes in a very short
    time to production and target individual customer segments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Insights**: You must have strong analytics capabilities and combine data
    and metrics from different sources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are not there yet, don’t worry. Many teams I work with are not. Just
    keep on improving your capabilities and check that your metrics show results.
    DevOps is a journey and not a goal, and you must take it one step at a time.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ve learned the basics of experimentation, A|B testing,
    and hypothesis-driven development, and I introduced some tools that can help you
    build solutions for it.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn the basics of GitHub – hosting options,
    pricing, and how you can integrate it into your existing toolchain and your enterprise.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These are the references and links from this chapter that you can also use
    to get more information on the topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The Scientific method*: https://en.wikipedia.org/wiki/Scientific_method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ring-based deployments*: [https://docs.microsoft.com/en-us/azure/devops/migrate/phase-rollout-with-rings](https://docs.microsoft.com/en-us/azure/devops/migrate/phase-rollout-with-rings)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Optimizely*: [https://www.optimizely.com/](https://www.optimizely.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hubspot*: [https://www.hubspot.com/](https://www.hubspot.com/ )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*GrowthBook*: [https://github.com/growthbook/growthbook](https://github.com/growthbook/growthbook)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Flagger*: [https://flagger.app/](https://flagger.app/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stefan Prodan: *GitOps recipe for progressive delivery with Flux v2, Flagger,
    and Istio*: [https://github.com/stefanprodan/gitops-istio](https://github.com/stefanprodan/gitops-istio)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
