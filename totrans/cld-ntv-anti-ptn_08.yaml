- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Don’t Get Lost in the Data Jungle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data is at the crux of everything we do. Most operations in cloud native applications
    relate to generating, consuming, and modifying data in myriad forms. Choosing
    the right places to store our data in the cloud, knowing how to ingest data, and
    maintaining data integrity are paramount. While much of the value of the applications
    we produce lives in the business logic, fundamentally, that business logic operates
    on data. Therefore, the way we store data is instrumental in the operation of
    our application. Unlike traditional on-premise services, cloud native services
    present new and exciting opportunities that can reduce our operational and maintenance
    overhead significantly. However, when used incorrectly, these services can just
    as quickly hamper our efforts through some insidious anti-patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following main anti-patterns that
    are present when persisting data in the cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: Picking the wrong database or storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data replication from production to development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backup and recovery should theoretically work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manual data ingestion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No observability for data transfer errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a solid understanding of cloud native
    data storage options for operational purposes and the trade-offs between them.
  prefs: []
  type: TYPE_NORMAL
- en: Picking the wrong database or storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “*When all you have is a hammer, everything looks like a nail*” is a refrain
    commonly used to describe overreliance on the same tool for every job. Having
    preferences is acceptable, but when teams pick a database or storage solution,
    we often see the same developers repeatedly reaching for the same tools. While
    familiarity with a particular toolset might be advantageous for rapid onboarding
    and development, it can lead to suboptimal solutions and anti-patterns. Cloud
    native applications have a wide range of databases and storage methods, so a well-rounded
    cloud application should consider all the available options. Before we dive into
    these options, let’s explore some required background knowledge to frame our conversations.
  prefs: []
  type: TYPE_NORMAL
- en: Framing the conversation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When discussing databases, it is essential to start by exploring the **consistency,
    availability, and partition tolerance** (**CAP**) theorem, normal forms, and time
    complexity. These three concepts explain the trade-offs and approaches to designing
    data models for myriad solutions.
  prefs: []
  type: TYPE_NORMAL
- en: CAP theorem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As previously mentioned, the CAP theorem stands for consistency, availability,
    and partition tolerance, specifically concerning distributed datastores. The consensus
    is that a distributed database solution can only genuinely address two of these
    capabilities simultaneously:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consistency** ensures that when a database read occurs, it will return the
    database state that results from all actions committed before we request the read.
    Strongly consistent databases maintain this paradigm, whereas eventually consistent
    databases will return a state that may or may not have all applied writes propagated;
    it represents the state of the database from some point in the past.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Availability** means that every request received by a valid database node
    must return a non-error response. In the context of a distributed datastore, this
    might conflict with the guarantee of consistency. How can we ensure that our system
    has received all transactions from all other nodes, especially in scenarios where
    we might have network partitions or delays? This brings us to partition tolerance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partition tolerance** guarantees that the system will continue operating
    despite unreliable or late message delivery between nodes. If one of our nodes
    suffers catastrophic network failure, our datastore should keep operating. This
    is primarily an issue in distributed databases with multi-master configurations,
    such as some of the high-availability options discussed later in the chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In an ideal world, our chosen datastore would have all three of these properties,
    and some recent developments in this space push the limits of this exclusivity.
    However, this pattern is generally closely reflected in reality.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22364_08_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Euler diagram for exclusivity of the CAP theorem elements
  prefs: []
  type: TYPE_NORMAL
- en: Normal forms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Normal forms** refer to how we construct data in our database systems. Fundamentally,
    normal forms are a measure of normalization in our database. We will quickly review
    normal forms and use a common theme to provide examples for each. One point to
    keep in mind as we go through this section is that even though it may appear that
    the higher our normal form is, the better our database design is, in most cases,
    we also need to consider the performance and querying of our data and access patterns.
    We will only discuss the first three normal forms here as, typically, this is
    where most of the differences between cloud native databases lie:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first normal form of data (**1NF**) defines each cell as a unit that only
    contains a single value, and the names of columns in our data storage should be
    unique. Many storage solutions that support nested or unstructured data already
    fail this criterion. The following table shows a first normal form dataset for
    order information:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **InvoiceItems** |'
  prefs: []
  type: TYPE_TB
- en: '| **InvoiceId (key)** | **ItemId (key)** | **Qty** | **SalespersonID** | **Salesperson**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 123 | 312 | 10 | 10 | Aiden |'
  prefs: []
  type: TYPE_TB
- en: '| 123 | 432 | 5 | 10 | Aiden |'
  prefs: []
  type: TYPE_TB
- en: '| 456 | 321 | 20 | 8 | Gerald |'
  prefs: []
  type: TYPE_TB
- en: '| 789 | 432 | 10 | 8 | Gerald |'
  prefs: []
  type: TYPE_TB
- en: Table 8.1 – Invoices, items, and salespeople stored in a single table
  prefs: []
  type: TYPE_NORMAL
- en: The second normal form (`Salesperson` column in the first table. In this scenario,
    our result only depends on the part of the key, the invoice ID.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **InvoiceItems** |'
  prefs: []
  type: TYPE_TB
- en: '| **InvoiceId (key)** | **ItemId (key)** | **Qty** |'
  prefs: []
  type: TYPE_TB
- en: '| 123 | 312 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| 123 | 432 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 456 | 321 | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| 789 | 432 | 10 |'
  prefs: []
  type: TYPE_TB
- en: Table 8.2 – Invoices and items; note we have removed two columns in this table
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s add a new table to satisfy the second normal form by storing salespeople
    against invoice IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **InvoiceSalesperson** |'
  prefs: []
  type: TYPE_TB
- en: '| **InvoiceId (key)** | **SalespersonID** | **Salesperson** |'
  prefs: []
  type: TYPE_TB
- en: '| 123 | 10 | Aiden |'
  prefs: []
  type: TYPE_TB
- en: '| 456 | 8 | Gerald |'
  prefs: []
  type: TYPE_TB
- en: '| 789 | 8 | Gerald |'
  prefs: []
  type: TYPE_TB
- en: Table 8.3 – Invoices and their relation to salespeople; note that we are storing
    less data now but can reconstruct the same level of detail
  prefs: []
  type: TYPE_NORMAL
- en: 'The third normal form (`InvoiceSalesperson` table is based on the `InvoiceId`
    key. However, the salesperson’s name depends on `SalespersonID`, which is a transitive
    dependency. To rectify this, let’s add a `Salesperson` table (*Table 8.5*):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **InvoiceItems** |'
  prefs: []
  type: TYPE_TB
- en: '| **InvoiceId (key)** | **ItemId (key)** | **Qty** |'
  prefs: []
  type: TYPE_TB
- en: '| 123 | 312 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| 123 | 432 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 456 | 321 | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| 789 | 432 | 10 |'
  prefs: []
  type: TYPE_TB
- en: Table 8.4 – Invoices and items; this scenario is unchanged from our previous
    example
  prefs: []
  type: TYPE_NORMAL
- en: We then have the same invoice salesperson mapping; however, we use an identifier
    rather than the salesperson’s name.
  prefs: []
  type: TYPE_NORMAL
- en: '| **InvoiceSalesperson** |'
  prefs: []
  type: TYPE_TB
- en: '| **InvoiceId (key)** | **SalespersonID** |'
  prefs: []
  type: TYPE_TB
- en: '| 123 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| 456 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 789 | 8 |'
  prefs: []
  type: TYPE_TB
- en: Table 8.5 – Invoices and their relation to salespeople; however, we have removed
    the transitive dependency
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we add a table with each of the salespeople in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Salesperson** |'
  prefs: []
  type: TYPE_TB
- en: '| **SalespersonID (key)** | **Salesperson** |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | Aiden |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Gerald |'
  prefs: []
  type: TYPE_TB
- en: Table 8.6 – Maps salespeople IDs to their names; this once again reduces the
    data we store but can still be reconstructed with the right access patterns
  prefs: []
  type: TYPE_NORMAL
- en: Our solution has now evolved to comply with the third normal form. As you can
    see, high levels of normalization require increasing dependence on relationships
    but provide greater consistency in our data.
  prefs: []
  type: TYPE_NORMAL
- en: Time complexity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, we need to discuss time complexity and Big O notation. Big O notation
    describes the upper bound of a system’s execution time in relation to the size
    of the dataset being processed. A system with a constant lookup time for a record,
    regardless of its dataset, is *O(1)*. A system that linearly scales its lookup
    time with the number of items in our dataset is *O(n)*.
  prefs: []
  type: TYPE_NORMAL
- en: A good example is a naive database implementation that checks every row in a
    database to see whether it matches our selection criteria. In this case, the implementation
    would be *O(n)* complexity; as the number of records grows, so does the number
    of checks we need to make on each lookup linearly. In reality, most database solutions
    will lie somewhere between these values. Complexity can scale at rates greater
    than *O(n)*, but you should find another one if a database ever offers that complexity.
  prefs: []
  type: TYPE_NORMAL
- en: The right database for the right purpose
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We see four key types of databases utilized in cloud native systems for bulk
    data storage: relational, NoSQL, key-value, and graph (there are many other solutions,
    such as ledger/blockchain databases, hierarchical databases, and vector databases,
    but they are outside the scope of this section). Each has advantages and is useful
    for different data types but requires different approaches. A common anti-pattern
    is developers choosing the wrong cloud databases for their applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Relational databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Relational databases** are the tried-and-true traditional database solution.
    They allow you to establish records and model the relationships between them.
    In this solution, the database usually conforms to a strict, predefined set of
    relationships and structures defined as a part of its schema. However, more and
    more relational database engines are providing the ability to store semi-structured
    and unstructured data. Due to their highly structured data models, relational
    databases make it very easy to maintain consistency and integrity of the data.
    Their inbuilt support of relationships makes it easy to query normalized data.
    In the cloud world, these databases are often offered as a *service* and may even
    have “*serverless*” offerings (more on why that’s quoted in a few paragraphs);
    however, we run into issues when we try to scale these systems. Typically, the
    scaling model involves adding additional capacity to these services through vertical
    scaling.'
  prefs: []
  type: TYPE_NORMAL
- en: Some newer solutions provide automated, transparent sharding capability priced
    at a premium. At vast scales, with massive datasets, this can cause issues that
    can result in higher cloud bills. It’s also essential to note that in these systems,
    we’re typically limited to certain index types, such as binary trees, which have
    a time complexity of *O(log(n))*. When we query data in a relational database,
    a typical pattern is to join records and perform aggregations to return the result
    in the format we want. This pattern can be instrumental in scenarios where you
    know the structure of the data you want to store but not the access patterns of
    how you will query that data. The flexible access patterns allow you to expand
    your offerings without significant changes to the underlying database. You can
    provide new insights with new queries.
  prefs: []
  type: TYPE_NORMAL
- en: The services that provide relational databases in the hyperscalers cover all
    familiar SQL flavors, such as MySQL, PostgreSQL, and SQL Server. Typically, these
    solutions focus on being consistent and partition-tolerant. However, many new
    services by hyperscalers also provide high availability.
  prefs: []
  type: TYPE_NORMAL
- en: NoSQL databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**NoSQL databases** provide an alternative to traditional relational databases.
    They are denormalized to some degree, and rather than allowing for flexible access
    patterns, they rely on access patterns designed into the data model itself.'
  prefs: []
  type: TYPE_NORMAL
- en: 'All the hyperscalers have offerings in this space: Azure has Cosmos DB, GCP
    has Firestore, and AWS has DynamoDB. Unlike our strictly formatted SQL tables,
    NoSQL databases have no enforced schema. Columns can mix data types, and data
    can be deeply nested. There are compelling arguments for why you should do away
    with separate tables and instead put all your data into one big table. These services
    offer extreme scalability and performance at a low price point. However, they
    require fundamental shifts in thinking from the traditional relational database
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: We must design our access patterns upfront to get the best value from our NoSQL
    database solution. This requirement can make development slightly more complicated
    because adding a new access pattern is more than just a case of writing a new
    query. We may require significant changes to our database design. Some database
    solutions in the NoSQL space (such as DynamoDB, Firestore, and Cosmos DB) can
    achieve close to *O(1)* complexity for properly structured access patterns but
    incur a penalty of *O(n)* complexity for improperly structured access patterns.
    Many of these solutions allow you to prioritize availability and partition tolerance
    or consistency and partition tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: Key-value stores
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Key-value stores** are a straightforward type of database. Essentially, we
    provide a way to address (key) our stored data (value). NoSQL databases still
    allow for complex access patterns. Our key-value store has one access pattern:
    use the key to get the value stored at an address. These are typically high-performance
    in-memory datastores that may or may not offer some form of persistence. The typical
    use case for these datastores is a cache for complex queries or computational
    outputs from other systems. They can be helpful in our cloud arsenal when we have
    complex requests with low cardinality.'
  prefs: []
  type: TYPE_NORMAL
- en: Graph databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final database type we will discuss is `OrderID` field is referenced on
    the order record, the shipping manifest, and the payment record. The shipping
    manifest and payment record contain foreign keys to the order record; however,
    the actual relationship is stored on the records themselves. In a graph database,
    the relationships are first-class objects. We have our objects (vertices) and
    our relationships (edges), and the data model is optimized for extremely fast
    traversal of relationships, allowing us to follow paths through our dataset in
    a performant way. This property can be advantageous when objects interact with
    each other in arbitrary ways, for example, with users on a social media site,
    interacting with other users, posts, communities, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Other database types
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Exploring other supporting services or nonstandard database types can also be
    advantageous. A key type of database that is often ignored is time-series databases.
    These might be implemented as standalone products or extensions to the previous
    database types. These databases are optimized for chronological access patterns
    and storage rather than the structures mentioned previously. Another common type
    of database or database extension is spatial databasing, specifically looking
    at geometric and geographic properties in queries. The key here is not to limit
    yourself to the preceding database structures but to also explore the options
    available for your edge cases.
  prefs: []
  type: TYPE_NORMAL
- en: In one example I worked on, the client used a Postgres database to store a list
    of customer addresses and identifiers. However, this system’s access patterns
    are unsuitable for a relational database. First, the data was not relational;
    each record was wholly independent, and second, the Postgres keyword `LIKE` was
    significantly used within the database’s query patterns. The client’s quick solution
    was to put a **generalized inverted index** (**GIN**) on every column. This enabled
    searching on arbitrary strings but made modifying the database unwieldy. Using
    a search service such as OpenSearch to store the queriable documents would have
    been straightforward, likely resulting in a lower cloud bill and better performance.
  prefs: []
  type: TYPE_NORMAL
- en: Manual, managed, serverless, and truly serverless databases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When choosing databases, we must establish the need for the database types discussed
    earlier and the method by which we are going to consume the database in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: The naive approach to this from the on-premises mindset might be that we simply
    need to provision a cloud VM, install a database, and be good to go. While this
    manual approach will work, it must present a compelling value proposition. In
    this scenario, you are solely responsible for backups, patching the DB version
    and OS, and provisioning new machines. How you install, run, and maintain databases
    is unlikely to be a value differentiator for your business. Therefore, this manual
    option is generally considered an anti-pattern unless you need specific functionality
    or configurations that aren’t available in managed services. Instead, the baseline
    deployment of a database is typically as a managed service.
  prefs: []
  type: TYPE_NORMAL
- en: This deployment method is where we see most companies start their cloud database
    adoption, as these managed services provide a way for them to use familiar tools
    (Postgres, MySQL, and SQL Server) while allowing the cloud provider to take care
    of backups, patching, and maintenance using battle tested and resilient methodologies.
    Many companies never find a compelling reason to leave this level, which is perfectly
    acceptable. We can also start to set up resilient architectures in this development
    mode with read replicas, automated failover, and multi-master configurations.
  prefs: []
  type: TYPE_NORMAL
- en: In the managed system, we typically see applications that have consistent, predictable
    patterns. However, some businesses have unpredictable traffic and usage, so you
    should move to a more scalable solution. This situation is where “serverless”
    solutions come into play. I use quotes in this scenario because they are serverless
    (i.e., they will automatically scale). Still, they do not scale down to zero,
    which many people consider true serverless. An anti-pattern we commonly see in
    this space is people migrating to these “serverless” solutions without considering
    non-relational data models.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have truly serverless databases. These are typically NoSQL or document
    databases (such as DynamoDB, Firestore, and Cosmos DB in the major cloud providers
    in the **online transaction processing** (**OLTP**) space) that make trade-offs
    in ease of use for extreme scalability, cost-effectiveness, and performance. The
    anti-pattern we often see in this space is teams seeing this option and holding
    it as the pinnacle of achievement to build a system that utilizes this cloud native
    unique option without considering the downsides. Namely, your data is less portable,
    will be harder to hire for, and requires upfront knowledge of your access patterns.
    This combination can lead to bad initial experiences that cause teams to return
    to the familiar land of relational databases and not consider these databases
    for use cases where they would be a good fit.
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring storage requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common anti-pattern is using traditional storage mechanisms in the cloud without
    considering other options. Conventional filesystems evolved out of the need for
    on-device storage and provide considerable functionality. Network filesystems,
    such as FTP and NFS, became the de facto projection of these services into a multi-machine
    environment. The core principle in these systems is that a central server is responsible
    for coordinating access to the underlying storage. A common theme in this book
    is that centralization is usually an anti-pattern.
  prefs: []
  type: TYPE_NORMAL
- en: When we start to design a system that utilizes storage in the cloud, the first
    question we should ask is, “*Can we use blob storage?*” **Blob storage** is decentralized
    and scales horizontally, with much higher resiliency and durability than conventional
    network filesystems. In Azure, this service is Azure Blob Storage, GCP has Cloud
    Storage, and AWS has S3.
  prefs: []
  type: TYPE_NORMAL
- en: You can think of blob storage as a key-value store that can store enormous values.
    For most cloud native use cases, this provides more than enough capability. Do
    you still need metadata? Put it in your database. Do you need locks? Use your
    database. Need backups? Use version history. Blob storage is likely the answer
    to your storage needs. There are cases where specialized or traditional filesystems
    still provide benefits, such as in high-performance computing, low-latency applications,
    and conventional filesystem migrations. So, remember that no one tool is the right
    solution to every problem.
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring the life cycle and archive policy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Storing data is easy. We send a request to our storage provider of choice and
    then forget about it until we need to use it. Therein lies the anti-pattern: failing
    to maintain the life cycle of your data appropriately can lead to severe consequences.'
  prefs: []
  type: TYPE_NORMAL
- en: However, we might want to save some money here because we don’t necessarily
    want to access this data; we just want to keep it on file. This requirement is
    where the concept of storage tiers comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take an example: we work at a large firm that has an internal tax function.
    Throughout the year, people upload receipts. We must access these receipts repeatedly
    during tax time as various functions perform their duties. Then, after the tax
    period, we just need to keep a copy in case of discrepancies. In all cloud providers,
    we can group their storage tiers into one of three broad categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hot** is for data that needs to be accessed regularly and available at a
    moment’s notice. Typically, this tier strikes a good balance between cost to store
    and cost to retrieve. Consider this where we want our receipts to be at tax time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cold** is for data that needs to be accessed at a moment’s notice but is
    unlikely to be accessed often. We pay a little more when we want to access items
    in this tier but reap the benefits of our infrequent access with lower storage
    costs. This tier might be where we store all of our receipts during the year as
    they’re submitted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Archive** is for data that we want to keep but do not have specific access
    speed requirements. This tier offers the most cost-effective storage solution
    with the highest access cost and slowest retrieval time (this might be on the
    order of hours rather than milliseconds). When we are done with all of our receipts
    for the year and just need to keep a record for posterity, we will move them to
    this tier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some data may need to be retained to comply with regulatory requirements, while
    other data may only need to be stored short-term as its validity rapidly decreases.
    We accomplish these use cases through data life cycles. Life cycle policy and
    management tools allow us to automate this process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, we take two actions in life cycle policies: we either change the
    storage tier of our data or delete our data. A life cycle policy might mix these
    two actions. For example, imagine we work for a company that creates detailed
    financial reports. Every month, we release a new report that is accessed frequently,
    then infrequently, and then it needs to be archived for six years. Our life cycle
    policy might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a file in the hot tier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait 31 days (report cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Move the file to the cold tier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait 334 days.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Move the file to the archive tier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait six years.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Delete the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we kept our file in the hot tier, we would be paying for the convenience
    of frequent access without actually accessing the file. Therefore, our life cycle
    policy has allowed us to optimize our cloud storage spending.
  prefs: []
  type: TYPE_NORMAL
- en: Data replication from production to development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We all need data to ensure that the systems we build in our development environment
    match all the weird and wonderful types of data that our users generate in production.
  prefs: []
  type: TYPE_NORMAL
- en: This section is one of the few sections with an anti-pattern that is serious
    enough to name the entire section after. Under no circumstance should you copy
    user-generated data from production to development environments. While it may
    seem easy to get real-world use cases for your lower environment, lower environments
    typically have more lax security controls and broader access to developers. A
    few recent data breaches have directly involved this anti-pattern; real-world
    user data was available on test systems, and these test systems were breached.
    Instead, in this section, we will go through some alternatives to testing on production
    data and some common anti-patterns in creating data for test environments.
  prefs: []
  type: TYPE_NORMAL
- en: But we mask our production data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first anti-pattern we will discuss is using masked data from production
    systems in test environments. This procedure is only marginally better than using
    production data directly. The fallacy in this scenario is that we are coming from
    an insecure position (unmasked production data), applying a transform (our masking
    procedure), and assuming the output is secure (masked data). To illustrate why
    this is a problem, let us look at a parallel example, one based on FaaS. I was
    working with a client who had produced an authentication and logging wrapper for
    lambda functions. The wrapper applied some functionality that could be enabled
    with flags in the lambda function code. One of the flags enabled authentication.
    This pattern meant that, fundamentally, any created lambda functions started as
    insecure functions and then had to opt in to become secure. Instead, we inverted
    that dependency. We made all of the functions secure by default, and you could
    use a flag to turn authentication off for unauthenticated functions. This change
    made being insecure a conscious choice rather than an unconscious mistake. When
    we mask data, we risk making unconscious mistakes because we start from an insecure
    position. The solution is to start from a secure position and explicitly make
    any insecure additions to our data choices. So, we have to start from a secure
    position, which means we need to know our schema and generate data that tests
    its limits.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with synthetic data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed earlier, the easiest way to ensure that the data you use is
    safe for lower environments is to ensure it doesn’t originate from production
    systems. Therefore, we need a reliable way to generate fake data for our system.
    Luckily, we are not the first people to have this issue! A multitude of open source
    libraries exist with the sole purpose of generating completely fake data. Usually,
    for cloud projects, JavaScript is used at some point in the development cycle,
    be it for frontend applications or backend servers with a runtime such as Node.js,
    Bun, or Deno, so it usually forms a good baseline language. In this case, the
    Faker.js ([fakerjs.dev](http://fakerjs.dev)) library provides a comprehensive
    set of generators to create fake data for testing. The other common language we
    see in testing frameworks is Python, which also has its own Faker library ([https://faker.readthedocs.io/en/master/](https://faker.readthedocs.io/en/master/)).
  prefs: []
  type: TYPE_NORMAL
- en: These libraries form an excellent foundation upon which to build. These allow
    us to create bulk data to see how our system handles when under heavy load. We
    can use our production system’s utilization metrics to develop synthetic data.
    Synthetic data retains the schema and structure of our production data, but the
    contents of the records are pure fiction, making it great for functional testing.
    This approach allows us to load a similar amount of data present in production
    into lower environments, ensuring that the conditions we are testing under in
    our lower environments are similar to those under our higher environment. A common
    anti-pattern we see here is attempting to use only a small data set in lower environments.
    This anti-pattern is an issue because you first test the system at the production
    scale when you deploy it to production. Under this paradigm, scenarios and edge
    behaviors that only become present at the scale of the production system remain
    hidden during testing. These problems might be a poorly optimized SQL query or
    a missing index on a column. In these scenarios, small datasets are unlikely to
    be exposed to the issue.
  prefs: []
  type: TYPE_NORMAL
- en: Perfect synthetic data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When creating synthetic data, it can be easy to fall into the anti-pattern of
    developing *perfect synthetic data*. This anti-pattern only injects the data,
    formats, and usage patterns we expect to see in our production systems. While
    this might test our systems’ happy path, unfortunately, users are fantastic at
    stressing our systems in ways we never intended. What happens if the user signs
    up with an address and then that address is deleted or gets subdivided into an
    A/B block or any other myriad of problems? We can take a leaf from the domain
    of chaos engineering here. Instead of creating perfect data, we make data with
    a certain amount of corruption, usually expressed as a percentage of the total
    synthetic data. Perfect data only addresses usage by perfectly homogenous users,
    and we all know that our user base consists of a wildly different collection of
    individuals.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some simple guidelines for creating synthetic data that I like to
    follow. I generally split these into two layers: one for structured data (SQL
    and Parquet) and one for unstructured/semi-structured data (NoSQL, CSV, JSON,
    and TXT). The unstructured data corruptions should be treated as an extension
    of the structured corruptions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Structured data can be corrupted in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Missing records**: What happens if we receive a partial object in a request
    to our service? What if a dependency is missing? What if the dependency existed
    when we created the record but manually deleted it afterward?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unreachable state**: We may have transitive dependencies that are unreachable
    from a code perspective but permissible from a database perspective. What happens
    if we reach this state?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Corrupted records**: This is data that fundamentally does not make sense
    but is permissible by the system. What if the user accidentally entered their
    credit card number in the cardholder name field? What if one row of our CSV has
    an unescaped comma in a string?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Large records**: What happens when a user connects an infinite number of
    monkeys with an endless number of typewriters to your profanity filter?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unstructured data can be corrupted in the following additional ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Duplicate records**: What happens when we try to insert duplicate records,
    or multiple records that represent the same object?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extra fields**: What happens when our system receives extra data from the
    client that it wasn’t expecting?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing fields**: What happens when our system doesn’t receive data from
    the client that it was expecting?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Syntactically incorrect data**: This is data that doesn’t agree with the
    rules of the data medium in use (for example, not valid CSV or JSON). Missing
    a column? Forgot a curly brace?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From this, we see that perfect testing data should be imperfect by design. This
    allows us to discover our system’s edge behavior. Our test data should identify
    issues we might encounter before encountering them in production. However, it
    is impossible to be perfectly prophetic about the data issues we might see in
    production. The best type of corrupted data is when we find something in production.
    In that case, copy the methodology of the corrupted data (not the data itself!)
    into your synthetic data generation tool. This process allows us to find other
    ways in which this might impact production. For example, we have an issue where
    an invalid card number is entered. Then, the customer could rectify the card number,
    and all is good. If we add the pattern to our synthetic data, we can see how that
    data would have affected our system if it had been allowed to flow through to
    our billing run system or other application areas.
  prefs: []
  type: TYPE_NORMAL
- en: Backup and recovery should theoretically work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “*The best-laid plans of mice and men oft go awry*,” goes the famous line from
    Robert Burns’ “To a Mouse.” The nugget of wisdom here is that no matter how carefully
    we plan for every eventuality, we cannot be confident of its success until we
    execute it. We touched on this topic in [*Chapter 7*](B22364_07.xhtml#_idTextAnchor198)
    when we discussed ignoring reliability. We will go into this topic in more detail
    and explore how to address this anti-pattern with a specific focus on data. As
    discussed before, not testing your data resiliency will lead to unwanted downtime
    when you least expect it. Let’s dive into some ways to mitigate this.
  prefs: []
  type: TYPE_NORMAL
- en: Have a plan
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having a plan is the first step toward resilient data architectures, and the
    key to that plan is understanding the shared responsibility model. If you are
    running your data solution self-hosted in the cloud against the recommendations
    of the first section of this chapter, then you are responsible for everything
    yourself. We often come across a disconnect when people shift to managed services.
    Inevitably, someone will find a checkbox on their managed cloud database instance
    that says **Enable backups** and see that it is ticked. Then, they will rest easy
    at night, thinking their data is safe because it is nebulously “handled by the
    cloud.” If this sounds all too familiar (even if it doesn’t), you probably need
    to consider putting together a recovery action plan.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key factors need to be considered when creating this plan, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define your **recovery time objective** (**RTO**) and **recovery point objective**
    (**RPO**). Respectively, these two measures are the answers to the following questions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How much time can I afford for my service to be down?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How much data can I afford to lose when the service goes down?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A common anti-pattern here is to answer “None” and “Nothing.” Realistically,
    the costs of maintaining such a strategy are incongruent with reality. Typically,
    the question is answered in an order of magnitude, such as seconds, minutes, hours,
    or days.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once you’ve outlined the parameters for your recovery plan, you must design
    an architectural solution to achieve this. Suppose the scales we are looking at
    involve second or minute granularity. In that case, you likely need to look into
    live read replicas that can take over as the main DB in the case of failure or
    even multi-master DB configurations for ultra-low downtime applications. A solid
    incremental backup system is enough if we look at the order of hours or days (in
    the first scenario, we’d still have this requirement for resilience in depth).
    We can test our resilience architecture by detailing stressors our system may
    experience, such as a failure in a database instance or an entire cloud region
    going down. We then draft the system response when that stressor occurs, making
    changes to our architecture as required. There is an interesting parallel here
    between the stressors we choose to simulate and the actual resilience of our system.
    In the research and subsequent book by Barry O’Reilly, *Residues: Time, Change,
    and Uncertainty in Software Architecture*, he states that often our stressors
    do not exist in mutual exclusivity; for example, a network failure and a tsunami
    wiping out a data center are likely to have commonalities in the way our architecture
    will respond. Therefore, our stressor list does not need to be exhaustive. We
    just need to list and simulate stressors until our resultant architecture no longer
    requires any changes to support recovery from the stressor.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once we design the resilience architecture, we can start reviewing the action
    plan. The action plan is a detailed step-by-step manual for restoring service;
    think of it as the user guide to your resilient architecture. Identifying all
    the functional and non-functional requirements needed to complete the operation
    is essential. Some good questions to ask yourself are the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Who is going to act?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Where will they get the credentials?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the resource identifiers that they will be acting on?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the impacts on customers going to be?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What steps will they need to perform?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The final step is to run through your action plan. This dry run can be in a
    lower environment or a copy of production. Still, it’s essential to carry out
    the operations in your action plan to identify any gaps in the documentation.
    Ideally, you would do this with team members who are not involved in designing
    the action plan. This process prevents the team from performing the actions based
    on intent rather than the documentation itself. You can do this as often as required
    to refine the action plan until everyone is comfortable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Game day
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When soldiers train for combat, they don’t do it solely in a classroom or through
    reading copious, possibly outdated documentation. A core part of their readiness
    comes from training activities that simulate real-world scenarios as closely as
    possible. This regime means that when they respond to situations, they don’t just
    know what to do in theory; they have real-world knowledge. Your team should practice
    responding to incidents similarly, using conditions close to real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first stage with any game day is planning. At its inception, the game day
    should have a clearly defined scope and boundaries to ensure the safety of the
    scenario. The last thing you want is a hypothetical incident response becoming
    an actual incident! The planning should include a scenario that tests a specific
    action plan. These scenarios can be as real or as fake as you want, and your list
    of stressors from designing your architecture might be an excellent place to start.
    Some of my favorites are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A senior engineer with too many permissions (does this sound familiar from [*Chapter
    6*](B22364_06.xhtml#_idTextAnchor165)?) had production database access. They accidentally
    dropped one of our tables. How can we get the data back?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your database needs critical security updates that require a database restart.
    How will you ensure continuity?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Someone scheduled the deletion of the blob storage encryption key. Did we detect
    it? How will we prevent it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though the scenarios may be fake, the tools and processes used should be
    the same as those we use in a real incident. The response should be as close as
    possible to the required real-world response.
  prefs: []
  type: TYPE_NORMAL
- en: Remember those RTO and RPO goals we defined when formulating the plan? The game
    day is a perfect litmus test for those goals. Going into the event, everyone should
    be aware of these objectives, the deadline should be enforced, and, ideally, meeting
    the objective should be incentivized.
  prefs: []
  type: TYPE_NORMAL
- en: A game day is a great way to build inter-team communication and break down silos
    within the business. Involve all affected teams, even non-technical teams. How
    will sales operate with missing data? Does the marketing team need to create a
    statement? The implications of an actual event likely spread beyond the confines
    of the technical team, so why not utilize your simulated event to manage the complete
    response? Your technical team will need additional technical-only game days, but
    a full-scale game day can be highly productive to test your entire business’s
    resilience.
  prefs: []
  type: TYPE_NORMAL
- en: 'Executing the game day is fun: set up your simulated scenario, inform your
    operational team of the situation, and then watch them perform the recovery strategy.
    Make sure that the team is aware of the scope and boundaries of the game day before
    they begin executing to avoid the consequences we mentioned earlier. While testing
    your incident response, you should document your team’s actions. This process
    enables you to identify gaps in your existing action plan and refine it for future
    game days or an actual incident.'
  prefs: []
  type: TYPE_NORMAL
- en: This process should be followed by a healthy and blameless postmortem for both
    the simulated event (e.g., how did this theoretical event occur in the first place?
    How can we stop it from happening in the real world?) and the actual response
    itself (e.g., did we meet our RTO and RPO? Was our procedure efficient?).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the documentation generated during the execution phase after the
    event for a post-game day retrospective. This retrospective can follow the standard
    Agile retrospective format:'
  prefs: []
  type: TYPE_NORMAL
- en: What went well?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What went wrong?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What did we learn?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What changes can we make to do better next time?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can usually separate the points raised through this retrospective into two
    distinct categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Points about the execution of the recovery plan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Points about the execution of the game day itself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both are important to collect, but use the first set to feed into improving
    your recovery plan and the second set to host an even better game day next time!
  prefs: []
  type: TYPE_NORMAL
- en: The real thing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you follow the preceding advice when an actual incident occurs, the response
    should be that of a well-oiled machine rolling into action. That does not absolve
    you of your surrounding responsibilities. You should still do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Execute according to procedure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document all actions taken
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to achieve RTO and RPO targets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conduct a postmortem and retrospective
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will (hopefully!) get very few of these opportunities to execute the recovery
    plan for real, so this is where you will get your most valuable data.
  prefs: []
  type: TYPE_NORMAL
- en: Manual data ingestion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When talking to other engineers about problems they experience when writing
    code, they will often say that the computer is not doing what they are telling
    it to do. My answer is usually the same: “*Computers will do exactly what you
    tell them to do*.” There is an old joke that illustrates this point very well.
    A programmer’s partner asks them to go to the shops and pick up a loaf of bread,
    and if they have eggs, get a dozen. The programmer returns with a dozen loaves
    of bread. When questioned why, they reply, “*Well, they had eggs*.” Computers
    are literal, but when you finally have the computer exhibiting the behavior that
    you want, the good news is that it will execute the actions precisely the same
    ad infinitum, barring some external influence. The downside is that computers
    are bad at performing actions that we haven’t predicted. On the other hand, humans
    have evolved to excel at performing in situations we haven’t anticipated. However,
    you lose the perfect execution criteria of computers.'
  prefs: []
  type: TYPE_NORMAL
- en: What does this have to do with data? What would you choose if we want our data
    to be ingested the same way every time? A fallible human who might be able to
    sort out the edge cases on the fly or a significantly less fallible automated
    system that is deterministic in the way that the same input will always produce
    the same output?
  prefs: []
  type: TYPE_NORMAL
- en: The first data ingestion pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first stage of shifting to an automated data ingestion system is to define
    the happy path. We discussed this concept when talking about synthetic data. How
    would you want the system to operate if all your data was perfect? This allows
    you to feed perfect data into the system and receive perfect results. In an ideal
    world, we wouldn’t need to ever progress beyond this state. In my experience,
    I have never encountered a data source that met the perfect criteria. So, let
    us start pushing data through our pipeline, and if our data doesn’t hit our perfect
    criteria, we can deal with the issues as they arise. This might involve removing
    invalid records from the source dataset or manipulating the data to meet our perfect
    data criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'This has enabled us to combine the best of both worlds. Our automated system
    processes all of our well-formatted data to produce deterministic results, and
    our human operators can intervene when the computerized system cannot process
    the records. This allows the human element to exercise their judgment when required
    to allow all records to be correctly ingested. However, this setup still has one
    key issue: cloud services can quickly ingest our data, processing millions of
    records per second. On the other hand, while being more versatile, humans move
    at a glacial pace.'
  prefs: []
  type: TYPE_NORMAL
- en: Failure granularity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When ingesting data, we want to ensure we choose the correct failure granularity
    for our data ingestion pipeline. A naive approach would be to fail the pipeline
    whenever an error is encountered. As our datasets grow and our ingestion pipelines
    become more complex, the chances of the pipeline not experiencing a failure rapidly
    approaches zero. It is an infrequent case, in my experience, that a data pipeline
    provides value through an all-or-nothing approach.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, an incomplete dataset still offers more value than no data at all,
    and that is where this naive approach falls over. This is where it is crucial
    to consider your failure granularity. This means we need to discover the smallest
    unit of data that becomes non-functional when there is an error. This might mean
    we fail a single file, row/column, or cell in our dataset. By constraining the
    failure to the smallest unit of non-functional data, we can still leverage our
    dataset for other purposes, collect the failing units of data, and then process
    those failures asynchronously, enhancing the dataset as time goes on by using
    human judgment to deal with these edge cases.
  prefs: []
  type: TYPE_NORMAL
- en: This might consist of an automatic prefiltering stage that determines whether
    the data matches our specifications. Records that do match are passed onto our
    data ingestion pipeline, and records that do not match our specification are passed
    to a dead letter queue for later triaging.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling the pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Human labor for mundane tasks will always be the most expensive to scale. The
    human requirement to scale experiences hysteresis with the time required to hire,
    onboard, and train new resources. With the adoption of cloud native services,
    we barely even have to lift a finger to increase our throughput. In fact, with
    auto-scaling, even those few mouse clicks and keyboard strokes may be redundant!
  prefs: []
  type: TYPE_NORMAL
- en: Once the initial pipeline is built, the dead letter queue becomes a valuable
    resource. As we fix issues with data in the dead letter queue, we understand the
    types of problems we expect to see with our data. By analyzing how our human experts,
    with domain knowledge, rectify this problem, we can begin to provide edge case
    automation for these cases, codifying their knowledge into instructions that our
    pipeline can execute. As our pipeline scales, this automation allows it to improve
    its resilience, allowing our adaptable human element to deal with new problems
    requiring their expertise.
  prefs: []
  type: TYPE_NORMAL
- en: Automating these cases also allows us to increase the recency of our data. Rather
    than waiting for a human to come and rectify these errors after they have been
    detected as errors, we have extended our specification to include these types
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: Making the jump to streaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As our pipeline becomes increasingly automatic, and if our upstream data sources
    support it, we can increase the frequency of our data ingestion to be closer to
    real time. Instead of a manual ingestion process performed once a week due to
    human limitations, we can shift to running our pipeline much more frequently.
    We have seen clients achieve a shift from monthly data ingestions to hourly data
    ingestions with this process.
  prefs: []
  type: TYPE_NORMAL
- en: The final stage is rather than a schedule-driven process that pulls all data
    that has occurred in a period, we shift to a streaming model where the presence
    of new data kicks off the ingestion pipeline. The advantage of using cloud native
    services in this space is that, often, the scheduled pipelines you have already
    created can be run as streaming pipelines with minimal changes.
  prefs: []
  type: TYPE_NORMAL
- en: No observability for data transfer errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I will repeat the mantra used countless times throughout this book, “*You can’t
    fix what you can’t measure*.” The same is valid for data transfer. You need to
    be able to view the state of your data transfers so you can make informed decisions
    based on the data you have. The observability method is up to the user, but it
    is important to note that simply getting the observability data is half the battle.
    The other half is getting it in front of the eyes that will most impact the quality
    of your data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The data integrity dependency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let me pose a hypothetical scenario we have seen play out at clients all too
    often. You have a successful app with a great dev team. To better understand your
    customers, you create a new data team to track how users interact with your application.
    To accomplish this, your developers quickly cobble together some cloud native
    data pipeline tools to feed data to the data team. The data team struggles to
    make progress because the data coming through is of poor quality, so the data
    team spends excessive time simply getting the data to a usable state. This causes
    the data team to be less effective due to both lack of time and lack of good quality
    data. The development team is just throwing data over the metaphorical fence and
    letting the data team deal with the fallout. The development team is the beneficiary
    of the data, as they will be the ones who can consume the data artifacts that
    the data team produces to understand better what they are building. Here, we see
    the dichotomy: the data team is rarely the team that will benefit from the data,
    but they are the ones who need to ensure that the data is correct to show that
    they are doing their jobs.'
  prefs: []
  type: TYPE_NORMAL
- en: Inverting the dependency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I worked for a client previously in a company with a very large (non-software)
    engineering function. These engineers are tasked with ensuring that specific safety
    parameters are met. Part of that included ingesting sensor data from the field.
    One data engineer was responsible for maintaining the data pipeline. This configuration
    is all good in a static environment, but as we all know thanks to Werner Vogels,
    “*Everything fails all the time*.” What happened was that some sensors, data loggers,
    or even networking equipment would fail and be replaced, changing the topology
    of the data. The data would then show up as unrecognized, and the data engineer
    would go and chase down the responsible engineer for the correct parameters to
    ingest the data correctly. In this scenario, the data engineer did not benefit
    from the data but was responsible for reactively fixing the data. Alongside this
    client, we designed a solution that monitored pipeline health, found inconsistencies,
    and told the engineer responsible that the data was not being appropriately ingested.
    They could then log in to a UI to fix the data topology so it would be ingested
    correctly on the next run. As the responsibility for this data sat with the engineer,
    we noticed that not only did they reactively fix the data they were responsible
    for but they proactively went and updated the topology to prevent future pipeline
    failures. We had inverted the dependency!
  prefs: []
  type: TYPE_NORMAL
- en: This is the power of having the right eyes on the observability data and empowering
    the beneficiaries to maintain it themselves. This lets our data engineers focus
    instead on the bigger picture and deal with problems in the data domain rather
    than playing catchup with other domains.
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining the dependency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have inverted the data dependency between our producers and consumers,
    we can start to examine how to preserve the integrity of the link. As developers
    move forward, they rarely stop to think about their changes’ impact on the broader
    data ecosystem, of which their data is only a tiny part. The key to negotiating
    this minefield is typically through data contracts. A data contract is a specification
    that defines the format of the data that the application will produce. These specifications
    represent a mutual understanding of the underlying schema between data producers
    and consumers. If we use a common specification framework, such as JSON Schema,
    we can add tests for conformity of our data as part of the definition of done.
    This definition allows us to identify when we will cause breaking changes and
    preemptively notify downstream users that the schema is changing.
  prefs: []
  type: TYPE_NORMAL
- en: Mature operations in this space also allow for the adoption of more modern tools,
    such as data catalogs. These catalogs will enable us to register the data and
    its schema so that it can be utilized by anyone who needs it within the organization.
    It is also vital to centrally track these new dependencies as they grow so that
    we know who to notify when a data contract requires a breaking change.
  prefs: []
  type: TYPE_NORMAL
- en: So, now we have a solid understanding of how data observability is important
    for reacting to pipeline failures, preemptively acting, and treating our data
    services as first-class citizens in our application stack.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The cloud offers all new ways for us to manage one of our most important assets:
    our data! However, falling into the anti-patterns in this chapter can not only
    have implications for your bottom line but also for the durability, availability,
    and security of your data. By understanding the concepts in this chapter, you
    are well equipped to navigate the cloud native data jungle and build effective
    architectures. Next, we will look at how we can connect all the parts of our architecture
    together.'
  prefs: []
  type: TYPE_NORMAL
