<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer055">&#13;
			<h1 id="_idParaDest-59" class="chapter-number"><a id="_idTextAnchor060"/>3</h1>&#13;
			<h1 id="_idParaDest-60"><a id="_idTextAnchor061"/>Running Kubernetes with Other Cloud Pals</h1>&#13;
			<p>Chances are that throughout this book, and perhaps even so far, you’re going to get whiplash by finding out about the number of places and different ways you can deploy Kubernetes. The reality is, you’re going to get even more whiplash in the real world. Whether you’re a full-time Kubernetes engineer or a consultant, every company that you go to is going to feel a bit different in the ways you’re deploying Kubernetes and where you’re <span class="No-Break">deploying it.</span></p>&#13;
			<p>In the previous chapter, you learned about the three major Kubernetes cloud services – AKS, EKS, and GKE. However, there are a ton of other great options in the wild that are on private clouds and <strong class="bold">Platform-as-a-Service</strong> (<strong class="bold">PaaS</strong>) solutions. Although you’ll see a lot of organizations, ranging from start-ups to Fortune 200 companies and up, using popular Kubernetes cloud-based services such as AKS, EKS, and GKE, more and more organizations are starting to use private clouds for secondary Kubernetes clusters or even to save money because the larger cloud providers are typically far more expensive. Taking it to another level, some organizations are completely ditching the idea of having a Kubernetes cluster in the cloud and going PaaS, and you’ll see why in the <span class="No-Break">upcoming sections.</span></p>&#13;
			<p>By the end of this chapter and with the help of the previous chapter, you’ll be able to identify what solution your organization should go with and why it would be useful. From an individual perspective, you’ll walk away from this chapter with the know-how of multiple managed Kubernetes offerings. That way, you’ll be far more marketable in the job space and use all the <span class="No-Break">different platforms.</span></p>&#13;
			<p>In this chapter, we’re going to cover the <span class="No-Break">following topics:</span></p>&#13;
			<ul>&#13;
				<li>Understanding Linode <span class="No-Break">Kubernetes Engine</span></li>&#13;
				<li>Exploring DigitalOcean <span class="No-Break">Managed Kubernetes</span></li>&#13;
				<li>What is Kubernetes PaaS and how does <span class="No-Break">it differ?</span></li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-61"><a id="_idTextAnchor062"/>Technical requirements</h1>&#13;
			<p>For this chapter, you should already know a bit about cloud technologies. The gist is that all clouds are more or less the same. There are differences in the names of the services, but they’re all doing the same thing more <span class="No-Break">or less.</span></p>&#13;
			<p>If you’re comfortable with the cloud and have worked in a few cloud-based services, you’ll be successful in navigating <span class="No-Break">this chapter.</span></p>&#13;
			<p>To work inside the cloud-based services, you will need <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li>A <span class="No-Break">Linode account</span></li>&#13;
				<li>A <span class="No-Break">DigitalOcean account</span></li>&#13;
				<li>A Red <span class="No-Break">Hat account</span></li>&#13;
				<li>An AWS account (for the final section of <span class="No-Break">this chapter)</span></li>&#13;
			</ul>&#13;
			<p>You can sign up for all of these services and get free credit. Just ensure that you shut down the Kubernetes environments when you’ve finished running them to <span class="No-Break">save money.</span></p>&#13;
			<p>The code for this chapter can be found in this book’s GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch3"><span class="No-Break">https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch3</span></a><span class="No-Break">.</span></p>&#13;
			<h1 id="_idParaDest-62"><a id="_idTextAnchor063"/>Understanding Linode Kubernetes Engine</h1>&#13;
			<p>Linode, recently<a id="_idIndexMarker112"/> acquired by Akami Technologies, is a developer-friendly private cloud that is very well-known for its easy dashboard and feature-rich platform that isn’t overly complex. Linode focuses on ease of use with a cloud -for -all mindset. Some key callouts for Linode include transparent pricing with almost zero guesswork, easily scalable workloads, a full/public API, and a GUI-based cloud manager. Most of all, Linode is known for its <em class="italic">always human</em> <span class="No-Break">customer support.</span></p>&#13;
			<p>When it comes to comparing Linode and other private cloud providers, Linode sticks out by offering cloud GPUs and high outbound transfer speeds, along with its <span class="No-Break">customer support.</span></p>&#13;
			<p>In this section, you’re going to learn about why you’d want to use <strong class="bold">Linode Kubernetes Engine</strong> (<strong class="bold">LKE</strong>) and <a id="_idIndexMarker113"/>how to set up the LKE portal, create a Kubernetes cluster in LKE manually, take the same manual process to automate it, and deploy your <span class="No-Break">Kubernetes workloads.</span></p>&#13;
			<h2 id="_idParaDest-63"><a id="_idTextAnchor064"/>Why LKE?</h2>&#13;
			<p>When you’re<a id="_idIndexMarker114"/> choosing a cloud, the last thing you want is to have to guess how much your monthly bill is going to be. This is why people are nervous about the cloud and even more nervous about serverless technologies. The monthly cost can be unknown, which isn’t the best answer to give to a CFO. With Linode, costs are bundled together, so you know exactly what you’re going to pay for, and that’s very important for billing administrators and engineers alike. When it comes to scaling, both horizontally and vertically, the last thing that any engineer wants to have to sit and figure out manually is how much the environment is going to cost the company <span class="No-Break">every month.</span></p>&#13;
			<p>Another large cost saving is with the Control Plane. Much like any Kubernetes cloud service, the idea is to abstract the Kubernetes Control Plane/API server away from you. That way, you don’t have to worry about managing anything other than worker nodes and the application(s). Linode doesn’t charge for the Control Plane, whereas other clouds do. For example, EKS and GKE charge a per cluster management fee of $10 per hour or $73.00 per month. Although this may not seem like a lot, for a start-up that’s getting by with bootstrap funding and has enough bills, they most likely don’t want <span class="No-Break">one more.</span></p>&#13;
			<h2 id="_idParaDest-64"><a id="_idTextAnchor065"/>Setting up LKE manually</h2>&#13;
			<p>Now that <a id="_idIndexMarker115"/>you know the theory behind why you’d want to choose Linode, along with some pricing metrics and other aspects that make Linode great, it’s time to get hands-on and learn about setting <span class="No-Break">up LKE.</span></p>&#13;
			<p>For this section, ensure that you are signed into Linode via a web browser of your choosing. Follow <span class="No-Break">these steps:</span></p>&#13;
			<ol>&#13;
				<li value="1">On the Linode dashboard, <span class="No-Break">choose </span><span class="No-Break"><strong class="bold">Kubernetes</strong></span><span class="No-Break">:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer029" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_01.jpg" alt="Figure 3.1 – The LKE portal&#13;&#10;" width="364" height="637"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – The LKE portal</p>&#13;
			<ol>&#13;
				<li value="2">Click the <a id="_idIndexMarker116"/>blue <strong class="bold">Create </strong><span class="No-Break"><strong class="bold">Cluster</strong></span><span class="No-Break"> button:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer030" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_02.jpg" alt="Figure 3.2 – The Create Cluster button&#13;&#10;" width="666" height="449"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – The Create Cluster button</p>&#13;
			<ol>&#13;
				<li value="3">Choose a<a id="_idIndexMarker117"/> name for your cluster, what region/location you want the LKE cluster to reside in, and the Kubernetes <span class="No-Break">API version:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer031" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_03.jpg" alt="Figure 3.3 – Adding a Cluster label, region, and Kubernetes version&#13;&#10;" width="557" height="326"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – Adding a Cluster label, region, and Kubernetes version</p>&#13;
			<ol>&#13;
				<li value="4">When choosing node pools, you have a <span class="No-Break">few options:</span><ul><li><strong class="bold">Dedicated CPU</strong>: Good for workloads where consistent performance is crucial for <span class="No-Break">daily workflows</span></li><li><strong class="bold">Shared CPU</strong>: Good for medium workflows, such as a secret engine (something that isn’t getting a lot <span class="No-Break">of traffic)</span></li><li><strong class="bold">High Memory</strong>: Good for RAM-intensive applications, such as older Java applications, in-memory databases, and <span class="No-Break">cached data</span></li></ul></li>&#13;
			</ol>&#13;
			<p>For this section, you can choose <strong class="bold">Shared CPU</strong> as it’s the <span class="No-Break">most cost-effective:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer032" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_04.jpg" alt="Figure 3.4 – Worker node size&#13;&#10;" width="1018" height="611"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – Worker node size</p>&#13;
			<ol>&#13;
				<li value="5">To<a id="_idIndexMarker118"/> continue to keep things cost-effective, choose the <strong class="bold">Linode 2 GB</strong> option and ensure you scale it down to <span class="No-Break"><em class="italic">1</em></span><span class="No-Break"> node:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer033" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_05.jpg" alt="Figure 3.5 – The Add Node Pools page&#13;&#10;" width="985" height="278"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – The Add Node Pools page</p>&#13;
			<ol>&#13;
				<li value="6">In any production-level environment, you always want to think about <strong class="bold">high availability</strong> (<strong class="bold">HA</strong>). When it<a id="_idIndexMarker119"/> comes to Kubernetes, it’s no different. LKE offers the ability to enable HA for the Kubernetes Control Plane. For production environments, you’ll 100% want to implement this. For lab/dev environments (which is what you’re building now for learning purposes), you don’t have to enable HA. Once <a id="_idIndexMarker120"/>done, click the blue <strong class="bold">Create </strong><span class="No-Break"><strong class="bold">Cluster</strong></span><span class="No-Break"> button:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer034" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_06.jpg" alt="Figure 3.6 – HA Control Plane&#13;&#10;" width="317" height="716"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 – HA Control Plane</p>&#13;
			<p>Now that you’re familiar with the manual process of creating an LKE cluster, it’s time to learn how to automate it and make the process repeatable for <span class="No-Break">production-level environments.</span></p>&#13;
			<h2 id="_idParaDest-65"><a id="_idTextAnchor066"/>Automating LKE deployments</h2>&#13;
			<p>Now that you <a id="_idIndexMarker121"/>know how to create an LKE cluster manually, it’s time to learn how to create it with Terraform so you can ensure repeatable processes throughout your environment. In many production-level cases, you’ll run the following Terraform code within a CI/CD pipeline to ensure repeatability. For this section, you can run <span class="No-Break">it locally.</span></p>&#13;
			<p>First, you’ll see the <strong class="source-inline">main.tf</strong> configuration and then look <span class="No-Break">at </span><span class="No-Break"><strong class="source-inline">variables.tf</strong></span><span class="No-Break">.</span></p>&#13;
			<p>First, there’s<a id="_idIndexMarker122"/> the Terraform provider. The provider will utilize the newest version of the Linode Terraform provider. For Terraform to interact with the Linode API, you’ll need to pass in an API key that you can create from your <span class="No-Break">Linode account:</span></p>&#13;
			<pre class="source-code">&#13;
terraform {&#13;
  required_providers {&#13;
    linode = {&#13;
      source = "linode/linode"&#13;
    }&#13;
  }&#13;
}&#13;
provider "linode" {&#13;
  token = var.token&#13;
}</pre>&#13;
			<p>Next, there’s the <strong class="source-inline">linode_lke_cluster</strong> resource, which will create the LKE cluster. Within the dynamic block, you’ll see a <strong class="source-inline">for_each</strong> loop that specifies how many worker nodes will be created based on the pool amount. The pool amount is the number of worker nodes you want to deploy (between 3 to 4 is recommended <span class="No-Break">for production):</span></p>&#13;
			<pre class="source-code">&#13;
resource "linode_lke_cluster" "packtlke" {&#13;
    k8s_version = var.apiversion&#13;
    region = var.region&#13;
    dynamic "pool" {&#13;
        for_each = var.pools&#13;
        content {&#13;
            type  = pool.value["type"]&#13;
            count = pool.value["count"]&#13;
        }&#13;
    }&#13;
}</pre>&#13;
			<p>The last piece of code is the output of <strong class="source-inline">kubeconfig</strong>, which contains all of the authentication and authorization configurations to connect to the <span class="No-Break">Kubernetes cluster:</span></p>&#13;
			<pre class="source-code">&#13;
output "kubeconfig" {&#13;
   value = linode_lke_cluster.packtlke.kubeconfig&#13;
   sensitive = true&#13;
}</pre>&#13;
			<p>Now that <a id="_idIndexMarker123"/>you have the main Terraform configuration, you’ll need variables to pass in. These variables allow your code to stay repeatable so that you don’t have to continuously change hardcoded values or create new configurations for each environment. The reason why is that due to formatting, it may look out of the ordinary on your page while you’re reading <span class="No-Break">this chapter.</span></p>&#13;
			<p>For this section, you can take a look at the variables<a id="_idIndexMarker124"/> on GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/blob/main/Ch3/LKE/variables.tf"><span class="No-Break">https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/blob/main/Ch3/LKE/variables.tf</span></a><span class="No-Break">.</span></p>&#13;
			<p>Although these are all standard Terraform variables and don’t require much explanation, the one variable to point out is the <strong class="source-inline">pools</strong> variable. Notice how there’s a list type specified for the variable, which includes how many worker nodes and the size of the worker nodes on Linode. The reason why the variable is a list type is that in the <strong class="source-inline">main.tf</strong> configuration, the <strong class="source-inline">dynamic “pool”</strong> block calls for a list when using the <span class="No-Break"><strong class="source-inline">for</strong></span><span class="No-Break"> loop.</span></p>&#13;
			<p>One thing to keep in mind when it comes to LKE is understanding Linode. Although Linode is a great cloud provider, the truth is, it’s not going to have as many services and features for Kubernetes as, for example, EKS. Taking EKS as an example, there are IAM roles and RBAC-related permissions you can configure, DNS management with Route53, Secrets management, a container registry, and Fargate profiles for serverless Kubernetes. Even Azure and GCP have very similar services. With a provider such as Linode, however, they don’t. That’s not to discount Linode or say that they aren’t a good Kubernetes provider because the truth is, they very much are. However, a situation such as Linode not having IAM/RBAC built-in capabilities may be a deal breaker for many production <a id="_idIndexMarker125"/>engineering and <span class="No-Break">security teams.</span></p>&#13;
			<p>Now that you know how to create an LKE cluster both manually and automatically, it’s time to move on to the <span class="No-Break">next section.</span></p>&#13;
			<h1 id="_idParaDest-66"><a id="_idTextAnchor067"/>Exploring DigitalOcean Managed Kubernetes</h1>&#13;
			<p>DigitalOcean, much<a id="_idIndexMarker126"/> like Linode, markets toward the notion of an easy cloud to use compared to other large clouds with (what feels like) millions of services to choose from. DigitalOcean’s slogan is <em class="italic">Simpler cloud. Happier devs. Better results</em>. Over the years, DigitalOcean wasn’t only known for its cloud platform, but its blogs and how-to guides. DigitalOcean, for many engineers, became the standard go-to online location for learning how to do something in a hands-on fashion. Many writers use the DigitalOcean Technical Writing Guidelines that DigitalOcean created for writer/blogger <span class="No-Break">best practices.</span></p>&#13;
			<p>In this section, you’re going to learn about why you’d want to use <strong class="bold">DigitalOcean Managed Kubernetes</strong>, the pros<a id="_idIndexMarker127"/> of the Kubernetes service, setting up DigitalOcean Managed Kubernetes manually, and taking the same manual process, but doing it in an automated fashion <span class="No-Break">with Terraform.</span></p>&#13;
			<h2 id="_idParaDest-67"><a id="_idTextAnchor068"/>Why DigitalOcean Kubernetes Engine?</h2>&#13;
			<p>Since DigitalOcean<a id="_idIndexMarker128"/> was founded in 2011, developers around the globe have been using it for its ease of use and straightforward deployments. A lot of engineers even use DigitalOcean for hosting their projects (personal websites, blogs, servers, and so on). It’s far easier in many cases than having to worry about creating a bunch of services in a large <span class="No-Break">public cloud.</span></p>&#13;
			<p>From an ease-of-use perspective, DigitalOcean Kubernetes Engine does not disappoint. Much like any other Kubernetes service, the purpose is to abstract away the need to manage the underlying Control Plane/API server. The whole idea here is to lower the barrier of entry when it comes to using a <span class="No-Break">Kubernetes service.</span></p>&#13;
			<p>Compared to other products such as EKS/GKE/AKS, DigitalOcean Kubernetes Engine is more focused on the Day Two operations piece of Kubernetes. The complexities of a lot of the Kubernetes services out in the wild sometimes make engineers run away because they want<a id="_idIndexMarker129"/> something that <em class="italic">just works</em> out of <span class="No-Break">the box.</span></p>&#13;
			<p class="callout-heading">Important note</p>&#13;
			<p class="callout">DigitalOcean Managed Kubernetes, although easy to use, appears to be a bit out of date from a Kubernetes API perspective compared to its counterparts. Whereas many Kubernetes services offer Kubernetes API version v1.23 and above, DigitalOcean only offers up to v1.22.8 at the time of writing. Keep this in mind and remember to check as you may need different API versions. </p>&#13;
			<h2 id="_idParaDest-68"><a id="_idTextAnchor069"/>Setting up DigitalOcean Managed Kubernetes manually</h2>&#13;
			<p>Now that you <a id="_idIndexMarker130"/>know the theory behind why you’d want to choose DigitalOcean, along with some pricing metrics and other aspects that make DigitalOcean great, it’s time to get hands-on and learn about setting up DigitalOcean <span class="No-Break">Kubernetes Engine.</span></p>&#13;
			<p>For this section, ensure that you are signed into DigitalOcean via a web browser of your choosing. Follow <span class="No-Break">these steps:</span></p>&#13;
			<ol>&#13;
				<li value="1">On the DigitalOcean dashboard, <span class="No-Break">choose </span><span class="No-Break"><strong class="bold">Kubernetes</strong></span><span class="No-Break">:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer035" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_07.jpg" alt="Figure 3.7 – DigitalOcean Managed Kubernetes&#13;&#10;" width="599" height="651"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 – DigitalOcean Managed Kubernetes</p>&#13;
			<ol>&#13;
				<li value="2">Click<a id="_idIndexMarker131"/> the blue <strong class="bold">Create a Kubernetes </strong><span class="No-Break"><strong class="bold">Cluster</strong></span><span class="No-Break"> button:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer036" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_08.jpg" alt="Figure 3.8 – The Create a Kubernetes Cluster button&#13;&#10;" width="1099" height="415"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 – The Create a Kubernetes Cluster button</p>&#13;
			<ol>&#13;
				<li value="3">Choose your region, VPC name, and Kubernetes API version. The recommended API version, in general, is to always go with the latest unless you have a specific reason<a id="_idIndexMarker132"/> not to (the same rule applies to any <span class="No-Break">Kubernetes environment):</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer037" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_09.jpg" alt="Figure 3.9 – Adding cluster details&#13;&#10;" width="973" height="655"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9 – Adding cluster details</p>&#13;
			<ol>&#13;
				<li value="4">Choose the cluster capacity. The two very important sections here are as follows: <ul><li><strong class="bold">Machine type</strong>: For this, you’ll have to choose what’s best for you and your production environment. Although DigitalOcean doesn’t have as many options as Linode, you can choose from a basic node, Intel-based node, or AMD-based node from a CPU perspective. </li><li><strong class="bold">High availability Control Plane</strong>: For this, you’ll always want to ensure that the Control Plane is highly available. The Control Plane holds the scheduler, etcd, and many other important Kubernetes components. Without them, Kubernetes <span class="No-Break">wouldn’t work.</span></li></ul></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer038" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_10.jpg" alt="Figure 3.10 – Worker node size&#13;&#10;" width="1014" height="720"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.10 – Worker node size</p>&#13;
			<ol>&#13;
				<li value="5">Confirm <a id="_idIndexMarker133"/>your cluster by reviewing the monthly charge and clicking the green <strong class="bold">Create </strong><span class="No-Break"><strong class="bold">Cluster</strong></span><span class="No-Break"> button:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer039" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_11.jpg" alt="Figure 3.11 – Finalizing the cluster&#13;&#10;" width="636" height="647"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.11 – Finalizing the cluster</p>&#13;
			<p>Now that you’re<a id="_idIndexMarker134"/> familiar with the manual process of creating a DigitalOcean Kubernetes Engine cluster, it’s time to learn how to automate it and make the process repeatable for <span class="No-Break">production-level environments.</span></p>&#13;
			<h2 id="_idParaDest-69"><a id="_idTextAnchor070"/>Automating DigitalOcean Managed Kubernetes</h2>&#13;
			<p>From an <a id="_idIndexMarker135"/>automation perspective, you have a few options. Two of the most popular are the DigitalOcean CLI and <strong class="bold">Infrastructure as Code</strong> (<strong class="bold">IaC</strong>). In this <a id="_idIndexMarker136"/>section, you’ll learn how to create a DigitalOcean Managed Kubernetes cluster <span class="No-Break">using Terraform.</span></p>&#13;
			<p>In many production-level cases, you’ll run the following Terraform code within a CI/CD pipeline to ensure repeatability. For this section, you can run <span class="No-Break">it locally.</span></p>&#13;
			<p>Like we did for LKE, first, we’ll see the <strong class="source-inline">main.tf</strong> configuration and then you’ll take a look at <span class="No-Break"><strong class="source-inline">variables.tf</strong></span><span class="No-Break">.</span></p>&#13;
			<p>The Terraform configuration starts as all others do: with the provider. The DigitalOcean Terraform provider requires you to pass in a DigitalOcean API token, which you can generate from the <span class="No-Break">DigitalOcean UI:</span></p>&#13;
			<pre class="source-code">&#13;
terraform {&#13;
  required_providers {&#13;
    digitalocean = {&#13;
      source = "digitalocean/digitalocean"&#13;
    }&#13;
  }&#13;
}&#13;
provider "digitalocean" {&#13;
  token = var.do_token&#13;
}</pre>&#13;
			<p>Next, one<a id="_idIndexMarker137"/> resource block is needed, which is used to create the entire cluster and the node pools. These are DigitalOcean Droplets that end up being Kubernetes worker nodes. It also creates horizontal auto-scaling. For some DigitalOcean accounts, the maximum Droplet amount is three, so you’ll most likely want to increase that for <span class="No-Break">production environments:</span></p>&#13;
			<pre class="source-code">&#13;
resource "digitalocean_kubernetes_cluster" "packtdo" {&#13;
  name    = var.cluster_name&#13;
  region  = var.region&#13;
  version = var.k8s_version&#13;
  node_pool {&#13;
    name       = "autoscale-worker-pool"&#13;
    size       = "s-2vcpu-2gb"&#13;
    auto_scale = true&#13;
    min_nodes  = 2&#13;
    max_nodes  = 3&#13;
  }&#13;
}</pre>&#13;
			<p>Now that <a id="_idIndexMarker138"/>you have the Terraform configuration, you’ll need variables to pass in. These variables allow your code to stay repeatable so that you don’t have to continuously change hardcoded values or create new configurations for <span class="No-Break">each environment.</span></p>&#13;
			<p>There are four variables, <span class="No-Break">as follows:</span></p>&#13;
			<ul>&#13;
				<li><strong class="source-inline">region</strong>: The region that the DigitalOcean Kubernetes Engine cluster will <span class="No-Break">run in.</span></li>&#13;
				<li><strong class="source-inline">cluster_name</strong>: The name of the <span class="No-Break">Kubernetes cluster.</span></li>&#13;
				<li><strong class="source-inline">K8s_version</strong>: The Kubernetes <span class="No-Break">API version.</span></li>&#13;
				<li><strong class="source-inline">do_token</strong>: The DigitalOcean API token. For production-level environments, you’ll want to store this in some type of secret store and have Terraform retrieve it with a data block. Writing an API token into a variable and pushing it up to source control is a <span class="No-Break">big no-no:</span></li>&#13;
			</ul>&#13;
			<pre class="source-code">&#13;
variable "region" {&#13;
    type = string&#13;
    default = "nyc1"&#13;
}&#13;
variable "cluster_name" {&#13;
    type = string&#13;
    default = "packtdo01"&#13;
}&#13;
variable "k8s_version" {&#13;
    type = string&#13;
    default = "1.22.11-do.0"&#13;
}&#13;
variable "do_token" {&#13;
    type = string&#13;
    default = ""&#13;
    sensitive = true&#13;
}</pre>&#13;
			<p>Wrapping <a id="_idIndexMarker139"/>up this section on DigitalOcean, one thing to keep in mind is the same piece that was said in the <em class="italic">Understanding Linode Kubernetes Engine</em> section – bigger cloud providers are going to have more services that can tie into the managed Kubernetes offerings. This is something you’ll have to keep in mind as you decide what’s going to work best for <span class="No-Break">your environment.</span></p>&#13;
			<p>In the next and final section of this chapter, you’ll learn about PaaS with OpenShift from a theoretical and <span class="No-Break">hands-on perspective.</span></p>&#13;
			<h1 id="_idParaDest-70"><a id="_idTextAnchor071"/>What is Kubernetes PaaS and how does it differ?</h1>&#13;
			<p>Deploying Kubernetes clusters in different ways felt to engineers like it came in waves. First, there were raw Kubernetes clusters. You’d have to deploy everything manually, ranging from the Control Plane<a id="_idIndexMarker140"/> to the <strong class="bold">Certificate Authority</strong> (<strong class="bold">CA</strong>) and everything in between. After that, there were Kubernetes services in the cloud, such as AKS, GKE, and EKS. Now, there are serverless Kubernetes such as GKE AutoPilot and EKS Fargate, which you learned about in the <span class="No-Break">previous chapter.</span></p>&#13;
			<p>Another option that stands out, especially in the enterprise, is PaaS-based Kubernetes solutions such<a id="_idIndexMarker141"/> as Red <span class="No-Break">Hat’s </span><span class="No-Break"><strong class="bold">OpenShift</strong></span><span class="No-Break">.</span></p>&#13;
			<p>In this section, you’re going to learn about why you’d want to use OpenShift, how enterprises are utilizing PaaS-based Kubernetes such as OpenShift, and how to get started with a Dev environment right on your local computer with OpenShift, develop and deploy production-ready OpenShift clusters in major cloud providers, and deploy production-ready applications inside <span class="No-Break">of OpenShift.</span></p>&#13;
			<h2 id="_idParaDest-71"><a id="_idTextAnchor072"/>OpenShift</h2>&#13;
			<p>OpenShift is<a id="_idIndexMarker142"/> an odd paradox between full-blown Kubernetes and its own orchestration system. Underneath the hood, OpenShift uses Kubernetes. If you write a Kubernetes manifest for a Deployment, Pod, and so on, you can use it on OpenShift. Fundamentally, nothing changes when it comes to Kubernetes and OpenShift. However, there are differences in how you manage OpenShift versus how you manage Kubernetes. OpenShift is a PaaS, whereas Kubernetes can be managed with a cloud provider, so it sort of feels<a id="_idIndexMarker143"/> like <strong class="bold">Software as a Service</strong> (<strong class="bold">SaaS</strong>) and can be managed from a bare-metal perspective. Because Kubernetes is such a versatile platform, it can’t be put into <span class="No-Break">one category.</span></p>&#13;
			<p>One thing you must remember when it comes to OpenShift is that it’s enterprise-specific. There’s no reason that an engineer would run OpenShift for a lab environment other than to learn (which is what you’re doing in this chapter). With Kubernetes, you have far more deployment options and options regarding where you can deploy it. With OpenShift, you’re limited to a certain type of virtual machine and where/how you can deploy it. This isn’t necessarily a bad thing in the slightest. OpenShift wasn’t meant for engineers to do labs with like minikube and Docker Desktop. It was built with enterprise customers in mind. If you’re interested in diving deeper into this topic, I highly recommend reading <a id="_idIndexMarker144"/>this blog post from Tomasz Cholewa on comparing Kubernetes with <span class="No-Break">OpenShift: </span><a href="https://blog.cloudowski.com/articles/10-differences-between-openshift-and-kubernetes/"><span class="No-Break">https://blog.cloudowski.com/articles/10-differences-between-openshift-and-kubernetes/</span></a><span class="No-Break">.</span></p>&#13;
			<p>The definition of OpenShift, as per Red Hat, is that “<em class="italic">Red Hat OpenShift delivers a complete application platform for both traditional and cloud-native applications, allowing them to run anywhere. Built on Red Hat Enterprise Linux and compatible with Red Hat Ansible Automation Platform, Red Hat OpenShift enables automation inside and outside your </em><span class="No-Break"><em class="italic">Kubernetes clusters.</em></span><span class="No-Break">”</span></p>&#13;
			<p>Simply put, it allows you to orchestrate and manage containerized applications in a <span class="No-Break">PaaS environment.</span></p>&#13;
			<h2 id="_idParaDest-72"><a id="_idTextAnchor073"/>OpenShift in the enterprise</h2>&#13;
			<p>At this point, you <a id="_idIndexMarker145"/>may be wondering why anyone would want to use OpenShift over a standard Kubernetes deployment. Kubernetes has a ton of support, is supported by all major cloud providers, and is the latest and greatest. When it comes to the enterprise, Kubernetes is thought of a <span class="No-Break">bit differently.</span></p>&#13;
			<p>To leadership teams, Kubernetes is often thought of as a black box of magic and mystery that’s going to cost them a ton of money to maintain and support. The reality is that in enterprise environments, leadership teams want the ability to call a support number or contact an account executive when something breaks. They want the <em class="italic">enterprise software</em> so <a id="_idIndexMarker146"/>that if (when) something goes wrong, they know that the engineering teams have someone to call. Even though engineers will most likely spend more time waiting to hear back from support than doing it themselves, <em class="italic">enterprise licensing</em> gives leadership teams peace of mind. However, with <a id="_idIndexMarker147"/>peace of mind comes cost. OpenShift licensing is very expensive and remember, you have to run it somewhere, which will cost you money as well. If you run OpenShift in, for example, AWS, you’re paying for the cloud infrastructure running in AWS, OpenShift licensing, and Red Hat support. If you decide to go the OpenShift route, ensure that your leadership teams understand <span class="No-Break">the cost.</span></p>&#13;
			<p>From a technical and engineering perspective, OpenShift isn’t doing anything differently than what Kubernetes can do. Sure, to have Kubernetes do exactly what OpenShift does would require some work and engineering efforts to build it, but it’s all very much doable. Although OpenShift is a great platform, it’s not doing anything overly extraordinary compared <span class="No-Break">to Kubernetes.</span></p>&#13;
			<h2 id="_idParaDest-73"><a id="_idTextAnchor074"/>Getting started with OpenShift Sandbox</h2>&#13;
			<p>Before spending money on OpenShift, you can test it out using OpenShift ReadyContainers<a id="_idIndexMarker148"/> in a<a id="_idIndexMarker149"/> sandbox environment. Although the sandbox environment is not production-ready, it’s a great way to test out and familiarize yourself with how OpenShift works. It’s also great for lab environments! Follow <span class="No-Break">these steps:</span></p>&#13;
			<p class="callout-heading">Important note</p>&#13;
			<p class="callout">If you’re on an M1 Mac, OpenShift ReadyContainers<a id="_idIndexMarker150"/> are not currently supported for ARM devices, so this lab won’t work <span class="No-Break">for you.</span></p>&#13;
			<ol>&#13;
				<li value="1">Log into the <a id="_idIndexMarker151"/>Red Hat <span class="No-Break">console: </span><a href="https://console.redhat.com/"><span class="No-Break">https://console.redhat.com/</span></a><span class="No-Break">.</span></li>&#13;
				<li>Click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">OpenShift</strong></span><span class="No-Break">:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer040" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_12.jpg" alt="Figure 3.12 – Red Hat console&#13;&#10;" width="863" height="626"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.12 – Red Hat console</p>&#13;
			<ol>&#13;
				<li value="3">Choose the <span class="No-Break"><strong class="bold">Clusters</strong></span><span class="No-Break"> option:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer041" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_13.jpg" alt="Figure 3.13 – Clusters&#13;&#10;" width="1107" height="1013"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.13 – Clusters</p>&#13;
			<ol>&#13;
				<li value="4">Under<a id="_idIndexMarker152"/> the <strong class="bold">Clusters</strong> option, you’ll see three options – <strong class="bold">Cloud</strong>, <strong class="bold">Datacenter</strong>, and <strong class="bold">Local</strong>. <span class="No-Break">Choose </span><span class="No-Break"><strong class="bold">Local</strong></span><span class="No-Break">:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer042" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_14.jpg" alt="Figure 3.14 – Local cluster&#13;&#10;" width="1650" height="505"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.14 – Local cluster</p>&#13;
			<ol>&#13;
				<li value="5">Download OpenShift locally by clicking the blue <strong class="bold">Download OpenShift </strong><span class="No-Break"><strong class="bold">Local</strong></span><span class="No-Break"> button:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer043" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_15.jpg" alt="Figure 3.15 – Download OpenShift Local&#13;&#10;" width="1650" height="665"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.15 – Download OpenShift Local</p>&#13;
			<ol>&#13;
				<li value="6">Once <a id="_idIndexMarker153"/>OpenShift Local has been installed, you will need to run two commands (the instructions for installing CRC can be found <span class="No-Break">at </span><a href="https://crc.dev/crc/#minimum-system-requirements-operating-system_gsg"><span class="No-Break">https://crc.dev/crc/#minimum-system-requirements-operating-system_gsg</span></a><span class="No-Break">):</span><ol><li><strong class="source-inline">crc setup</strong>: Set up the configuration to authenticate to <span class="No-Break">Red Hat</span></li><li><strong class="source-inline">crc start</strong>: Start the local <span class="No-Break">OpenShift cluster:</span></li></ol></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer044" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_16.jpg" alt="Figure 3.16 – OpenShift Local setup&#13;&#10;" width="1650" height="411"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.16 – OpenShift Local setup</p>&#13;
			<ol>&#13;
				<li value="7">Once you run the <strong class="source-inline">crc start</strong> command, you’ll see an output on your terminal similar to <span class="No-Break">the following:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer045" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_17.jpg" alt="Figure 3.17 – Starting OpenShift Local&#13;&#10;" width="1645" height="488"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.17 – Starting OpenShift Local</p>&#13;
			<p>Depending<a id="_idIndexMarker154"/> on when you’re reading this and based on version changes, you may need some configurations, including passing in an OpenShift token to authenticate from your localhost. To keep these steps brief and since this information is already available from Red <a id="_idIndexMarker155"/>Hat, you can follow the installation instructions <span class="No-Break">here: </span><a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_local/2.5/html/getting_started_guide/installation_gsg#installing_gsg"><span class="No-Break">https://access.redhat.com/documentation/en-us/red_hat_openshift_local/2.5/html/getting_started_guide/installation_gsg#installing_gsg</span></a><span class="No-Break">.</span></p>&#13;
			<p>Although we didn’t touch on it in this section, there’s a second lab environment option known as OpenShift Sandbox, which is different than ReadyContainers. You can set up OpenShift Sandbox<a id="_idIndexMarker156"/> <span class="No-Break">here: </span><a href="https://developers.redhat.com/developer-sandbox"><span class="No-Break">https://developers.redhat.com/developer-sandbox</span></a><span class="No-Break">.</span></p>&#13;
			<p>Now that you know how to get an OpenShift sandbox up and running, let’s learn how to set up a production-ready OpenShift cluster <span class="No-Break">on AWS.</span></p>&#13;
			<h2 id="_idParaDest-74"><a id="_idTextAnchor075"/>OpenShift on AWS</h2>&#13;
			<p>CodeReady containers are amazing because they allow you to utilize your local computer to learn <a id="_idIndexMarker157"/>OpenShift, much like Minikube allows you to learn Kubernetes and<a id="_idIndexMarker158"/> Docker Desktop allows you to learn Docker <span class="No-Break">for free.</span></p>&#13;
			<p>Now that you know about the free version, let’s quickly dive into how to deploy OpenShift to the cloud. In this example, you’ll learn about AWS, but the other cloud providers that are supported have the <span class="No-Break">same workflow.</span></p>&#13;
			<p>For this section, ensure that you are logged into your AWS console via the AWS CLI and that you are also logged into your Red Hat account. Follow <span class="No-Break">these steps:</span></p>&#13;
			<ol>&#13;
				<li value="1">Log into <a id="_idIndexMarker159"/>the Red Hat<a id="_idIndexMarker160"/> <span class="No-Break">console: </span><a href="https://console.redhat.com/"><span class="No-Break">https://console.redhat.com/</span></a><span class="No-Break">.</span></li>&#13;
				<li>Click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">OpenShift</strong></span><span class="No-Break">:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer046" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_18.jpg" alt="Figure 3.18 – OpenShift manager&#13;&#10;" width="863" height="686"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.18 – OpenShift manager</p>&#13;
			<ol>&#13;
				<li value="3">Click the <strong class="bold">Clusters</strong> button and then click the blue <strong class="bold">Create </strong><span class="No-Break"><strong class="bold">cluster</strong></span><span class="No-Break"> button:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer047" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_19.jpg" alt="Figure 3.19 – Creating a cluster&#13;&#10;" width="1205" height="473"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.19 – Creating a cluster</p>&#13;
			<ol>&#13;
				<li value="4">You’ll see<a id="_idIndexMarker161"/> several options to choose from, including Azure and IBM <a id="_idIndexMarker162"/>Cloud. Click the blue <strong class="bold">Create cluster</strong> button under the <span class="No-Break">AWS option:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer048" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_20.jpg" alt="Figure 3.20 – AWS ROSA&#13;&#10;" width="1192" height="703"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.20 – AWS ROSA</p>&#13;
			<ol>&#13;
				<li value="5">The first page you’ll see associates your AWS account with Red Hat if you haven’t done so already. To do that, click the <strong class="bold">Select an account</strong> button and go through the walk-through of configuring ROSA, which is the Red Hat OpenShift service <span class="No-Break">on AWS:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer049" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_21.jpg" alt="Figure 3.21 – The Create a ROSA Cluster page&#13;&#10;" width="1046" height="719"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.21 – The Create a ROSA Cluster page</p>&#13;
			<ol>&#13;
				<li value="6">Once you’ve<a id="_idIndexMarker163"/> set up the AWS account permissions and roles for ROSA, the <a id="_idIndexMarker164"/>next page is all about configuring the cluster, which includes the cluster name, OpenShift version, region, and availability options. One of the cool options that OpenShift gives you is the ability to encrypt etcd and create persistent volumes with customer keys. This added security is typically looked at closely within <span class="No-Break">the enterprise:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer050" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_22.jpg" alt="Figure 3.22 – Adding cluster details&#13;&#10;" width="957" height="786"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.22 – Adding cluster details</p>&#13;
			<ol>&#13;
				<li value="7">Choose your <a id="_idIndexMarker165"/>machine pool, which includes the AWS EC2 instance size and<a id="_idIndexMarker166"/> <span class="No-Break">autoscaling capabilities:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer051" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_23.jpg" alt="Figure 3.23 – Worker node size&#13;&#10;" width="933" height="585"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.23 – Worker node size</p>&#13;
			<ol>&#13;
				<li value="8">Next are your networking options, which include whether you want the Kubernetes Control Plane/API server to be public or private, and whether you want to create a new AWS VPC for OpenShift or install the ROSA cluster into an existing VPC. Once you <a id="_idIndexMarker167"/>choose the <strong class="bold">Virtual Private Cloud (VPC)</strong> option, you’ll have to choose the<a id="_idIndexMarker168"/> CIDR ranges for the internal Kubernetes Pod networking and cluster <span class="No-Break">IP ranges:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer052" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_24.jpg" alt="Figure 3.24 – Networking configuration&#13;&#10;" width="1110" height="544"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.24 – Networking configuration</p>&#13;
			<ol>&#13;
				<li value="9">For cluster roles and policies, you have the option to manually set up the roles and policies or have OpenShift automatically do it <span class="No-Break">for you:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer053" class="IMG---Figure">&#13;
					<img src="Images/B19116_03_25.jpg" alt="Figure 3.25 – Cluster roles and policies&#13;&#10;" width="1165" height="506"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.25 – Cluster roles and policies</p>&#13;
			<ol>&#13;
				<li value="10">For the last<a id="_idIndexMarker169"/> step, you<a id="_idIndexMarker170"/> can choose how you want to implement updates for the ROSA cluster. The updates <a id="_idIndexMarker171"/>that occur are based on CVE scores from the <strong class="bold">National Vulnerability </strong><span class="No-Break"><strong class="bold">Database</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">NVD</strong></span><span class="No-Break">):</span><div id="_idContainer054" class="IMG---Figure"><img src="Images/B19116_03_26.jpg" alt="Figure 3.26 – Vulnerability scanning&#13;&#10;" width="1159" height="437"/></div></li>&#13;
			</ol>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.26 – Vulnerability scanning</p>&#13;
			<ol>&#13;
				<li value="11">Once you’ve <a id="_idIndexMarker172"/>filled in all your options, you can <a id="_idIndexMarker173"/>officially create your <span class="No-Break">ROSA cluster.</span></li>&#13;
			</ol>&#13;
			<p>Now that you know how to get up and running with OpenShift on-premises and in the cloud, let’s summarize <span class="No-Break">this chapter.</span></p>&#13;
			<h1 id="_idParaDest-75"><a id="_idTextAnchor076"/>Summary</h1>&#13;
			<p>Regardless of what option you decide to go with when deploying Kubernetes, whether it’s in a big cloud, a smaller cloud, or a PaaS solution, the goal is always the same and never changes – build an orchestration platform that can manage your <span class="No-Break">containerized applications.</span></p>&#13;
			<p>There are a lot of fancy tools out there, tons of different platforms, and many promises that each new and fancy platform will make your life easier from a Kubernetes perspective. The truth is, in one way or another, they all have the <span class="No-Break">same goal.</span></p>&#13;
			<p>The goal is to use Kubernetes to orchestrate and manage containerized applications. Ensure that as you go through each platform and tool, you have this in mind – <em class="italic">orchestrate my containerized applications</em>. If you keep that in mind, it’ll make choosing and getting through the marketing fluff <span class="No-Break">much easier.</span></p>&#13;
			<p>In the next chapter, we’ll be learning about on-premises Kubernetes and how understanding the underlying components of Kubernetes clusters is important, as well <span class="No-Break">as why.</span></p>&#13;
			<h1 id="_idParaDest-76"><a id="_idTextAnchor077"/>Further reading</h1>&#13;
			<p>To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break">following resource:</span></p>&#13;
			<ul>&#13;
				<li><em class="italic">Learning OpenShift</em>, by Denis Zuev, Artemii Kropachev, and Aleksey <span class="No-Break">Usov: </span><a href="https://www.packtpub.com/product/learn-openshift/9781788992329"><span class="No-Break">https://www.packtpub.com/product/learn-openshift/9781788992329</span></a></li>&#13;
			</ul>&#13;
		</div>&#13;
	</div></body></html>