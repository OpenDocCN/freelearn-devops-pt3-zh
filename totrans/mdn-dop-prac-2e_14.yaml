- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding Key Performance Indicators (KPIs) for Your Production Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we looked at the core concepts of modern DevOps –
    **Continuous Integration** (**CI**) and **Continuous Deployment/Delivery** (**CD**).
    We also looked at various tools and techniques that can help us enable a mature
    and secure DevOps channel across our organization. In this rather theory-focused
    chapter, we’ll try to understand some **key performance indicators** (**KPIs**)
    for operating our application in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the importance of reliability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SLOs, SLAs, and SLIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error budgets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recovery Time Objective (RPO) and Recovery Point Objective (RTO)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running distributed applications in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the importance of reliability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing software is one thing, and running it in production is another. The
    reason behind such a disparity is that most development teams cannot simulate
    production conditions in non-production environments. Therefore, many bugs are
    uncovered when the software is already running in production. Most issues encountered
    are non-functional – for example, the services could not scale properly with additional
    traffic, the amount of resources assigned to the application was suboptimal, thereby
    crashing the site, and many more. These issues need to be managed to make the
    software more reliable.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the importance of software reliability, let’s look at an example
    retail banking application. Software reliability is critically important for several
    reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User satisfaction**: Reliable software ensures a positive user experience.
    Users expect software to work as intended, and when it doesn’t, it can lead to
    frustration, loss of trust, and a poor reputation for the software or the organization
    behind it. For a bank’s retail customer, it might mean customers cannot do essential
    transactions and, therefore, may face hassles in payments and receipts, leading
    to a loss in user satisfaction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Business reputation**: Software failures can tarnish a company’s reputation
    and brand image. For our bank, if the issues are frequent, customers will look
    for other options, resulting in considerable churn and loss of business.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Financial impact**: Software failures can be costly. They can result in lost
    sales, customer support expenses, and even legal liabilities in cases where software
    failures cause harm or financial losses to users. This becomes especially critical
    for banking applications as customers'' money is involved. If transactions don’t
    happen in time, it can result in a loss of customer business, which will hurt
    the bank in the long run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Competitive advantage**: Reliable software can provide a competitive edge.
    Users are more likely to choose and stick with a bank with robust online banking
    software that consistently meets their needs and expectations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Productivity and efficiency**: Within organizations, reliable software is
    essential for maintaining productivity. Imagine the pain that the customer support
    and front office staff would have in such a disruption! You would also need more
    resources to manage these issues, which can disrupt operations, leading to wasted
    time and resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: Reliable software is often more secure. Attackers can exploit
    vulnerabilities and bugs in unreliable software. In the case of a bank, security
    is of prime importance because any breach can result in direct financial impact
    and loss. Ensuring reliability is a fundamental part of cybersecurity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compliance**: In some industries, especially banking, there are regulatory
    requirements related to software reliability. Failing to meet these requirements
    can result in legal and financial penalties.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customer trust**: Trust is a critical factor in software usage, especially
    in the case of a banking application. Users must trust that their money and data
    will be handled securely and that the software will perform as expected. Software
    reliability is a key factor in building and maintaining this trust.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maintainability**: Reliable software is typically easier to maintain. When
    software is unreliable, fixing bugs and updating becomes more challenging, potentially
    leading to a downward spiral of increasing unreliability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scaling and growth**: As software usage grows, reliability becomes even more
    critical. Software that works well for a small user base may struggle to meet
    the demands of a larger user base without proper reliability measures in place.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, software reliability is not just a technical concern; it has wide-reaching
    implications for user satisfaction, business success, and even legal and financial
    aspects. Therefore, investing in ensuring the reliability of software is a prudent
    and strategic decision for organizations.
  prefs: []
  type: TYPE_NORMAL
- en: Historically, running and managing software in production was the job of the
    Ops team, and most organizations still use it. The Ops team comprises a bunch
    of **system administrators** (**SysAdmins**) who must deal with the day-to-day
    issues of running the software in production. They implement scaling and fault
    tolerance with software, patch and upgrade software, work on support tickets,
    and keep the systems running so the software application functions well.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve all experienced the divide between Dev and Ops teams, each with its own
    goals, rules, and priorities. Often, they found themselves at odds because what
    benefited Dev (software changes and rapid releases) created challenges for Ops
    (stability and reliability).
  prefs: []
  type: TYPE_NORMAL
- en: However, the emergence of DevOps has changed this dynamic. In the words of Andrew
    Shafer and Patrick Debois, DevOps is a culture and practice in software engineering
    aimed at bridging the gap between software development and operations.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at DevOps from an Ops perspective, Google came up with **site reliability
    engineering** (**SRE**) as an approach that embodies DevOps principles. It encourages
    shared ownership, the use of common tools and practices, and a commitment to learning
    from failures to prevent recurring issues. The primary objective is to develop
    and maintain a dependable application without sacrificing the speed of delivery
    – a balance that was once thought contradictory (that is, *create better* *software
    faster*).
  prefs: []
  type: TYPE_NORMAL
- en: The idea of SRE is a novel thought about what would happen if we allowed software
    engineers to run the production environment. So, Google devised the following
    approach for running its Ops team.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Google, an ideal candidate for joining the SRE team should exhibit two
    key characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, they quickly become disinterested in manual tasks and seek opportunities
    to automate them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, they possess the requisite skills to develop software solutions, even
    when faced with complex challenges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, SREs should share an academic and intellectual background with
    the broader development organization. Essentially, SRE work, traditionally within
    the purview of operations teams, is carried out by engineers with strong software
    expertise. This strategy hinges on the natural inclination and capability of these
    engineers to design and implement automation solutions, thus reducing reliance
    on manual labor.
  prefs: []
  type: TYPE_NORMAL
- en: By design, SRE teams maintain a strong engineering focus. Without continuous
    engineering efforts, the operational workload escalates, necessitating an expansion
    of the team to manage the increasing demands. In contrast, a conventional operations-centric
    group scales in direct proportion to the growth of the service. If the services
    they support thrive, operational demands surge with increased traffic, compelling
    the hiring of additional personnel to perform repetitive tasks.
  prefs: []
  type: TYPE_NORMAL
- en: To avert this scenario, the team responsible for service management must incorporate
    coding into their responsibilities; otherwise, they risk becoming overwhelmed.
  prefs: []
  type: TYPE_NORMAL
- en: Accordingly, Google establishes a 50% upper limit on the aggregate “Ops” work
    allocated to all SREs, encompassing activities such as handling tickets, on-call
    duties, and manual tasks. This constraint guarantees that SRE teams allocate a
    substantial portion of their schedules to enhancing the stability and functionality
    of the service. While this limit serves as an upper bound, the ideal outcome is
    that, over time, SREs carry minimal operational loads and primarily engage in
    development endeavors as the service evolves to a self-sustaining state. Google’s
    objective is to create systems that are not merely automated but inherently self-regulating.
    However, practical considerations such as scaling and introducing new features
    continually challenge SREs.
  prefs: []
  type: TYPE_NORMAL
- en: SREs are meticulous in their approach, relying on measurable metrics to track
    progress toward specific goals. For instance, stating that a website is *running
    slowly* is vague and unhelpful in an engineering context. However, declaring that
    the 95th percentile of response time has exceeded the **service-level objective**
    (**SLO**) by 10% provides precise information. SREs also focus on reducing repetitive
    tasks, known as **toil**, by automating them to prevent burnout. Now, let’s look
    at some of the key SRE performance indicators.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding SLIs, SLOs, and SLAs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the realm of site reliability, three crucial parameters guide SREs: the
    **indicators of availability** – **service-level indicators** (**SLIs**), the
    **definition of availability** –SLOs, and the **consequences of unavailability**
    – **service-level agreements** (**SLAs**). Let’s start by exploring SLIs in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: SLIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SLIs serve as quantifiable reliability metrics. Google defines them as “*carefully
    defined quantitative measures of some aspect of the level of service provided.*”
    Common examples include request latency, failure rate, and data throughput. SLIs
    are specific to user journeys, which are sequences of actions users perform to
    achieve specific goals. For instance, a user journey for our sample Blog App might
    involve creating a new blog post.
  prefs: []
  type: TYPE_NORMAL
- en: 'Google, the original advocate of SRE, has identified four golden signals that
    apply to most user journeys:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latency**: This measures the time it takes for your service to respond to
    user requests'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Errors**: This indicates the percentage of failed requests, highlighting
    issues in service reliability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Traffic**: Traffic represents the demand directed toward your service, reflecting
    its usage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Saturation**: Saturation assesses how fully your infrastructure components
    are utilized'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One recommended approach by Google to calculate SLIs is by determining the
    ratio of good events to valid events:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: A perfect SLI score of 100 implies everything functions correctly, while a score
    of 0 signifies widespread issues.
  prefs: []
  type: TYPE_NORMAL
- en: A valuable SLI should align closely with the user experience. For example, a
    lower SLI value should correspond to decreased customer satisfaction. If this
    alignment is absent, the SLI may not provide meaningful insights or be worth measuring.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the following figure to understand this better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – Good versus bad SLI](img/B19877_14_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.1 – Good versus bad SLI
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the CPU Utilisation SLI does not reflect customer satisfaction
    in any way; that is, there is no direct correlation between increasing CPU Utilisation
    and decreased customer satisfaction except after it crosses the 80% threshold.
    In contrast, the Latency SLI directly correlates with customer satisfaction, which
    reduces with increasing latency and significantly after the 300ms and 500ms levels.
    Therefore, it is a good idea to use Latency as an SLI over CPU Utilization.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also advisable to limit the number of SLIs to a manageable quantity. Too
    many SLIs can lead to team confusion and trigger numerous false alarms. It’s best
    to focus on four or five metrics directly linked to customer satisfaction. For
    instance, instead of monitoring CPU and memory usage, prioritize metrics such
    as request latency and error rate.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, prioritizing user journeys is essential, giving higher importance
    to journeys that significantly impact customers and lower importance to those
    with less of a customer impact. For example, ensuring a seamless create and update
    post experience in our Blog App is more critical than the reviews and ratings
    service. SLIs alone do not make much sense as they are just measurable indicators.
    We need to set objectives for SLIs. So, let’s look at SLOs.
  prefs: []
  type: TYPE_NORMAL
- en: SLOs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google’s definition of SLOs states that they “*establish a target level for
    the reliability of your service.*” They specify the percentage of compliance with
    SLIs required to consider your site reliable. SLOs are formulated by combining
    one or more SLIs.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if you have an SLI that mandates *request latency to remain below
    500ms within the last 15 minutes with a 95th percentile measurement*, an SLO would
    necessitate *the SLI to be met 99% of the time for a* *99% SLO*.
  prefs: []
  type: TYPE_NORMAL
- en: While every organization aims for 100% reliability, setting a 100% SLO is not
    a practical goal. A system with a 100% SLO tends to be costly, technically complex,
    and often unnecessary for most applications to be deemed acceptable by their users.
  prefs: []
  type: TYPE_NORMAL
- en: In the realm of software services and systems, the pursuit of 100% availability
    is generally misguided because users cannot feel any practical distinction between
    a system that is 100% available and one that is 99.999% available. Multiple intermediary
    systems exist between the user and the service, such as their personal computer,
    home Wi-Fi, **Internet Service Provider** (**ISP**), and the power grid, and these
    collectively exhibit availability far lower than 99.999%. Consequently, the negligible
    difference between 99.999% and 100% availability becomes indistinguishable amidst
    the background noise of other sources of unavailability. Thus, investing substantial
    effort to attain that last 0.001% availability yields no noticeable benefit to
    the end user.
  prefs: []
  type: TYPE_NORMAL
- en: 'In light of this understanding, a question arises: if 100% is an inappropriate
    reliability target, what constitutes the right reliability target for a system?
    Interestingly, this is not a technical inquiry but rather a product-related one,
    necessitating consideration of the following factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User satisfaction**: Determining the level of availability that aligns with
    user contentment, considering their typical usage patterns and expectations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alternatives**: Evaluating the availability of alternatives available to
    dissatisfied users, should they seek alternatives due to dissatisfaction with
    the product’s current level of availability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User behavior**: Examining how users’ utilization of the product varies at
    different availability levels, recognizing that user behavior may change in response
    to fluctuations in availability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreover, a completely reliable application leaves no room for the introduction
    of new features, as any new addition has the potential to disrupt the existing
    service. Therefore, some margin for error must be built into your SLO.
  prefs: []
  type: TYPE_NORMAL
- en: SLOs represent internal objectives that require consensus among the team and
    internal stakeholders, including developers, product managers, SREs, and CTOs.
    They necessitate the commitment of the entire organization. Not meeting an SLO
    does not carry explicit or implicit penalties.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a customer cannot claim damages if an SLO is not met, but it may
    lead to dissatisfaction within organizational leadership. This does not imply
    that failing to meet an SLO should be consequence-free. Falling short of an SLO
    typically results in fewer changes and reduced feature development, potentially
    indicating a decline in quality and increased emphasis on the development and
    testing functions.
  prefs: []
  type: TYPE_NORMAL
- en: SLOs should be realistic, with the team actively working to meet them. They
    should align with the customer experience, ensuring that if the service complies
    with the SLO, customers do not perceive any service quality issues. If performance
    falls below the defined SLOs, it may affect the customer experience, but not to
    the extent that customers raise support tickets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some organizations implement two types of SLOs: **achievable** and **aspirational**.
    The achievable SLO represents a target the entire team should reach, while the
    aspirational SLO sets a higher goal and is part of an ongoing improvement process.'
  prefs: []
  type: TYPE_NORMAL
- en: SLAs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: According to Google, SLAs are “*formal or implicit agreements with your users
    that outline the repercussions of meeting (or failing to meet) the* *contained
    SLOs.*”
  prefs: []
  type: TYPE_NORMAL
- en: These agreements are of a more structured nature and represent business-level
    commitments made to customers, specifying the actions that will be taken if the
    organization fails to fulfill the SLA. SLAs can be either explicit or implicit.
    An explicit SLA entails well-defined consequences, often in terms of service credits,
    in case the expected reliability is not achieved. Implicit SLAs are evaluated
    in terms of potential damage to the organization’s reputation and the likelihood
    of customers switching to alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: SLAs are typically established at a level that is sufficient to prevent customers
    from seeking alternatives, and consequently, they tend to have lower thresholds
    compared to SLOs. For instance, when considering the request latency SLI, the
    SLO might be defined at a *300ms* SLI value, while the SLA could be set at a *500ms*
    SLI value. This distinction arises from the fact that SLOs are internal targets
    related to reliability, whereas SLAs are external commitments. By striving to
    meet the SLO, the team automatically satisfies the SLA, providing an added layer
    of protection for the organization in case of unexpected failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the correlation between SLIs, SLOs, and SLAs, let’s look at the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – SLIs, SLOs, and SLAs](img/B19877_14_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.2 – SLIs, SLOs, and SLAs
  prefs: []
  type: TYPE_NORMAL
- en: This figure shows how customer experience changes with the level of latency.
    If we keep the latency SLO at *300ms* and meet it, everything is good! Anything
    between *300ms* to *500ms* and the customer starts experiencing some degradation
    in performance, but that is not enough for them to lose their cool and start raising
    support tickets. Therefore, keeping the SLA at *500ms* is a good strategy. As
    soon we cross the *500ms* threshold, unhappiness sinks in, and the customer starts
    raising support tickets for service slowness. If things cross the *10s* mark,
    then it is a cause of worry for your Ops team, and *Everything is burning* at
    this stage. However, as we know, the wording of SLOs is slightly different from
    what we imagine here. When we say that we have an SLO for *300ms* latency, it
    does not mean anything. A realistic SLO for an SLI mandating *request latency
    to remain below 300ms within the last 15 minutes with a 95th percentile measurement*
    would be to meet *the SLI x% of the time*. What should that x be? Should it be
    *99%*, or should it be *95%*? How do we decide this number? Well, for that, we’ll
    have to look at **error budgets**.
  prefs: []
  type: TYPE_NORMAL
- en: Error budgets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As defined by Liz Fong-Jones and Seth Vargo, error budgets represent “*a quantitative
    measure shared between product and SRE teams to balance innovation* *and stability.*”
  prefs: []
  type: TYPE_NORMAL
- en: In simpler terms, an error budget quantifies the level of risk that can be taken
    to introduce new features, conduct service maintenance, perform routine enhancements,
    manage network and infrastructure disruptions, and respond to unforeseen situations.
    Typically, the monitoring system measures the uptime of your service, while SLOs
    establish the target you aim to achieve. The error budget is the difference between
    these two metrics and represents the time available to deploy new releases, provided
    it falls within the error budget limits.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is precisely why a *100%* SLO is not usually set initially. Error budgets
    serve the crucial purpose of helping teams strike a balance between innovation
    and reliability. The rationale behind error budgets lies in the SRE perspective
    that failures are a natural and expected part of system operations. Consequently,
    whenever a new change is introduced into production, there is an inherent risk
    of disrupting the service. Therefore, a higher error budget allows for introducing
    more features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For instance, if your SLO is *99%*, your error budget would be *1%*. If you
    calculate this over a month, assuming *30 days/month* and *24 hours/day*, you
    will have a *7.2-hour* error budget to allocate for maintenance or other activities.
    For a *99.9%* SLO, the error budget would be *43.2 minutes* per month, and for
    a *99.99%* SLO, it would be *4.32 minutes* monthly. You can refer to the following
    figure for more details:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.3 – Error budgets versus SLOs](img/B19877_14_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.3 – Error budgets versus SLOs
  prefs: []
  type: TYPE_NORMAL
- en: These periods represent actual downtime, but if your services have redundancy,
    high availability measures, and disaster recovery plans in place, you can potentially
    extend these durations because the service remains operational while you patch
    or address issues with one server.
  prefs: []
  type: TYPE_NORMAL
- en: Now, whether you want to keep on adding *9s* within your SLO or aim for a lower
    number would depend on your end users, business criticality, and availability
    requirements. A higher SLO is more costly and requires more resources than a lower
    SLO. However, sometimes, just architecting your application correctly can help
    you get to a better SLO target.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand SLOs, SLIs, SLAs, and error budgets, let’s talk about
    disaster recovery.
  prefs: []
  type: TYPE_NORMAL
- en: Disaster recovery, RTO, and RPO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Disaster recovery** is a comprehensive strategy that’s designed to ensure
    an organization’s resilience in the face of unexpected, disruptive events, such
    as natural disasters, cyberattacks, or system failures. It involves the planning,
    policies, procedures, and technologies necessary to quickly and effectively restore
    critical IT systems, data, and operations to a functional state. A well-implemented
    disaster recovery plan enables businesses to minimize downtime, data loss, and
    financial impact, helping them maintain business continuity, protect their reputation,
    and swiftly recover from adversities, ultimately safeguarding their long-term
    success.'
  prefs: []
  type: TYPE_NORMAL
- en: Every organization incorporates disaster recovery to varying degrees. Some opt
    for periodic backups or snapshots, while others invest in creating failover replicas
    of their production environment. Although failover replicas offer increased resilience,
    they come at the expense of doubling infrastructure costs. The choice of disaster
    recovery mechanism that an organization adopts hinges on two crucial KPIs – **Recovery
    Time Objective** (**RTO)** and **Recovery Point** **Objective** (**RPO)**.
  prefs: []
  type: TYPE_NORMAL
- en: RTO and RPO are crucial metrics in disaster recovery and business continuity
    planning. RTO represents the maximum acceptable downtime for a system or application,
    specifying the time within which it should be restored after a disruption. It
    quantifies the acceptable duration of service unavailability and drives the urgency
    of recovery efforts.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, RPO defines the maximum tolerable data loss in the event
    of a disaster. It signifies the point in time to which data must be recovered
    to ensure business continuity. Achieving a low RPO means that data loss is minimized,
    often by frequent data backups and replication. The following figure explains
    RTO and RPO beautifully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.4 – RTO and RPO](img/B19877_14_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.4 – RTO and RPO
  prefs: []
  type: TYPE_NORMAL
- en: A shorter RTO and RPO demand a more robust disaster recovery plan, which, in
    turn, results in higher costs for both infrastructure and human resources. Therefore,
    balancing RTO and RPO is essential to ensure a resilient IT infrastructure. Organizations
    must align their recovery strategies with these objectives to minimize downtime
    and data loss, thereby safeguarding business operations and data integrity during
    unforeseen disruptions.
  prefs: []
  type: TYPE_NORMAL
- en: Running distributed applications in production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we’ve been discussing KPIs for running an application in production,
    taking inspiration from SRE principles. Now, let’s understand how we will put
    these thoughts in a single place to run a distributed application in production.
  prefs: []
  type: TYPE_NORMAL
- en: A **distributed application** or a **microservice** is inherently different
    from a monolith. While managing a monolith revolves around ensuring all operational
    aspects of one application, the complexity increases manyfold with microservices.
    Therefore, we should take a different approach to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the perspective of SRE, running a distributed application in production
    entails focusing on ensuring the application’s *reliability*, *scalability*, and
    *performance*. Here’s how SREs approach this task:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SLOs**: SREs begin by defining clear SLOs that outline the desired level
    of reliability for the distributed application. SLOs specify the acceptable levels
    of *latency*, *error rates*, and *availability*. These SLOs are crucial in guiding
    the team’s efforts and in determining whether the system is meeting its reliability
    goals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SLIs**: SREs establish SLIs, which are quantifiable metrics that are used
    to measure the reliability of the application. These metrics could include response
    times, error rates, and other performance indicators. SLIs provide a concrete
    way to assess whether the application meets its SLOs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error budgets**: Error budgets are a key concept in SRE. They represent the
    permissible amount of downtime or errors that can occur before the SLOs are violated.
    SREs use error budgets to strike a balance between reliability and innovation.
    If the error budget is exhausted, it may necessitate a focus on stability and
    reliability over introducing new features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and alerting**: SREs implement robust monitoring and alerting
    systems to continuously track the application’s performance and health. They set
    up alerts based on SLIs and SLOs, enabling them to respond proactively to incidents
    or deviations from desired performance levels. In the realm of distributed applications,
    using a service mesh such as **Istio** or **Linkerd** can help. They help you
    visualize parts of your application through a single pane of glass and allow you
    to monitor your application and alert on it with ease.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Capacity planning**: SREs ensure that the infrastructure supporting the distributed
    application can handle the expected load and traffic. They perform capacity planning
    exercises to scale resources as needed, preventing performance bottlenecks during
    traffic spikes. With modern public cloud platforms, automating scalability with
    traffic is all the more easy to implement, especially with distributed applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated remediation**: Automation is a cornerstone of SRE practices. SREs
    develop automated systems for incident response and remediation. This includes
    *auto-scaling*, *self-healing mechanisms*, and *automated rollback procedures*
    to minimize downtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chaos engineering**: SREs often employ chaos engineering practices to introduce
    controlled failures into the system deliberately. This helps identify weaknesses
    and vulnerabilities in the distributed application, allowing for proactive mitigation
    of potential issues. Some of the most popular chaos engineering tools are Chaos
    Monkey, Gremlin, Chaos Toolkit, Chaos Blade, Pumba, ToxiProxy, and Chaos Mesh.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**On-call and incident management**: SREs maintain on-call rotations to ensure
    24/7 coverage. They follow well-defined incident management processes to resolve
    issues quickly and learn from incidents to prevent recurrence. Most SRE development
    backlogs come from this process as they learn from failures and, therefore, automate
    repeatable tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous improvement**: SRE is a culture of continuous improvement. SRE
    teams regularly conduct **post-incident reviews** (**PIRs**) and **root cause
    analyses** (**RCAs**) to identify areas for enhancement. Lessons learned from
    incidents are used to refine SLOs and improve the overall reliability of the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Documentation and knowledge sharing**: SREs document *best practices*, *runbooks*,
    and *operational procedures*. They emphasize knowledge sharing across teams to
    ensure that expertise is not siloed and that all team members can effectively
    manage and troubleshoot the distributed application. They also aim to automate
    the runbooks to ensure that manual processes are kept at a minimum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, SRE’s approach to running a distributed application in production
    focuses on *reliability*, *automation*, and *continuous improvement*. It sets
    clear goals, establishes metrics for measurement, and employs proactive monitoring
    and incident management practices to deliver a highly available and performant
    service to end users.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter covered SRE and the KPIs for running our service in production.
    We started by understanding software reliability and examined how to manage an
    application in production using SRE. We discussed the three crucial parameters
    that guide SREs: SLI, SLO, and SLA. We also explored error budgets and their importance
    in introducing changes within the system. Then, we looked at software disaster
    recovery, RPO, and RTO and how they define how complex or costly our disaster
    recovery measures will be. Finally, we looked at how DevOps or SRE will use these
    concepts to manage a distributed application in production.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will put what we’ve learned to practical use and explore
    how to manage all these aspects using a service mesh called Istio.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Answer the following questions to test your knowledge of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following is a good example of an SLI?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. The response time should not exceed 300ms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. The 95th percentile of response time in a 15-minute window should not exceed
    300ms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. 99% of all requests should respond within 300ms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. The number of failures should not exceed 1%.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A mature organization should have a 100% SLO. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SLOs are not tied to any customer-initiated punitive action. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of the following should you consider while deciding on an SLO? (Choose
    three)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. User satisfaction
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Alternatives
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. User behavior
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. System capacity
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: SLAs are generally kept to a stricter SLI value than SLOs. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of the following should you consider while defining SLIs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. CPU, memory, and disk utilization
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Latency, errors, traffic, and saturation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Utilization, capacity, and scale
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: An error budget of 1% provides how much scope for downtime per month?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. 72 hours
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. 43.2 minutes
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. 7.2 hours
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. 4.32 minutes
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: An SRE is a software developer doing Ops. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What minimum percent of the time should an SRE allocate to development work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. 30%
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. 40%
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. 50%
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. 60%
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are the answers to this chapter’s questions:'
  prefs: []
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A, B, C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
