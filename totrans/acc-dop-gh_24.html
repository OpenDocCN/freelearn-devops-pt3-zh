<html><head></head><body>
		<div id="_idContainer250">
			<h1 id="_idParaDest-366"><em class="italic"><a id="_idTextAnchor368"/>Chapter 19</em>: Experimentation and A|B Testing</h1>
			<p>In this chapter, we will discuss how you can evolve and continuously improve your products by conducting experiments to validate hypotheses through evidence-based DevOps practices, such as <strong class="bold">A|B testing</strong>. This is sometimes called <strong class="bold">hypothesis-driven development</strong> or just <strong class="bold">experimentation</strong>.</p>
			<p>The following topics will be covered in this chapter:</p>
			<ul>
				<li>Conducting experiments with the scientific method</li>
				<li>Effective A|B testing with GrowthBook and Flagger</li>
				<li>Experimentation and OKR</li>
			</ul>
			<h1 id="_idParaDest-367"><a id="_idTextAnchor369"/>Conducting experiments with the scientific method</h1>
			<p>Traditionally, requirements management was more guesswork than science. The closest that <a id="_idIndexMarker1216"/>came to a scientific approach<a id="_idIndexMarker1217"/> were interviews or market research in general. The problem with this approach is that you cannot ask people what they do not yet know. You can ask them what they want but not what they need, as they probably won’t know that yet, especially in a market segment that gets disrupted.</p>
			<p>The idea of hypothesis-driven development is to apply the <strong class="bold">scientific method</strong> to product management, an empirical method of acquiring evidence-based knowledge.</p>
			<p>The scientific method is a process of experimentation used to explore observations and answer questions that aim to discover cause-and-effect relationships. It follows certain process steps (see <em class="italic">Figure 19.1</em>):</p>
			<div>
				<div id="_idContainer243" class="IMG---Figure">
					<img src="image/B17827_19_001.jpg" alt="Figure 19.1 – The scientific method&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 19.1 – The scientific method</p>
			<p>We will look at the various steps in detail:</p>
			<ol>
				<li><strong class="bold">Observation</strong>: Observing<a id="_idIndexMarker1218"/> reality using all five senses: smell, sight, sound, touch, and taste.</li>
				<li><strong class="bold">Question</strong>: Formulating a question<a id="_idIndexMarker1219"/> based on observation and existing research or previous experiments.</li>
				<li><strong class="bold">Hypothesis</strong>: Stating a hypothesis<a id="_idIndexMarker1220"/> based on knowledge obtained while formulating the question. The hypothesis is a prediction of what you think will occur based on observation and research. Hypotheses are often written in the <em class="italic">if … then …</em> form, for example: “<em class="italic">if</em> we modify this variable, <em class="italic">then</em> we expect this change to be observable.”</li>
				<li><strong class="bold">Experiment</strong>: The experiment<a id="_idIndexMarker1221"/> proves or disproves the hypothesis. In the experiment, you have different variables. <strong class="bold">Independent variables</strong> are <a id="_idIndexMarker1222"/>the ones you change to trigger a result. <strong class="bold">Dependent variables</strong> are the <a id="_idIndexMarker1223"/>things you measure and expect to change. In the experiment, you collect <strong class="bold">qualitative data</strong> through <a id="_idIndexMarker1224"/>observations and <strong class="bold">quantitative data</strong> by <a id="_idIndexMarker1225"/>measuring and collecting metrics.</li>
			</ol>
			<p>Experiments<a id="_idIndexMarker1226"/> also use control groups to prove that the variance is more than just chance. To test treatment with a drug, you must design an experiment in which a portion of the population – the <strong class="bold">control group</strong> – is left <a id="_idIndexMarker1227"/>untreated and given a placebo, while the <strong class="bold">experimental group</strong> is <a id="_idIndexMarker1228"/>treated with the potential drug (see <em class="italic">Figure 19.2</em>).</p>
			<div>
				<div id="_idContainer244" class="IMG---Figure">
					<img src="image/B17827_19_002.jpg" alt="Figure 19.2 – Conducting a scientific experiment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 19.2 – Conducting a scientific experiment</p>
			<p>To have a<a id="_idIndexMarker1229"/> good experiment, you should only change <em class="italic">one variable at a time</em> while keeping all other variables the same. You should also <em class="italic">avoid bias</em>. No matter how hard you try, bias can sneak so easily into your observations and conclusions.</p>
			<ol>
				<li value="5"><strong class="bold">Conclusion</strong>: After the<a id="_idIndexMarker1230"/> experiment, you analyze the results and compare the actual results to the expected ones. What have you learned from the experiment? Can you verify or refute your hypothesis? Is there a new hypothesis or new question to formulate? Or do you need more experiments to be sure?</li>
				<li><strong class="bold">Results</strong>: The final <a id="_idIndexMarker1231"/>step is to share your results. Even if your hypothesis was refuted, it is still valuable learning. </li>
			</ol>
			<p>The scientific method is an<a id="_idIndexMarker1232"/> iterative, empirical method, but the steps do not necessarily occur in that order. At any point, you can modify your question and change your hypothesis – and observation is going on all the time. Instead of a clear cycle, the process diagram looks more like <em class="italic">Figure 19.3</em>:</p>
			<div>
				<div id="_idContainer245" class="IMG---Figure">
					<img src="image/B17827_19_003.jpg" alt="Figure 19.3 – There is no strict order of the steps in the process&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 19.3 – There is no strict order of the steps in the process</p>
			<p>The scientific approach is very important in our industry – not only for building the right things. You should also use the approach when hunting down bugs or production issues: formulate a hypothesis based on facts observed. Conduct an experiment by changing only one thing at a time, normally a configuration value. Perform a cross-check to ensure that there is no other system or variable interfering with your experiment. Make a conclusion and document your results before starting with the next hypothesis.</p>
			<p>Let’s have a look at how you can use the scientific method to evolve and continuously improve your software.</p>
			<h2 id="_idParaDest-368"><a id="_idTextAnchor370"/>Observation – gathering and analyzing the data</h2>
			<p>You can <a id="_idIndexMarker1233"/>watch people using your application for <a id="_idIndexMarker1234"/>observation. We’ve talked in <a href="B17827_12_Epub.xhtml#_idTextAnchor267"><em class="italic">Chapter 12</em></a>, <em class="italic">Shift Left Testing for Increased Quality</em>, about <strong class="bold">usability testing</strong> techniques, such as <strong class="bold">h</strong><strong class="bold">allway testing</strong> or <strong class="bold">guerrilla usability</strong>. However, normally users are scattered across the<a id="_idIndexMarker1235"/> world and it’s <a id="_idIndexMarker1236"/>easier to look at the data they produce than to interview them.</p>
			<p>Data is the most important ingredient <a id="_idIndexMarker1237"/>for <strong class="bold">hypothesis-driven development</strong>! The more you experiment, the more data you will collect over time.</p>
			<p>When observing data, you should not only focus on the data points at hand. Ask yourself what the data does not tell you. If your goal was to increase the number of active users every month, you should not only focus your observation on the data about the current users. Check the data for failed login attempts. How many users would like to use your applications but are locked out and can’t recover their password or second authentication factor? How many people do not come back after they need to verify their mail or mobile number? How many cancel the registration process and how long do they wait before doing so?</p>
			<p>To answer these kinds of questions, you cannot simply look into your usage data. You have to combine data from all sources available (see <em class="italic">Figure 19.4</em>):</p>
			<div>
				<div id="_idContainer246" class="IMG---Figure">
					<img src="image/B17827_19_004.jpg" alt="Figure 19.4 – Logging sources for gathering data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 19.4 – Logging sources for gathering data</p>
			<p>This <a id="_idIndexMarker1238"/>quantitative data can then be combined with qualitative data, such as customer surveys, data from your customer service center, or any kind of analytics data. <em class="italic">Figure 19.5</em> shows the different data sources that you can use to gain insights and formulate the question:</p>
			<div>
				<div id="_idContainer247" class="IMG---Figure">
					<img src="image/B17827_19_005.jpg" alt="Figure 19.5 – Data sources for observation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 19.5 – Data sources for observation</p>
			<p>With these questions in mind, you can then start to formulate a hypothesis.</p>
			<h2 id="_idParaDest-369"><a id="_idTextAnchor371"/>Formulating the hypothesis</h2>
			<p>The <a id="_idIndexMarker1239"/>hypothesis is the prediction of what you think will occur based on your observation and research. Hypotheses can be written in a simple <em class="italic">if … then …</em> form: <em class="italic">If</em> <em class="italic">&lt;we modify this variable&gt;</em>, <em class="italic">then</em> <em class="italic">&lt;we expect this change to be observable&gt;</em>.</p>
			<p><em class="italic">If</em> we shorten our registration form by deleting fields such as phone number and mailing address, <em class="italic">then</em> the number of people canceling the registration process (<em class="italic">abandonment rate</em>) will decrease.</p>
			<p>Since you<a id="_idIndexMarker1240"/> will have many hypotheses on your backlog, it is common to have a fix form, similar to user stories, which includes the customer segment and the feature name. This makes your hypotheses more discoverable on a backlog:</p>
			<p><em class="italic">We believe</em> <em class="italic">{customer segment}</em></p>
			<p><em class="italic">want</em> <em class="italic">{feature}</em> </p>
			<p><em class="italic">because</em> <em class="italic">{value proposition}</em></p>
			<p>This form also forces you to bring three aspects into your hypothesis:</p>
			<ul>
				<li><strong class="bold">Who</strong>: for whom are we changing the application?</li>
				<li><strong class="bold">What</strong>: what are we changing?</li>
				<li><strong class="bold">How</strong>: how will this change impact the users?</li>
			</ul>
			<p>These ingredients make up a good hypothesis:</p>
			<p><em class="italic">We believe</em> new users</p>
			<p><em class="italic">want</em> a shorter registration form with fewer input fields </p>
			<p><em class="italic">because</em> this allows them to test the application and gain confidence before revealing their personal data.</p>
			<p>Note that focusing on the value proposition leads to a more abstract description of the <strong class="bold">how</strong> with a greater focus on the <strong class="bold">why</strong>. In marketing, you often find details like this in your hypothesis:</p>
			<ul>
				<li>What is the impact?</li>
				<li>By how much/how great it is?</li>
				<li>After what period of time?</li>
			</ul>
			<p>This results in a<a id="_idIndexMarker1241"/> one-to-one relationship between hypotheses and experiments. Especially when starting with experimentation, I think separating the experiment from the underlaying hypothesis helps. You will probably need multiple experiments before you finally can say with certainty whether the hypothesis was true or false.</p>
			<h2 id="_idParaDest-370"><a id="_idTextAnchor372"/>Building the experiment</h2>
			<p>When defining your<a id="_idIndexMarker1242"/> experiment, you should try to keep as many variables fixed as possible. The best thing is to look at your baseline data. How will weekends and vacations impact your data? How will political and macroeconomic trends impact your experiment?</p>
			<p>Also, make sure that both your control group and your experimental group are big enough. If you only experiment on a small group, your results might not be representative. If your control group is too small, you might not have enough data to compare your results to, especially if there are other external factors that you did not foresee. A good experiment should contain the following information:</p>
			<ul>
				<li>What’s the change?</li>
				<li>What’s the expected impact?</li>
				<li>Who is the audience or the customer segment?</li>
				<li>How much change are we expecting?</li>
				<li>How long do we run the experiment?</li>
				<li>What is the baseline we compare the data to (a control group or historical data)?</li>
			</ul>
			<p>Here is an example.</p>
			<p>The new, <em class="italic">shorter registration form</em> (<strong class="bold">what’s the change</strong>) will <em class="italic">reduce the abandonment rate</em> of the registration form (<strong class="bold">impact</strong>) for <em class="italic">50%</em> of our <em class="italic">new users</em> (<strong class="bold">for whom</strong>) by more than <em class="italic">15%</em> (<strong class="bold">how much</strong>) after <em class="italic">14 days</em> (<strong class="bold">after how long</strong>) compared to our control group (<strong class="bold">baseline</strong>).</p>
			<p>With the <a id="_idIndexMarker1243"/>experiment defined, you can start implementing and running it. If you develop with feature flags (see <a href="B17827_10_Epub.xhtml#_idTextAnchor239"><em class="italic">Chapter 10</em></a>, <em class="italic">Feature Flags and the Feature Lifecycle</em>), this is as simple as writing a new feature. The only difference is that you do not turn the feature on for all users but for your experimentation group instead.</p>
			<h2 id="_idParaDest-371"><a id="_idTextAnchor373"/>Validating the results</h2>
			<p>After the <a id="_idIndexMarker1244"/>experiment, you analyze the results and compare the actual results to the expected ones. What have you learned from the experiment? Can you verify or falsify your hypothesis or do you need more experiments to be sure? Is there a new hypothesis or new question to formulate?</p>
			<p>The retrospective study of the results is an important part. Do not skip it and just assume that the hypothesis is true or false because your metrics exceed a threshold. Analyze the data and check for unexpected influences, strays, and statistical outliers.</p>
			<p>Learning from your hypotheses and experiments should lead to new ideas and complete the build-measure-learn loop (see <em class="italic">Figure 19.6</em>):</p>
			<div>
				<div id="_idContainer248" class="IMG---Figure">
					<img src="image/B17827_19_006.jpg" alt="Figure 19.6 – Hypothesis-driven experimentation with the build-measure-learn loop&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 19.6 – Hypothesis-driven experimentation with the build-measure-learn loop</p>
			<p>There are many tools available that can help you with effective A|B testing and experimentation.</p>
			<h1 id="_idParaDest-372"><a id="_idTextAnchor374"/>Effective A|B testing with GrowthBook and Flagger</h1>
			<p>GitHub does not have the tooling to help you with A|B testing but there are many tools available on the market. The problem is that many of these tools have completely different scopes. Some are more like web experience tools, which you can use to build your website using a <strong class="bold">content management system</strong> (<strong class="bold">CMS</strong>) or to build A|B tests using a visual editor to create and test your<a id="_idIndexMarker1245"/> variations (for example, <strong class="bold">Optimizely</strong> – see https://www.optimizely.com/). Some are more focused on marketing, landing pages, and campaign management, such <a id="_idIndexMarker1246"/>as <strong class="bold">HubSpot</strong> (https://www.hubspot.com/). These tools are great but probably not the right choice for an engineering team.</p>
			<p>A better solution is provided by the tools for doing feature flags, such as <strong class="bold">LaunchDarkly</strong>, <strong class="bold">VWO</strong>, or <strong class="bold">Unleash</strong>. I have covered these tools in<em class="italic"> </em><a href="B17827_10_Epub.xhtml#_idTextAnchor239"><em class="italic">Chapter 10</em></a>, <em class="italic">Feature Flags and the Feature Lifecycle</em>, so I will not cover them again here. If you are using one of these solutions for feature flags, this is the first place you should look for a solution for A|B testing.</p>
			<p>In this chapter, I will focus on <strong class="bold">GrowthBook</strong> and <strong class="bold">Flagger</strong>, two open source projects with a strong focus on experimentation, but with a completely different approach.</p>
			<h3>GrowthBook</h3>
			<p><strong class="bold">GrowthBook</strong> (<a href="https://github.com/growthbook/growthbook">https://github.com/growthbook/growthbook</a>) is a <a id="_idIndexMarker1247"/>solution with <a id="_idIndexMarker1248"/>a free and open core. It is also available as a SaaS and Enterprise plan. It provides an SDK for <strong class="bold">React</strong>, <strong class="bold">JavaScript</strong>, <strong class="bold">PHP</strong>, <strong class="bold">Ruby</strong>, <strong class="bold">Python</strong>, <strong class="bold">Go</strong>, and <strong class="bold">Kotlin</strong>.</p>
			<p>The solution design of <a id="_idIndexMarker1249"/>GrowthBook is completely <a id="_idIndexMarker1250"/>containerized. If you want to try it out, you just have to clone the repository and then run the following:</p>
			<pre class="source-code">docker-compose up -d</pre>
			<p>Once up, you can access the Growthbook on <a href="http://localhost:3000">http://localhost:3000</a>.</p>
			<p class="callout-heading">Running GrowthBook in GitHub Codespaces</p>
			<p class="callout">If you want to try out GrowthBook, you can run it in GitHub Codespaces. For this to work, you have to configure <strong class="source-inline">docker-compose.yml</strong> to use the correct DNS names, since GrowthBook uses localhost to connect to its MongoDB. Set <strong class="source-inline">APP_ORIGIN</strong> under <strong class="source-inline">environment</strong> to your local address of port <strong class="source-inline">3000</strong> and <strong class="source-inline">API_HOST</strong> to your local address of port <strong class="source-inline">3001</strong> and make port <strong class="source-inline">3001</strong> visible.</p>
			<p>Once <a id="_idIndexMarker1251"/>connected, you can use it to serve feature flags or build <a id="_idIndexMarker1252"/>experiments. To build experiments, you have to connect a data source to GrowthBook – for example, <strong class="bold">BigQuery</strong>, <strong class="bold">Snowflake</strong>, <strong class="bold">Redshift</strong>, or <strong class="bold">Google Analytics</strong>, among many others. There are predefined data schemas and you can also build your own. You then create metrics based on your data source. Metrics can be any of the following:</p>
			<ul>
				<li><strong class="bold">Binomial</strong>: A simple yes or no conversation (for example, <strong class="source-inline">Account Created</strong>)</li>
				<li><strong class="bold">Count</strong>: Multiple conversations per user (for example, <strong class="source-inline">Page Visits</strong>)</li>
				<li><strong class="bold">Duration</strong>: How much time something takes on average (for example, <strong class="source-inline">Time on Site</strong>)</li>
				<li><strong class="bold">Revenue</strong>: The revenue gained or lost on average (for example, <strong class="source-inline">Revenue per User</strong>)</li>
			</ul>
			<p>To run an experiment, you would normally use your feature flags. You could also run an inline experiment directly with one of the SDKs. This is what an experiment would look like in JavaScript:</p>
			<pre class="source-code">const { value } = growthbook.run({</pre>
			<pre class="source-code">  key: “my-experiment”,</pre>
			<pre class="source-code">  variations: [“red”, “blue”, “green”],</pre>
			<pre class="source-code">});</pre>
			<p>The experiment runs based on your defined metrics and the results look like they do in <em class="italic">Figure 19.7</em>:</p>
			<div>
				<div id="_idContainer249" class="IMG---Figure">
					<img src="image/B17827_19_007.jpg" alt="Figure 19.7 – The results of an experiment in GrowthBook&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 19.7 – The results of an experiment in GrowthBook</p>
			<p>You can <a id="_idIndexMarker1253"/>add and remove metrics to the experiment and<a id="_idIndexMarker1254"/> also export it as a Jupyter notebook.</p>
			<p>GrowthBook also comes with the Google Chrome extension <strong class="bold">GrowthBook DevTools</strong> for JavaScript and the React SDK, which allows you to directly interact with your feature flags in the browser. A visual editor is currently in beta.</p>
			<p>GrowthBook is straightforward and also based on feature flags like the solutions introduced in <a href="B17827_10_Epub.xhtml#_idTextAnchor239"><em class="italic">Chapter 10</em></a>.</p>
			<h2 id="_idParaDest-373"><a id="_idTextAnchor375"/>Flagger</h2>
			<p>A completely<a id="_idIndexMarker1255"/> different approach uses <strong class="bold">Flagger</strong> (<a href="https://flagger.app/">https://flagger.app/</a>). It’s a <a id="_idIndexMarker1256"/>delivery <a id="_idIndexMarker1257"/>operator for <strong class="bold">Kubernetes</strong> and can be used with the <strong class="bold">service mesh</strong> <strong class="bold">Istio</strong>. Flagger is more often used for <strong class="bold">canary releases</strong> to Kubernetes clusters but it can also route traffic on<a id="_idIndexMarker1258"/> HTTP match conditions.</p>
			<p>You could create an experiment for all users with an <strong class="source-inline">insider</strong> cookie for 20 minutes, like so:</p>
			<pre class="source-code">analysis:</pre>
			<pre class="source-code">  # schedule interval (default 60s)</pre>
			<pre class="source-code">  interval: 1m</pre>
			<pre class="source-code">  # total number of iterations</pre>
			<pre class="source-code">  iterations: 20</pre>
			<pre class="source-code">  # max number of failed metric checks before rollback</pre>
			<pre class="source-code">  threshold: 2</pre>
			<pre class="source-code">  # canary match condition</pre>
			<pre class="source-code">  match:</pre>
			<pre class="source-code">    - headers:</pre>
			<pre class="source-code">        cookie:</pre>
			<pre class="source-code">          regex: “<strong class="bold">^(.*?;)?(type=insider)(;.*)?$</strong>”</pre>
			<p>You can<a id="_idIndexMarker1259"/> combine Flagger with metrics from <strong class="bold">Prometheus</strong>, <strong class="bold">Datadog</strong>, <strong class="bold">Dynatrace,</strong> among <a id="_idIndexMarker1260"/>many others. I’m not going to go into more detail here. See the Flagger documentation (<a href="https://docs.flagger.app/">https://docs.flagger.app/</a>) for more information. There is also a good tutorial from Stefan Prodan: <em class="italic">GitOps recipe for Progressive Delivery with Flux v2, Flagger and Istio</em> (see <a href="https://github.com/stefanprodan/gitops-istio">https://github.com/stefanprodan/gitops-istio</a>).</p>
			<p>A solution with Flagger and Istio brings great flexibility but it is also quite complex and not suited to beginners. If you are already on Kubernetes and Istio and perform canary releases, then Flagger might be a powerful framework for you.</p>
			<p>As you can see, there are many solutions out there that can help you run experiments and A|B tests. From CMS- and campaign-focused tools to Kubernetes operators, there is a wide range of solutions that have completely different approaches. The best solution for you depends on a lot of things – mostly your existing toolchain, pricing, and support. I think it is more important to focus on the process and data analytics. Serving two versions of your application should not be the challenge – making sense of your data probably is.</p>
			<h1 id="_idParaDest-374"><a id="_idTextAnchor376"/>Experimentation and OKR</h1>
			<p>In <em class="italic">Chapter 1</em>, <em class="italic">Metrics That Matter</em>, I introduced you to <strong class="bold">Objectives and Key Results</strong> (<strong class="bold">OKRs</strong>) as a framework to define <a id="_idIndexMarker1261"/>and track objectives and their outcomes in a transparent way. OKRs help organizations achieve high alignment on <a id="_idIndexMarker1262"/>strategic goals while keeping a maximum level of autonomy for the individual teams.</p>
			<p>Engineering teams are an expensive resource and a lot of stakeholders are requesting things from them all the time: testers submitting bugs, customers requesting new features, and management wanting to catch up with the competition and make promises to important customers. How should a team ever find the freedom to conduct experiments? And what experiments would be the best to start with?</p>
			<p>OKRs can give you the ability to have a strong alignment with higher-level goals by simultaneously preserving the autonomy to decide <em class="italic">what</em> to build and <em class="italic">how</em> to build it. </p>
			<p>Let’s assume your company wants to be the market leader with a market share of 75% and it will need a constant growth rate of newly registered users to achieve that. The <strong class="bold">key result</strong> for your team is a growth rate of 20% each month. This will then set the priority for your team. Of course, there will be other things to do, but the priority will be the OKR. The team probably first investigates how many people come to the registration page in the first place and from what referral. How many people click on the <strong class="bold">Register Now</strong> button? How many finish the dialog? At what point do they not come back? And at that point, they are automatically starting to formulate hypotheses and can run experiments to prove them.</p>
			<p>OKRs are also good for cross-team collaboration, as teams probably have OKRs with high synergy effects, as they are aligned to higher-level goals. In this example, the team probably wants to talk with marketing, as they will have similar OKRs. They might have their own ideas for experiments to help drive the engagement rate for the landing pages that lead to your registration site.</p>
			<p>OKRs are a <a id="_idIndexMarker1263"/>great tool to <a id="_idIndexMarker1264"/>grant people the freedom to experiment by ensuring alignment with ot<a id="_idTextAnchor377"/>her teams and higher-level goals.</p>
			<h1 id="_idParaDest-375"><a id="_idTextAnchor378"/>Summary</h1>
			<p>Experimentation, A|B testing, and hypothesis-driven development are difficult topics as they require a high level of maturity in many areas:</p>
			<ul>
				<li><strong class="bold">Management</strong>: Your teams need the autonomy to decide on their own <em class="italic">what</em> to build and <em class="italic">how</em> to build it.</li>
				<li><strong class="bold">Culture</strong>: You need a culture of trust where people are not afraid to fail.</li>
				<li><strong class="bold">Cross-team collaboration</strong>: Your teams must be able to work interdisciplinarily, as experimentation often requires the collaboration of different departments.</li>
				<li><strong class="bold">Technical capabilities</strong>: You must be able to release changes in a very short time to production and target individual customer segments.</li>
				<li><strong class="bold">Insights</strong>: You must have strong analytics capabilities and combine data and metrics from different sources.</li>
			</ul>
			<p>If you are not there yet, don’t worry. Many teams I work with are not. Just keep on improving your capabilities and check that your metrics show results. DevOps is a journey and not a goal, and you must take it one step at a time.</p>
			<p>In this chapter, you’ve learned the basics of experimentation, A|B testing, and hypothesis-driven development, and I introduced some tools that can help you build solutions for it.</p>
			<p>In the next chapter, you will learn the basics of GitHub – hosting options, pricing, and how you can integrate it into your existing toolchain and your enterprise.</p>
			<h1 id="_idParaDest-376"><a id="_idTextAnchor379"/>Further reading</h1>
			<p>These are the references and links from this chapter that you can also use to get more information on the topics:</p>
			<ul>
				<li><em class="italic">The Scientific method</em>: https://en.wikipedia.org/wiki/Scientific_method</li>
				<li><em class="italic">Ring-based deployments</em>: <a href="https://docs.microsoft.com/en-us/azure/devops/migrate/phase-rollout-with-rings">https://docs.microsoft.com/en-us/azure/devops/migrate/phase-rollout-with-rings</a></li>
				<li><em class="italic">Optimizely</em>: <a href="https://www.optimizely.com/">https://www.optimizely.com/</a></li>
				<li><em class="italic">Hubspot</em>: <a href="https://www.hubspot.com/ ">https://www.hubspot.com/</a></li>
				<li><em class="italic">GrowthBook</em>: <a href="https://github.com/growthbook/growthbook">https://github.com/growthbook/growthbook</a></li>
				<li><em class="italic">Flagger</em>: <a href="https://flagger.app/">https://flagger.app/</a></li>
				<li>Stefan Prodan: <em class="italic">GitOps recipe for progressive delivery with Flux v2, Flagger, and Istio</em>: <a href="https://github.com/stefanprodan/gitops-istio">https://github.com/stefanprodan/gitops-istio</a></li>
			</ul>
		</div>
	</body></html>