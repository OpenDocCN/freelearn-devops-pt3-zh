<html><head></head><body>
		<div id="_idContainer033">
			<h1 class="chapter-number" id="_idParaDest-172"><a id="_idTextAnchor208"/>9</h1>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor209"/>Backing Up Data and Metadata</h1>
			<p>In this chapter, we’ll explore the critical importance of backing up both data and metadata as part of a comprehensive Salesforce DevOps strategy. We’ll discuss the substantial costs that organizations face from data loss and system outages in terms of lost productivity, revenue, and reputation. Viable backups can drastically reduce disruption by enabling rapid restoration after incidents. Key capabilities such as external storage, automation, integration, security, and smart restoration functionality will be explored to help you architect backups <span class="No-Break">for Salesforce.</span></p>
			<p>This chapter stresses capturing both metadata and data within backups to fully protect business continuity. Metadata’s indispensable role in securing data, providing context, enabling automation, and preserving configurations will be highlighted. Similarly, effective techniques for backing up Salesforce’s flexible data model at scale will be covered. You will learn how to integrate backup tools into development workflows to amplify their value beyond just protection, and architect streamlined <span class="No-Break">recovery processes.</span></p>
			<p>We will cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li><em class="italic">Why backups should be part of your DevOps process</em>: We’ll start by looking at the fundamental role that regular and accurate backups play as part of an overall <span class="No-Break">DevOps strategy.</span></li>
				<li><em class="italic">Metadata backups</em>: In this section, we’ll look at how it’s important to have a plan for backing up your metadata schema, not just the data contained <span class="No-Break">within it.</span></li>
				<li><em class="italic">Data backups</em>: Next, we’ll discuss the considerations you need to factor into your <span class="No-Break">data backups.</span></li>
				<li><em class="italic">The recovery process</em>: Backups are only half the challenge – in this section, we’ll tackle the steps and processes to consider when restoring <span class="No-Break">from backups.</span></li>
				<li><em class="italic">Incident and disaster recovery planning</em>: Efficient data recovery is driven by effective planning for when things go wrong. This section covers factors to take into account when you’re planning <span class="No-Break">your approach.</span></li>
				<li><em class="italic">Securing backup data</em>: Your backups need to be as secure as your Salesforce implementation. So, in this section, we look at how to enforce security around <span class="No-Break">your backups.</span></li>
				<li><em class="italic">Navigating GDPR and CCPA regulations for data backups</em>: The data you back up is subject to the same rules and regulations as your working data. In this section, we’ll touch upon the two most common <a id="_idIndexMarker363"/>regulations that may apply to your <a id="_idIndexMarker364"/>data backups – the <strong class="bold">General Data Protection Regulation</strong> (<strong class="bold">GDPR</strong>) and the <strong class="bold">California Consumer Privacy </strong><span class="No-Break"><strong class="bold">Act</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">CCPA</strong></span><span class="No-Break">).</span></li>
				<li><em class="italic">Data retention considerations</em>: In this section, we’ll review some of the areas that may affect your data retention policy for how long you keep your <span class="No-Break">backed-up data.</span></li>
				<li><em class="italic">Options for Salesforce backup</em>: Finally, with the aforementioned topics covered, we’ll look at the current options available for effective <span class="No-Break">Salesforce backups.</span></li>
			</ul>
			<p>By the end of this chapter, you will understand the pivotal role effective data and metadata backups play in limiting business disruption and gain practical insights to evaluate and implement robust backup solutions tailored for the <span class="No-Break">Salesforce platform.</span></p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor210"/>Technical requirements</h1>
			<p>There are no technical requirements for this chapter since the principles discussed here can be used with several Salesforce backup solutions on <span class="No-Break">the market.</span></p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor211"/>Why backups should be part of your DevOps process</h1>
			<p>Salesforce<a id="_idIndexMarker365"/> often serves as the critical system of engagement and operations for many organizations. The data within Salesforce provides a 360-degree view of customers and enables marketing, sales, and service activities. As a result, any prolonged downtime or data loss within Salesforce can cripple an organization’s ability to do business. Customer relationships suffer, revenue-generating activities halt, and productivity grinds to <span class="No-Break">a standstill.</span></p>
			<p>According to multiple studies, the average cost of an hour of downtime for most companies exceeds $300,000. For some larger enterprises, this hourly cost can be in the millions. Extrapolated over several hours or days, major outages create substantial financial and reputational damage. In the worst cases, companies without adequate data protection and recovery plans can be forced completely out of business by catastrophic <span class="No-Break">data loss.</span></p>
			<p>However, viable and integrated data backups provide organizations with the capability to quickly restore service and limit disruption. Rather than permanently losing data and productivity, companies can restore from recent backups and continue operations with only a brief period of constrained activity during recovery. Sophisticated tools can automate backup processes and make <span class="No-Break">restoration seamless.</span></p>
			<p>Beyond just protecting against disasters, fully incorporating backup tools and procedures into the DevOps toolchain has additional benefits. Backups give developers more confidence to innovate and experiment rapidly, knowing their work is protected. Integrated backups also encourage using consistent processes and environments across development, testing, and production to minimize confusion when restoration <span class="No-Break">is required.</span></p>
			<p>Treating <a id="_idIndexMarker366"/>backups as an essential element of DevOps for Salesforce limits potential business disruption empowers developers, and assures organizations that their critical Salesforce data is <span class="No-Break">fully protected.</span></p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor212"/>The cost of data loss</h2>
			<p>The average <a id="_idIndexMarker367"/>hourly cost of critical system downtime can vary significantly depending on the size of the organization, industry, and specific systems affected. However, research indicates that large enterprises experience average downtime costs ranging from $300,000 to $400,000 per hour (source: <a href="https://www.statista.com/statistics/753938/worldwide-enterprise-server-hourly-downtime-cost/">https://www.statista.com/statistics/753938/worldwide-enterprise-server-hourly-downtime-cost/</a>). For small and medium-sized businesses, hourly downtime costs typically range from $10,000 <span class="No-Break">to $23,000.</span></p>
			<p>These figures represent averages, but actual costs can be far higher for revenue-critical systems or in heavily regulated industries. Some key factors that contribute to downtime costs include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Lost productivity and revenue</strong>: Employees cannot work and revenue <span class="No-Break">generation halts</span></li>
				<li><strong class="bold">Customer dissatisfaction</strong>: Outages erode customer trust, satisfaction, <span class="No-Break">and loyalty</span></li>
				<li><strong class="bold">Recovery and remediation costs</strong>: Restoring systems and data recovery accrues sizable <span class="No-Break">IT expenses</span></li>
				<li><strong class="bold">Regulatory non-compliance fines</strong>: Healthcare, finance, and other regulated sectors carry <span class="No-Break">steep penalties</span></li>
				<li><strong class="bold">Reputational damage</strong>: Brand reputation suffers from extended outages, impacting <span class="No-Break">future business</span></li>
			</ul>
			<p>Given the<a id="_idIndexMarker368"/> enormous potential cost of critical system downtime, business continuity planning is imperative for minimizing impact. Strategies such as redundant systems, comprehensive disaster recovery plans, regular backups, testing, monitoring, and maintenance help reduce downtime costs. Investing in continuity helps organizations maintain operations and customer service <span class="No-Break">during outages.</span></p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor213"/>Backups limit disruption</h2>
			<p>While data <a id="_idIndexMarker369"/>loss and system outages cannot be prevented entirely, viable data backups provide organizations with the capability to quickly restore service and limit disruption. Rather than permanently losing data and enduring prolonged productivity losses, companies can restore from recent backups and resume operations after a brief period of constrained activity <span class="No-Break">during recovery.</span></p>
			<p>With proper backup systems in place, organizations can reduce the <strong class="bold">recovery time objective</strong> (<strong class="bold">RTO</strong>) to<a id="_idIndexMarker370"/> just hours or minutes for mission-critical systems such as Salesforce. For example, sophisticated backup tools optimized for the Salesforce platform enable administrators to roll back to a pre-outage state in under an hour in <span class="No-Break">many cases.</span></p>
			<p>Well-designed backup solutions incorporate a set of key capabilities to enable rapid yet secure restoration after outages, while also integrating seamlessly into day-to-day <span class="No-Break">development workflows.</span></p>
			<p>For example, external cloud-based backup storage is essential for ensuring access to data copies, even when primary systems are fully down. By storing backups in different locations than live data, businesses are protected against disasters impacting both systems simultaneously. Configuring automated, recurring backups on daily or intraday schedules also provides constant protection by capturing point-in-time snapshots consistently without manual oversight that could lead <span class="No-Break">to gaps.</span></p>
			<p>Backup solutions focused on aligning with modern development practices allow on-demand backups to be triggered right before risky release events, providing a failsafe restore point if issues emerge. Advanced analytics help identify anomalies by detecting unusual data changes across backups, enabling investigation of potential corruption <span class="No-Break">or loss.</span></p>
			<p>Integration with existing developer tools, environments, and workflows simplifies restoration when needed by utilizing familiar interfaces. This reduces confusion during incidents, which speeds up recovery. Robust restoration engines optimized over thousands of real-world recoveries ensure that pre-outage configurations can be <span class="No-Break">recreated rapidly.</span></p>
			<p>Enterprise-grade security, including access controls and strong encryption for data at rest and in transit, is essential for protecting highly sensitive backup copies. By incorporating these types of sophisticated capabilities, backup solutions can augment development outcomes while still delivering <span class="No-Break">reliable protection.</span></p>
			<p>By incorporating <a id="_idIndexMarker371"/>solutions with these types of capabilities into regular operations, teams can quickly diagnose issues, rapidly restore service, and limit the business disruption caused by data loss or system outages. The overall cost and reputational damage are contained to the brief recovery window required to restore from <span class="No-Break">viable backups.</span></p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor214"/>Backups complement development</h2>
			<p>Incorporating <a id="_idIndexMarker372"/>backup tools and procedures fully into the development life cycle and DevOps toolchain provides benefits far beyond just disaster recovery preparedness. When backups are treated as an integral part of the development process rather than an afterthought, it enhances developer agility, augments testing, and accelerates <span class="No-Break">release cycles.</span></p>
			<p>For developers, viable backups provide a safety net that enables increased innovation velocity and freedom. Rather than cautiously limiting changes due to fear of unintended consequences, developers can rapidly implement new features and evolve system architectures, knowing they have a backup snapshot to roll back to in case of issues. Backups shift developer mindsets from tentatively changing systems to boldly <span class="No-Break">advancing them.</span></p>
			<p>Backups also facilitate creating more realistic and reproducible test and staging environments. Capturing both data and configuration details allows you to refresh sandbox environments so that they match production more closely. This enables more accurate testing and training simulations. Backup data can also be used to generate masked test datasets for development and quality assurance testing without <span class="No-Break">compromising security.</span></p>
			<p>During daily <a id="_idIndexMarker373"/>development operations, integrated backups provide additional advantages. Anomaly detection algorithms help identify data issues that were introduced during coding cycles early for rapid remediation. Streamlined integration with <strong class="bold">continuous integration/continuous delivery</strong> (<strong class="bold">CI/CD</strong>) pipelines<a id="_idIndexMarker374"/> allows backups to be captured with ease with each new build for subsequent restoration if pipeline changes have <span class="No-Break">unanticipated impacts.</span></p>
			<p>In the case of applications where data (rather than metadata) determines business logic (such as Salesforce CPQ), the ability to roll back quickly after deployment for developers may well be more efficient than running large quantities of end-to-end automated tests pre-deployment on <span class="No-Break">each change.</span></p>
			<p>For developers building on scratch orgs, restoration capabilities keep them productive by reverting to previous known good states, when necessary, after aggressive customization experiments. Comprehensive backups that include code, configurations, and dependencies aid developer troubleshooting and <span class="No-Break">debugging efforts.</span></p>
			<p>For administrators and release managers, on-demand backups right before deployment provide an instant rollback point if new changes or upgrades cause problems in production environments. This safety net enables CD with reduced risk. Release managers gain confidence to incrementally improve systems on an ongoing basis knowing rollbacks <span class="No-Break">are possible.</span></p>
			<p>By facilitating <a id="_idIndexMarker375"/>developer freedom, augmenting testing, and accelerating release cycles, integrated backups enhance business agility and time-to-market for new innovations. Treating backups as complementary processes to development and operations, rather than just as disaster recovery tools, amplifies their value in modern <span class="No-Break">DevOps practices.</span></p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor215"/>Backups protect against errors</h2>
			<p>Even the <a id="_idIndexMarker376"/>most careful administrators and developers will inevitably make mistakes that impact data integrity. Accidental configuration changes or inadvertent code deployments happen at some point, despite best intentions. More concerning, malicious actions by compromised users also pose a threat to organizations. When human errors or intentional tampering occurs, viable backups serve as the last line of defense for <span class="No-Break">rapid recovery.</span></p>
			<p>On the development side, even extensively tested code changes may still have unanticipated downstream impacts when they’re deployed to production, affecting data accuracy or availability. Backups allow for rapidly rolling back code changes to regain system stability and data reliability. Code-related incidents can be diagnosed properly by comparing backups before and after deployments to pinpoint <span class="No-Break">root causes.</span></p>
			<p>Data accidents can also occur, such as bulk record deletions or modifications executed against the wrong dataset. Without backups, re-entry of lost data can take many hours or days, if possible at all. Data retention policies may also legally require long-term backup of certain datasets if originals are lost. With regular backup schedules and storage, rapid restoration of information minimizes <span class="No-Break">business disruption.</span></p>
			<p>In cases of internal security threats and malicious actions, backups may be the only option for recovery after destructive events such as mass data deletion. While prevention is ideal, backups provide an insurance policy to counteract even worst-case scenarios. Forensic analysis of backups can also aid investigations into suspicious activities that compromised security in the <span class="No-Break">first place.</span></p>
			<p>Notable Salesforce-specific incidents such as the NA14 outage in 2016 and the <strong class="bold">Permageddon</strong> permission issue<a id="_idIndexMarker377"/> in 2019, in which Salesforce had to reset permissions for teams that integrated Pardot, to protect user data, ultimately requiring admins to rebuild profiles and permissions, highlight the need for viable backups, even when you’re relying on a major cloud platform. Without accessible backups, companies can face massive costs <span class="No-Break">and disruption.</span></p>
			<p>Ultimately, even <a id="_idIndexMarker378"/>human errors and intentional misuse must be planned for as realities when managing large complex systems such as Salesforce. Maintaining viable backups, securing access appropriately, and implementing checks and alerts on changes can help organizations quickly undo the impacts of inevitable mistakes or malice. Having reliable data recovery capabilities provides greater peace of mind day-to-day for administrators <span class="No-Break">and developers.</span></p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor216"/>Backups verify releases</h2>
			<p>The rapid pace of <a id="_idIndexMarker379"/>change within modern development environments means new features and upgrades are constantly being released to users. While extensive testing occurs before releases, anticipating how all users will interact with changes is impossible within even large-scale test environments. As a result, unanticipated impacts inevitably occur over time as real-world usage differs from <span class="No-Break">predicted patterns.</span></p>
			<p>To enable the continuous delivery of incremental improvements in this dynamic environment, development teams need a way to swiftly diagnose and resolve issues introduced by new releases. Backups provide this capability by capturing a known good state immediately before any given release deployment. If users experience problems after a release, administrators can pinpoint whether the root cause stems from that release by rapidly restoring to the pre-deployment <span class="No-Break">backup snapshot.</span></p>
			<p>With this capability to roll back and conduct root cause analysis based on backups, development teams gain confidence in shipping changes frequently. There is less reluctance to continuously deliver incremental improvements, new features, and upgrades since they know that releases can be rolled back if substantial issues appear. Release velocity and <span class="No-Break">innovation accelerate.</span></p>
			<p>On-demand backups right before each deployment also create reproducible release testing environments. Release candidates can be deployed repeatedly to staging environments that have been restored from the same backup to verify all changes precisely. Automating on-demand backups as part of CI/CD pipelines further enhances <span class="No-Break">release verification.</span></p>
			<p>For users, rapid resolution of release-related defects based on backups maintains confidence and satisfaction. There is no need to wait for lengthy debugging and new patched versions when backups can be used to quickly restore the previous stable state. The overall quality and responsiveness of the development <span class="No-Break">process improve.</span></p>
			<p>Viable backups<a id="_idIndexMarker380"/> enable development teams to accelerate innovation and gain assurance that continuous delivery risks can be managed. Release changes can be made boldly knowing any adverse impacts <span class="No-Break">are reversible.</span></p>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor217"/>Metadata backups</h1>
			<p>In today’s digital age, data often <a id="_idIndexMarker381"/>holds the title of <em class="italic">new gold</em>. However, in the Salesforce realm, if data is considered gold, then metadata can be viewed as the intricate blueprint that illustrates how that gold is extracted, refined, and molded into valuable entities. This comparison underscores the immense significance of metadata in the elaborate world <span class="No-Break">of Salesforce.</span></p>
			<p>Metadata isn’t just a solitary layer of information; it is a comprehensive mosaic of configurations, definitions, and automations that breathe life into data. Metadata determines how each object and field behaves, their interrelations, and their presentation to users. It acts as the DNA of a Salesforce org, controlling form <span class="No-Break">and function.</span></p>
			<p>The ability to interpret and make sense of data is inextricably linked to the contextual foundation that metadata establishes. Envision a scenario with millions of records that are devoid of any indication of their structure or interconnections. It’s akin to attempting a complex billion-piece jigsaw puzzle without the guiding image on the box. Data, such as account records, only achieves its full potential value when coupled with its associated metadata. This fusion ensures that data isn’t merely a cluster of numbers and characters but a coherent and impactful compilation driving decisions and operations. From a practical sense, this means that data restoration can often be entirely dependent on having the matching metadata state as well. Records, which include values for fields that no longer exist, such as picklist values, can be problematic, as can records stored before new required fields have <span class="No-Break">been added.</span></p>
			<p>Moreover, for Salesforce administrators and developers, the real concern isn’t just the potential loss of data itself, but the possible erasure of months or years of meticulously crafted customizations. The prospect of reconstructing hundreds of custom objects, redefining thousands of page layout configurations, recalibrating complex user permissions, and redeveloping sophisticated flows and triggers is a daunting scenario. Such large-scale rebuilding represents substantial time, effort, and resources required to regain previous levels of business process automation. In this context, a robust metadata backup emerges as a crucial safety net, ensuring swift recovery <span class="No-Break">from disruptions.</span></p>
			<p>We cannot emphasize the pivotal role of metadata in safeguarding an organization’s data security and access controls enough. While data may represent discrete records, it’s the metadata that encapsulates the policies that dictate which users can view and modify those records and how. Metadata doesn’t just dictate which raw data a user can access but also modulates how they interact with it and its relationship with other data components. In the absence of viable metadata backups, you might possess a treasure trove of data but lack the necessary mechanisms to properly secure and control access <span class="No-Break">to it.</span></p>
			<p>Furthermore, while data at rest provides tangible historical records, it’s the metadata that propels it into dynamic action via automation. From streamlining routine tasks such as lead assignments to orchestrating complex multi-step processes crossing departments, metadata diligently operates behind the scenes enabling Salesforce’s responsive nature. Rules, validations, and triggers encoded in metadata ensure data accuracy <span class="No-Break">and consistency.</span></p>
			<p>Within the vast framework of Salesforce, data and metadata are intertwined elements that paint a comprehensive tableau of an organization. Although backing up data is vital to preserve historical records and insights, the relevance, security, and actionable nature of this data is firmly anchored in the surrounding metadata. As enterprises continue to pour significant resources into tailoring Salesforce deployments to their unique requirements, the <a id="_idIndexMarker382"/>indispensable nature of comprehensive metadata backups becomes glaringly evident. A genuinely robust backup strategy must seamlessly integrate both data and metadata, ensuring that neither foundational element is <span class="No-Break">left vulnerable.</span></p>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor218"/>Data backups</h1>
			<p>While metadata captures the<a id="_idIndexMarker383"/> structural blueprint and logic in Salesforce, organizations must also back up their data to fully protect business operations. Data consists of the actual records and content that users create, update, and manage within Salesforce as part of their daily work. This includes accounts, contacts, leads, opportunities, cases, and <span class="No-Break">custom objects.</span></p>
			<p>The data in Salesforce often represents the core customer information, sales transactions, service issue history, and other operational datasets on which organizations run their business. Losing access to current production data could set back operations by months or years, depending on <span class="No-Break">the volume.</span></p>
			<p>However, backing up data at scale within Salesforce introduces unique challenges. The flexible data model, when coupled with complex relationships spanning hundreds of objects and massive data volumes, requires backup techniques and tools that are specifically optimized <span class="No-Break">for Salesforce.</span></p>
			<p>For example, the declarative, configurable architecture of Salesforce allows interlinked multi-object data graphs to model business domains to be created. An account record could have hundreds of related child records across contacts, opportunities, cases, and other objects. Backup solutions designed for Salesforce can preserve these complex webs of object relationships when capturing and <span class="No-Break">restoring data.</span></p>
			<p>The volume of data also demands optimized backup performance and granular restore capabilities. Salesforce production instances often contain terabytes of data across billions of records. Full data backups and restores must be executed efficiently to fit within limited maintenance windows. Granular restore features help apply surgical changes by restoring only select records or fields that have been <span class="No-Break">modified inadvertently.</span></p>
			<p>Lastly, Salesforce’s flexible schema requires maintaining data integrity when objects and fields mutate over time. Backup tools need to map backups to the current structure rather than forcing rigid schemas that break when data models evolve. Tracking field history helps properly align changing data types <span class="No-Break">and attributes.</span></p>
			<p>Backing up data is as important as metadata for fully protecting business continuity in Salesforce. But to be effective, data backup solutions must be tailored for Salesforce’s unique data modeling, massive scale, and flexible schema capabilities. The nuances of the platform demand data backup tools designed <span class="No-Break">for purpose.</span></p>
			<p>At first glance, you might assume that backing up every element in your org is the most prudent approach. However, this isn’t always the case and can sometimes be counterproductive. There are <a id="_idIndexMarker384"/>certain objects that, while frequently updated, don’t hold significant value in backups. Objects such as <strong class="source-inline">AuthSession</strong>, <strong class="source-inline">LoginGeo</strong>, and <strong class="source-inline">LoginIp</strong>, for instance, undergo frequent changes and don’t contain vital <span class="No-Break">business data.</span></p>
			<p>Similarly, if you’re considering backing up a custom object such as <strong class="source-inline">Case</strong>, the associated <strong class="source-inline">CaseFeed</strong>, <strong class="source-inline">CaseHistory</strong>, and <strong class="source-inline">CaseShare</strong> objects may not provide much backup value for the same rationale. Omitting these objects can result in more compact backups, facilitating quicker backup and restoration processes – in the case of sharing tables, these are dynamically recreated once the records to which they relate are restored. Employing a backup solution that allows you to track and assess your data’s modifications will be even more effective if you exclude high-churn objects. This exclusion streamlines your <a id="_idIndexMarker385"/>monitoring process, making it easier to identify impactful changes to <span class="No-Break">essent<a id="_idTextAnchor219"/>ial data<a id="_idTextAnchor220"/>.</span></p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor221"/>The recovery process</h1>
			<p>In the event of a crisis, when Salesforce<a id="_idIndexMarker386"/> data becomes compromised or lost, the immediate availability of backups is a lifeline. Yet, merely having backups at the ready isn’t the entire solution for ensuring business continuity. An intricate architecture encompassing the recovery process, its validation, and the seamless restoration of data <span class="No-Break">is paramount.</span></p>
			<p>The initial step of this process is rooted in validating the backup’s viability. It’s a common pitfall to just assume the most recent backup is devoid of errors and ready for use. This assumption can lead to further complications, emphasizing the necessity for continuous monitoring of backup integrity. By automating and continuously monitoring backups, organizations can promptly detect anomalies, ensuring that a reliable and untainted backup is always <span class="No-Break">at hand.</span></p>
			<p>However, even before leaping into recovery mode, organizations must adopt a strategic stance. Analyzing the nature and extent of the data loss is critical. Delving into backup analytics can shed light on whether the data was corrupted or deleted entirely. It helps pin down which specific records and objects were affected, and when the anomaly occurred. This thorough examination ensures that the restoration process zeroes in on the affected areas, safeguarding any unaffected data from being <span class="No-Break">inadvertently overwritten.</span></p>
			<p>Once the scope of data requiring restoration is clear, the restoration process itself can be planned. For metadata, it is advisable to restore components incrementally in a logical order matching dependencies. Foundational elements such as core custom objects and fields should be addressed first, providing a base for permissions, business logic, and presentation layers to follow. Breaking restoration into smaller sections avoids error cascades and <span class="No-Break">simplifies troubleshooting.</span></p>
			<p>When substantial metadata <a id="_idIndexMarker387"/>volumes exist, a phased approach to restoring prioritized categories reduces risk. The recommended sequence is <span class="No-Break">as follows:</span></p>
			<ol>
				<li><strong class="bold">Data tier</strong>: The core custom objects, fields, and schema that define the foundation of the org’s <span class="No-Break">data structure.</span></li>
				<li><strong class="bold">Security</strong>: Permission sets, profiles, and sharing rules that control user access and <span class="No-Break">data isolation.</span></li>
				<li><strong class="bold">Programmability</strong>: Any Apex classes, triggers, components, and tests that enable custom <span class="No-Break">business logic.</span></li>
				<li><strong class="bold">Presentation</strong>: Visualforce, Lightning, and layout metadata that constitute the UI <span class="No-Break">presentation layer.</span></li>
				<li><strong class="bold">Other</strong>: Additional metadata such as emails, reports, and documents that customize and configure <span class="No-Break">the org.</span></li>
			</ol>
			<p>This workflow matches the platform’s inherent hierarchy from raw data components up through <span class="No-Break">complex overlays.</span></p>
			<p>Similarly, when restoring large data volumes, core objects should be prioritized first, while still capturing related data dependencies. Salesforce’s recommended priority is <span class="No-Break">as follows:</span></p>
			<ol>
				<li><span class="No-Break">Users</span></li>
				<li><span class="No-Break">Accounts</span></li>
				<li><span class="No-Break">Campaigns</span></li>
				<li><span class="No-Break">Contacts</span></li>
				<li><span class="No-Break">Opportunities</span></li>
				<li><span class="No-Break">Cases</span></li>
				<li><span class="No-Break">Price books</span></li>
				<li><span class="No-Break">Products</span></li>
				<li><span class="No-Break">Leads</span></li>
				<li><span class="No-Break">Contracts</span></li>
			</ol>
			<p>Such a division of object data is further encouraged by the Salesforce platform, depending on the restoration mechanism. If you’re using Apex, the behavior around record chunking should be well understood by architects – be sure to read and consider the <em class="italic">Creating Records for Multiple Object Types</em> section of the Salesforce documentation <span class="No-Break">at </span><a href="https://developer.salesforce.com/docs/atlas.en-us.apexcode.meta/apexcode/langCon_apex_dml_limitations.htm#:~:text=Creating%20Records%20for%20Multiple%20Object%20Types"><span class="No-Break">https://developer.salesforce.com/docs/atlas.en-us.apexcode.meta/apexcode/langCon_apex_dml_limitations.htm#:~:text=Creating%20Records%20for%20Multiple%20Object%20Types</span></a><span class="No-Break">.</span></p>
			<p>Robust backup solutions can restore interconnected data across multiple objects to maintain relationship integrity. Segmenting very large datasets by owner or timeframe helps avoid platform limits when restoring manually. Architects should also be aware of the concepts of lookup skew, ownership skew, and the platform’s behavior around record locking when designing a <span class="No-Break">restoration process.</span></p>
			<p>Lookup skew<a id="_idIndexMarker388"/> in<a id="_idIndexMarker389"/> Salesforce refers to a situation where many records (usually tens of thousands or more) are associated with a single record through a lookup relationship. This can lead to performance issues and can negatively impact database operations such as queries, reports, and data loads. Ownership skew <a id="_idIndexMarker390"/>in Salesforce is a different but related concept to lookup skew. It occurs when many records in a Salesforce organization are owned by a single user or a small group of users. This can create performance issues and operational challenges, like those experienced with <span class="No-Break">lookup skew.</span></p>
			<p>By following a structured, phased restoration workflow specific to data loss circumstances, organizations can streamline recovery, troubleshoot issues early, and minimize errors caused by oversights in <span class="No-Break">addressing interdependencies.</span></p>
			<p>You might be wondering about the best way to perfect this restoration process. The answer lies in the utilization of Salesforce sandboxes. By conducting dry runs in these environments, teams can refine and optimize recovery procedures. Over time, these repeated practices forge a sort of <em class="italic">muscle memory</em>, enabling swift and efficient responses to real-time crises. Moreover, detailed documentation of this recovery blueprint serves a dual purpose: it’s a valuable reference guide and an essential training tool for onboarding new <span class="No-Break">team members.</span></p>
			<p>For organizations that have integrated DevOps into their operations, the recovery process can be streamlined further. Harnessing familiar tools and environments expedites restoration efforts. When teams are equipped to use tools and interfaces they interact with daily, it diminishes the likelihood of errors borne out <span class="No-Break">of unfamiliarity.</span></p>
			<p>While the importance<a id="_idIndexMarker391"/> of robust backups cannot be understated, an organization’s resilience in the face of data crises hinges on more than just backups. It requires a harmonious blend of meticulous planning, rigorous testing, comprehensive documentation, and continuous training. Only then can organizations be confident of a swift and effective response to data mishaps. Preparedness isn’t just a strategy; it’s the bedrock of data security in the <span class="No-Break">Salesforce ecosystem.</span></p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor222"/>Incident and disaster recovery planning</h1>
			<p>Robust backup solutions are an essential foundation for data protection, but organizations must also develop detailed plans for responding to and recovering from incidents. Preparing incident response and disaster recovery procedures alongside backup tools provides a comprehensive approach to minimizing <span class="No-Break">business disruption.</span></p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor223"/>Incident response plans</h2>
			<p>Incident response plans outline<a id="_idIndexMarker392"/> the immediate actions that are taken during a crisis such as a data breach or system outage. They establish a clear protocol for investigation, containment, communication, and initial recovery steps. Here are some of the <span class="No-Break">key elements:</span></p>
			<ul>
				<li>Defining roles for assembling an incident response team with <span class="No-Break">cross-functional expertise</span></li>
				<li>Checklists for evidence gathering, damage assessment, and root cause analysis <span class="No-Break">of incidents</span></li>
				<li>Plans for internal and external communication with <span class="No-Break">affected stakeholders</span></li>
				<li>Compliance protocols for reporting incidents to <span class="No-Break">regulatory bodies</span></li>
				<li>Initial recovery workflows for restoring from <span class="No-Break">backups safely</span></li>
				<li>Documentation for lessons learned and plan improvement <span class="No-Break">after incidents</span></li>
			</ul>
			<p>Having an incident <a id="_idIndexMarker393"/>response plan helps you avoid making rushed, stressed decisions during crises, instead enabling a deliberate, <span class="No-Break">optimized reaction.</span></p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor224"/>Disaster recovery plans</h2>
			<p>Disaster recovery plans<a id="_idIndexMarker394"/> focus on long-term processes for restoring normal business operations after major outages. They set policies and technology strategies for resilience, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li>Identifying maximum acceptable downtime thresholds <span class="No-Break">for systems</span></li>
				<li>Architecting infrastructure redundancies and <span class="No-Break">alternative sites</span></li>
				<li>Defining backup schedule frequency to limit potential <span class="No-Break">data loss</span></li>
				<li>Calculating how long restoration from backups <span class="No-Break">will take</span></li>
				<li>Creating procedures for orderly restoration backed <span class="No-Break">by testing</span></li>
				<li>Training personnel and conducting simulations to refine <span class="No-Break">recovery capabilities</span></li>
				<li>Ensuring compliance with regulatory requirements <span class="No-Break">around availability</span></li>
				<li>Documenting plans, system diagrams, dependencies, <span class="No-Break">and procedures</span></li>
			</ul>
			<p>Careful disaster recovery planning reduces the operational, financial, legal, and reputational consequences of catastrophic <span class="No-Break">system disruptions.</span></p>
			<p>By developing coordinated incident response and disaster recovery plans alongside data backups, organizations can minimize business disruption, enhance resilience, and <span class="No-Break">improve responsiveness.</span></p>
			<p>Of course, we all know that Salesforce has considerations that may not always exist in other software systems. For example, architects should consider how to handle reinstating data that’s further along the business process than the typical new record well in advance of any data restoration exercise. Think about a rule that Opportunity records cannot be created as Closed Won. However, if you’re restoring a backup in the event of a data loss, you may very well want to <span class="No-Break">do that.</span></p>
			<p>Validation rules are another simple example here, and some DevOps solutions can deactivate these before restoration. However, more complex frameworks have grown around flows and<a id="_idIndexMarker395"/> triggers – and managed packages may have further custom solutions, given that managed flows and triggers cannot be deactivated. Therefore, when designing business logic to handle data validation, ensure there’s also the ability to insert data in any state during a data <span class="No-Break">restoration exercise.</span></p>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor225"/>Securing backup data</h1>
			<p>Backup data often<a id="_idIndexMarker396"/> contains sensitive information such as customer details, employee records, intellectual property, and other proprietary or regulated data types. As such, properly securing backup data is imperative for mitigating compliance, contractual, and <span class="No-Break">cybersecurity risks.</span></p>
			<p>For storage, cloud-based backups are common given Salesforce’s cloud nature, but jurisdictional data sovereignty policies may dictate on-premises backups. In any case, backups should be stored separately from live data to limit exposure from single points <span class="No-Break">of failure.</span></p>
			<p>Managing access to backups is critical. This can be done with role-based controls, minimum privileges, and strict identity management. This ensures that only authorized individuals can view or modify backups. Encryption safeguards for data in transit and at rest using industry standards such as AES-256 are a must. Encryption keys should have robust life cycle management <span class="No-Break">as well.</span></p>
			<p>Monitoring backup operations, access, anomalies, and changes provides valuable activity audit logs to demonstrate compliance and support incident investigation <span class="No-Break">if needed.</span></p>
			<p>Backup platforms must assist with evolving compliance needs around retention policies, data residency, right-to-be-forgotten, and regulatory mandates. Providers should have rigorous cybersecurity programs that cover software development, incident response, vulnerability management, and <span class="No-Break">business continuity.</span></p>
			<p>By holistically addressing backup security and compliance requirements upfront, organizations can appropriately safeguard sensitive data while enabling streamlined recovery <span class="No-Break">when needed.</span></p>
			<h1 id="_idParaDest-188"><a id="_idTextAnchor226"/>Navigating GDPR and CCPA regulations for data backups</h1>
			<p>In an era marked by heightened sensitivity to data protection and privacy, businesses operating across Europe and the US are subject to two major regulatory frameworks: GDPR and CCPA. These regulations directly influence how organizations manage and store <span class="No-Break">backup data.</span></p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor227"/>GDPR overview</h2>
			<p>The <strong class="bold">GDPR</strong> empowers <a id="_idIndexMarker397"/>individuals within <a id="_idIndexMarker398"/>the <strong class="bold">European Union</strong> (<strong class="bold">EU</strong>) and <strong class="bold">European Economic Area</strong> (<strong class="bold">EEA</strong>) with<a id="_idIndexMarker399"/> expansive rights over their data. Among the many provisions, the right to erasure stands out prominently when considering backup strategies. As stipulated in Article 17 of the GDPR, individuals possess the authority to mandate virtually any company – even those outside the EU and EEA – to eliminate their data from the company’s archives within a stipulated 30-day period. Consequently, businesses catering to EU and EEA clientele must have the proficiency to pinpoint and expunge specific records from <span class="No-Break">their backups.</span></p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor228"/>Insights into CCPA</h2>
			<p>Coming into <a id="_idIndexMarker400"/>force in 2020, the <strong class="bold">California Consumer Privacy Act</strong> (<strong class="bold">CCPA</strong>) bestows comparable rights to California’s residents. While the CCPA’s purview is somewhat narrower than the GDPR’s, it imposes specific criteria that companies must fulfill before being accountable for CCPA-related requests. Notably, like its European counterpart, the CCPA empowers individuals to inquire about their held data and demand its removal. As these regulations become increasingly standard, businesses should anticipate the need to adhere to either or both the GDPR and CCPA. Thus, evaluating backup strategies for compliance with these data protection norms <span class="No-Break">is indispensable.</span></p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor229"/>Data retention considerations</h1>
			<p>An important decision in architecting<a id="_idIndexMarker401"/> backup solutions involves establishing data retention policies to govern how long backup data persists before deletion. Organizations take varied approaches to retention depending on business needs, legal factors, and <span class="No-Break">budget constraints.</span></p>
			<p>Many default to indefinite data retention to maximize the historical snapshots available for recovery. With no set deletion schedules, all backup data is perpetually available. This ensures confidence that even very old backup copies can be accessed if needed to restore lost production data or address a historical audit. However, indefinite retention leads to continuously expanding storage consumption and cost over time as new <span class="No-Break">backups accumulate.</span></p>
			<p>Other<a id="_idIndexMarker402"/> companies choose to implement limited data retention windows that automatically delete backups after a set period. Retention may be mandated by industry regulations requiring data to be kept only for a specific number of years due to privacy statutes. Contractual clauses with customers or partners may also dictate maximal retention timeframes. From a budget perspective, deleting backups periodically helps control the otherwise unlimited growth in storage expenses that’s inevitable with indefinite <span class="No-Break">retention approaches.</span></p>
			<p>Some organizations align backup data retention schedules with their consumer data retention policies, which limit how long they persist personal information. This ensures backups reflect corporate deletion commitments. However, it can pose risks if valuable historical business data is deleted prematurely alongside expired consumer records. A better approach is to retain business data indefinitely while deleting only consumer data from backups based on <span class="No-Break">data types.</span></p>
			<p>The tradeoff with limited retention policies is that the availability of snapshots for historical recovery diminishes over time. While recent backup data is maintained, older backups eventually get deleted forever. So, retention schedules imply a tolerance for permanently losing access to dated records after the retention <span class="No-Break">period elapses.</span></p>
			<p>In regulated industries such as financial services and healthcare, retention minimums may be mandated for certain data types, allowing early deletion of other data past a specific age. Compliance factors should be incorporated when defining <span class="No-Break">retention policies.</span></p>
			<p>Ultimately, organizations need to weigh the pros and cons of varied retention approaches based on their unique business needs, data recovery requirements, growth constraints, and legal obligations. However, having explicit formal data retention policies for backups provides consistency, predictability, and alignment with corporate data <a id="_idIndexMarker403"/>governance standards. As with all data policies, retention guidelines should be <span class="No-Break">reviewed regularly.</span></p>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor230"/>Options for Salesforce backup</h1>
			<p>When designing a backup<a id="_idIndexMarker404"/> strategy, organizations have several approaches <span class="No-Break">to consider.</span></p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor231"/>No backups</h2>
			<p>Inaction can<a id="_idIndexMarker405"/> itself be an intentional action, but when it comes to backing up your data, it’s not recommended. The lack of any data protection exposes companies to major risks of business disruption, compliance violations, and even bankruptcy after data loss incidents. False confidence in native Salesforce recovery or recycling bin services leaves gaps that should not be <span class="No-Break">left open.</span></p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor232"/>Manual exports</h2>
			<p>Self-service data exports provide <a id="_idIndexMarker406"/>minimal protection at least, but require manual repetition, offer slow and limited restoration, and incur overhead for storage and compliance. A lack of metadata backups to complement the data backup and provide context only hinders <span class="No-Break">full recovery.</span></p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor233"/>Version control</h2>
			<p>The<a id="_idIndexMarker407"/> integration of <strong class="bold">version control systems</strong> (<strong class="bold">VCSs</strong>) as a<a id="_idIndexMarker408"/> mechanism for metadata and data backup represents a significant advancement over manual approaches. In scenarios where catastrophic failures occur, a VCS can act as a reliable source for disaster recovery, enabling teams to redeploy the entire metadata to a new Salesforce org, ensuring business continuity. Additionally, maintaining a detailed history of changes is crucial for audit trails and compliance requirements, which VCS facilitates by tracking changes in a structured and <span class="No-Break">accessible manner.</span></p>
			<p>However, the reliance on VCSs for backup in Salesforce DevOps is not without its challenges and limitations. The complexity of incorporating VCSs into Salesforce necessitates a deep understanding of both the Salesforce environment and version control practices, which can be daunting, especially for click-based admins who are not familiar with version control. The risk of overwriting existing code or introducing updates that conflict with the main repository is heightened when multiple developers work on the same project, even though VCSs offer a failsafe <span class="No-Break">for overwrites.</span></p>
			<p>The scope of VCSs is limited in capturing the nuances of Salesforce data, which might lead to incomplete backups and recovery challenges. The dependency on external tools introduces additional points of failure in the backup and recovery process, making it time and resource-intensive. VCSs might not be sufficient for full disaster recovery in cases of complete system failure or <span class="No-Break">data corruption.</span></p>
			<p>Achieving a seamless and efficient integration of VCS that is accessible and user-friendly for Salesforce admins presents a significant challenge. Ensuring compliance with data protection laws and security standards adds another layer of complexity to using a VCS as a backup solution. Inefficient use of a VCS can lead to increased technical debt, especially if deployments frequently encounter bugs and errors, and the potential for increased technical <a id="_idIndexMarker409"/>debt looms. Without a comprehensive VCS strategy, there may be a lack of complete visibility into the history and context of changes, hindering debugging, decision-making, and understanding the impact of specific changes in the <span class="No-Break">Salesforce environment.</span></p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor234"/>Native Salesforce tools</h2>
			<p>Salesforce’s backup solution, which<a id="_idIndexMarker410"/> is now generally available, offers automated backups, effortless data restoration, and robust encryption to enhance data security. However, a notable limitation is the confinement of data within the Salesforce infrastructure. It lacks an off-platform backup option, which could be a setback for organizations seeking diversified data backup strategies. Initially, Salesforce had a data recovery service that was discontinued, but due to significant customer feedback, it was reinstated, which led to the creation of the new Backup and <span class="No-Break">Restore service.</span></p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor235"/>Purpose-built backup solutions</h2>
			<p>Robust <a id="_idIndexMarker411"/>third-party tools designed specifically for Salesforce provide automation, data monitoring, fast full restores, security integrations, and other advanced capabilities tailored to the platform’s <span class="No-Break">unique needs.</span></p>
			<p>When evaluating options, the highest priorities are confidence in comprehensive and rapid recovery, minimal disruption during incidents, plus security and compliance. The budget must be weighed against the high costs of downtime. Integrated DevOps tools <span class="No-Break">maximize value.</span></p>
			<p>Many, if not all, of these solutions offer a free trial, which will allow you to ensure that they have coverage and ease of use – these two factors alone are critical <span class="No-Break">decision points.</span></p>
			<p>Restoring both data and metadata accurately can be very hard. Some metadata types and corresponding data in Salesforce cannot be restored. These considerations should be part of the architectural planning for backup and disaster recovery so that the business understands the limits of what can be restored. As a simple example, in most cases, you won’t get the original Salesforce record IDs back after restoring a record, so other systems should not necessarily rely on <span class="No-Break">record IDs.</span></p>
			<p>Factoring in any <a id="_idIndexMarker412"/>limitations informs the choice of tools for backup. If certain aspects are hard to do and important to the business, that should be at the top of any <span class="No-Break">vendor evaluation.</span></p>
			<h1 id="_idParaDest-198"><a id="_idTextAnchor236"/>Summary</h1>
			<p>In this chapter, we explored the critical need for comprehensive data and metadata backups within Salesforce to minimize business disruption from data <span class="No-Break">loss incidents.</span></p>
			<p>We discussed the potentially catastrophic costs of data loss and system outages in terms of lost productivity, revenue, reputation, and regulatory compliance if recovery is not swift. Viable backups limit damage by enabling <span class="No-Break">rapid restoration.</span></p>
			<p>Key capabilities such as external storage, automation, integration, smart restoration, and enterprise-grade security are required elements of a robust Salesforce backup solution. Metadata must be backed up alongside raw data to provide structure, access controls, and automation. Effective techniques for large-scale Salesforce data backup were <span class="No-Break">also covered.</span></p>
			<p>Incident response plans outline immediate actions during crises, while disaster recovery plans focus on long-term restoration procedures. Testing and documentation are key. Carefully securing sensitive backup data is imperative <span class="No-Break">as well.</span></p>
			<p>Comprehensive data protection combining backups, planning, security, compliance, and purpose-built solutions designed for Salesforce enables organizations to minimize disruption, safeguard information assets, and maintain business continuity and <span class="No-Break">customer trust.</span></p>
			<p>Ultimately, your backups are only as effective as your ability to restore them. Equally, you can’t restore what you haven’t backed up, so they should be extensive <span class="No-Break">and regular.</span></p>
			<p>In the next chapter, we’ll tackle another of the more operational aspects of DevOps, which is monitoring for changes. In Salesforce, changes can arrive from multiple sources, even production. Keeping tight control over these changes and ensuring all your environments are up to date and in sync through effective monitoring will be our <span class="No-Break">next topic.</span></p>
		</div>
	</body></html>