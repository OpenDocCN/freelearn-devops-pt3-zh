<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer145">
			<h1 id="_idParaDest-378" class="chapter-number"><a id="_idTextAnchor1812"/>14</h1>
			<h1 id="_idParaDest-379"><a id="_idTextAnchor1813"/>Understanding Key Performance Indicators (KPIs) for Your Production Service</h1>
			<p>In <a id="_idIndexMarker1541"/>the previous chapters, we looked at the core concepts of modern DevOps – <strong class="bold">Continuous Integration</strong> (<strong class="bold">CI</strong>) and <strong class="bold">Continuous Deployment/Delivery</strong> (<strong class="bold">CD</strong>). We <a id="_idIndexMarker1542"/>also looked at various tools and techniques that can help us enable a mature and secure DevOps channel across our organization. In this rather theory-focused chapter, we’ll try to understand<a id="_idIndexMarker1543"/> some <strong class="bold">key performance indicators</strong> (<strong class="bold">KPIs</strong>) for operating our application <span class="No-Break">in production.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topic<a id="_idTextAnchor1814"/>s:</span></p>
			<ul>
				<li>Understanding the importance <span class="No-Break">of reliability</span></li>
				<li>SLOs, SLAs, <span class="No-Break">and SLIs</span></li>
				<li><span class="No-Break">Error budgets</span></li>
				<li>Recovery Time Objective (RPO) and Recovery Point <span class="No-Break">Objective</span><span class="No-Break"> (RTO)</span></li>
				<li>Running<a id="_idIndexMarker1544"/> distributed <a id="_idIndexMarker1545"/>applications <span class="No-Break">in production</span></li>
			</ul>
			<p>So, let’s <span class="No-Break">get star<a id="_idTextAnchor1815"/><a id="_idTextAnchor1816"/>ted!</span></p>
			<h1 id="_idParaDest-380"><a id="_idTextAnchor1817"/>Understanding the importance of reliability</h1>
			<p>Developing <a id="_idIndexMarker1546"/>software is one thing, and running it in production is another. The reason behind such a disparity is that most development teams cannot simulate production conditions in non-production environments. Therefore, many bugs are uncovered when the software is already running in production. Most issues encountered are non-functional – for example, the services could not scale properly with additional traffic, the amount of resources assigned to the application was suboptimal, thereby crashing the site, and many more. These issues need to be managed to make the software <span class="No-Break">more reliable.</span></p>
			<p>To understand the importance<a id="_idIndexMarker1547"/> of software reliability, let’s look at an example retail banking application. Software reliability is critically important for <span class="No-Break">several reasons:</span></p>
			<ul>
				<li><strong class="bold">User satisfaction</strong>: Reliable software ensures a positive user experience. Users expect software to work as intended, and when it doesn’t, it can lead to frustration, loss of trust, and a poor reputation for the software or the organization behind it. For a bank’s retail customer, it might mean customers cannot do essential transactions and, therefore, may face hassles in payments and receipts, leading to a loss in <span class="No-Break">user satisfaction.</span></li>
				<li><strong class="bold">Business reputation</strong>: Software failures can tarnish a company’s reputation and brand image. For our bank, if the issues are frequent, customers will look for other options, resulting in considerable churn and loss <span class="No-Break">of business.</span></li>
				<li><strong class="bold">Financial impact</strong>: Software failures can be costly. They can result in lost sales, customer support expenses, and even legal liabilities in cases where software failures cause harm or financial losses to users. This becomes especially critical for banking applications as customers' money is involved. If transactions don’t happen in time, it can result in a loss of customer business, which will hurt the bank in the <span class="No-Break">long run.</span></li>
				<li><strong class="bold">Competitive advantage</strong>: Reliable software can provide a competitive edge. Users are more likely to choose and stick with a bank with robust online banking software that consistently meets their needs <span class="No-Break">and expectations.</span></li>
				<li><strong class="bold">Productivity and efficiency</strong>: Within organizations, reliable software is essential for maintaining productivity. Imagine the pain that the customer support and front office staff would have in such a disruption! You would also need more resources to manage these issues, which can disrupt operations, leading to wasted time <span class="No-Break">and resources.</span></li>
				<li><strong class="bold">Security</strong>: Reliable software is often more secure. Attackers can exploit vulnerabilities and bugs in unreliable software. In the case of a bank, security is of prime importance because any breach can result in direct financial impact and loss. Ensuring reliability is a fundamental part <span class="No-Break">of cybersecurity.</span></li>
				<li><strong class="bold">Compliance</strong>: In some industries, especially banking, there are regulatory requirements related to software reliability. Failing to meet these requirements can result in legal and <span class="No-Break">financial penalties.</span></li>
				<li><strong class="bold">Customer trust</strong>: Trust is a critical factor in software usage, especially in the case of a banking application. Users must trust that their money and data will be handled securely and that the software will perform as expected. Software reliability is a key factor in building and maintaining <span class="No-Break">this trust.</span></li>
				<li><strong class="bold">Maintainability</strong>: Reliable software is typically easier to maintain. When software is unreliable, fixing bugs and updating becomes more challenging, potentially leading to a downward spiral of <span class="No-Break">increasing unreliability.</span></li>
				<li><strong class="bold">Scaling and growth</strong>: As software usage grows, reliability becomes even more critical. Software that works well for a small user base may struggle to meet the demands of a <a id="_idIndexMarker1548"/>larger user base without proper reliability measures <span class="No-Break">in place.</span></li>
			</ul>
			<p>In summary, software reliability is not just a technical concern; it has wide-reaching implications for user satisfaction, business success, and even legal and financial aspects. Therefore, investing in ensuring the reliability of software is a prudent and strategic decision <span class="No-Break">for organizations.</span></p>
			<p>Historically, running and managing software in production was the job of the Ops team, and most organizations still use it. The Ops team comprises a bunch of <strong class="bold">system administrators</strong> (<strong class="bold">SysAdmins</strong>) who<a id="_idIndexMarker1549"/> must deal with the day-to-day issues of running the software in production. They implement scaling and fault tolerance with software, patch and upgrade software, work on support tickets, and keep the systems running so the software application <span class="No-Break">functions well.</span></p>
			<p>We’ve all experienced the divide between Dev and Ops teams, each with its own goals, rules, and priorities. Often, they found themselves at odds because what benefited Dev (software changes and rapid releases) created challenges for Ops (stability <span class="No-Break">and reliability).</span></p>
			<p>However, the emergence of DevOps has changed this dynamic. In the words of Andrew Shafer and Patrick Debois, DevOps<a id="_idIndexMarker1550"/> is a culture and practice in software engineering aimed at bridging the gap between software development <span class="No-Break">and operations.</span></p>
			<p>Looking at DevOps from an Ops perspective, Google came up with <strong class="bold">site reliability engineering</strong> (<strong class="bold">SRE</strong>) as an <a id="_idIndexMarker1551"/>approach that embodies DevOps principles. It encourages shared ownership, the use of common tools and practices, and a commitment to learning from failures to prevent recurring issues. The primary objective is to develop and maintain a dependable application without sacrificing the speed of delivery – a balance that was once thought contradictory (that is, <em class="italic">create better </em><span class="No-Break"><em class="italic">software faster</em></span><span class="No-Break">).</span></p>
			<p>The idea of SRE is a novel thought about what would happen if we allowed software engineers to run the production environment. So, Google devised the following approach for running its <span class="No-Break">Ops team.</span></p>
			<p>For Google, an ideal candidate for joining the SRE team should exhibit two <span class="No-Break">key characteristics:</span></p>
			<ul>
				<li>Firstly, they quickly become disinterested in manual tasks and seek opportunities to <span class="No-Break">automate them</span></li>
				<li>Secondly, they possess the requisite skills to develop software solutions, even when faced with <span class="No-Break">complex challenges</span></li>
			</ul>
			<p>Additionally, SREs should share an academic and intellectual background with the broader development organization. Essentially, SRE<a id="_idIndexMarker1552"/> work, traditionally within the purview of operations teams, is carried out by engineers with strong software expertise. This strategy hinges on the natural inclination and capability of these engineers to design and implement automation solutions, thus reducing reliance on <span class="No-Break">manual labor.</span></p>
			<p>By design, SRE teams maintain a strong engineering focus. Without continuous engineering efforts, the operational workload escalates, necessitating an expansion of the team to manage the increasing demands. In contrast, a conventional operations-centric group scales in direct proportion to the growth of the service. If the services they support thrive, operational demands surge with increased traffic, compelling the hiring of additional personnel to perform <span class="No-Break">repetitive tasks.</span></p>
			<p>To avert this scenario, the team responsible for service management must incorporate coding into their responsibilities; otherwise, they risk <span class="No-Break">becoming overwhelmed.</span></p>
			<p>Accordingly, Google establishes a 50% upper limit on the aggregate “Ops” work allocated to all SREs, encompassing activities such as handling tickets, on-call duties, and manual tasks. This constraint guarantees that SRE teams allocate a substantial portion of their schedules to enhancing the stability and functionality of the service. While this limit serves as an upper bound, the ideal outcome is that, over time, SREs carry minimal operational loads and primarily engage in development endeavors as the service evolves to a self-sustaining state. Google’s objective is to create systems that are not merely automated but inherently self-regulating. However, practical considerations such as scaling and introducing new features continually <span class="No-Break">challenge SREs.</span></p>
			<p>SREs are <a id="_idIndexMarker1553"/>meticulous in their approach, relying on measurable metrics to track progress toward specific goals. For instance, stating that a website is <em class="italic">running slowly</em> is vague and unhelpful in an engineering context. However, declaring that the 95th percentile of response time has exceeded <a id="_idIndexMarker1554"/>the <strong class="bold">service-level objective</strong> (<strong class="bold">SLO</strong>) by 10% provides precise information. SREs also focus on reducing repetitive tasks, known as <strong class="bold">toil</strong>, by <a id="_idIndexMarker1555"/>automating them to prevent burnout. Now, let’s look at some of the key SRE <span class="No-Break">performance indicators.</span></p>
			<h1 id="_idParaDest-381"><a id="_idTextAnchor1818"/>Understanding SLIs, SLOs, and SLAs</h1>
			<p>In the <a id="_idIndexMarker1556"/>realm of<a id="_idIndexMarker1557"/> site reliability, three crucial parameters guide SREs: the <strong class="bold">indicators of availability</strong> – <strong class="bold">service-level indicators</strong> (<strong class="bold">SLIs</strong>), the <strong class="bold">definition of availability</strong> –SLOs, and<a id="_idIndexMarker1558"/> the <strong class="bold">consequences of unavailability</strong> – <strong class="bold">service-level agreements</strong> (<strong class="bold">SLAs</strong>). Let’s start by exploring SLIs <span class="No-Break">in detail.</span></p>
			<h2 id="_idParaDest-382"><a id="_idTextAnchor1819"/>SLIs</h2>
			<p>SLIs serve as <a id="_idIndexMarker1559"/>quantifiable reliability metrics. Google defines them as “<em class="italic">carefully defined quantitative measures of some aspect of the level of service provided.</em>” Common examples include request latency, failure rate, and data throughput. SLIs are specific to user journeys, which are sequences of actions users perform to achieve specific goals. For instance, a user journey for our sample Blog App might involve creating a new <span class="No-Break">blog post.</span></p>
			<p>Google, the original advocate of SRE, has identified four golden signals that apply to most <span class="No-Break">user journeys:</span></p>
			<ul>
				<li><strong class="bold">Latency</strong>: This measures the time it takes for your service to respond to <span class="No-Break">user requests</span></li>
				<li><strong class="bold">Errors</strong>: This indicates the percentage of failed requests, highlighting issues in <span class="No-Break">service reliability</span></li>
				<li><strong class="bold">Traffic</strong>: Traffic represents the demand directed toward your service, reflecting <span class="No-Break">its usage</span></li>
				<li><strong class="bold">Saturation</strong>: Saturation assesses how fully your infrastructure components <span class="No-Break">are utilized</span></li>
			</ul>
			<p>One recommended approach by Google to calculate SLIs is by determining the ratio of good events to <span class="No-Break">valid events:</span></p>
			<pre class="source-code">
SLI = (Good Events * 100) / Valid Events</pre>			<p>A perfect <a id="_idIndexMarker1560"/>SLI score of 100 implies everything functions correctly, while a score of 0 signifies <span class="No-Break">widespread issues.</span></p>
			<p>A valuable SLI should align closely with the user experience. For example, a lower SLI value should correspond to decreased customer satisfaction. If this alignment is absent, the SLI may not provide meaningful insights or be <span class="No-Break">worth measuring.</span></p>
			<p>Let’s look at the following figure to understand <span class="No-Break">this better:</span></p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/B19877_14_1.jpg" alt="Figure 14.1 – Good versus bad SLI" width="1573" height="1325"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.1 – Good versus bad SLI</p>
			<p>As we can see, the CPU Utilisation <a id="_idIndexMarker1561"/>SLI does not reflect customer satisfaction in any way; that is, there is no direct correlation between increasing CPU Utilisation and decreased customer satisfaction except after it crosses the 80% threshold. In contrast, the Latency SLI directly correlates with customer satisfaction, which reduces with increasing latency and significantly after the 300ms and 500ms levels. Therefore, it is a good idea to use Latency as an SLI over <span class="No-Break">CPU Utilization.</span></p>
			<p>It’s also advisable to limit the number of SLIs to a manageable quantity. Too many SLIs can lead to team confusion and trigger numerous false alarms. It’s best to focus on four or five metrics directly linked to customer satisfaction. For instance, instead of monitoring CPU and memory usage, prioritize metrics such as request latency and <span class="No-Break">error rate.</span></p>
			<p>Furthermore, prioritizing user journeys is essential, giving higher importance to journeys that significantly impact customers and lower importance to those with less of a customer impact. For example, ensuring a seamless create and update post experience in our Blog App is more critical than the reviews and ratings service. SLIs <a id="_idIndexMarker1562"/>alone do not make much sense as they are just measurable indicators. We need to set objectives for SLIs. So, let’s look <span class="No-Break">at SLOs.</span></p>
			<h2 id="_idParaDest-383"><a id="_idTextAnchor1820"/>SLOs</h2>
			<p>Google’s definition<a id="_idIndexMarker1563"/> of SLOs states that they “<em class="italic">establish a target level for the reliability of your service.</em>” They specify the percentage of compliance with SLIs required to consider your site reliable. SLOs are formulated by combining one or <span class="No-Break">more SLIs.</span></p>
			<p>For instance, if you have an SLI that mandates <em class="italic">request latency to remain below 500ms within the last 15 minutes with a 95th percentile measurement</em>, an SLO would necessitate <em class="italic">the SLI to be met 99% of the time for a </em><span class="No-Break"><em class="italic">99% SLO</em></span><span class="No-Break">.</span></p>
			<p>While every organization aims for 100% reliability, setting a 100% SLO is not a practical goal. A system with a 100% SLO tends to be costly, technically complex, and often unnecessary for most applications to be deemed acceptable by <span class="No-Break">their users.</span></p>
			<p>In the realm of software services and systems, the pursuit of 100% availability is generally misguided because users cannot feel any practical distinction between a system that is 100% available and one that is 99.999% available. Multiple intermediary systems exist between the user and the service, such as their personal computer, home Wi-Fi, <strong class="bold">Internet Service Provider</strong> (<strong class="bold">ISP</strong>), and the<a id="_idIndexMarker1564"/> power grid, and these collectively exhibit availability far lower than 99.999%. Consequently, the negligible difference between 99.999% and 100% availability becomes indistinguishable amidst the background noise of other sources of unavailability. Thus, investing substantial effort to attain that last 0.001% availability yields no noticeable benefit to the <span class="No-Break">end user.</span></p>
			<p>In light of this understanding, a question arises: if 100% is an inappropriate reliability target, what constitutes the right reliability target for a system? Interestingly, this is not a technical inquiry but rather a product-related one, necessitating consideration of the <span class="No-Break">following factors:</span></p>
			<ul>
				<li><strong class="bold">User satisfaction</strong>: Determining the level of availability that aligns with user contentment, considering their typical usage patterns <span class="No-Break">and expectations</span></li>
				<li><strong class="bold">Alternatives</strong>: Evaluating the availability of alternatives available to dissatisfied users, should they seek alternatives due to dissatisfaction with the product’s current level <span class="No-Break">of availability</span></li>
				<li><strong class="bold">User behavior</strong>: Examining how users’ utilization of the product varies at different availability levels, recognizing that user behavior may change in response to fluctuations <span class="No-Break">in availability</span></li>
			</ul>
			<p>Moreover, a completely reliable application leaves no room for the introduction of new features, as any new addition has the potential to disrupt the existing service. Therefore, some margin for error must be built into <span class="No-Break">your SLO.</span></p>
			<p>SLOs <a id="_idIndexMarker1565"/>represent internal objectives that require consensus among the team and internal stakeholders, including developers, product managers, SREs, and CTOs. They necessitate the commitment of the entire organization. Not meeting an SLO does not carry explicit or <span class="No-Break">implicit penalties.</span></p>
			<p>For example, a customer cannot claim damages if an SLO is not met, but it may lead to dissatisfaction within organizational leadership. This does not imply that failing to meet an SLO should be consequence-free. Falling short of an SLO typically results in fewer changes and reduced feature development, potentially indicating a decline in quality and increased emphasis on the development and <span class="No-Break">testing functions.</span></p>
			<p>SLOs should be realistic, with the team actively working to meet them. They should align with the customer experience, ensuring that if the service complies with the SLO, customers do not perceive any service quality issues. If performance falls below the defined SLOs, it may affect the customer experience, but not to the extent that customers raise <span class="No-Break">support tickets.</span></p>
			<p>Some organizations <a id="_idIndexMarker1566"/>implement two types of SLOs: <strong class="bold">achievable</strong> and <strong class="bold">aspirational</strong>. The achievable SLO represents a target the entire team should reach, while the aspirational SLO sets a higher goal and is part of an ongoing <span class="No-Break">improvement process.</span></p>
			<h2 id="_idParaDest-384"><a id="_idTextAnchor1821"/>SLAs</h2>
			<p>According<a id="_idIndexMarker1567"/> to Google, SLAs are “<em class="italic">formal or implicit agreements with your users that outline the repercussions of meeting (or failing to meet) the </em><span class="No-Break"><em class="italic">contained SLOs.</em></span><span class="No-Break">”</span></p>
			<p>These agreements are of a more structured nature and represent business-level commitments made to customers, specifying the actions that will be taken if the organization fails to fulfill the SLA. SLAs can be either explicit or implicit. An explicit SLA entails well-defined consequences, often in terms of service credits, in case the expected reliability is not achieved. Implicit SLAs are evaluated in terms of potential damage to the organization’s reputation and the likelihood of customers switching <span class="No-Break">to alternatives.</span></p>
			<p>SLAs are<a id="_idIndexMarker1568"/> typically established at a level that is sufficient to prevent customers from seeking alternatives, and consequently, they tend to have lower thresholds compared to SLOs. For instance, when considering the request latency SLI, the SLO might be defined at a <em class="italic">300ms</em> SLI value, while the SLA could be set at a <em class="italic">500ms</em> SLI value. This distinction arises from the fact that SLOs are internal targets related to reliability, whereas SLAs are external commitments. By striving to meet the SLO, the team automatically satisfies the SLA, providing an added layer of protection for the organization in case of <span class="No-Break">unexpected failures.</span></p>
			<p>To understand the correlation between SLIs, SLOs, and SLAs, let’s look at the <span class="No-Break">following figure:</span></p>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/B19877_14_2.jpg" alt="Figure 14.2 – SLIs, SLOs, and SLAs" width="1224" height="628"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1<a id="_idTextAnchor1822"/><a id="_idTextAnchor1823"/>4.2 – SLIs, SLOs, and SLAs</p>
			<p>This figure shows how customer experience changes with the level of latency. If we keep the latency SLO at <em class="italic">300ms</em> and meet it, everything is good! Anything between <em class="italic">300ms</em> to <em class="italic">500ms</em> and the customer starts experiencing some degradation in performance, but that is not enough for them to lose their cool and start raising support tickets. Therefore, keeping the <a id="_idIndexMarker1569"/>SLA at <em class="italic">500ms</em> is a good strategy. As soon we cross the <em class="italic">500ms</em> threshold, unhappiness sinks in, and the customer starts raising support tickets for service slowness. If things cross the <em class="italic">10s</em> mark, then it is a cause of worry for your Ops team, and <em class="italic">Everything is burning</em> at this stage. However, as we know, the wording of SLOs is slightly different from what we imagine here. When we say that we have an SLO for <em class="italic">300ms</em> latency, it does not mean anything. A realistic SLO for an SLI mandating <em class="italic">request latency to remain below 300ms within the last 15 minutes with a 95th percentile measurement</em> would be to meet <em class="italic">the SLI x% of the time</em>. What should that x be? Should it be <em class="italic">99%</em>, or should it be <em class="italic">95%</em>? How do we decide this number? Well, for that, we’ll have to look<a id="_idIndexMarker1570"/> at <span class="No-Break"><strong class="bold">error budgets</strong></span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-385"><a id="_idTextAnchor1824"/>Error budgets</h1>
			<p>As defined by <a id="_idIndexMarker1571"/>Liz Fong-Jones and Seth Vargo, error budgets represent “<em class="italic">a quantitative measure shared between product and SRE teams to balance innovation </em><span class="No-Break"><em class="italic">and stability.</em></span><span class="No-Break">”</span></p>
			<p>In simpler terms, an error budget quantifies the level of risk that can be taken to introduce new features, conduct service maintenance, perform routine enhancements, manage network and infrastructure disruptions, and respond to unforeseen situations. Typically, the monitoring system measures the uptime of your service, while SLOs establish the target you aim to achieve. The error budget is the difference between these two metrics and represents the time available to deploy new releases, provided it falls within the error <span class="No-Break">budget limits.</span></p>
			<p>This is precisely why a <em class="italic">100%</em> SLO is not usually set initially. Error budgets serve the crucial purpose of helping teams strike a balance between innovation and reliability. The rationale behind error budgets lies in the SRE perspective that failures are a natural and expected part of system operations. Consequently, whenever a new change is introduced into production, there is an inherent risk of disrupting the service. Therefore, a higher error budget allows for introducing <span class="No-Break">more features:</span></p>
			<pre class="source-code">
Error Budget = 100% — SLO</pre>			<p>For instance, if your SLO is <em class="italic">99%</em>, your error budget would be <em class="italic">1%</em>. If you calculate this over a month, assuming <em class="italic">30 days/month</em> and <em class="italic">24 hours/day</em>, you will have a <em class="italic">7.2-hour</em> error budget to allocate for maintenance or other activities. For a <em class="italic">99.9%</em> SLO, the error budget would be <em class="italic">43.2 minutes</em> per month, and for a <em class="italic">99.99%</em> SLO, it would be <em class="italic">4.32 minutes</em> monthly. You can<a id="_idIndexMarker1572"/> refer to the following figure for <span class="No-Break">more details:</span></p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/B19877_14_3.jpg" alt="Figure 14.3 – Error budgets versus SLOs" width="1520" height="973"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.3 – Error budgets versus SLOs</p>
			<p>These periods represent actual downtime, but if your services have redundancy, high availability measures, and disaster recovery plans in place, you can potentially extend these durations because the service remains operational while you patch or address issues with <span class="No-Break">one server.</span></p>
			<p>Now, whether you want to keep on adding <em class="italic">9s</em> within your SLO or aim for a lower number would depend on your end users, business criticality, and availability requirements. A higher SLO is more costly and requires more resources than a lower SLO. However, sometimes, just architecting your application correctly can help you get to a better <span class="No-Break">SLO target.</span></p>
			<p>Now that we understand SLOs, SLIs, SLAs, and error budgets, let’s talk about <span class="No-Break">disaster recovery.</span></p>
			<h1 id="_idParaDest-386"><a id="_idTextAnchor1825"/>Disaster recovery, RTO, and RPO</h1>
			<p><strong class="bold">Disaster recovery</strong> is a <a id="_idIndexMarker1573"/>comprehensive strategy that’s designed to ensure an organization’s resilience in the face of unexpected, disruptive events, such as natural disasters, cyberattacks, or system failures. It involves the planning, policies, procedures, and technologies necessary to quickly and effectively restore critical IT systems, data, and operations to a functional state. A well-implemented disaster recovery plan enables businesses to minimize downtime, data loss, and financial impact, helping them maintain business continuity, protect their reputation, and swiftly recover from adversities, ultimately safeguarding their <span class="No-Break">long-term success.</span></p>
			<p>Every organization incorporates disaster recovery to varying degrees. Some opt for periodic backups or snapshots, while others invest in creating failover replicas of their production environment. Although failover replicas offer increased resilience, they come at the expense of doubling infrastructure costs. The choice of disaster recovery mechanism that an <a id="_idIndexMarker1574"/>organization adopts hinges on two crucial <a id="_idIndexMarker1575"/>KPIs – <strong class="bold">Recovery Time Objective</strong> (<strong class="bold">RTO)</strong> and <strong class="bold">Recovery Point </strong><span class="No-Break"><strong class="bold">Objective</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">RPO)</strong></span><span class="No-Break">.</span></p>
			<p>RTO and RPO are crucial metrics in disaster recovery and business continuity planning. RTO represents the maximum acceptable downtime for a system or application, specifying the time within which it should be restored after a disruption. It quantifies the acceptable duration of service unavailability and drives the urgency of <span class="No-Break">recovery efforts.</span></p>
			<p>On the other hand, RPO defines the maximum tolerable data loss in the event of a disaster. It signifies the point in time to which data must be recovered to ensure business continuity. Achieving a low RPO means that data loss is minimized, often by frequent data backups and replication. The following figure explains RTO and <span class="No-Break">RPO beautifully:</span></p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/B19877_14_4.jpg" alt="Figure 14.4 – RTO and RPO" width="1614" height="624"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.4 – RTO and RPO</p>
			<p>A shorter <a id="_idIndexMarker1576"/>RTO and<a id="_idIndexMarker1577"/> RPO demand a more robust disaster recovery plan, which, in turn, results in higher costs for both infrastructure and human resources. Therefore, balancing RTO and RPO is essential to ensure a resilient IT infrastructure. Organizations must align their recovery strategies with these objectives to minimize downtime and data loss, thereby safeguarding business operations and data integrity during <span class="No-Break">unforeseen disruptions.</span></p>
			<h1 id="_idParaDest-387"><a id="_idTextAnchor1826"/>Running distributed applications in production</h1>
			<p>So far, we’ve been discussing KPIs for running an application in production, taking inspiration from SRE principles. Now, let’s understand how we will put these thoughts in a single place to run a distributed application <span class="No-Break">in production.</span></p>
			<p>A <strong class="bold">distributed application</strong> or a <strong class="bold">microservice</strong> is <a id="_idIndexMarker1578"/>inherently different from a monolith. While managing a monolith<a id="_idIndexMarker1579"/> revolves around ensuring all operational aspects of one application, the complexity increases manyfold with microservices. Therefore, we should take a different approach <span class="No-Break">to it.</span></p>
			<p>From the perspective of SRE, running a distributed application in production entails focusing on ensuring the application’s <em class="italic">reliability</em>, <em class="italic">scalability</em>, and <em class="italic">performance</em>. Here’s how SREs approach <span class="No-Break">this task:</span></p>
			<ul>
				<li><strong class="bold">SLOs</strong>: SREs <a id="_idIndexMarker1580"/>begin by defining clear SLOs that outline <a id="_idIndexMarker1581"/>the desired level of reliability for the distributed application. SLOs specify the acceptable levels of <em class="italic">latency</em>, <em class="italic">error rates</em>, and <em class="italic">availability</em>. These SLOs are crucial in guiding the team’s efforts and in determining whether the system is meeting its <span class="No-Break">reliability goals.</span></li>
				<li><strong class="bold">SLIs</strong>: SREs<a id="_idIndexMarker1582"/> establish SLIs, which are quantifiable metrics that are used to measure the reliability of the application. These metrics <a id="_idIndexMarker1583"/>could include response times, error rates, and other performance indicators. SLIs provide a concrete way to assess whether the application meets <span class="No-Break">its SLOs.</span></li>
				<li><strong class="bold">Error budgets</strong>: Error<a id="_idIndexMarker1584"/> budgets are a key concept in SRE. They represent the permissible amount of downtime or errors that can occur before the SLOs <a id="_idIndexMarker1585"/>are violated. SREs use error budgets to strike a balance between reliability and innovation. If the error budget is exhausted, it may necessitate a focus on stability and reliability over introducing <span class="No-Break">new features.</span></li>
				<li><strong class="bold">Monitoring and alerting</strong>: SREs implement robust monitoring and alerting systems to continuously track the application’s performance and health. They set up alerts <a id="_idIndexMarker1586"/>based on SLIs and SLOs, enabling them to respond proactively to incidents or deviations from desired performance levels. In the realm of distributed applications, using a service mesh <a id="_idIndexMarker1587"/>such as <strong class="bold">Istio</strong> or <strong class="bold">Linkerd</strong> can help. They <a id="_idIndexMarker1588"/>help you visualize parts of your application through a single pane of glass and allow you to monitor your application and alert on it <span class="No-Break">with ease.</span></li>
				<li><strong class="bold">Capacity planning</strong>: SREs <a id="_idIndexMarker1589"/>ensure that the infrastructure supporting the distributed application can handle the expected load and traffic. They perform capacity planning<a id="_idIndexMarker1590"/> exercises to scale resources as needed, preventing performance bottlenecks during traffic spikes. With modern public cloud platforms, automating<a id="_idIndexMarker1591"/> scalability with traffic is all the more easy to implement, especially with <span class="No-Break">distributed applications.</span></li>
				<li><strong class="bold">Automated remediation</strong>: Automation<a id="_idIndexMarker1592"/> is a cornerstone of SRE practices. SREs <a id="_idIndexMarker1593"/>develop automated systems for incident response and remediation. This includes <em class="italic">auto-scaling</em>, <em class="italic">self-healing mechanisms</em>, and <em class="italic">automated rollback procedures</em> to <span class="No-Break">minimize downtime.</span></li>
				<li><strong class="bold">Chaos engineering</strong>: SREs often employ chaos engineering<a id="_idIndexMarker1594"/> practices to introduce controlled failures into the system deliberately. This helps identify weaknesses and vulnerabilities in the distributed application, allowing for <a id="_idIndexMarker1595"/>proactive mitigation of potential issues. Some of the most popular chaos engineering tools are Chaos Monkey, Gremlin, Chaos Toolkit, Chaos Blade, Pumba, ToxiProxy, and <span class="No-Break">Chaos Mesh.</span></li>
				<li><strong class="bold">On-call and incident management</strong>: SREs maintain on-call rotations<a id="_idIndexMarker1596"/> to ensure 24/7 coverage. They follow well-defined<a id="_idIndexMarker1597"/> incident <a id="_idIndexMarker1598"/>management processes to resolve issues quickly and learn from incidents to prevent recurrence. Most SRE development backlogs come from this process as they learn from failures and, therefore, automate <span class="No-Break">repeatable tasks.</span></li>
				<li><strong class="bold">Continuous improvement</strong>: SRE is a <a id="_idIndexMarker1599"/>culture of continuous<a id="_idIndexMarker1600"/> improvement. SRE teams regularly conduct <strong class="bold">post-incident reviews</strong> (<strong class="bold">PIRs</strong>) and <strong class="bold">root cause analyses</strong> (<strong class="bold">RCAs</strong>) to <a id="_idIndexMarker1601"/>identify areas for enhancement. Lessons learned from incidents are used to refine SLOs and improve the overall reliability of <span class="No-Break">the application.</span></li>
				<li><strong class="bold">Documentation and knowledge sharing</strong>: SREs document <em class="italic">best practices</em>, <em class="italic">runbooks</em>, and <em class="italic">operational procedures</em>. They emphasize knowledge sharing across teams<a id="_idIndexMarker1602"/> to ensure that expertise is not siloed and that all team members can effectively manage and troubleshoot the distributed application. They also aim to automate the runbooks to ensure that manual processes are kept at <span class="No-Break">a minimum.</span></li>
			</ul>
			<p>In summary, SRE’s approach to running a distributed application in production focuses on <em class="italic">reliability</em>, <em class="italic">automation</em>, and <em class="italic">continuous improvement</em>. It sets clear goals, establishes metrics for measurement, and employs proactive monitoring and incident management practices to deliver a highly available and performant service to <span class="No-Break">end users.</span></p>
			<h1 id="_idParaDest-388"><a id="_idTextAnchor1827"/>Summary</h1>
			<p>This chapter covered SRE and the KPIs for running our service in production. We started by understanding software reliability and examined how to manage an application in production using SRE. We discussed the three crucial parameters that guide SREs: SLI, SLO, and SLA. We also explored error budgets and their importance in introducing changes within the system. Then, we looked at software disaster recovery, RPO, and RTO and how they define how complex or costly our disaster recovery measures will be. Finally, we looked at how DevOps or SRE will use these concepts to manage a distributed application <span class="No-Break">in production.</span></p>
			<p>In the next chapter, we will put what we’ve learned to practical use and explore how to ma<a id="_idTextAnchor1828"/><a id="_idTextAnchor1829"/>nage all these aspects using a service mesh <span class="No-Break">called Istio.</span></p>
			<h1 id="_idParaDest-389"><a id="_idTextAnchor1830"/>Questions</h1>
			<p>Answer the following questions to test your knowledge of <span class="No-Break">this chapter:</span></p>
			<ol>
				<li>Which of the following is a good example of <span class="No-Break">an SLI?</span><p class="list-inset">A. The response time should not <span class="No-Break">exceed 300ms.</span></p><p class="list-inset">B. The 95th percentile of response time in a 15-minute window should not <span class="No-Break">exceed 300ms.</span></p><p class="list-inset">C. 99% of all requests should respond <span class="No-Break">within 300ms.</span></p><p class="list-inset">D. The number of failures should not <span class="No-Break">exceed 1%.</span></p></li>
				<li>A mature organization should have a 100% <span class="No-Break">SLO</span><span class="No-Break">. (True/False)</span></li>
				<li>SLOs are not tied to any customer-initiated punitive <span class="No-Break">action</span><span class="No-Break">. (True/False)</span></li>
				<li>Which of the following should you consider while deciding on an SLO? (<span class="No-Break">Choose three)</span><p class="list-inset">A. <span class="No-Break">User satisfaction</span></p><p class="list-inset"><span class="No-Break">B. Alternatives</span></p><p class="list-inset">C. <span class="No-Break">User behavior</span></p><p class="list-inset">D. <span class="No-Break">System capacity</span></p></li>
				<li>SLAs are generally kept to a stricter SLI value than <span class="No-Break">SLOs</span><span class="No-Break">. (True/False)</span></li>
				<li>Which of the following should you consider while <span class="No-Break">defining SLIs?</span><p class="list-inset">A. CPU, memory, and <span class="No-Break">disk utilization</span></p><p class="list-inset">B. Latency, errors, traffic, <span class="No-Break">and saturation</span></p><p class="list-inset">C. Utilization, capacity, <span class="No-Break">and scale</span></p></li>
				<li>An error budget of 1% provides how much scope for downtime <span class="No-Break">per month?</span><p class="list-inset">A. <span class="No-Break">72 hours</span></p><p class="list-inset">B. <span class="No-Break">43.2 minutes</span></p><p class="list-inset">C. <span class="No-Break">7.2 hours</span></p><p class="list-inset">D. <span class="No-Break">4.32 minutes</span></p></li>
				<li>An SRE is a software developer doing <span class="No-Break">Ops</span><span class="No-Break">. (True/False)</span></li>
				<li>What minimum percent of the time should an SRE <a id="_idTextAnchor1831"/><a id="_idTextAnchor1832"/>allocate to <span class="No-Break">development work?</span><p class="list-inset"><span class="No-Break">A. 30%</span></p><p class="list-inset"><span class="No-Break">B. 40%</span></p><p class="list-inset"><span class="No-Break">C. 50%</span></p><p class="list-inset"><span class="No-Break">D. 60%</span></p></li>
			</ol>
			<h1 id="_idParaDest-390"><a id="_idTextAnchor1833"/>Answers</h1>
			<p>Here are the answers to this <span class="No-Break">chapter’s questions:</span></p>
			<ol>
				<li>B</li>
				<li><span class="No-Break">False</span></li>
				<li><span class="No-Break">True</span></li>
				<li>A, <span class="No-Break">B, C</span></li>
				<li><span class="No-Break">False</span></li>
				<li>B</li>
				<li>C</li>
				<li><span class="No-Break">True</span></li>
				<li>C</li>
			</ol>
		</div>
	</div>
</div>
</body></html>