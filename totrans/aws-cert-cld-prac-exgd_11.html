<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer219">
			<h1 id="_idParaDest-220"><a id="_idTextAnchor223"/><a href="B17124_09_Final_SK_ePub.xhtml#_idTextAnchor223"><em class="italic">Chapter 9</em></a>: High Availability and Elasticity on AWS</h1>
			<p>Most applications follow a design pattern that comprises several layers—such as the network layer, the compute layer, and the storage and database layers. We call this a multi-tier application. So, for example, you can have a three-tier application stack comprising a web services layer that offers frontend web interface access, an application layer where perhaps all data processing happens, and a backend database layer to store and manage data.</p>
			<p>In this chapter, we start to bring together the various core <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>) services we have learned about so far to design and architect a complete <strong class="bold">end-to-end</strong> (<strong class="bold">E2E</strong>) solution. Furthermore, in previous chapters, we have only deployed single resource instances of various AWS services—for example, a single <strong class="bold">Elastic Compute Cloud</strong> (<strong class="bold">EC2</strong>) instance to offer compute capability, or as in the previous chapter, where we deployed a single Amazon <strong class="bold">Relational Database Service</strong> (<strong class="bold">RDS</strong>) database instance in the public subnet of our <strong class="bold">virtual private cloud</strong> (<strong class="bold">VPC</strong>).</p>
			<p>In real-world scenarios, you generally need to incorporate components that will help you achieve <strong class="bold">high availability</strong> (<strong class="bold">HA</strong>) and <strong class="bold">scalability</strong>. We can increase our application's availability by having more than one EC2 instance serving the same application or website. This way, if one of the EC2 instances fails, users can continue to access the services offered by being directed to other healthy EC2 instances in the fleet, and away from those that are unhealthy or in a failed state. We can also ensure that we place our EC2 instances across multiple <strong class="bold">Availability Zones</strong> (<strong class="bold">AZs</strong>), ensuring that if one AZ fails or just simply goes offline, users can be redirected to healthy EC2 instances in another AZ.</p>
			<p>Similarly, we need to offer solutions that are scalable. AWS offers services that are capable of automatically scaling out when required; for example, when we notice an increase in traffic, we can add more EC2 instances to cope with the load. On the flip side, the same AWS service can automatically scale in when demand drops, allowing us to save on the unnecessary expense of running underutilized servers in the cloud.</p>
			<p>Finally, we also need to consider the global availability of our services. Many companies have global customers, and while many AWS services are designed for regional availability and scalability options, other AWS services can help us deliver global availability and even offer resilience against regional outages.</p>
			<p>In this chapter, we discuss the following key concepts:</p>
			<ul>
				<li>Introduction to vertical and horizontal scaling concepts</li>
				<li>Overview of the <strong class="bold">Open Systems Interconnection</strong> (<strong class="bold">OSI</strong>) model</li>
				<li>Distributing web traffic with Amazon <strong class="bold">Elastic Load Balancing</strong> (<strong class="bold">ELB</strong>) </li>
				<li>Implementing elasticity with AWS Auto Scaling</li>
				<li>Designing multi-Region HA solutions</li>
			</ul>
			<h1 id="_idParaDest-221"><a id="_idTextAnchor224"/>Technical requirements</h1>
			<p>To complete this chapter and the exercises within, you need to have access to your AWS account and be logged in as <strong class="bold">Alice</strong>, our <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) user (administrator) that we created in <a href="B17124_04_Final_SK_ePub.xhtml#_idTextAnchor068"><em class="italic">Chapter 4</em></a>, <em class="italic">Identity and Access Management</em>. </p>
			<h1 id="_idParaDest-222"><a id="_idTextAnchor225"/>Introduction to vertical and horizontal scaling concepts</h1>
			<p>When you deploy a given EC2 instance in your VPC, you need to choose an instance type and one or more <a id="_idIndexMarker949"/>associated <strong class="bold">Elastic Block Store</strong> (<strong class="bold">EBS</strong>) (or instance store) volumes of specific sizes. Your EC2 instance will always need one root volume and one or more data volumes based on your application requirements.</p>
			<p>However, from time to time, you may need to upgrade your original configuration— perhaps you need more memory or <a id="_idIndexMarker950"/>more <strong class="bold">central processing units</strong> (<strong class="bold">CPUs</strong>) to cope with the load on your server. You may be running out of storage space and therefore need to increase the amount of storage on your EBS volumes. When<a id="_idIndexMarker951"/> upgrading to an instance of a higher specification, we call this <strong class="bold">vertical scaling</strong>. To perform most upgrades this way, you generally need to stop processing application requests, and most of the time, you may first need to shut the EC2 instance down. </p>
			<p>The actual<a id="_idIndexMarker952"/> upgrade can take anything from a few minutes to a few hours, depending on what you are upgrading. For example, upgrading the instance type usually involves shutting down the server, modifying the instance type, and restarting it again, as shown <a id="_idIndexMarker953"/> in the following screenshot:</p>
			<div>
				<div id="_idContainer186" class="IMG---Figure">
					<img src="Images/B17124_09_01.jpg" alt="Figure 9.1 – Changing EC2 instance type: vertical scaling&#13;&#10;" width="1016" height="625"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – Changing EC2 instance type: vertical scaling</p>
			<p>In the preceding screenshot, you will select the drop-down arrow next to <strong class="bold">Instance type</strong> to select a higher-specification EC2 instance type. Once selected, simply click on <strong class="bold">Apply</strong> and start up the EC2 instance again. The EC2 instance is started with the upgraded specifications.</p>
			<p>Similarly, you can also upgrade the storage volume attached to the server or attach additional volumes. You can modify existing volumes to increase the storage size or change the type of storage from <strong class="bold">General Purpose SSD (gp2)</strong> to <strong class="bold">Provisioned IOPS (io1)</strong>. When upgrading your storage, AWS needs to perform some optimization tasks, and this will take some time, depending on the size of the volume.</p>
			<p><strong class="bold">Vertical scaling</strong> does <a id="_idIndexMarker954"/>have its limitations, however. It cannot offer HA; so, if there is a problem with the EC2 instance and it fails, you will need to provision a new EC2 instance as a replacement. </p>
			<p>Rather than having a single EC2 instance host your application, you could consider hosting multiple EC2 instances with the same application offering. This way, if one EC2 instance fails, customers can be redirected to another EC2 instance that is in a healthy state. </p>
			<p>Often, you need more than one EC2 instance participating as a fleet to cope with demand and offer HA in case of failures of any instance. AWS offers a service called Auto Scaling (which we look at in detail later in this chapter), which can automatically launch (or terminate) an EC2 instance to cope with load based on performance parameters such as average CPU utilization across a fleet of servers. This ability to then launch additional EC2 instances serving the <a id="_idIndexMarker955"/>same application is known as <strong class="bold">horizontal scaling</strong>.</p>
			<p>With horizontal scaling, you can add more EC2 instances to your fleet when demand increases and terminate unnecessary instances when demand drops.</p>
			<p>This requires careful architectural design, as the application needs to be aware that it is being run from multiple EC2 instances. For example, if you have two copies of your WordPress blog running from two EC2 instances, content data is usually stored on the local storage attached to a single EC2 instance—in this case, the EBS volumes. If you remember from <a href="B17124_07_Final_SK_ePub.xhtml#_idTextAnchor157"><em class="italic">Chapter 7</em></a>, <em class="italic">AWS Compute Services – EC2, Lightsail, </em>EBS volumes can only be attached to one EC2 instance at a time. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Amazon EBS Multi-Attach<a id="_idIndexMarker956"/> is a new feature that enables you to attach a single Provisioned IOPS SSD (io1 or io2) volume to multiple instances located in the same AZ. However, it has several limitations and<a id="_idIndexMarker957"/> does not necessarily replace the use case for <strong class="bold">Elastic File System</strong> (<strong class="bold">EFS</strong>) volumes. For additional information, refer to <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html</a>.</p>
			<p>This means that any blog articles you write will be stored on one EC2 instance, and the other EC2 instance running WordPress will not be aware of this content, resulting in inconsistencies between the two servers. One way to handle this is to offer some ability to share data between the EC2 instances, such as having the application data hosted on an EFS volume that can act as a file share for multiple EC2 instances. The WordPress application will also<a id="_idIndexMarker958"/> have to be configured to store all blog content and related media on this central EFS volume instead, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="Images/B17124_09_02.jpg" alt="Figure 9.2 – Enabling horizontal scaling at the application layer&#13;&#10;" width="1168" height="729"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – Enabling horizontal scaling at the application layer</p>
			<p>In this section, we compared vertical scaling and horizontal scaling scalability options on AWS. Vertical scaling refers to an in-place upgrade to add more CPUs, memory, or storage. Vertical scaling does not offer any HA because if the EC2 instance fails, you cannot fail over to another instance. Horizontal scaling is designed to add more nodes (compute or otherwise) to your fleet and can help reduce the overall load on an individual instance. With horizontal scaling, you can also offer HA so that if one node fails, traffic can be redirected to other healthy nodes.</p>
			<p>Next, we introduce the <strong class="bold">OSI model</strong>, which is a reference model for how applications communicate over a network and how network traffic flows across from the physical layer of network cabling and Wi-Fi through to the application itself. Having a broad understanding of this reference model will help you appreciate and assist with troubleshooting communication issues between your applications across networks.</p>
			<h1 id="_idParaDest-223"><a id="_idTextAnchor226"/>Overview of the OSI model</h1>
			<p>The flow and<a id="_idIndexMarker959"/> distribution of network traffic across various devices and applications are defined by a concept known as the <strong class="bold">OSI model</strong>. Published in 1984, this model provides a visual description of how network traffic flows over a particular network system</p>
			<p>There are <strong class="bold">seven</strong> layers to the OSI model that flow from top to bottom, with layer 7 being at the top and layer 1 at the bottom. The OSI model is used as a reference point for various vendors to express which layer of network communication the product they are offering works on. This is because different hardware and software products operate at different layers. </p>
			<p>The OSI model also assists in identifying network problems. When analyzing the source of any network issue, identifying whether only a single user is affected or whether a network segment or the entire network is down can help identify potential equipment or mediums that are experiencing the fault. </p>
			<p>The following diagram illustrates the seven layers of the OSI model:</p>
			<div>
				<div id="_idContainer188" class="IMG---Figure">
					<img src="Images/B17124_09_03.jpg" alt="Figure 9.3 – OSI model&#13;&#10;" width="910" height="583"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3 – OSI model</p>
			<p>In the next section, we look at ELBs, which form a crucial part of the overall design architecture for HA and horizontal scalability.</p>
			<h1 id="_idParaDest-224"><a id="_idTextAnchor227"/>Distributing web traffic with Amazon ELB </h1>
			<p>When you<a id="_idIndexMarker960"/> have more than one EC2 instance that works <a id="_idIndexMarker961"/>as part of a fleet hosting a given application, you need a mechanism in place to distribute traffic to those instances in a manner that spreads the load across the fleet. At a very basic level, this is what Amazon ELBs are designed to do. Amazon ELBs distribute traffic across multiple targets, which can be <a id="_idIndexMarker962"/>EC2 instances, containers, <strong class="bold">Internet Protocol</strong> (<strong class="bold">IP</strong>) addresses, and even Lambda functions. They can handle varying traffic for your application, evenly distributing the load across those registered targets either in a single AZ or across multiple AZs within a given Region. This also means that ELBs can assist in designing architecture that offers HA and fault tolerance, as well as working with services such as Auto Scaling to deliver automatic scalability features to your applications. Note, however, that ELBs are regional-based only, so you cannot use an ELB to distribute traffic across Regions. </p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor228"/>Load balancers and VPCs</h2>
			<p>Amazon load balancers are<a id="_idIndexMarker963"/> designed to work with your VPC. AWS recommends you enable more than one<a id="_idIndexMarker964"/> AZ for your load balancer. Amazon <strong class="bold">Elastic Load Balancing</strong> (<strong class="bold">ELB</strong>) then<a id="_idIndexMarker965"/> creates <strong class="bold">load balancer nodes</strong> in the AZs you specify. The load balancer then distributes traffic to its nodes across the AZs. Its nodes then connect to the targets in the relevant AZs. </p>
			<p>An important point to note here is that your internet-based clients need to only connect to the load balancer, which then distributes traffic to targets in your VPC. This means you no longer need to place any targets such as EC2 instances in the public subnet unless you have a specific reason to do so. You can place your web servers, for instance, in a private subnet, and because they are registered to the load balancer, traffic will be routed to them. Furthermore, this also means that your web servers can function with just private IP addresses, reducing the overall attack surface by not having any public IP addresses, as depicted in the following diagram:</p>
			<div>
				<div id="_idContainer189" class="IMG---Figure">
					<img src="Images/B17124_09_04.jpg" alt="Figure 9.4 – Amazon ELB VPC configuration&#13;&#10;" width="918" height="771"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4 – Amazon ELB VPC configuration</p>
			<p>As seen in the <a id="_idIndexMarker966"/>preceding diagram, the web servers will receive traffic from the <strong class="bold">Application Load Balancer</strong> (<strong class="bold">ALB</strong>) (we look at the types of load <a id="_idIndexMarker967"/>balancers next). The ALB will distribute this traffic using the default round-robin method based on the number of AZs enabled for the load balancer. </p>
			<p>In addition, when you create a load balancer, you need to specify whether you are creating an <em class="italic">internet-facing</em> load balancer or an <em class="italic">internal</em> load balancer. These are outlined in more detail here:</p>
			<ul>
				<li><strong class="bold">Internet-facing load balancer</strong>—This has a<a id="_idIndexMarker968"/> publicly resolvable <strong class="bold">Domain Name System</strong> (<strong class="bold">DNS</strong>) name, enabling it to route requests from internet-based clients. The DNS name<a id="_idIndexMarker969"/> resolves client requests to public IP addresses of the load balancer nodes for your load balancer. In the preceding diagram, we can see an example of an internet-facing load balancer that accepts traffic from clients on the internet.</li>
				<li><strong class="bold">Internal load balancer</strong>—The nodes of an<a id="_idIndexMarker970"/> internal load balancer only have private IP addresses, and its DNS name is resolvable to the private IPs of the nodes. This means that internal load balancers can only route requests from clients that already have access to the VPC. </li>
			</ul>
			<p>Internal load balancers are <a id="_idIndexMarker971"/>particularly useful when you are designing a multi-tier application stack—for example, when you have multiple web servers that receive traffic from your internet-based clients and then need to send on that traffic for processing to application or database servers distributed across multiple AZs in private subnets, via another load balancer. In this case, the web servers would be registered against the internet-facing load balancers and the application/database servers would be registered against the internal load balancers, as depicted in the following diagram:</p>
			<div>
				<div id="_idContainer190" class="IMG---Figure">
					<img src="Images/B17124_09_05.jpg" alt="Figure 9.5 – Internal versus internet-facing ELBs&#13;&#10;" width="1107" height="751"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5 – Internal versus internet-facing ELBs</p>
			<p>Another important <a id="_idIndexMarker972"/>point to note is that for traffic to be accepted by an ELB, you need to configure security groups, specifying inbound rules that the port, protocol, and source of the traffic for that load balancer can accept. In addition, the security groups associated with your targets must also be configured to allow inbound traffic from the load balancers. You can usually do this by specifying the source of the traffic as being the security group associated with the load balancer itself.</p>
			<p>Amazon offers four types of ELBs, as listed here:</p>
			<ul>
				<li>ALB</li>
				<li><strong class="bold">Network Load Balancer</strong> (<strong class="bold">NLB</strong>)</li>
				<li><strong class="bold">Gateway Load Balancer</strong> (<strong class="bold">GWLB</strong>)</li>
				<li><strong class="bold">Classic Load Balancer</strong> (<strong class="bold">CLB</strong>)</li>
			</ul>
			<p>Let's look at these individually in some detail next. </p>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor229"/>ALB</h2>
			<p>We will start with the <a id="_idIndexMarker973"/>Amazon <strong class="bold">ALB</strong>, which is the most common type of load balancer to use for most applications. ALBs are designed to act as a single entry point for clients to connect to your applications running on targets such as a fleet of EC2 instances. It is recommended that your EC2 instances are based across multiple AZs, increasing the overall availability of your application in the case of a single AZ failure.</p>
			<p>ALBs are designed<a id="_idIndexMarker974"/> to distribute traffic at the application layer (using <strong class="bold">HyperText Transfer Protocol</strong> (<strong class="bold">HTTP</strong>) and <strong class="bold">HTTP Secure</strong> (<strong class="bold">HTTPS</strong>)). The <a id="_idIndexMarker975"/>application aayer is also known as the <em class="italic">seventh layer of the OSI model</em>. ALBs are therefore ideal for ensuring an even distribution of traffic to your web applications across the internet. </p>
			<p>Some key benefits of <a id="_idIndexMarker976"/>ALBs include the following:</p>
			<ul>
				<li>Support for path-based routing, allowing you to forward requests based on the <strong class="bold">Uniform Resource Locator</strong> (<strong class="bold">URL</strong>) in the request</li>
				<li>Support for host-based routing, allowing you to forward requests based on the URL in the request</li>
				<li>Support for routing requests to multiple applications on a single EC2 instance</li>
				<li>Support for registering Lambda functions as targets, as well as containerized applications, and much more</li>
			</ul>
			<p>An ALB has a configuration component <a id="_idIndexMarker977"/>called a <strong class="bold">listener</strong>. This listener service allows you to define rules on how the load balancer will route requests from clients to registered targets. These rules consist of a priority, actions, and any conditions that once met will enable the action to be performed. Your listener should have at least one default rule, and you can have additional rules. These rules also define which protocol and port to use to connect to your targets. </p>
			<p>When configuring an ALB, you also need to configure one or<a id="_idIndexMarker978"/> more <strong class="bold">target groups</strong> that will contain one or<a id="_idIndexMarker979"/> more <strong class="bold">targets</strong>, such as EC2 instances. You can have multiple target groups with an ALB, and this feature enables you to define complex routing rules based on the different application components. So, for example, if you have a corporate website that also hosts an e-commerce section to it, you can split the traffic so that one target group contains targets that host just the corporate pages of your website (for example, <strong class="source-inline">www.mycompany.com</strong>) and another target group contains targets that host the actual e-commerce portion of your website (for example, <strong class="source-inline">shop.mycompany.com</strong>). Another <a id="_idIndexMarker980"/>feature of ALBs is that you can also register a target with multiple target groups.</p>
			<p>This approach allows you to better manage traffic to your website and use different targets to ensure better performance and management. As an example, if your e-commerce portion is having a major upgrade that will take that portion of the site down for a few hours, users can still visit the corporate portion of your website to access the latest information or services offered, as illustrated in the following diagram: </p>
			<div>
				<div id="_idContainer191" class="IMG---Figure">
					<img src="Images/B17124_09_06.jpg" alt="Figure 9.6 – AWS ALB with multiple listener rules&#13;&#10;" width="1025" height="388"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6 – AWS ALB with multiple listener rules</p>
			<p>In the preceding diagram, you will note that we have three target groups, and one target is part of two groups. <strong class="bold">Target Group 1</strong> could be our primary corporate web pages, whereas <strong class="bold">Target Group 3</strong> could be our e-commerce site. Each listener will also contain a default rule, and the listener on the right contains an additional rule that routes requests to another group.</p>
			<h3>Health checks</h3>
			<p>In addition to defining listener rules to route traffic to appropriate targets within target groups, ALBs also perform health checks<a id="_idIndexMarker981"/> against your targets to determine whether they are in a healthy state. If a target such as an EC2 instance is not responding within a predefined set of requests based on the health check settings, it is marked as <em class="italic">unhealthy</em> and the ALB stops sending traffic to it, redirecting traffic to only those targets that are in a <em class="italic">healthy</em> state.</p>
			<p>Using this approach, end users are always directed to EC2 instances (or any other targets) that are functioning and responding to the ALB, reducing their chances of experiencing outages.</p>
			<h3>Traffic routing</h3>
			<p>When it comes to routing <a id="_idIndexMarker982"/>traffic to individual targets in a target group, ALBs use <strong class="bold">round robin</strong> as the<a id="_idIndexMarker983"/> default method, but you can also configure routing based on<a id="_idIndexMarker984"/> the <strong class="bold">least outstanding requests</strong> (<strong class="bold">LOR</strong>) routing algorithm.</p>
			<p>Previously, we mentioned that you use the ALB to split traffic across two target groups in our example of an e-commerce store. The first target group will host EC2 instances that offer access to the corporate website (<strong class="source-inline">www.mycompany.com</strong>), and the second target group could host the e-commerce portion (<strong class="source-inline">shop.mycompany.com</strong>). This is also known as <strong class="bold">host-based routing</strong>, which <a id="_idIndexMarker985"/>allows you to define target groups and listener rules based on the hostname of your domain (<strong class="source-inline">www</strong> versus <strong class="source-inline">shop</strong>). In addition to host-based routing, ALBs can be used to route traffic for the following use cases:</p>
			<ul>
				<li><strong class="bold">Path-based conditions</strong>—This <a id="_idIndexMarker986"/>allows you to direct traffic based on different sections of your web application to different target groups. Traffic is then routed based on that path of the URL. For example, you can send one portion of traffic to <strong class="source-inline">mycompany.com/store</strong> for users looking to purchase products, and another portion to <strong class="source-inline">mycompany.com/blog</strong> for users looking to read articles about the latest market trends.</li>
				<li><strong class="bold">Host-header conditions</strong>—This <a id="_idIndexMarker987"/>allows you to route traffic based on fields in the request URL—for example, query patterns or by source IP address.</li>
				<li><strong class="bold">Multiple applications on a single EC2 instance</strong>—This allows you to register more than one application on an EC2 instance using different port numbers.</li>
				<li><strong class="bold">Support for registering targets by IP address</strong>—This allows you to also redirect traffic from your ALB to your on-premises servers using private IP addressing over <strong class="bold">virtual private network</strong> (<strong class="bold">VPN</strong>) tunnels or Direct Connect connections.</li>
				<li><strong class="bold">Support for registering Lambda functions as targets</strong>—This allows you to configure <a id="_idIndexMarker988"/>Lambda functions as targets. Any traffic forwarded will invoke the Lambda function, passing <a id="_idIndexMarker989"/>any content in <strong class="bold">JavaScript Object Notation</strong> (<strong class="bold">JSON</strong>) format.</li>
				<li><strong class="bold">Support for containerized applications</strong>—This includes <strong class="bold">Elastic Container Service</strong> (<strong class="bold">ECS</strong>), where you<a id="_idIndexMarker990"/> can schedule and register tasks with a target group.</li>
			</ul>
			<p>In this section, we discussed how traffic can be routed with ALBs and saw different use cases. Next, we take a look at some security features of ALBs. </p>
			<h2 id="_idParaDest-227"><a id="_idTextAnchor230"/>ALB and WAF</h2>
			<p>Amazon also offers<a id="_idIndexMarker991"/> several security tools that we will examine in detail in a later chapter. One such tool is called <a id="_idIndexMarker992"/>the <strong class="bold">Web Application Firewall</strong> (<strong class="bold">WAF</strong>), which helps protect against common web exploits such as SQL injections<a id="_idIndexMarker993"/> and <strong class="bold">cross-site scripting</strong> (<strong class="bold">XSS</strong>). Amazon ALBs offer WAF integration to help you protect your applications from such common web attacks.</p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor231"/>NLB</h2>
			<p><strong class="bold">NLBs</strong> are <a id="_idIndexMarker994"/>designed to operate at the fourth layer of the OSI model and be able to handle millions of<a id="_idIndexMarker995"/> requests per second. NLBs are designed for load balancing of both <strong class="bold">Transmission Control Protocol</strong> (<strong class="bold">TCP</strong>) and <strong class="bold">User Datagram Protocol</strong> (<strong class="bold">UDP</strong>) traffic and<a id="_idIndexMarker996"/> maintain ultra-low latencies. With NLBs, you can preserve the client's source IP, allowing your backend services to see the IP address of the client, which may be a requirement for the application to function.</p>
			<p>NLBs also offer support for static IP addresses and elastic IP addresses, the latter allowing you to configure one fixed IP per AZ. Again, this may be a requirement for your application, and hence you would need NLBs.</p>
			<p>NLBs do not inspect the application layer and so are unaware of content types, cookie data, or any custom header information. </p>
			<p>Some key benefits <a id="_idIndexMarker997"/>of NLBs include the following:</p>
			<ul>
				<li>Ability to handle volatile workloads and handle millions of requests per second.</li>
				<li>Support for static IP addresses for your load balancer and one elastic IP address per subnet.</li>
				<li>You can register targets by IP address—this allows you to register targets outside the VPC such as in your on-premises environment.</li>
				<li>Support for routing requests to multiple applications on a single instance using multiple ports.</li>
				<li>Support for containerized applications such as those running on Amazon ECS.</li>
			</ul>
			<p>In this section, we discussed NLBs and learned about their various use cases, particularly when you need to support millions of requests per second and operate at the fourth layer of the OSI model over TCP and UDP protocols. In the next section, we look at GWLBs, designed to enable you to distribute traffic across various software appliances offered on the AWS Marketplace.</p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor232"/>GWLB</h2>
			<p>Before allowing traffic <a id="_idIndexMarker998"/>to enter your VPC, you may wish to perform security inspections and analysis of that traffic to block any kind of suspicious activity. Often, you could deploy your own security tools on EC2 instances to inspect that traffic to deploy third-party tools <a id="_idIndexMarker999"/>procured from the AWS Marketplace such as firewalls, <strong class="bold">intrusion detection systems/intrusion prevention systems</strong> (<strong class="bold">IDSes/IPSes</strong>), and so on. Managing traffic being routed via these third-party tools is made easier with the help of Amazon <strong class="bold">GWLBs</strong>. </p>
			<p>Amazon GLWBs <a id="_idIndexMarker1000"/>can manage the availability of these third-party virtual appliances and act as a single entry and exit point for all traffic destined for these services. This enables you to scale the availability and load-balance traffic across a fleet of your virtual appliances. GWLB operates at the third layer of the OSI model (the network layer) and exchanges application traffic with your <a id="_idIndexMarker1001"/>virtual appliances using the <strong class="bold">Generic Network Virtualization Encapsulation</strong> (<strong class="bold">GENEVE</strong>) protocol on port <strong class="source-inline">6081</strong>. Traffic is sent in both directions to the appliance, allowing it to perform stateful traffic processing.</p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor233"/>CLB</h2>
			<p>Amazon <a id="_idIndexMarker1002"/>CLB is a previous-generation ELB designed to operate at both layer 4 and 7 of the OSI model but without the extended features offered by the ALB or the level of throughput you can expect from an NLB. CLBs enable you to distribute traffic across EC2 instances that are in a single AZ or across multiple AZs. They are ideal for testing and for non-production<a id="_idIndexMarker1003"/> environments, or if your existing application is running in the <strong class="bold">EC2-Classic network mode</strong>.</p>
			<p>In this section, we looked at Amazon ELBs, which enable you to centrally distribute incoming traffic across multiple targets such as a fleet of EC2 instances offering access to a web application. Amazon ELBs can also perform health checks against your targets and redirect traffic away from unhealthy targets to ones that are responding and in a healthy state. This reduces the chances of your end users experiencing any kind of outage by inadvertently being connected to an instance that is not healthy. </p>
			<p>Amazon ELBs ultimately help you design your architecture for HA and support scalability features in conjunction with Amazon Auto Scaling, which we will discuss in the next section.</p>
			<h1 id="_idParaDest-231"><a id="_idTextAnchor234"/>Implementing elasticity with Amazon Auto Scaling</h1>
			<p>One of the most <a id="_idIndexMarker1004"/>amazing services on AWS is the <a id="_idIndexMarker1005"/>ability to automatically scale your workloads when demand increases and then scale back in when demand drops. This service is offered as part of various core technologies—for example, computing services such as EC2 and database services such as DynamoDB. </p>
			<p>Automatic scaling in response to a particular condition such as an increase in demand (for example, when average CPU utilization across your fleet of EC2 instances goes above a threshold such as 70%) can help provision additional capacity when it is required most. However, you are not stuck with the new size of your fleet. You can configure Auto Scaling so that if demand drops below a specific threshold value, it will terminate EC2 instances and therefore <em class="italic">save on costs</em>. Let's look at Auto Scaling for EC2 instances in detail next.</p>
			<p>Auto Scaling is a<a id="_idIndexMarker1006"/> regional service, and you can <a id="_idIndexMarker1007"/>scale across AZs within a given Region, allowing you to launch EC2 instances across AZs for <strong class="bold">HA</strong> and <strong class="bold">resilience</strong>. </p>
			<p>In the following diagram, we can see that two additional EC2 instances were added to a fleet across AZs <strong class="bold">2A</strong> and <strong class="bold">2B</strong>. This was due to the average CPU utilization rising above 80%. Once the new instances are part of the fleet, the average CPU utilization should start to fall as the load on the application is spread across six instances now instead of the original four:</p>
			<div>
				<div id="_idContainer192" class="IMG---Figure">
					<img src="Images/B17124_09_07.jpg" alt="Figure 9.7 – Auto Scaling service example&#13;&#10;" width="1014" height="526"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – Auto Scaling service example</p>
			<p>Amazon Auto Scaling<a id="_idIndexMarker1008"/> helps you provision necessary <a id="_idIndexMarker1009"/>EC2 instances on-demand and terminate them when the demand for your resources drops to or below a certain threshold. With Amazon Auto Scaling, you do not need to carry out complex capacity planning exercises. Next, we look at some core components of the Amazon Auto Scaling service.</p>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor235"/>Auto Scaling groups</h2>
			<p>When you configure Auto Scaling, you define a collection called an Auto Scaling group. This Auto Scaling group<a id="_idIndexMarker1010"/> will monitor and manage your fleet of EC2 instances. As part of the configuration, you need to define the following:</p>
			<ul>
				<li><strong class="bold">Minimum number of EC2 instances</strong>—This is the minimum size of the group, and Auto Scaling will ensure that the number of EC2 instances in your fleet never drops below this level. If an instance fails, taking the total count below this value, then the Auto Scaling service will launch additional EC2 instances.</li>
				<li><strong class="bold">Desired number of EC2 instances</strong>—If you specify the desired capacity (usually because you know that at this value, your users have optimal experience), then the Auto Scaling service will always try to ensure that you have the number of EC2 instances equal to the desired capacity. Note that your desired number can be the same as the minimum number, which would mean that the Auto Scaling service ensures that you always have this minimum number of EC2 instances.</li>
				<li><strong class="bold">Maximum number of EC2 instances</strong>—This is the maximum size of the fleet. You need to specify the maximum size that you would want to scale out to. This also has the effect of ensuring that Auto Scaling does not deploy more than the maximum number of instances if, say, a bug in the application running on those instances causes unnecessary scale-outs.</li>
			</ul>
			<p>In the following diagram, we can see how Amazon Auto Scaling will provision your desired capacity of EC2 instances and can then scale out to the maximum number of EC2 instances as per your Auto Scaling group configuration:</p>
			<div>
				<div id="_idContainer193" class="IMG---Figure">
					<img src="Images/B17124_09_08.jpg" alt="Figure 9.8 – Auto Scaling groups&#13;&#10;" width="489" height="383"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.8 – Auto Scaling groups</p>
			<p>The Auto Scaling <a id="_idIndexMarker1011"/>service will launch and/or terminate EC2 instances as part of the group based on the parameters you define and then scale out or scale back in based on the scaling policies you set. You also define how health checks are made against the EC2 instances in the group—this can be either with the Auto Scaling service performing health checks itself or using the health check services of the ELB that the fleet of EC2 instances is registered to. Depending on the health check results, your scaling policies will be triggered accordingly. </p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor236"/>Configuration templates</h2>
			<p>To set up AWS Auto Scaling, you need to<a id="_idIndexMarker1012"/> configure either a <strong class="bold">Launch Template</strong> or a <strong class="bold">Launch Configuration</strong>. Configuration templates<a id="_idIndexMarker1013"/> enable you to define specifications <a id="_idIndexMarker1014"/>of the EC2 instances to launch within the group. So, for example, the template will define the <strong class="bold">Amazon Machine Image</strong> (<strong class="bold">AMI</strong>) <strong class="bold">identifier</strong> (<strong class="bold">ID</strong>), instance type, key pairs, security groups, and block device mappings. </p>
			<p>Within the configuration template, you can also define scripts to be run at launch time, known as <strong class="bold">bootstrapping</strong>, which <a id="_idIndexMarker1015"/>will allow you to automatically configure the EC2 instance to participate in the fleet of existing instances, where possible. For example, at the launch of an EC2 instance, you can configure it with Apache Web Services so that it can function as a web server. These scripts can be defined in the <strong class="bold">user data</strong> section of the template and can be written in <strong class="bold">Bash</strong> for Linux <strong class="bold">operating systems</strong> (<strong class="bold">OSes</strong>) or <strong class="bold">PowerShell</strong> for Windows OSes. They are described in more detail here:</p>
			<ul>
				<li><strong class="bold">Launch Configuration</strong>—A Launch Configuration<a id="_idIndexMarker1016"/> is a basic template where you specify information for instances such as AMI IDs, instance types, key pairs, and security groups. You can associate your Launch Configuration with multiple Auto Scaling groups, but you can only launch one specific Launch Configuration for an Auto Scaling group at a time. Furthermore, once you have defined the parameters of a Launch Configuration, you cannot change it and you will have to re-create it if you need to modify it in any way. <p>Launch Configurations are the original way of defining configuration templates for your Auto Scaling groups and while still available, are now no longer recommended by Amazon. Instead, you are advised to use Launch Templates, which offer more features and flexibility and we'll look at next. </p></li>
				<li><strong class="bold">Launch Templates</strong>—Similar to <a id="_idIndexMarker1017"/>Launch Configurations in that you define the specifications for your EC2 instances, Launch Templates also offer additional features, including the following:<ul><li>Configuring multiple versions of a template</li><li>Hosting a default template and templates with variations for different use cases</li><li>Launching both Spot and On-Demand Instances</li><li>Specifying multiple instance types and multiple Launch Templates</li></ul><p>Launch Templates<a id="_idIndexMarker1018"/> also enable you to use newer features of<a id="_idIndexMarker1019"/> EC2, such<a id="_idIndexMarker1020"/> as newer <strong class="bold">EBS volumes</strong> (<strong class="source-inline">gp3</strong> and <strong class="source-inline">io2</strong>), <strong class="bold">EBS volume tagging</strong>, <strong class="bold">elastic inference</strong>, and <strong class="bold">Dedicated Hosts</strong>. You <a id="_idIndexMarker1021"/>cannot use <a id="_idIndexMarker1022"/>Launch Configurations to set up Dedicated Hosts.</p></li>
			</ul>
			<p>Launch Templates are the preferred option when configuring your templates for EC2 instances to be launched as they provide a lot more flexibility. Next, we look at different scaling options to suit different use cases.</p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor237"/>Scaling options</h2>
			<p>The final configuration component of your Auto Scaling service<a id="_idIndexMarker1023"/> is to determine your scaling policy. Scaling refers to the automatic addition or termination of your compute capacity to meet the demands of your end users and the load on your application. Scaling actions are triggered by an event—for example, the average CPU utilization across your fleet of servers has gone above 80% for the last 20 minutes and users will start to experience poor performance. Based on this event, Auto Scaling can be configured to launch one or more EC2 instances to bring the average CPU utilization down to below 60%.</p>
			<p>Depending on your business use case, you have several scaling options to choose from, as outlined here:</p>
			<ul>
				<li><strong class="bold">Always maintain current instance levels</strong>—Your scaling options can be configured to maintain the number of EC2 instances in the fleet, which is where you do not scale in or out; instead, AWS Auto Scaling simply replaces any failed or unhealthy EC2 instances to maintain the fleet size.</li>
				<li><strong class="bold">Scale manually</strong>—You can change the minimum, maximum, and desired number of instances and Auto Scaling will make the necessary modifications to reflect your change.</li>
				<li><strong class="bold">Scale based on schedule</strong>—You can configure Auto Scaling to automatically launch new EC2 instances or terminate existing ones at predefined schedules, whereby you specify a date and time for the scaling action to take place. For example, a large payroll company could scale out the number of EC2 instances running a payroll application in the third week of the month when clients need to submit all their payroll data before a specific deadline. During other weeks, the payroll company can operate on a small number of instances and still offer the optimal client experience.</li>
				<li><strong class="bold">Dynamic scaling (scale on-demand)</strong>—This is ideal when you are not able to predict demand. Dynamic scaling<a id="_idIndexMarker1024"/> will be triggered when an event occurs, such as CPU<a id="_idIndexMarker1025"/> utilization rising above a predefined threshold value for a period of time. Likewise, if demand drops, you can then automatically scale back in. There are three different forms of dynamic scaling on offer, outlined as follows:<ul><li><strong class="bold">Target tracking scaling policy</strong>—Auto Scaling <a id="_idIndexMarker1026"/>will launch or terminate EC2 instances in the fleet based on a target value of a specific metric. So, for example, if you know that average CPU utilization of 45% is ideal for the end user experience and anything above this threshold affects performance, then you can set your target tracking scaling policy for CPU utilization to 45%. If demand on your application increases, causing this metric to rise, then additional EC2 instances are launched. Likewise, if demand drops, causing the metric to fall much below 45%, Auto Scaling can terminate EC2 instances. You can think of a target-tracking scaling policy as a home thermostat where you try to maintain an ideal room temperature at home.</li><li><strong class="bold">Step scaling</strong>—Here, an <a id="_idIndexMarker1027"/>increase or decrease in capacity is based on a series of <em class="italic">step adjustments</em>, where the size of the breach of threshold specified determines the amount of scaling action.</li><li><strong class="bold">Simple scaling</strong>—This is <a id="_idIndexMarker1028"/>where capacity is increased or decreased based on a single scaling metric.</li></ul></li>
				<li><strong class="bold">Predictive scaling</strong>—A more <a id="_idIndexMarker1029"/>advanced form of scaling that uses <strong class="bold">load forecasting</strong>, <strong class="bold">scheduled scaling actions</strong>, and <strong class="bold">maximum capacity behavior</strong>. Maximum<a id="_idIndexMarker1030"/> capacity behavior enables you to override the maximum number of instances in the fleet if the forecast capacity is higher than this maximum capacity value.</li>
			</ul>
			<p>In this section, we looked at the AWS Auto Scaling service, which enables you to automatically scale your compute resources (and other resources such as databases) across multiple AZs in a given Region. You can scale out as well as scale back in to cope with demand and ensure that you always have the right number of resources to offer the best end user experience. By automatically scaling back in when demand is low, you can also ensure effective cost management.</p>
			<p>In the next section, we move to examine how we can offer global HA and fault tolerance of the AWS resources that power your application.</p>
			<h1 id="_idParaDest-235"><a id="_idTextAnchor238"/>Designing multi-Region HA solutions</h1>
			<p>In <a href="B17124_06_Final_SK_ePub.xhtml#_idTextAnchor122"><em class="italic">Chapter 6</em></a>, <em class="italic">AWS Networking Services – VPCs, Route53, and CloudFront</em>, we looked at Amazon Route 53, which offers DNS and traffic routing policies to help design highly available and<a id="_idIndexMarker1031"/> resilient architectures incorporating configurations that increase performance and security best practices. We also looked at how Amazon CloudFront can help cache content locally closer to end users, which reduces latency and improves overall performance.</p>
			<p>While Amazon Auto Scaling and ELB services help you offer HA and scalable services within a given Region on their own, there is no provision for global availability of services. If you were to host your application in a single Region alone and if that Region were to fail, your end users would not be able to access your applications until the Region came back online and resources made available. </p>
			<p>Services such as Route 53 and CloudFront, however, enable you to extend your application's availability to be even more resilient on a global scale. In this section, we look at one such option to offer global availability if your primary Region experiences a major outage and where you perhaps have a global customer base. </p>
			<p>Specifically, Amazon Route 53 offers several routing policies, one of which is known as a failover routing policy, which helps you design an active/passive configuration for your application availability. </p>
			<p>In the following <a id="_idIndexMarker1032"/>diagram, we deploy two copies of the application across two AZs. We then configure Amazon Route 53 with a <strong class="bold">failover</strong> routing policy, where the<a id="_idIndexMarker1033"/> primary version of your site is based in the London Region and the secondary site is in the Sydney Region. The following diagram and associated key points highlight the proposed architecture:</p>
			<div>
				<div id="_idContainer194" class="IMG---Figure">
					<img src="Images/B17124_09_09.jpg" alt="Figure 9.9 – Route 53 configured with failover routing policy, enabling an active/passive solution for application architecture&#13;&#10;" width="1431" height="664"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.9 – Route 53 configured with failover routing policy, enabling an active/passive solution for application architecture</p>
			<p>In the <a id="_idIndexMarker1034"/>preceding diagram, we have a <strong class="bold">primary site based in the London Region</strong> with the following deployment:</p>
			<ul>
				<li>EC2 instances are deployed as part of an Auto Scaling group. In the London Region (our primary site), we have a desired/minimum capacity of four EC2 instances that can expand to a maximum of six EC2 instances to support demand as required. </li>
				<li>Traffic is distributed via the ALB to the EC2 instances across two AZs in the London Region. The source of that traffic is routed via Route 53 from end users on the internet.</li>
				<li>Route 53 performs health checks against the primary site. Health checks are run against each EC2 instance via the ALB, and if the primary site is reachable, Route 53 continues to direct traffic to the primary site only.</li>
				<li>If there is a regional outage or if the Auto Scaling group fails to maintain healthy EC2 instances behind the load balancer, Route 53 marks the site as unhealthy and performs a failover.</li>
				<li>During failover, Route 53 redirects all traffic to the secondary site in Sydney, shown in <em class="italic">Figure 9.9</em> as the dotted yellow traffic lines on the right.</li>
				<li>While<a id="_idIndexMarker1035"/> traffic is being redirected to the secondary site, which may start off with a minimum number of instances, Auto Scaling can scale out the number of nodes in the fleet to cope with demand up to the maximum specified number of instances. </li>
			</ul>
			<p>The preceding example and diagram illustrate how we can combine regional services such as Auto Scaling and ALBs along with global services such as Route 53 to design a highly available and resilient application architecture.</p>
			<p>In this section, we learned about designing application solutions that offer multi-regional HA options using both ELBs and Route 53 services specifically. </p>
			<p>In the next section, we examine a series of hands-on exercises that will help you configure the <a id="_idIndexMarker1036"/>various services you have learned about so far, incorporating IAM, <strong class="bold">Simple Storage Service</strong> (<strong class="bold">S3</strong>), VPCs, EC2, RDS, ELBs, and Auto Scaling, to build a two-tier application solution. This two-tier application solution will comprise a web/application tier and a backend database tier.</p>
			<p>To complete the upcoming exercises, it is vital that you have completed all previous exercises in all the previous chapters. Furthermore, to complete the exercise, you will need to access the source code files of the application, which are available at the <em class="italic">Packt Publishing</em> GitHub repository: <a href="https://github.com/PacktPublishing/AWS-Certified-Cloud-Practitioner-Exam-Guide">https://github.com/PacktPublishing/AWS-Certified-Cloud-Practitioner-Exam-Guide</a></p>
			<p>Finally, as you <a id="_idIndexMarker1037"/>carry out each exercise, you will be provided with some background details to help you appreciate the architecture and reasons behind the deployment. All exercises need to be done in the <strong class="source-inline">us-east-1</strong> Region, which is where<a id="_idIndexMarker1038"/> you have already built <a id="_idIndexMarker1039"/>your <strong class="bold">production VPC</strong> and host your <strong class="bold">RDS database</strong>.</p>
			<p>For all exercises, ensure that you are logged in to your AWS account as our IAM user <strong class="bold">Alice</strong>.</p>
			<h1 id="_idParaDest-236"><a id="_idTextAnchor239"/>Extended exercises – setting the scene </h1>
			<p>The upcoming exercises are based on the following scenario. You work for a fictitious company called <strong class="bold">The Vegan Studio</strong>. The<a id="_idIndexMarker1040"/> company is in the hospitality industry. Specifically, the company runs a chain of cafes and restaurants across the <strong class="bold">United States</strong> (<strong class="bold">US</strong>), serving only vegan dishes for those looking to indulge in meat-free cuisine. The company employs over 4,000 employees across its business, and keeping everyone engaged and feeling part of a large family is something the business takes great pride in.</p>
			<p>Every year, they run several contests for their employees to participate in. This year, they are running a <em class="italic">My Good Deed for the Month</em> contest. A web application has been designed by one of the developers, which you need to now deploy in a highly available and scalable manner on AWS in the <strong class="source-inline">us-east-1</strong> Region. The contest will run for a month and all employees are encouraged to submit a statement of any good deeds they carried out. Five winners will be chosen from a list of entries and awarded a special hamper prize. Participants must back up their good deeds with evidence if requested (just to be sure no one is fooling around)!</p>
			<p>Next, we will walk you through a series of exercises to deploy the application designed by your developer with HA and scalability features. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Some AWS services, such as ELBs <a id="_idIndexMarker1041"/>and <strong class="bold">Network Address Translation</strong> (<strong class="bold">NAT</strong>) gateways, are chargeable. We suggest you complete all the exercises in reasonably quick succession and then perform the cleanup exercise at the end. Overall, the cost should not be more than $5. To ensure costs are kept to a minimum, we will not be configuring the RDS database you deployed in the previous chapter with Multi-AZ.</p>
			<p>The following exercises will make use of a multi-tier application design that will be deployed in the <strong class="bold">production VPC</strong> that <a id="_idIndexMarker1042"/>you already built in the previous chapters. Recall that the VPC comprises both public and private subnets, spanning across two AZs. From the previous chapters, you have already built a foundation architecture as per the following diagram:</p>
			<div>
				<div id="_idContainer195" class="IMG---Figure">
					<img src="Images/B17124_09_10.jpg" alt="Figure 9.10 – Production VPC architecture prior to deploying the &quot;Good Deed of the Month&quot; contest application&#13;&#10;" width="1060" height="791"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.10 – Production VPC architecture prior to deploying the "Good Deed of the Month" contest application</p>
			<p>As per the previous diagram, your<a id="_idIndexMarker1043"/> current architecture is comprised of the following key AWS services and resources:</p>
			<ul>
				<li>A VPC created in the <strong class="source-inline">us-east-1</strong> Region with public and private subnets across two AZs. The private subnets have been designed to support a two-tier application solution comprising a web/application tier and a database tier.</li>
				<li>The public subnets will normally be used to deploy bastion hosts for remote administration and NAT gateways. For the upcoming series of exercises, we will not be deploying any bastion hosts as this is not required for the labs in these exercises. However, we will amend the bastion host security group to allow inbound <strong class="bold">Secure Shell</strong> (<strong class="bold">SSH</strong>) connections if you wish to later deploy bastion hosts. Furthermore, because we will be deploying Linux servers, remote administration requires SSH access on port <strong class="source-inline">22</strong>. </li>
				<li>The application tier <a id="_idIndexMarker1044"/>private subnets do not currently have any EC2 instances deployed.</li>
				<li>The database tier private subnets currently host a single instance MySQL RDS database in the <strong class="source-inline">us-east-1a</strong> AZ.</li>
			</ul>
			<p>Through the upcoming exercises in this chapter, we will build on the architecture to design a fully functional application solution with HA and scalability features.</p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor240"/>Exercise 9.1 – setting up an Amazon S3 bucket to host source files</h2>
			<p>In <a id="_idIndexMarker1045"/>this exercise, you will first create an Amazon S3 bucket that will be used to host your source files for your application. You need to first download the source file, which is available in a ZIP folder format, and extract its contents into a new folder or onto the desktop of your computer for easy access. </p>
			<p>The first step is to prepare your source code files. Your source code files contain a database connection file that will need to be amended to the specific RDS database you configured in the previous chapter. Follow these next steps: </p>
			<ol>
				<li>Once you unzip the downloaded folder, you can see the contents of the main <strong class="source-inline">vegan-php-files</strong> folder, as per the following screenshot:<div id="_idContainer196" class="IMG---Figure"><img src="Images/B17124_09_11.jpg" alt="Figure 9.11 – vegan-php-files source code&#13;&#10;" width="698" height="292"/></div><p class="figure-caption">Figure 9.11 – vegan-php-files source code</p></li>
				<li>You will<a id="_idIndexMarker1046"/> note that in the <strong class="source-inline">v5</strong> directory, there is a file called <strong class="source-inline">db</strong> that is a <strong class="bold">PHP: Hypertext Preprocessor</strong> (<strong class="bold">PHP</strong>) file. This file contains default database connection string details that you will first need to amend before you upload the source code to your S3 bucket. Specifically, you will need to provide the RDS database connection details, which include the RDS endpoint DNS name, master username, password, and database name. <em class="italic">Recall that you made a note of these values in the last chapter</em>.</li>
				<li>In a notepad or text editor tool, open the <strong class="source-inline">db.php</strong> file from the <strong class="source-inline">v5</strong> folder.</li>
				<li>Within the PHP file, you will need to edit the values of the placeholders with the appropriate database connection values. In the following screenshot, you will see the placeholders:<div id="_idContainer197" class="IMG---Figure"><img src="Images/B17124_09_12.jpg" alt="Figure 9.12 – db.php file&#13;&#10;" width="1101" height="364"/></div><p class="figure-caption">Figure 9.12 – db.php file</p></li>
				<li>You will <a id="_idIndexMarker1047"/>need to replace the placeholders with the connection details to your database, making sure to place all the values within single quotation marks. Do not make any other changes to the code. Here is a screenshot of where you can obtain these values after you create your database. The database endpoint is visible on the main <strong class="bold">Connectivity &amp; security</strong> tab, and you will find the username and database name in the <strong class="bold">Configuration</strong> tab. Note that the password is not visible, as you should have made a note of it when launching the database instance:<div id="_idContainer198" class="IMG---Figure"><img src="Images/B17124_09_13.jpg" alt="Figure 9.13 – Amazon RDS database settings&#13;&#10;" width="1046" height="499"/></div><p class="figure-caption">Figure 9.13 – Amazon RDS database settings</p></li>
				<li>Save the <a id="_idIndexMarker1048"/>file in its original location. </li>
				<li>Log in to your AWS Management Console and navigate to the Amazon S3 dashboard.</li>
				<li>In the left-hand menu, click <strong class="bold">Buckets</strong>.</li>
				<li>Click <strong class="bold">Create bucket</strong> in the right-hand pane of the dashboard.</li>
				<li>For the <strong class="bold">Bucket name</strong> field, provide any name of choice. You will need to choose a unique name as bucket names are obtained on a first-come, first-served basis. For example, my bucket name is <strong class="source-inline">vegan-good-deed</strong>. Ensure that the Region selected is the <strong class="source-inline">us-east-1</strong> Region.</li>
				<li>Scroll to the bottom of the page, leaving all settings at their default values, and click the <strong class="bold">Create bucket</strong> button. Your Amazon S3 bucket will be created, and you will be redirected back to the list of available buckets. </li>
				<li>Click on the bucket you just created, and you will be redirected to the <strong class="bold">Objects </strong>listing page, where you will note that there are currently no objects, as per the following <a id="_idIndexMarker1049"/>screenshot:<div id="_idContainer199" class="IMG---Figure"><img src="Images/B17124_09_14.jpg" alt="Figure 9.14 – New bucket creation&#13;&#10;" width="1373" height="720"/></div><p class="figure-caption">Figure 9.14 – New bucket creation</p></li>
				<li>Click the <strong class="bold">Upload</strong> button.</li>
				<li>Next, you want to try to resize your browser page with the S3 bucket <strong class="bold">Upload</strong> page visible, and next to it resize the <strong class="source-inline">vegan-php-files</strong> folder so that you can easily drag and drop all the folders and files into the S3 bucket's <strong class="bold">Object</strong> area, as per the <a id="_idIndexMarker1050"/>following screenshot. You need to ensure that the folder hierarchy is maintained for the application to work:<div id="_idContainer200" class="IMG---Figure"><img src="Images/B17124_09_15.jpg" alt="Figure 9.15 – Copying files and folders to the S3 bucket&#13;&#10;" width="1650" height="740"/></div><p class="figure-caption">Figure 9.15 – Copying files and folders to the S3 bucket</p></li>
				<li>Your Amazon S3 <strong class="bold">Upload</strong> page will provide a summary of files and folders to be uploaded. You will need to then click on the <strong class="bold">Upload</strong> button at the bottom of the page.</li>
				<li>Once all the<a id="_idIndexMarker1051"/> files and folders have been uploaded, you receive an <strong class="bold">Upload succeeded</strong> message.</li>
			</ol>
			<p>Now that your source code and files for your application have been uploaded, we will move on to the next exercise. As part of this series of exercises, you will need to configure your EC2 instances to download the source code files for the application. Using Bash scripts at the time of launching your EC2 instances, you will download the source code from the Amazon S3 bucket and place it in the appropriate folders within the EC2 instances to serve the application. </p>
			<p>Because your EC2 instance would need to have permissions to access the previous S3 bucket we created and download the source code, we need to configure an IAM role that your EC2 instance will use to authenticate to Amazon S3.</p>
			<h2 id="_idParaDest-238"><a id="_idTextAnchor241"/>Exercise 9.2 – creating an IAM role</h2>
			<p>In this exercise, you will <a id="_idIndexMarker1052"/>create an IAM role that your EC2 instances will use to authenticate and access the source code files in your Amazon S3 bucket. Proceed as follows:</p>
			<ol>
				<li value="1">Ensure that you are logged in to your AWS account and navigate to the IAM dashboard.</li>
				<li>Click on <strong class="bold">Roles</strong> from the left-hand menu.</li>
				<li>Click <strong class="bold">Create role</strong>, as per the following screenshot:<div id="_idContainer201" class="IMG---Figure"><img src="Images/B17124_09_16.jpg" alt="Figure 9.16 – Creating an IAM role&#13;&#10;" width="1150" height="585"/></div><p class="figure-caption">Figure 9.16 – Creating an IAM role</p></li>
				<li>In the <strong class="bold">Select type of trusted entity</strong> field, click the <strong class="bold">AWS services</strong> option, and under <strong class="bold">Choose a use case</strong>, select <strong class="bold">EC2</strong> under <strong class="bold">Common use cases</strong>.</li>
				<li>Click the <strong class="bold">Next: Permissions</strong> button at the bottom of the page.</li>
				<li>On the <strong class="bold">Attach permissions policies</strong> page, filter the list by searching for <strong class="source-inline">S3</strong>. Next, select the <strong class="source-inline">AmazonS3ReadOnlyAccess</strong> policy and click the <strong class="bold">Next: Tags</strong> button, as <a id="_idIndexMarker1053"/>per the following screenshot:<div id="_idContainer202" class="IMG---Figure"><img src="Images/B17124_09_17.jpg" alt="Figure 9.17 – Creating an IAM role (continued)&#13;&#10;" width="1117" height="822"/></div><p class="figure-caption">Figure 9.17 – Creating an IAM role (continued)</p></li>
				<li>Set a key-value pair to tag your role with a key of <strong class="bold">Name</strong> and a value of <strong class="source-inline">EC2-to-S3-Read-Access</strong>. This allows us to easily identify the role. Click the <strong class="bold">Next: Review</strong> button <a id="_idIndexMarker1054"/>on the bottom right-hand corner of the page.</li>
				<li>In the <strong class="bold">Review</strong> section, provide a name for your role, such as <strong class="source-inline">EC2-to-S3-Read-Access</strong>, and a description.</li>
				<li>Finally, click the <strong class="bold">Create role</strong> button.</li>
			</ol>
			<p>AWS will now <a id="_idIndexMarker1055"/>create your IAM role, and you will need to reference this role when we launch our EC2 instances in a later chapter.</p>
			<p>At this stage, you have the following AWS services configured:</p>
			<ul>
				<li>An Amazon VPC with public and private subnets across two AZs. You have a public subnet to host bastion hosts and NAT gateways, and four private subnets—two<a id="_idIndexMarker1056"/> for your web/application servers located in the <strong class="bold">web/application tier</strong> and another two for <a id="_idIndexMarker1057"/>your <strong class="bold">database tier</strong>.</li>
				<li>An RDS database that will store all data such as the <em class="italic">good deeds of the month</em> that the employees of The Vegan Studio will submit. </li>
				<li>An Amazon S3 bucket with the source code files configured to point to the RDS database.</li>
				<li>An IAM role to allow your EC2 instances to download the source code files from the S3 bucket.</li>
			</ul>
			<p>When you deploy your application, you will install Apache Web Services and host your application files on EC2 instances. Specifically, you will be deploying two EC2 instances that will be placed across two AZs. To distribute traffic across those EC2 instances, you will need to configure an ALB that will be configured to accept inbound HTTP (port <strong class="source-inline">80</strong>) traffic from the internet and distribute them to your EC2 instances. In the next exercise, you will need to configure your ALB. </p>
			<h2 id="_idParaDest-239"><a id="_idTextAnchor242"/>Exercise 9.3 – configuring an ALB</h2>
			<p>In this exercise, you will be configuring an<a id="_idIndexMarker1058"/> ALB that will be used to accept inbound traffic from your users on the internet and distribute them across the EC2 instances you deploy later in this chapter. </p>
			<p>ALBs, as discussed earlier in this chapter, can be used to distribute web and application traffic using HTTP and HTTPS protocols. You will configure an internet-facing load balancer so that you can accept inbound requests from the internet. </p>
			<p>CLBs and ALBs require you to also configure a security group within which you define which traffic would be permitted inbound to those load balancers. Therefore, the first step is to revisit the VPC dashboard and create a new security group for your ALB, as follows:</p>
			<ol>
				<li value="1">Navigate to the VPC dashboard and ensure you are still in the <strong class="source-inline">us-east-1</strong> Region.</li>
				<li>From the left-hand menu, click on <strong class="bold">Security Groups</strong>.</li>
				<li>Click the <strong class="bold">Create security group</strong> button in the top right-hand corner of the screen.</li>
				<li>Provide the <a id="_idIndexMarker1059"/>security group with a name such as <strong class="source-inline">ALB-SG</strong> and a description such as <strong class="source-inline">Allow inbound HTTP traffic from Internet</strong>.</li>
				<li>Ensure that you select <strong class="bold">ProductionVPC</strong> from the VPC drop-down list.</li>
				<li>Click the <strong class="bold">Add rule</strong> button under <strong class="bold">Inbound rules</strong>.</li>
				<li>For <strong class="bold">Type</strong>, select <strong class="bold">HTTP</strong> from the <a id="_idIndexMarker1060"/>drop-down list, and set the source to <strong class="bold">Custom</strong>, with a <strong class="bold">classless inter-domain routing</strong> (<strong class="bold">CIDR</strong>) block of <strong class="source-inline">0.0.0.0/0</strong>. This source denotes the public internet.</li>
				<li>Provide an optional description and then click on the <strong class="bold">Create security group</strong> button on the bottom right-hand corner of the screen.</li>
			</ol>
			<p>AWS will now create your security group, which we will use to configure our ALB. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">ELBs do not fall under the Free Tier offering from AWS, and you must ensure you delete them once you have completed all the labs.</p>
			<p>Now that we have configured a security group, we can move on to configuring our load balancing service. However, your ALB requires a <strong class="bold">target group</strong> to send traffic to. The target group will be used to register the EC2 instances that will accept traffic from the ALB. So, the first step is to create your target group, as follows:</p>
			<ol>
				<li value="1">Navigate to the EC2 dashboard and ensure that you are in the <strong class="source-inline">us-east-1</strong> Region.</li>
				<li>From the left-hand menu, click on <strong class="bold">Target Groups</strong>, under the <strong class="bold">Load Balancing</strong> menu.</li>
				<li>From the right-hand pane, click on <strong class="bold">Create target group</strong>.</li>
				<li>Next, you are <a id="_idIndexMarker1061"/>presented with a two-step wizard. In <em class="italic">Step 1</em>, select <strong class="bold">Instances</strong> and then scroll further down to provide a <strong class="bold">Target group name</strong> value. I have named my target group <strong class="source-inline">Production-TG</strong>.</li>
				<li>Under <strong class="bold">Protocol</strong>, ensure that <strong class="bold">HTTP</strong> is selected and the port is set to <strong class="source-inline">80</strong>.</li>
				<li>Next, under <strong class="bold">VPC</strong>, ensure you select <strong class="source-inline">Production-VPC</strong>.</li>
				<li>Scroll further down till you reach the <strong class="bold">Health checks</strong> section.</li>
				<li>Next, set the <strong class="bold">Health check protocol</strong> to <strong class="bold">HTTP</strong>.</li>
				<li>For the <strong class="bold">Health check path</strong> field, type in <strong class="source-inline">/health.html</strong> as per the following screenshot:<div id="_idContainer203" class="IMG---Figure"><img src="Images/B17124_09_18.jpg" alt="Figure 9.18 – Load balancer target group health checks&#13;&#10;" width="858" height="357"/></div><p class="figure-caption">Figure 9.18 – Load balancer target group health checks</p></li>
				<li>Next, expand the <strong class="bold">Advanced health check settings</strong> field.</li>
				<li>Set the <strong class="bold">Port</strong> value to <strong class="bold">Traffic Port</strong>.</li>
				<li>Set the <strong class="bold">Healthy threshold</strong> value to <strong class="source-inline">3</strong>.</li>
				<li>Set the <strong class="bold">Unhealthy threshold</strong> value to <strong class="source-inline">2</strong>.</li>
				<li>Next, set <a id="_idIndexMarker1062"/>the <strong class="bold">Timeout</strong> value to <strong class="source-inline">2</strong>.</li>
				<li>Finally, set the <strong class="bold">Interval</strong> value to <strong class="source-inline">10</strong> seconds.</li>
				<li>Click the <strong class="bold">Next</strong> button at the bottom of the page.</li>
				<li>This will take you to <em class="italic">Step 2</em>, where you would normally register any EC2 instances. However, as we have not launched any EC2 instances yet, you can ignore this step and simply click on the <strong class="bold">Create target group</strong> button at the bottom of the page.<p>Next, now that we have the <strong class="bold">target group</strong> configured, we can launch our ALB. ELBs are configured in the EC2 management console or via the <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>).</p></li>
				<li>From the left-hand menu, click on <strong class="bold">Load Balancers</strong> under the <strong class="bold">Load Balancing</strong> category.</li>
				<li>Next, click on the <strong class="bold">Create Load Balancer</strong> button at the top of the screen in the right-hand pane.</li>
				<li>Click on the <strong class="bold">Create</strong> button in the <strong class="bold">Application Load Balancer</strong> section of the page, as per the following screenshot:<div id="_idContainer204" class="IMG---Figure"><img src="Images/B17124_09_19.jpg" alt="Figure 9.19 – Selecting Application Load Balancer as load balancer type&#13;&#10;" width="1374" height="611"/></div><p class="figure-caption">Figure 9.19 – Selecting Application Load Balancer as load balancer type</p></li>
				<li>In <strong class="bold">Step 1: Configure Load Balancer</strong>, proceed as follows:<ol><li>Set the name of the load balancer to <strong class="source-inline">Production-ALB</strong>.</li><li>Ensure that <a id="_idIndexMarker1063"/>the <strong class="bold">Scheme</strong> field is set to <strong class="bold">Internet facing</strong> and that <strong class="bold">IP address type</strong> is set to <strong class="bold">ipv4</strong>.</li><li>Next, under <strong class="bold">Network mapping</strong>, select <strong class="source-inline">Production-VPC</strong> under the <strong class="bold">VPC</strong> heading.</li><li>Under <strong class="bold">Mappings</strong>, you need to select which AZs will be enabled for the ALB. </li><li>Select the checkboxes next to both the <strong class="source-inline">us-east-1a</strong> and <strong class="source-inline">us-east-1b</strong> AZs.</li><li>In the <strong class="bold">Subnet</strong> drop-down list for the <strong class="source-inline">us-east-1a</strong> AZ, select the <strong class="bold">Public Subnet One</strong> subnet. </li><li>Next, in the <strong class="bold">Subnet</strong> drop-down list for the <strong class="source-inline">us-east-1b</strong> AZ, select the <strong class="bold">Public Subnet Two</strong> subnet.</li></ol><p>AWS will then deploy the ALB <em class="italic">nodes</em> in these public subnets, routing incoming traffic from the internet to the EC2 instances in the private subnets that we registered as targets for the load balancer. Internet-facing load balancers should be created in subnets that have been configured with an internet gateway such as in this case: the public subnets of your VPC.</p></li>
				<li>Next, under <strong class="bold">Security Groups</strong>, select the <strong class="source-inline">ALB-SG</strong> security group from the drop-down list. You can also delete the <strong class="bold">default</strong> security group that was pre-selected by clicking on the <strong class="bold">X</strong> sign next to the group. This is because we only want to associate the load balancer with the <strong class="source-inline">ALB-SG</strong> security group.</li>
				<li>In the <strong class="bold">Listener</strong> section, ensure that the <strong class="bold">Protocol</strong> field is set to <strong class="bold">HTTP</strong> and the <strong class="bold">Port</strong> field is set to <strong class="source-inline">80</strong>. Next, under <strong class="bold">Default Action</strong>, select the <strong class="source-inline">Production-TG</strong> target group you created earlier from the drop-down list.</li>
				<li>Finally, scroll <a id="_idIndexMarker1064"/>further down and click on the <strong class="bold">Create load balancer</strong> button.</li>
				<li>You will receive a confirmation message stating that the load balancer has been created successfully. Click on <strong class="bold">View load balancers</strong>, which will take you back to the list of load balancers deployed, and you should find your <strong class="source-inline">Production-ALB</strong> load balancer in the list. After a few moments, the status of the load balancer should change from <strong class="bold">Provisioning</strong> to <strong class="bold">Active</strong>.</li>
			</ol>
			<p>At this point, we have now configured our ALB. Let's go ahead and look at our architectural diagram to see how our configuration is coming along:</p>
			<div>
				<div id="_idContainer205" class="IMG---Figure">
					<img src="Images/B17124_09_20.jpg" alt="Figure 9.20 – Production VPC architecture after configuring S3 bucket, IAM role, and ALB&#13;&#10;" width="1066" height="795"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.20 – Production VPC architecture after configuring S3 bucket, IAM role, and ALB</p>
			<p>For traffic to be <a id="_idIndexMarker1065"/>allowed inbound to the application servers, we need to ensure that the security groups associated with those servers have been correctly configured. Specifically, the <strong class="source-inline">AppServers-SG</strong> security group must allow traffic on the HTTP protocol (port <strong class="source-inline">80</strong>) from the ALB we deployed in the previous exercise.</p>
			<p>Furthermore, in <a href="B17124_07_Final_SK_ePub.xhtml#_idTextAnchor157"><em class="italic">Chapter 7</em></a>, <em class="italic">AWS Compute Services</em>, you configured the <strong class="source-inline">AppServers-SG</strong> security group to accept traffic from the <strong class="source-inline">BastionHost-SG</strong> security group. This was to enable inbound traffic on <strong class="bold">Remote Desktop Protocol</strong> (<strong class="bold">RDP</strong>) (port <strong class="source-inline">3389</strong>), which enables<a id="_idIndexMarker1066"/> the Windows Remote Desktop client to perform remote access operations. Although we will not be deploying any bastion hosts in the remaining exercises in this chapter, we will amend the inbound rule on the <strong class="source-inline">AppServers-SG</strong> security group such that the protocol and port used to accept traffic from the <strong class="source-inline">BastionHost-SG</strong> security<a id="_idIndexMarker1067"/> group will be set to the SSH protocol on port <strong class="source-inline">22</strong>. This is because we will be deploying Linux EC2 instances to host our application, and any remote management of Linux servers requires you to configure SSH access.</p>
			<h2 id="_idParaDest-240"><a id="_idTextAnchor243"/>Exercise 9.4 – amending the Production-VPC security group</h2>
			<p>In this exercise, we will <a id="_idIndexMarker1068"/>amend the RDP inbound rule in the <strong class="source-inline">AppServers-SG</strong> security group such that it is configured to accept traffic on the SSH protocol (port <strong class="source-inline">22</strong>) from the <strong class="source-inline">BastionHost-SG</strong> security group. Next, we will add a new rule to accept traffic on the HTTP protocol (port <strong class="source-inline">80</strong>) from the ALB's security group, <strong class="source-inline">ALB-SG</strong>. Finally, we will amend the <strong class="source-inline">BastionHost-SG</strong> security group such that it is configured to accept traffic on the SSH protocol (port <strong class="source-inline">22</strong>) from the internet. This is useful if you later wish to perform any remote administration of your Linux servers. </p>
			<p>Amend the <strong class="source-inline">BastionHost-SG</strong> security group, as follows:</p>
			<ol>
				<li value="1">Navigate to the VPC dashboard and ensure that you are in the <strong class="source-inline">us-east-1</strong> Region.</li>
				<li>From the left-hand menu, click on <strong class="bold">Security Groups</strong>.</li>
				<li>In the middle pane, select the checkbox next to the <strong class="bold">Security group ID</strong> value associated with the <strong class="source-inline">BastionHost-SG</strong> security group.</li>
				<li>In the pane below, click on <strong class="bold">Inbound rules</strong> and then click on the <strong class="bold">Edit inbound rules</strong> button.</li>
				<li>Next, delete the existing <strong class="bold">RDP</strong> rule by clicking on the <strong class="bold">Delete</strong> button on the far right of the page.</li>
				<li>Click the <strong class="bold">Add rule</strong> button. </li>
				<li>For the type, select <strong class="bold">SSH</strong> from the drop-down list. Next, ensure that the <strong class="bold">Custom</strong> option is selected in the <strong class="bold">Source</strong> column, and in the search box next to it, type in <strong class="source-inline">0.0.0.0/0</strong>. </li>
				<li>Finally, click on the <strong class="bold">Save rules</strong> button in the bottom right-hand corner of the page.<p>Amend the <strong class="source-inline">AppServers-SG</strong> security group.</p></li>
				<li>Click on the <strong class="bold">Security Groups</strong> link from the left-hand menu again to see all your security groups in the VPC.</li>
				<li>In the middle pane, select the checkbox next to the <strong class="bold">Security group ID</strong> value associated with the <strong class="source-inline">AppServers-SG</strong> security group.</li>
				<li>In the pane <a id="_idIndexMarker1069"/>below, click on <strong class="bold">Inbound rules</strong> and then click on the <strong class="bold">Edit inbound rules</strong> button.</li>
				<li>Next, delete the existing <strong class="bold">RDP</strong> rule by clicking on the <strong class="bold">Delete</strong> button on the far right of the page.</li>
				<li>Click the <strong class="bold">Add rule</strong> button. </li>
				<li>For the type, select <strong class="bold">SSH</strong> from the drop-down list. Next, ensure that the <strong class="bold">Custom</strong> option is selected in the <strong class="bold">Source</strong> column, and in the search box next to it, start by typing in <strong class="source-inline">sg-</strong>. You will notice that a list of your security groups will become visible. Select the <strong class="source-inline">BastionHost-SG</strong> security group from this list. </li>
				<li>Next, click the <strong class="bold">Add rule</strong> button again.</li>
				<li>For the type, select <strong class="bold">HTTP</strong> from the drop-down list. Next, ensure that the <strong class="bold">Custom</strong> option is selected in the <strong class="bold">Source</strong> column, and in the search box next to it, start by typing in <strong class="source-inline">sg-</strong>. You will notice that a list of your security groups will become visible. This time, select the <strong class="source-inline">ALB-SG</strong> security group from the list.</li>
				<li>Finally, click on the <strong class="bold">Save rules</strong> button in the bottom right-hand corner of the page.</li>
			</ol>
			<p>We will not need to amend the <strong class="source-inline">Database-SG</strong> security group because this has already been configured to only accept traffic from the <strong class="source-inline">AppServers-SG</strong> security group using the MySQL port <strong class="source-inline">3306</strong>.</p>
			<p>Recall from the architectural diagram in <em class="italic">Figure 9.20</em> that the web/application EC2 instances are going to be placed in a private subnet. Our The Vegan Studio employees will be able to access the <em class="italic">Good Deed of the Month</em> contest application on those EC2 instances via the ALB. However, the EC2 instances will need access to the internet to download updates as well as the source code files stored on the Amazon S3 bucket. </p>
			<p>Remember that, unlike the public subnet, the private subnet does not grant direct access to the internet. Any EC2 instance in the private subnet would need to direct internet-bound traffic via an AWS NAT gateway, as discussed in <a href="B17124_06_Final_SK_ePub.xhtml#_idTextAnchor122"><em class="italic">Chapter 6</em></a>, <em class="italic">AWS Networking Services – VPCs, Route53, and CloudFront</em>.</p>
			<p>In the next exercise, we will deploy a NAT gateway.</p>
			<h2 id="_idParaDest-241"><a id="_idTextAnchor244"/>Exercise 9.5 – deploying a NAT gateway</h2>
			<p>In this exercise, we <a id="_idIndexMarker1070"/>will deploy a NAT gateway in the <strong class="bold">Public Subnet One</strong> subnet of our production VPC. Ideally, you want to deploy multiple NAT gateways in each public <a id="_idIndexMarker1071"/>subnet across the AZs you have resources in to avoid a <strong class="bold">single point of failure</strong> (<strong class="bold">SPOF</strong>). However, for the purposes of this lab, we will use a single NAT gateway.</p>
			<p>In addition, you will need to configure your <strong class="bold">main route table</strong> with a new route that will allow outbound traffic to the internet via this NAT gateway. </p>
			<p>We will start this exercise by first allocating an elastic IP address for our AWS account, which is a requirement to configure a NAT gateway. To do this, follow these steps:</p>
			<ol>
				<li value="1">Navigate to your VPC dashboard and ensure that you are in the <strong class="source-inline">us-east-1</strong> Region.</li>
				<li>NAT gateways require an elastic IP address, and so you will need to allocate one first to your AWS account. From the left-hand menu, click on <strong class="bold">Elastic IPs</strong>. In the right-hand pane, click the <strong class="bold">Allocate Elastic IP address</strong> button.</li>
				<li>You will be presented with the <strong class="bold">Allocate Elastic IP address</strong> page. Ensure that <strong class="bold">Amazon's pool of IPv4 addresses</strong> is selected and then click the <strong class="bold">Allocate</strong> button.</li>
			</ol>
			<p>AWS will allocate an elastic IP address from its pool of available addresses for your AWS account. Next, you will need to set up your NAT gateway, as follows:</p>
			<ol>
				<li value="1">From the left-hand menu, click on <strong class="bold">NAT Gateways</strong>. </li>
				<li>In the right-hand pane, click the <strong class="bold">Create NAT gateway</strong> button.</li>
				<li>On the <strong class="bold">Create NAT gateway</strong> page, proceed as follows:<ol><li>Provide a<a id="_idIndexMarker1072"/> name for your NAT gateway—for example, <strong class="source-inline">Production-NAT</strong>.</li><li>Next, from the <strong class="bold">Subnet</strong> drop-down list, select the <strong class="bold">Public Subnet One</strong> subnet.</li><li>Next, from the drop-down list under <strong class="bold">Elastic IP allocation ID</strong>, select the elastic IP you allocated to your account moments ago.</li><li>Finally, click the <strong class="bold">Create NAT gateway</strong> button at the bottom of the page.</li></ol></li>
			</ol>
			<p>The NAT gateway will take a couple of minutes to be provisioned. Once ready, the NAT gateway state will be set to <strong class="bold">Available</strong>, as per the following screenshot:</p>
			<div>
				<div id="_idContainer206" class="IMG---Figure">
					<img src="Images/B17124_09_21.jpg" alt="Figure 9.21 – NAT gateway&#13;&#10;" width="1583" height="480"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.21 – NAT gateway</p>
			<p>Now that you have deployed your NAT gateway, you will need to configure your main route table with a route to the internet that uses the NAT gateway, as follows:</p>
			<ol>
				<li value="1">From the left-hand menu in the VPC dashboard, select <strong class="bold">Route Tables</strong>.</li>
				<li>Click on the checkbox next to <strong class="bold">Main Route Table</strong>.</li>
				<li>In the bottom pane, click on the <strong class="bold">Routes</strong> tab.</li>
				<li>Next, click the <strong class="bold">Edit routes</strong> button on the far right-hand side of the page.</li>
				<li>You will be presented with the <strong class="bold">Edit routes</strong> page. Click the <strong class="bold">Add route</strong> button.</li>
				<li>Under the <strong class="bold">Destination</strong> column, provide the destination as <strong class="source-inline">0.0.0.0/0</strong>.</li>
				<li>Next, click on the <strong class="bold">Target</strong> search box to open up a list of potential targets. Select the <strong class="bold">NAT Gateway</strong> target, and AWS will display available NAT gateways associated with this VPC. You should find the <strong class="source-inline">Production-NAT</strong> NAT gateway in the list. Go<a id="_idIndexMarker1073"/> ahead and select this. </li>
				<li>Finally, click on the <strong class="bold">Save changes</strong> button.</li>
			</ol>
			<p>Your main route table has now been configured with a route to the internet that will use the NAT gateway. </p>
			<p>Now that you have configured your NAT gateway and the main route table correctly, we can proceed with deploying our EC2 instances that will host the <em class="italic">Good Deed of the Month</em> application in the next exercise. </p>
			<h2 id="_idParaDest-242"><a id="_idTextAnchor245"/>Exercise 9.6 – deploying your application servers with Amazon Auto Scaling</h2>
			<p>In this exercise, we will <a id="_idIndexMarker1074"/>configure the Amazon Auto Scaling service to define a <strong class="bold">Launch Configuration</strong> for our deployment, which will include a script to configure our EC2 instances with the Apache web service and download the application source files from the Amazon S3 bucket. As part of the exercise, you will also create an EC2 instance profile that will be used to contain the IAM role you created earlier and allow the EC2 instance to assume that role.</p>
			<p>The EC2 instances will also be provisioned as targets in the <strong class="source-inline">Production-TG</strong> target group we created earlier in the ALB exercise. The <strong class="source-inline">Production-ALB</strong> ALB will then be able to distribute inbound traffic from our The Vegan Studio employees on the internet to those EC2 instances, enabling them to submit any good deeds they carried out for review by our panel.</p>
			<p>In addition, we will configure Auto Scaling policies to always ensure that we always have two running EC2 instances, one in each private subnet, across the two AZs, <strong class="source-inline">us-east-1a</strong> and <strong class="source-inline">us-east-1b</strong>. In terms of health checks, these will be performed both at the EC2 level and via the ALBs using the health check parameters you defined in <em class="italic">Exercise 9.3</em> earlier.</p>
			<h3>Creating an Auto Scaling Launch Configuration</h3>
			<p>As part of <a id="_idIndexMarker1075"/>this exercise, you will need access to a Bash script that we have included in the GitHub repository <a href="https://github.com/PacktPublishing/AWS-Certified-Cloud-Practitioner-Exam-Guide">https://github.com/PacktPublishing/AWS-Certified-Cloud-Practitioner-Exam-Guide</a>. In the <strong class="source-inline">vegan-php-files.zip</strong> file you downloaded earlier, which you unzipped, you will find a file called <strong class="source-inline">userdata-script</strong> in the top-level folder. You will need to amend this script to match your configuration. Open the script file in a notepad or text editor application and change the last line of the script, replacing <strong class="source-inline">[Source Bucket]</strong> with the actual name of your bucket. So, for example, if your bucket name is <strong class="source-inline">vegan-good-deed</strong>, then the last line should be changed from <strong class="source-inline">aws s3 cp s3://[Source Bucket] /var/www/html –recursive</strong> to <strong class="source-inline">aws s3 cp s3://vegan-good-deed /var/www/html –recursive</strong>. Make sure to save the file.</p>
			<p>Next, we look at the steps required to set up our AWS Auto Scaling Launch Configuration, as follows:</p>
			<ol>
				<li value="1">Navigate to the EC2 dashboard and ensure that you are in the <strong class="source-inline">us-east-1</strong> Region.</li>
				<li>From the left-hand menu, select <strong class="bold">Launch Configurations</strong> from the <strong class="bold">Auto Scaling</strong> category.</li>
				<li>In the right-hand pane, click on the <strong class="bold">Create Launch Configuration</strong> button.</li>
				<li>You will be presented with the <strong class="bold">Create Launch Configuration</strong> page.</li>
				<li>Provide a name for your Launch Configuration—for example, <strong class="source-inline">Production-LC</strong>.</li>
				<li>Next, you need to search for the Amazon Linux 2 AMI. It might be difficult to find the AMI in the new <strong class="bold">user interface</strong> (<strong class="bold">UI</strong>). To identify the AMI ID, open another browser window to access your AWS account and navigate to the EC2 dashboard. Click on <strong class="bold">Instances</strong> from the left-hand menu and then click on the <strong class="bold">Launch instances</strong> button in the far right-hand corner of the screen. You will find a list of quickstart AMIs. From this page, <em class="italic">make a note of the AMI ID for the Amazon Linux 2 instance</em>. Ensure that the AMI ID is for the <strong class="source-inline">64-bit x86</strong> architecture, which I have highlighted as per the following screenshot:<div id="_idContainer207" class="IMG---Figure"><img src="Images/B17124_09_22.jpg" alt="Figure 9.22 – AMI ID for Amazon Linux 2 instance&#13;&#10;" width="1246" height="199"/></div><p class="figure-caption">Figure 9.22 – AMI ID for Amazon Linux 2 instance</p></li>
				<li>Back in the <a id="_idIndexMarker1076"/>previous browser window where you are configuring your Auto Scaling Launch Configuration, click on the drop-down arrow under <strong class="bold">AMI</strong> and paste in the AMI ID you copied previously in the search box. You should then be able to find the relevant AMI to use. Make sure you select this AMI.</li>
				<li>Next, under <strong class="bold">Instance type</strong>, click on the <strong class="bold">Choose instance type</strong> button, and in the search box, type in <strong class="source-inline">t2.micro</strong>. You can then select the <strong class="source-inline">t2.micro</strong> instance type from the filtered list. Go ahead and click the <strong class="bold">Choose</strong> button.</li>
				<li>Next, under the <strong class="bold">Additional configuration</strong> option, click the drop-down arrow under <strong class="bold">IAM instance profile</strong> and select the <strong class="source-inline">EC2-to-S3-Read-Access</strong> instance profile that contains the IAM role you created earlier.</li>
				<li>Next, expand the <strong class="bold">Advanced details</strong> section.</li>
				<li>Under <strong class="bold">User Data</strong>, ensure that <strong class="bold">As Text </strong>is the selected option and, in the textbox provided, go ahead and paste in a copy of the Bash script file you amended a few moments ago.</li>
				<li>Next, under <strong class="bold">IP address type</strong>, ensure that you select <strong class="bold">Do not assign a public IP address to any instances</strong>. This is because the EC2 instances are going to be launched in the private subnets and will not require a public IP address.</li>
				<li>Leave the settings in the <strong class="bold">Storage (volumes)</strong> field at their default values.</li>
				<li>Next, under <strong class="bold">Security groups</strong>, click on the <strong class="bold">Select an existing security group</strong> option, and from the list of available security groups, select the security group ID associated with the <strong class="source-inline">AppServers-SG</strong> security group.</li>
				<li>In the <strong class="bold">Key pair (login)</strong> section, select <strong class="bold">Choose an existing key pair</strong> from the <strong class="bold">Key pair options</strong> drop-down list.</li>
				<li>In the drop-down list under <strong class="bold">Existing key pair</strong>, ensure that you select the key pair you <a id="_idIndexMarker1077"/>created earlier. In my example, this is the <strong class="source-inline">USEC2Keys</strong> key pair.</li>
				<li>Next, tick the box to acknowledge that you have access to the private key file that you downloaded earlier in <a href="B17124_07_Final_SK_ePub.xhtml#_idTextAnchor157"><em class="italic">Chapter 7</em></a>, <em class="italic">AWS Compute Services</em>.</li>
				<li>Finally, click on the <strong class="bold">Create Launch Configuration</strong> button at the bottom of the screen.</li>
			</ol>
			<p>At this point, you have successfully created your first Auto Scaling Launch Configuration, as per the following screenshot:</p>
			<div>
				<div id="_idContainer208" class="IMG---Figure">
					<img src="Images/B17124_09_23.jpg" alt="Figure 9.23 – Amazon Auto Scaling Launch Configuration&#13;&#10;" width="1619" height="338"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.23 – Amazon Auto Scaling Launch Configuration</p>
			<p>Now that you have created your Launch Configuration, you can proceed to configure your Auto Scaling groups.</p>
			<h3>Configuring Auto Scaling groups</h3>
			<p>As part of creating your <a id="_idIndexMarker1078"/>Auto Scaling groups, you can define Auto Scaling policies. Because we will not be performing any real load testing on our application servers, we will simply configure our Auto Scaling policy to ensure that we always have a minimum of two EC2 instances across the two AZs. Proceed as follows:</p>
			<ol>
				<li value="1">From the left-hand menu of the EC2 dashboard, click on the <strong class="bold">Auto Scaling Groups</strong> link under <strong class="bold">Auto Scaling</strong>.</li>
				<li>Click the <strong class="bold">Create an Auto Scaling group</strong> button in the right-hand pane of the screen.</li>
				<li>In <strong class="bold">Step 1, Choose Launch Template or configuration</strong>, provide a name to identify your Auto Scaling group—for example, <strong class="source-inline">Production-ASG</strong>.</li>
				<li>In the next section of the screen, you will have an option to select a Launch Template from a drop-down list. However, instead of a Launch Template, we have configured <a id="_idIndexMarker1079"/>a Launch Configuration. To access your Launch Configuration, click on the <strong class="bold">Switch to Launch Configuration</strong> link on the far right-hand side of the screen.</li>
				<li>Next, under <strong class="bold">Launch Configuration</strong>, select the <strong class="source-inline">Production-LC</strong> Launch Configuration you created earlier.</li>
				<li>Click the <strong class="bold">Next</strong> button to move on to <em class="italic">Step 2</em>.</li>
				<li>In <strong class="bold">Step 2, Configure settings</strong>, select <strong class="source-inline">Production-VPC</strong> from the <strong class="bold">VPC </strong>drop-down list.</li>
				<li>In the drop-down list under <strong class="bold">Subnets</strong>, ensure that you select both the <strong class="bold">Private Subnet One - App</strong> and <strong class="bold">Private Subnet Two - App</strong> subnets, as per the following screenshot:<div id="_idContainer209" class="IMG---Figure"><img src="Images/B17124_09_24.jpg" alt="Figure 9.24 – Auto Scaling group subnet selection&#13;&#10;" width="1200" height="809"/></div><p class="figure-caption">Figure 9.24 – Auto Scaling group subnet selection</p></li>
				<li>Click the <strong class="bold">Next</strong> button.</li>
				<li>In <strong class="bold">Step 3, Load balancing – optional</strong>, we will be using the ALB you created earlier. Select<a id="_idIndexMarker1080"/> the <strong class="bold">Attach to an existing load balancer</strong> option.</li>
				<li>Next, ensure that <strong class="bold">Choose from your load balancer target groups</strong> is selected under the <strong class="bold">Attach to an existing load balancer</strong> section.</li>
				<li>From the drop-down list under <strong class="bold">Existing load balancer target groups</strong>, select the <strong class="source-inline">Production-TG</strong> target group that is associated with the <strong class="source-inline">Production-ALB</strong> ALB.</li>
				<li>Next, under the <strong class="bold">Health checks – optional</strong> section, select the <strong class="bold">ELB</strong> checkbox. This is to enable ELB health checks in addition to the EC2 health checks.</li>
				<li>Click the <strong class="bold">Next</strong> button at the bottom of the screen.</li>
				<li>In <strong class="bold">Step 4, Configure group size and scaling policies</strong>, under <strong class="bold">Group size</strong>, set the <strong class="bold">Desired</strong>, <strong class="bold">Minimum</strong>, and <strong class="bold">Maximum</strong> capacity values to <strong class="source-inline">2</strong> each. We want to always maintain two EC2 instances in our fleet.</li>
				<li>Under <strong class="bold">Scaling policies – optional</strong>, ensure that <strong class="bold">None</strong> is selected, and then click the <strong class="bold">Next</strong> button.</li>
				<li>In <strong class="bold">Step 5, Add notifications</strong>, do not add any notifications and click on the <strong class="bold">Next</strong> button.</li>
				<li>In <strong class="bold">Step 6, Add tags</strong>, click the <strong class="bold">Add tag</strong> button. Specify a key-value pair to set the name of the<a id="_idIndexMarker1081"/> servers you launch such that the <strong class="bold">Key</strong> field is set to <strong class="source-inline">Name</strong> and the <strong class="bold">Value</strong> field is set to <strong class="source-inline">Production-Servers</strong>.</li>
				<li>Click the <strong class="bold">Next</strong> button to continue.</li>
				<li>You are then presented with a <strong class="bold">Review</strong> page. Review the configuration settings that you have defined to make sure you followed the preceding series of steps correctly. When you are satisfied, go ahead and click the <strong class="bold">Create Auto Scaling group</strong> button at the bottom of the page.</li>
			</ol>
			<p>AWS will then start configuring your Auto Scaling group and proceed to launch two EC2 instances based on the parameters of the groups. The EC2 instances will be configured as per the configuration you defined in the Launch Configuration earlier.</p>
			<p>Once Auto Scaling has completed the deployment of your EC2 instances, you will be able to see the details of your deployment, as per the following screenshot:</p>
			<div>
				<div id="_idContainer210" class="IMG---Figure">
					<img src="Images/B17124_09_25.jpg" alt="Figure 9.25 – Auto Scaling group deployment completed&#13;&#10;" width="1578" height="776"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.25 – Auto Scaling group deployment completed</p>
			<p>At this point, your <a id="_idIndexMarker1082"/>application has now been deployed across two EC2 instances. If you click on the <strong class="bold">Activity</strong> tab, you will see the AWS Auto Scaling service launched two EC2 instances in response to the fact that the minimum and desired capacity were not met before the launch of those EC2 instances. The Auto Scaling service will always try to ensure you have the desired number of EC2 instances in your fleet. Next, we will review the deployment and access the application.</p>
			<h3>Reviewing your deployment and accessing your application</h3>
			<p>You can check whether the <a id="_idIndexMarker1083"/>Auto Scaling service has correctly deployed your application. Specifically, you can check whether two EC2 instances have been deployed and registered with your ALB. Furthermore, you can also check whether the ALB has marked those EC2 instances as healthy, indicating that the health checks have passed as well.</p>
			<p>Here are the steps to perform these checks and then access the application:</p>
			<ol>
				<li value="1">In the EC2 dashboard, click on the <strong class="bold">Target Groups</strong> link from the left-hand menu under <strong class="bold">Load Balancing</strong>.</li>
				<li>In the right-hand pane, click on the <strong class="source-inline">Production-TG</strong> target group that you created earlier.</li>
				<li>On the details page of the <strong class="source-inline">Production-TG</strong> target group, you will note that two EC2 instances have been launched and both are in a <strong class="bold">healthy</strong> state, as per the following screenshot:<div id="_idContainer211" class="IMG---Figure"><img src="Images/B17124_09_26.jpg" alt="Figure 9.26 – Healthy EC2 instances registered to load balancer target group&#13;&#10;" width="1584" height="860"/></div><p class="figure-caption">Figure 9.26 – Healthy EC2 instances registered to load balancer target group</p></li>
				<li>Next, click<a id="_idIndexMarker1084"/> on the <strong class="bold">Instances</strong> link from the left-hand menu.</li>
				<li>You will notice that two instances with the name <strong class="source-inline">Production-Servers</strong> have been launched, with one EC2 instance in the <strong class="source-inline">us-east-1a</strong> AZ and the other in the <strong class="source-inline">us-east-1b</strong> AZ, as per the following screenshot:<div id="_idContainer212" class="IMG---Figure"><img src="Images/B17124_09_27.jpg" alt="Figure 9.27 – Auto Scaling group successfully launched two EC2 instances&#13;&#10;" width="1298" height="214"/></div><p class="figure-caption">Figure 9.27 – Auto Scaling group successfully launched two EC2 instances</p></li>
				<li>Next, we can<a id="_idIndexMarker1085"/> access our application. From the left-hand menu, click on the <strong class="bold">Load Balancers</strong> link under <strong class="bold">Load Balancing</strong>.</li>
				<li>In the right-hand pane, you will find your ALB details, as per the following screenshot:<div id="_idContainer213" class="IMG---Figure"><img src="Images/B17124_09_28.jpg" alt="Figure 9.28 – ALB details for Production-ALB&#13;&#10;" width="1263" height="699"/></div><p class="figure-caption">Figure 9.28 – ALB details for Production-ALB</p></li>
				<li>In the bottom pane, you will find a <strong class="bold">DNS name</strong> link for your ALB. Copy this URL and paste it into a new browser window. If you have successfully completed all of the previous exercises, you will be able to access the <strong class="bold">Good Deed of the Month Contest</strong> web application, as per the following screenshot:<div id="_idContainer214" class="IMG---Figure"><img src="Images/B17124_09_29.jpg" alt="Figure 9.29 – Good Deed of the Month Contest web application&#13;&#10;" width="1159" height="937"/></div><p class="figure-caption">Figure 9.29 – Good Deed of the Month Contest web application</p></li>
				<li>You can test<a id="_idIndexMarker1086"/> your application by entering details of some potential good deeds you have done yourself. Once you have filled in the form in the middle of the web page, click the <strong class="bold">SUBMIT</strong> button.</li>
				<li>You will note that when the web page reloads after you click on the <strong class="bold">Submit</strong> button, your <em class="italic">good deed of the month</em> is read back from the MySQL RDS database and presented on the page. If you submit more entries, these are also reported back. This demonstrates how the application can write to and read from the <a id="_idIndexMarker1087"/>backend RDS database.</li>
			</ol>
			<p>Let's take another look at the application architectural diagram to see how you have built this multi-tier solution:</p>
			<div>
				<div id="_idContainer215" class="IMG---Figure">
					<img src="Images/B17124_09_30.jpg" alt="Figure 9.30 – Multi-tier application architecture&#13;&#10;" width="1061" height="801"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.30 – Multi-tier application architecture</p>
			<p>The key components of your architecture include the following:</p>
			<ul>
				<li>A VPC in the <strong class="source-inline">us-east-1</strong> Region that consists of public and private subnets across the <strong class="source-inline">us-east-1a</strong> and <strong class="source-inline">us-east-1b</strong> AZs. The VPC consists of six subnets in total: two public subnets to host your ALB nodes and NAT gateway, two private subnets to host your application tier, and another two private subnets to host the database tier. </li>
				<li>You also have an Amazon S3 bucket to host all the application source code and files.</li>
				<li>An IAM role that will allow your EC2 instances to gain authorization to read and download the application source code from the S3 bucket.</li>
				<li>An RDS database deployed in the <strong class="source-inline">us-east-1a</strong> AZ as a single database instance. Ideally, you would want to configure your database with Multi-AZ for HA.</li>
				<li>Two EC2 instances deployed using an Auto Scaling group and Launch Configuration. The Launch Configuration contains the necessary <strong class="bold">bootstrapping</strong> script to set up and configure the EC2 instances as web servers and automatically serve the application to your users. Furthermore, the Auto Scaling group automatically registers any EC2 instances deployed to the ALB target group. The target group runs health checks against the EC2 instances, marking them as healthy or unhealthy based on the health checks defined. </li>
				<li>Finally, the application code has the necessary database connection to access the backend RDS database and store any application data. Note that storing database connection details within the application is not considered best practice, and <a id="_idIndexMarker1088"/>AWS offers several options such as the <strong class="bold">AWS Systems Manager Parameter Store</strong> or <strong class="bold">AWS Secrets Manager</strong> to <a id="_idIndexMarker1089"/>manage such pieces of sensitive data. To keep this lab simple, we stored the database connection details within the application code. </li>
			</ul>
			<p>Next, we look at how to test the AWS Auto Scaling service by simulating a failure of an EC2 instance.</p>
			<h3>Testing the Auto Scaling service</h3>
			<p>In this part of the<a id="_idIndexMarker1090"/> exercise, you will stop an EC2 instance to simulate failure. When the EC2 instance is in a stopped state, it will not respond to the load balancer health checks. The load balancer will then mark the EC2 instances as unavailable. This will send a notification to the Auto Scaling group, confirming that there are fewer than two EC2 instances in the group, which is less than the desired capacity. Auto Scaling should then replace the instance and new server. Let's proceed with simulating this failure of an EC2 instance, as follows:</p>
			<ol>
				<li value="1">In the EC2 dashboard, click on the <strong class="bold">Instances</strong> link from the left-hand menu.</li>
				<li>Next, in the right-hand pane of the screen, you will note that you have two EC2 instances running. Select the instance that is in the <strong class="source-inline">us-east-1b</strong> AZ, as per the following screenshot:<div id="_idContainer216" class="IMG---Figure"><img src="Images/B17124_09_31.jpg" alt="Figure 9.31 – EC2 instances in Running state&#13;&#10;" width="1421" height="198"/></div><p class="figure-caption">Figure 9.31 – EC2 instances in Running state</p></li>
				<li>From the <strong class="bold">Instance state</strong> drop-down list at the top right-hand corner of the screen, select <strong class="bold">Stop instance</strong> while ensuring the EC2 instance in the <strong class="source-inline">us-east-1b</strong> AZ is selected.</li>
				<li>You will be prompted with a dialog box to confirm whether you want to stop the selected instance. Go ahead and click the <strong class="bold">Stop</strong> button.</li>
				<li>AWS will then <a id="_idIndexMarker1091"/>perform a shutdown of your EC2 instance, which will take a couple of minutes. Wait until the EC2 instance is in a <strong class="bold">Stopped</strong> state, and then proceed to click on the <strong class="bold">Auto Scaling Group</strong> link under the <strong class="bold">Auto Scaling</strong> category from the left-hand menu.</li>
				<li>Next, in the right-hand pane, click the <strong class="source-inline">Production-ASG</strong> Auto Scaling group.</li>
				<li>Next, click on the <strong class="bold">Activity</strong> tab.</li>
				<li>You will find additional activities that clearly show that the Auto Scaling service terminated the stopped EC2 instance. This is because, in a stopped state, it cannot respond to health checks. This is then followed by the launch of a new EC2 instance to replace the one that got terminated (see the following screenshot), in order to maintain our desired capacity at two instances as per the Auto Scaling group configuration. You will note that the Auto Scaling service will not try to restart the stopped instance. The Auto Scaling group will use the same Launch Configuration to configure the server with the application and register it to the<a id="_idIndexMarker1092"/> ALB's target group, as illustrated here:<div id="_idContainer217" class="IMG---Figure"><img src="Images/B17124_09_32.jpg" alt="Figure 9.32 – Auto Scaling activity history&#13;&#10;" width="1529" height="360"/></div><p class="figure-caption">Figure 9.32 – Auto Scaling activity history</p></li>
				<li>Now that the Auto Scaling service has replaced your EC2 instance, you can visit your application via the ALB URL to confirm that your application is still functioning as expected. Note that when one of the EC2 instances was stopped, the application was still accessible via the ALB URL because traffic would have been forwarded onto the other EC2 instance that was still running in the <strong class="source-inline">us-east-1a</strong> AZ.</li>
			</ol>
			<p>Congratulations! Well done on completing the series of exercises to get to this stage. You have now learned how to design and architect a multi-tier application solution using a combination of AWS services to help you build an HA and scalable application. </p>
			<p>In the next exercise, you will perform a cleanup operation to terminate unwanted resources so that you do not incur any further charges.</p>
			<h2 id="_idParaDest-243"><a id="_idTextAnchor246"/>Exercise 9.7 – cleanup</h2>
			<p>In this exercise, you will terminate the various resources you deployed in the previous exercises. The first step is to delete the <a id="_idIndexMarker1093"/>Auto Scaling group, which will terminate your EC2 instances. If you try terminating the EC2 instances manually, then the Auto Scaling group will simply launch new ones. Proceed as follows:</p>
			<ol>
				<li value="1">From the EC2 dashboard, click on <strong class="bold">Auto Scaling Group</strong> from the left-hand menu under <strong class="bold">Auto Scaling</strong>.</li>
				<li>From the right-hand pane, select the <strong class="source-inline">Production-ASG</strong> Auto Scaling group. Click the <strong class="bold">Delete</strong> button and confirm the delete request by typing in <strong class="source-inline">delete</strong> in the textbox and clicking the <strong class="bold">Delete</strong> button.</li>
				<li>Next, click on <strong class="bold">Launch Configuration</strong> from the left-hand menu under the <strong class="bold">Auto Scaling</strong> service.</li>
				<li>Next, select the <strong class="source-inline">Production-LC</strong> Launch Configuration, and from the <strong class="bold">Actions</strong> menu, click <strong class="bold">Delete Launch Configuration</strong>. Confirm the delete request.</li>
				<li>Next, click <strong class="bold">Load Balancers</strong> under the <strong class="bold">Load Balancing</strong> menu. </li>
				<li>From the right-hand pane, select the <strong class="source-inline">Production-ALB</strong> load balancer, and from the <strong class="bold">Actions</strong> drop-down list, click <strong class="bold">Delete</strong>.</li>
				<li>Next, click on <strong class="bold">Target Groups</strong> under <strong class="bold">Load Balancing</strong> in the left-hand menu. In the right-hand pane, select the <strong class="source-inline">Production-TG</strong> target group, and from the <strong class="bold">Actions</strong> drop-down list, click <strong class="bold">Delete</strong> and delete the target group.</li>
			</ol>
			<p>Your load balancer and the Auto Scaling group have been removed from your account. Next, navigate to the Amazon RDS console, as follows:</p>
			<ol>
				<li value="1">From the left-hand menu, click on <strong class="bold">Databases</strong>.</li>
				<li>In the right-hand pane, select the database that you created earlier in <a href="B17124_08_Final_SK_ePub.xhtml#_idTextAnchor189"><em class="italic">Chapter 8</em></a>, <em class="italic">AWS Databases Services</em>.</li>
				<li>From the <strong class="bold">Actions</strong> drop-down list, click <strong class="bold">Delete</strong>.</li>
				<li>Uncheck the <strong class="bold">Create final snapshot?</strong> box and click the acknowledgment box that states that upon deletion, automated backups, including system snapshots <a id="_idIndexMarker1094"/>and <strong class="bold">point-in-time recovery</strong> (<strong class="bold">PITR</strong>), will no longer be available. Next, type <strong class="source-inline">delete me</strong> in the confirmation textbox and click the <strong class="bold">Delete</strong> button. Your Amazon RDS database will now be deleted.</li>
				<li>Next, we should <a id="_idIndexMarker1095"/>also remove the database subnet group created previously. From the left-hand menu, click on <strong class="bold">Subnet groups</strong>.</li>
				<li>In the right-hand pane, select the database subnet group you created previously and click the <strong class="bold">Delete</strong> button.<p>Now that your database has also been deleted, we can delete the VPC. Navigate to the VPC console.</p></li>
				<li>Before we can delete the VPC, you need to delete the NAT gateway. From the left-hand menu, click on <strong class="bold">NAT Gateways</strong>. In the right-hand pane, select the <strong class="source-inline">Production-NAT</strong> NAT gateway, and from the <strong class="bold">Actions</strong> drop-down list, click <strong class="bold">Delete NAT gateway</strong>. You will then be presented with a dialog box to confirm the deletion. Type <strong class="source-inline">delete</strong> in the confirmation box and click the <strong class="bold">Delete</strong> button.</li>
				<li>Next, you need to check whether there are any network interfaces still attached to your VPC. Usually, you will find that the Amazon RDS network interface (<strong class="source-inline">RDSNetworkInterface</strong>) may still be attached to the <strong class="source-inline">Database-SG</strong> security group. If that is the case, you will first need to delete this interface before you can delete the VPC, as follows:<ol><li>Navigate to the EC2 console and select <strong class="bold">Network Interfaces</strong> from the left-hand menu.</li><li>Check whether there are any interfaces still attached to your VPC by cross-referencing the VPC ID with your <strong class="source-inline">Production-VPC</strong> security group ID. Select the network interface and then, from the <strong class="bold">Actions</strong> menu, click the <strong class="bold">Delete</strong> button. The following screenshot shows the attached network interface:</li></ol></li>
			</ol>
			<div>
				<div id="_idContainer218" class="IMG---Figure">
					<img src="Images/B17124_09_33.jpg" alt="Figure 9.33 – Network interfaces attached to your VPC&#13;&#10;" width="1140" height="172"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.33 – Network interfaces attached to your VPC</p>
			<ol>
				<li value="9">Navigate back to the VPC console. Next, from the left-hand menu, click on <strong class="bold">Your VPCs</strong>.</li>
				<li>From the <a id="_idIndexMarker1096"/>right-hand pane, select the <strong class="source-inline">Production-VPC</strong> security group, and then, from the <strong class="bold">Actions</strong> drop-down list, select <strong class="bold">Delete VPC</strong>.</li>
				<li>You will be presented with a list of all components of your VPC that will be deleted. Confirm your delete request by typing <strong class="source-inline">delete</strong> into the confirmation textbox and then clicking the <strong class="bold">Delete</strong> button.</li>
			</ol>
			<p>Your VPC should now get deleted. Within the VPC console, there is still one more component you need to delete, and that is the elastic IP address you allocated to your AWS account. This is because elastic IP addresses are only free if they are associated with running instances (or in our case, the NAT gateway). Proceed as follows:</p>
			<ol>
				<li value="1">From the left-hand menu, click on <strong class="bold">Elastic IPs</strong>.</li>
				<li>In the right-hand pane, select the IP address you allocated to your AWS account, and from the <strong class="bold">Actions</strong> drop-down list, click the <strong class="bold">Release Elastic IP addresses</strong> link. Next, in the <strong class="bold">Release Elastic IP addresses</strong> dialog box, click the <strong class="bold">Release</strong> button. </li>
			</ol>
			<p>At this point, your elastic IP address has been released back to AWS. You will not incur any charges on unused elastic IP addresses in your account.</p>
			<p>This completes your cleanup exercise for this chapter, and you can now rest assured that you will not incur any further costs associated with this lab. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">You still have an Amazon S3 bucket that hosts all the source code for the application you deployed in this chapter. While you could delete that resource, we advise you to keep the bucket as we will be using it for the exercises in the next chapter.</p>
			<p>Next, we provide a summary of this chapter and the key concepts to remember for the exam. </p>
			<h1 id="_idParaDest-244"><a id="_idTextAnchor247"/>Summary</h1>
			<p>In this chapter, you learned about the differences between vertical scaling and horizontal scaling. We discussed options to increase an EC2 instance's specification and capacity. We then examined the AWS ELB service and how it can be used to evenly distribute incoming application traffic across a fleet of EC2 instances. You learned about the different types of ELBs and their use cases—ALBs, NLBs, GWLBs, and CLBs. We discussed how, using ELB, you can distribute the placement of EC2 instances that power your application across multiple AZs, thereby offering HA of services in case of AZ failures or outages.</p>
			<p>Next, we examined how we can automatically scale out (add more EC2 instances to our fleet of servers that support an application) using the Amazon Auto Scaling service. Auto Scaling can help us scale out when demand increases and equally scale back in when demand drops, ensuring that you always have the right number of EC2 instances to provide the best <strong class="bold">user experience</strong> (<strong class="bold">UX</strong>) for your application.</p>
			<p>Both ELB and Auto Scaling, however, are Regional services only. This means using these two services alone cannot offer global resilience. To offer HA across Regions, we discussed how we can use Route 53 and other global services such as CloudFront. Route 53 offers several routing policies to make it possible to distribute traffic to application servers spread across the globe and offers options to build complete <strong class="bold">disaster recovery</strong> (<strong class="bold">DR</strong>) and business continuity solutions for your business.</p>
			<p>In the next chapter, we look at a number of AWS services designed to help you build applications on AWS that move away from traditional monolith architectures in favor of modern decoupled architecture and microservices.</p>
			<h1 id="_idParaDest-245"><a id="_idTextAnchor248"/>Questions</h1>
			<ol>
				<li value="1">You are planning on developing a website in multiple languages such that you have one fleet of EC2 instances that serves the English version of your site and another fleet that serves the Spanish version of your site. For each language version, you will be configuring URLs with different paths such that the English version of your site will contain <strong class="source-inline">/en/</strong> in the path and the Spanish version will contain <strong class="source-inline">/es/</strong>.<p>Which type of load balancer would you use to route traffic to ensure users connect to the site in their desired language?</p><ol><li>CLB</li><li>NLB</li><li>ALB</li><li>Path-based load balancer</li></ol></li>
				<li>You are building a multi-tier architecture with web servers placed in the public subnet and application servers placed in the private subnet of your VPC. You need to deploy ELBs to distribute traffic to both the web server farm and the application server farm. Which type of load balancer would you choose to distribute traffic to your application servers?<ol><li>Internet-facing</li><li>Internal load balancer</li><li>Dynamic load balancer</li><li>Static load balancer</li></ol></li>
			</ol>
			<ol>
				<li value="3">Which ELB is ideal for handling volatile workloads and can scale to millions of requests per second?<ol><li>ALB</li><li>NLB</li><li>CLB</li><li>Premium load balancer</li></ol></li>
				<li>Which configuration feature of the AWS Auto Scaling service enables you to define a maximum number of EC2 instances that can be launched in your fleet?<ol><li>Auto Scaling group</li><li>Auto Scaling Launch Configuration</li><li>Auto Scaling max fleet size</li><li>Auto Scaling policy</li></ol></li>
				<li>When an ELB detects an unhealthy EC2 instance, which action does it perform regarding distributing incoming traffic?<ol><li>It continues to send traffic to the failed instance.</li><li>It terminates the failed instance so that it is not part of the ELB target group.</li><li>It only sends traffic to the remaining healthy instances.</li><li>It restarts the unhealthy EC2 instance.</li></ol></li>
				<li>Which service does an AWS ALB integrate with to protect your applications from common web attacks?<ol><li>WAF</li><li>Shield</li><li>Inspector</li><li><strong class="bold">Key Management Service</strong> (<strong class="bold">KMS</strong>)</li></ol></li>
			</ol>
		</div>
	</div></body></html>