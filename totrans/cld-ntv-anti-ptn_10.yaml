- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Observing Our Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the cloud native landscape, observability and incident management are often
    treated as secondary concerns until they no longer are. All too often, organizations
    only realize the importance of proper monitoring and response processes when an
    unexpected outage or performance issue brings everything to a halt. The damage
    is usually already done by that point: trust is shaken, financial losses accrue,
    and teams are left scrambling to repair systems and reputations. This chapter
    delves into the common pitfalls, or anti-patterns, that cloud native organizations
    encounter when scaling their architectures without giving observability the attention
    it demands.'
  prefs: []
  type: TYPE_NORMAL
- en: Knowing something is wrong in today’s complex ecosystems is not enough. We need
    to know where it’s going wrong, how it’s affecting our services, and potential
    downstream impacts. Moreover, observability can no longer be purely reactive;
    with the advent of advanced services powered by machine learning (ML) and artificial
    intelligence (AI), organizations can now predict incidents before they happen
    and identify anomalies before they evolve into critical issues. This proactive
    approach is essential for organizations navigating the ever-increasing complexity
    of hybrid workloads, microservices architectures, and multi-cloud environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will explore several anti-patterns in cloud native observability
    and incident management and the practical remediations that can help overcome
    these challenges. These include:'
  prefs: []
  type: TYPE_NORMAL
- en: Incomplete Observability Coverage for Distributed Tracing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lack of Real-Time Monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ignoring Out-of-the-Box ML Features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Failure to Implement Holistic Views for Distributed Tracing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not Splitting Impact and Diagnostic Metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we will detail these anti-patterns, providing actionable
    strategies and remediations to help organizations develop a robust observability
    framework. Addressing these common pitfalls will give you the clarity needed to
    maintain operational excellence and avoid potential issues in even the most complex
    cloud native environments.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s Capture Everything in the Log Aggregator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section begins by exploring the risks of Incomplete Observability Coverage
    for Distributed Tracing, followed by the critical need for Real-Time Monitoring
    to ensure timely detection and response. The first instinct for many organizations
    new to cloud native architectures is to collect as much data as possible.
  prefs: []
  type: TYPE_NORMAL
- en: “*Let’s capture everything*,” they say as if the sheer volume of logs will magically
    make everything clear. Unfortunately, this mindset often leads to operational
    chaos rather than clarity. When capturing logs, Log aggregation tools can be powerful
    allies, but only when used with purpose. Capturing every log entry from every
    system, service, and application into a single, all-encompassing aggregator may
    sound ideal, but it quickly becomes unmanageable. What begins as a noble attempt
    to enhance visibility is a quagmire of irrelevant data, burying critical signals
    needed to troubleshoot issues under a mountain of logs with no practical value.
  prefs: []
  type: TYPE_NORMAL
- en: Take for example a fluentbit or fluentd, great tools to capture logs but without
    filtering said logs, the thousands upon million logs that can be present, are
    impossible to discphier.
  prefs: []
  type: TYPE_NORMAL
- en: Why Indiscriminate Logging Fails
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Indiscriminate logging** assumes all data is equally important, but not all
    logs are created equal. Some data is essential for diagnosing system health or
    debugging issues, while other logs are merely noise. Logging every heartbeat of
    a service might seem helpful, but wading through thousands of heartbeat logs during
    an issue diagnosis is counterproductive.'
  prefs: []
  type: TYPE_NORMAL
- en: Take for example logging an OK status vs WARN/FAIL status in an app. The sheer
    amount of OK may be considered noise to some, costing more than it’s worth in
    cloud storage. This approach inflates operational costs, as cloud storage and
    processing are not free, and logging everything can quickly become a financial
    burden. More data means more processing power is required to analyze it, leading
    to escalating costs and diminishing returns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Smart Logging: A More Effective Approach'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Organizations need to be deliberate about what they log instead of capturing
    everything. Log retention should be front and center; however, the key is to focus
    on actionable data logs relevant to business-critical operations or that provide
    insight into system health. Setting log levels (e.g., DEBUG, INFO, WARN, ERROR)
    appropriately helps filter out unnecessary data, ensuring only meaningful information
    is captured.
  prefs: []
  type: TYPE_NORMAL
- en: 'Context is also key: logs should be structured to trace issues across different
    services and environments. Capturing metadata such as request IDs, user sessions,
    or transaction IDs helps stitch logs into a coherent narrative. Tools like AWS
    CloudWatch Logs Insights or Datadog can be used for centralized log management
    and visualization, reducing noise and prioritizing critical issues. This allows
    organizations to maintain operational efficiency and quickly resolve incidents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Smart Logging: An Example'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider a cloud native e-commerce application hosted on AWS. The system consists
    of several microservices: a user service for handling authentication, a product
    service for managing inventory, a payment service for processing transactions,
    and a delivery service. The application handles millions of daily requests, so
    effective logging is essential to maintain performance and troubleshoot issues
    quickly. Here is how smart logging can be applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '`DEBUG`: During development, the engineering team enables `DEBUG` logs to capture
    detailed information about user authentication, such as API calls to AWS Cognito.
    In production, `DEBUG` logs are disabled to avoid unnecessary clutter.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`INFO`: In the product service, `INFO` logs capture successful actions like
    a user adding an item to their cart or completing an order. For instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'WARN: When a transient issue arises, such as a timeout during a payment request
    to AWS RDS, a WARN log is generated:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'user_id`, `session_id`, `transaction_id`, and `request_id`. For example, the
    payment service logs may include:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'ERROR: Delivery failed - transaction_id=txn001, delivery_id=delv789, error_code=DELIV_ERRORte,'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: from opentelemetry import metrics
  prefs: []
  type: TYPE_NORMAL
- en: from opentelemetry.sdk.metrics import MeterProvider
  prefs: []
  type: TYPE_NORMAL
- en: from opentelemetry.sdk.metrics.export import ConsoleMetricExporter, PeriodicExportingMetricReader
  prefs: []
  type: TYPE_NORMAL
- en: Set up MeterProvider and Metric Exporter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: provider = MeterProvider()
  prefs: []
  type: TYPE_NORMAL
- en: metrics.set_meter_provider(provider)
  prefs: []
  type: TYPE_NORMAL
- en: exporter = ConsoleMetricExporter()
  prefs: []
  type: TYPE_NORMAL
- en: reader = PeriodicExportingMetricReader(exporter)
  prefs: []
  type: TYPE_NORMAL
- en: provider.add_metric_reader(reader)
  prefs: []
  type: TYPE_NORMAL
- en: Create a meter for recording metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: meter = metrics.get_meter(__name__)
  prefs: []
  type: TYPE_NORMAL
- en: 'Define metrics: Impact and Diagnostic'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: impact_latency = meter.create_histogram("impact_latency", unit="ms", description="Request
    Latency")
  prefs: []
  type: TYPE_NORMAL
- en: diagnostic_cpu_usage = meter.create_observable_gauge("diagnostic_cpu_usage",
    description="CPU Usage")
  prefs: []
  type: TYPE_NORMAL
- en: Function to simulate recording of impact metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'def record_impact_metrics(latency_value):'
  prefs: []
  type: TYPE_NORMAL
- en: impact_latency.record(latency_value)
  prefs: []
  type: TYPE_NORMAL
- en: 'print(f"Recorded impact latency: {latency_value}ms")'
  prefs: []
  type: TYPE_NORMAL
- en: Function to observe diagnostic metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'def observe_diagnostic_metrics():'
  prefs: []
  type: TYPE_NORMAL
- en: import psutil
  prefs: []
  type: TYPE_NORMAL
- en: return psutil.cpu_percent(interval=None)
  prefs: []
  type: TYPE_NORMAL
- en: Register diagnostic metric callback
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: meter.create_observable_gauge("diagnostic_cpu_usage", callbacks=[observe_diagnostic_metrics])
  prefs: []
  type: TYPE_NORMAL
- en: Simulating metric recording
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: record_impact_metrics(120)  # Simulating a latency of 120ms
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: 'Key Points to observe in the code are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`impact_latency histogram` tracks the length of a request, a key metric for
    user experience. In this example, we record a latency of 120ms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`diagnostic_cpu_usage` observable gauge monitors CPU usage. We use the ``psutil``
    library to gather CPU statistics, a proper diagnostic metric to understand system
    resource usage during incidents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By collecting and analyzing impact and diagnostic metrics, teams can quickly
    detect performance issues while gathering the information necessary to diagnose
    and resolve root causes. This combined approach ensures that cloud native systems
    remain resilient and performant, even under pressure.
  prefs: []
  type: TYPE_NORMAL
- en: Proper metrics dictate the relevance of results; the next section will go into
    a real-world scenario of what happens when we neglect distributed tracing.
  prefs: []
  type: TYPE_NORMAL
- en: Real World Consequences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During a recent engagement with a leading e-commerce retailer, we were called
    in to address significant performance issues that emerged during a high-traffic
    sale. The retailer’s microservices architecture managed critical operations like
    inventory management and payment processing, but their observability was fragmented.
    Relying solely on logs and metrics from individual services, they couldn’t trace
    transactions end-to-end, making it impossible to quickly identify the source of
    latency when checkout times began to slow down under the increased load. Hours
    into the incident, we implemented distributed tracing, immediately revealing a
    third-party payment API as the bottleneck causing the delays.
  prefs: []
  type: TYPE_NORMAL
- en: Introducingm distributed tracing gave the retailer real-time visibility into
    the entire transaction flow across all microservices. The integration of this
    tracing allowed the operations team to pinpoint and resolve issues much faster,
    avoiding prolonged outages that could have been identified within minutes instead
    of hours. Our intervention reduced downtime and restored customer trust by ensuring
    that future peak traffic periods would be handled with better performance monitoring
    and faster response times. The image below shows a simple Kubernetes microservice
    followed by a list of issue we had discovered
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22364_10_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 - Simple Kubernetes Microservice
  prefs: []
  type: TYPE_NORMAL
- en: 'Key issues we identified included:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lack of end-to-end transaction visibility**: The system couldn’t trace the
    customer journey through microservices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Isolated logs and metrics**: Metrics were limited to individual services
    and offered no insight into how they interacted in a transaction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unmonitored third-party dependencies**: The third-party API causing the delays
    was not adequately monitored, making it invisible until distributed tracing was
    implemented.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Slow incident response times**: Without distributed tracing, the team relied
    on manual troubleshooting, prolonging the resolution of critical issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**To address these challenges, organizations need a comprehensive observability
    strategy that integrates distributed tracing, centralized logging, and robust
    monitoring across all system components. The list below provides** **more context:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Implemented Distributed Tracing**: We deployed OpenTelemetry as our tracing
    solution, providing end-to-end visibility of transactions across the microservices
    architecture. This allowed the team to detect bottlenecks immediately, including
    the slow third-party API causing checkout delays. The key to consistency was to
    package OpenTelemtry SDK (in this case, python) with all code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhanced Log Aggregation**: We integrated the tracing system with CloudWatch
    for centralized log aggregation, ensuring all logs from the various microservices
    were collected and searchable in one place. This improved the team’s ability to
    correlate logs with tracing data, making incident detection and diagnosis faster
    and more accurate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-Time Monitoring and Alerts**: We set up monitoring dashboards and real-time
    alerts using Prometheus and Grafana, configuring them to display system health,
    performance metrics, and traces. This gave the operations team proactive insights
    into system performance, reducing their reliance on reactive troubleshooting during
    high-traffic events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring of Third-Party Dependencies**: We established specific monitors
    for third-party APIs, which included setting latency thresholds and failure rate
    alerts. This ensured that external dependencies were continuously tracked for
    performance degradation, preventing them from silently becoming bottlenecks in
    the future.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By leveraging these tools and strategies, we improved the retailer’s system
    visibility and reduced their response times to critical incidents. These solutions
    can be replicated using equivalent services in the major cloud providers, such
    as AWS XRay + CloudWatch, Azure Application Insights, and Google Cloud Operations
    Suite.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss best practices and what to consider when building distributed
    tracing to tie together all we have learned up to this point.
  prefs: []
  type: TYPE_NORMAL
- en: Best Practices for Resolving Tracing Neglect
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neglecting distributed tracing is like trying to navigate a complex city with
    incomplete maps; inevitably, you’ll get lost. End-to-end tracing acts as your
    GPS, connecting the dots between microservices, identifying bottlenecks, and illuminating
    paths that might otherwise remain hidden in the shadows of fragmented logs. Distributed
    tracing must be treated as a foundational practice, not an afterthought, to ensure
    a cloud native system remains agile, scalable, and responsive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following when applying building tracing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Start Early and Be Consistent**: One of the most critical best practices
    is implementing tracing from the beginning. Retroactively adding tracing to an
    existing architecture is akin to patching holes in a sinking ship; it is inefficient
    and prone to leaving gaps. Consistency is key; every microservice, dependency,
    and path that requests take through your system should be covered by tracing.
    This ensures that no part of the system is left in the dark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trace the Entire Journey**: Successful tracing doesn’t stop at the surface.
    It needs to span the entire request lifecycle, from the user-facing frontend through
    the backend services and to external dependencies like third-party APIs. This
    complete visibility ensures that any latency or failure can be tracked down to
    its root cause, whether it is inside your system or external to it. The result
    is faster diagnostics, quicker resolutions, and fewer headaches when incidents
    arise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integrate with Metrics and Logs**: Tracing is robust, but it becomes a force
    multiplier when combined with metrics and logs. Use impact metrics to identify
    when user experience is degrading and diagnostic metrics to figure out why. Aggregating
    all three pillars of observability, tracing, metrics, and logs creates a comprehensive
    view of your system, empowering teams with the insights they need to stay proactive
    rather than reactive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automate and Set Guardrails**: Manually instrumenting every service for tracing
    can be daunting, especially in dynamic, cloud native environments. Automation
    tools and frameworks like OpenTelemetry simplify this process, ensuring that tracing
    is baked into every new microservice or deployment by default. Setting automated
    guardrails for latency thresholds, error rates, and critical service performance
    can trigger alerts before incidents spiral out of control.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitor What Matters Most**: Not every trace, log, or metric needs to be
    treated equally. Prioritize tracing for the critical paths within your system,
    such as user transactions, payment flows, or any service that directly impacts
    customer experience. This focus ensures that your team’s attention is on what
    matters most while still keeping an eye on the broader system’s health.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adhering to these best practices can help traditional organizations transform
    distributed tracing from a reactive tool into a proactive asset when shifting
    to cloud native. When tracing is holistic and integrated, the result is a cloud
    native architecture that is resilient, transparent, and able to meet the demands
    of modern applications. While tracing forms the backbone of a well-functioning
    cloud native system, the actual test of resilience lies in how an organization
    responds when things go wrong. Even with the best tracing practices, the system’s
    ability to recover and maintain stability is compromised without a mature process
    for handling alerts and incidents.
  prefs: []
  type: TYPE_NORMAL
- en: In the final section of this chapter, we will explore how immature processes
    for alerts and incidents can undermine even the most robust architectures and
    how addressing these shortcomings is essential for sustaining operational excellence
    in cloud native environments.
  prefs: []
  type: TYPE_NORMAL
- en: Immature processes for alerts & incidents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While offering agility and scalability, cloud native organizations can often
    suffer from immature processes when handling alerts and incidents. In environments
    that manage thousands of microservices, the noise from redundant alerts, incomplete
    observability setups, and ineffective incident response protocols can overwhelm
    teams. As organizations modernize their infrastructure, they often forget a fundamental
    truth: **alerting and incident management are not about gathering all available
    metrics but focusing on the right metrics, responding to the right signals, and
    ensuring these processes function smoothly across all environments, not just**
    **in production**.'
  prefs: []
  type: TYPE_NORMAL
- en: At the heart of many cloud native failures is a “*collect everything*” mindset,
    gathering every possible metric and sending alerts for every anomaly. This approach
    often leads to chaos, leaving engineering and operations teams drowning in data
    without actionable insights. The issue is not a lack of metrics; it’s the absence
    of purposeful, well-aligned metrics and alerts. By understanding the dangers of
    metric dumping, we can become more cautious and aware, ensuring that every alert
    has a clear purpose, and each metric gathered should address a specific use case
    tied to both business and technical objectives.
  prefs: []
  type: TYPE_NORMAL
- en: This section will provide a guide through building a mature, effective system
    for alerting and incident response, highlighting common pitfalls and strategies
    for overcoming them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Purpose-driven metrics and alerts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The trap of metric dumping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shifting left in observability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alert fatigue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incident response maturity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These topics will help develop a resilient, proactive approach to alerting and
    incident management, enabling teams to respond swiftly and intelligently when
    challenges arise.
  prefs: []
  type: TYPE_NORMAL
- en: Purpose-driven Metrics and Alerts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The sheer volume of metrics available can be overwhelming. However, not every
    metric is useful, and collecting everything without a clear purpose, leads to
    noise, alert fatigue, and inefficiency. Purpose-driven metrics focus on gathering
    data that aligns directly with specific business or technical objectives, ensuring
    that every metric serves a tangible purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'Metrics should be chosen based on their ability to provide actionable insights,
    not just because they’re easy to collect. For example, instead of gathering CPU
    usage for all instances, consider: *“What are we trying to achieve by monitoring
    this? Are we looking to understand performance under load? Predict infrastructure
    scaling needs? Optimize resource utilization?*” Once the goal is clear, we can
    design metrics and alerts that align with it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Purpose-driven Metrics and Alerts: An Example'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For instance, let’s consider a microservices-based e-commerce platform. One
    critical business objective is ensuring a seamless customer checkout experience.
    In this case, purpose-driven metrics could include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latency of the checkout service**: This metric tracks the time it takes for
    the checkout process to complete, directly impacting user experience. The business
    goal is to ensure customers aren’t abandoning their carts due to slow performance.
    The associated alert could trigger when latency exceeds a defined threshold, prompting
    engineers to investigate slowdowns before they impact many users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error rate during payment processing**: Monitoring this metric helps ensure
    that failed payment transactions don’t disrupt sales. Instead of sending alerts
    for every failed transaction, an alert might be triggered when the error rate
    surpasses a certain percentage over a specific time window, allowing teams to
    differentiate between minor blips and real issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inventory update delays**: If inventory updates after sales are delayed,
    this can result in overselling, impacting revenue and customer satisfaction. A
    targeted metric around the time it takes to sync inventory after an order is placed
    serves a business goal of operational accuracy. Alerts are triggered if these
    delays exceed acceptable limits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By defining metrics like these, we do not just collect data for the sake of
    it. Instead, we ensure that each metric serves a well-defined purpose, allowing
    teams to focus on what truly matters to the business. Now that we’ve seen the
    value of purpose-driven metrics, we must avoid the opposite approach, which we
    call the “metric dumping” trap.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will examine the trap of metric dumping and how it can derail even
    the best-intentioned cloud native monitoring strategies.
  prefs: []
  type: TYPE_NORMAL
- en: The Trap of Metric Dumping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the rush to embrace monitoring tools and gather insights, many organizations
    fall into the trap of metric dumping. This occurs when every possible metric is
    collected without considering its value or purpose. On the surface, this might
    seem like a way to guarantee complete visibility. However, it leads to data overload,
    alert fatigue, and reduced system performance, making it harder for teams to respond
    to critical issues promptly. **Metric dumping** is the process of collecting every
    available metric, whether it’s CPU usage, memory, network latency, or disk I/O,
    without considering how these metrics will be used or whether they contribute
    to achieving business goals. Teams may believe that collecting more data gives
    them more control and insight.
  prefs: []
  type: TYPE_NORMAL
- en: 'Metric Dumping: An Example'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For example, imagine an organization that monitors the CPU usage of every instance
    across hundreds of microservices, regardless of whether CPU usage is relevant
    to the service’s performance. They collect this data at a highly granular level
    (every second), even though the service has no history of CPU-related performance
    issues. Over time, this approach generates vast amounts of data that clog dashboards,
    increase storage costs, and create an alert system constantly firing off non-critical
    warnings. This is a classic case of metric dumping, collecting more data than
    is necessary or actionable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Metric dumping creates two significant problems that slow down operations:
    operational inefficiency and alert fatigue. Here’s how these issues manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Operational Inefficiency**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The flood of unnecessary metrics slows down monitoring systems, as every metric
    must be processed, stored, and analyzed.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This results in massive data pipelines that consume more resources than needed.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Engineers and operations teams must sift through irrelevant data, making it
    harder to find important insights.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Critical alerts may be delayed, buried, or missed, potentially allowing issues
    to escalate into outages.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alert Fatigue**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring too many metrics leads to excessive alerts, overwhelming teams with
    a stream of non-critical notifications.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Over time, teams become desensitized, increasing the risk of missing or ignoring
    truly important alerts.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This reduces team effectiveness and increases the likelihood of system downtime,
    longer recovery times, and a negative impact on customer experience.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s how you can move away from metric dumping and toward a more focused,
    efficient monitoring strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Action** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| **Define Clear Business and** **Technical Goals** | Ensure every metric has
    a well-defined purpose. Start by asking, “What business or operational problem
    are we trying to solve?” Collect metrics supporting these objectives. |'
  prefs: []
  type: TYPE_TB
- en: '| **Prioritize** **Actionable Metrics** | Focus on metrics that provide actionable
    insights. Avoid collecting data just because it’s available. Ensure metrics help
    the team make decisions or take action. |'
  prefs: []
  type: TYPE_TB
- en: '| **Regularly Review and** **Prune Metrics** | Periodically audit the metrics
    being collected. Retire those that are no longer relevant, reducing noise and
    keeping the monitoring system efficient. |'
  prefs: []
  type: TYPE_TB
- en: '| **Create** **Threshold-based Alerts** | Design alerts that trigger only when
    critical thresholds are crossed. This reduces unnecessary alerts and helps teams
    focus on the most important issues. |'
  prefs: []
  type: TYPE_TB
- en: '| **Use** **Aggregated Metrics** | Aggregate metrics to get a high-level view,
    avoiding excessive granularity. Monitor averages over time to identify meaningful
    patterns and reduce noise. |'
  prefs: []
  type: TYPE_TB
- en: '| **Focus on Key Performance** **Indicators (KPIs)** | Align metrics with KPis
    that measure the health and performance of critical systems, like user experience,
    transaction success rates, and service latency. |'
  prefs: []
  type: TYPE_TB
- en: Table 10.5 - Metric dumping resolution table
  prefs: []
  type: TYPE_NORMAL
- en: By following these steps, you can eliminate the inefficiencies of metric dumping
    and create a streamlined monitoring system that delivers clear, actionable insights.
    This will improve response times, reduce alert fatigue, and enable teams to focus
    on the most critical aspects of your cloud native environment. The next section
    will dive into the observability side by addressing *Shifting left* *in Observability*.
  prefs: []
  type: TYPE_NORMAL
- en: Shifting Left in Observability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most overlooked aspects of cloud native observability is the failure
    to extend monitoring and alerting into the early stages of the software development
    lifecycle(SDLC). This oversight leads to an anti-pattern where full observability
    is treated as a production-only concern. In cloud native environments, where microservices
    sprawl and deployment velocity are high, waiting until production to catch issues
    is akin to letting the fuse burn down on a bomb. This is where the practice of
    shifting left comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: Shifting left in observability means embedding monitoring, alerting, and diagnostics
    into earlier environments, such as development, testing, and **UAT** (**User Acceptance
    Testing**), instead of waiting until the code reaches production. By doing so,
    organizations can catch performance bottlenecks, scaling issues, or misconfigurations
    earlier, long before they disrupt production services or, worse, customers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine a scenario in which a cloud native e-commerce application is being
    deployed. In production, the company uses a robust observability platform like
    Prometheus and Grafana for monitoring and alerting system health. However, in
    pre-production environments, like staging or UAT, there’s only a basic setup:
    maybe some logs or simple uptime monitoring.'
  prefs: []
  type: TYPE_NORMAL
- en: This means that while the application undergoes various stages of testing, critical
    performance metrics such as API latency or resource saturation are not being monitored.
    The development team is unaware that under load, a particular microservice starts
    exhibiting high latency after a specific number of concurrent users. This issue
    only surfaces once the application is live in production, where latency spikes
    impact real users, leading to a scramble to mitigate the issue under the stress
    of a live incident.
  prefs: []
  type: TYPE_NORMAL
- en: Had observability been shifted left, this problem could have been identified
    much earlier. With the right metrics in place, developers would have seen that
    API latency gradually degrades with increasing load during the load-testing phase,
    allowing them to resolve the issue before production deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key to shifting left in observability is realizing that monitoring is crucial
    in all environments, not just production. Here’s how to start:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Action** | **Description** | **Benefits of** **Shifting Left** |'
  prefs: []
  type: TYPE_TB
- en: '| **Instrument Early** | Add monitoring and tracing from the start, ensuring
    every feature or service has observability baked in during development across
    all environments (development, Cl, staging). | - Early issue detection- Improved
    developer ownership |'
  prefs: []
  type: TYPE_TB
- en: '| **Monitor** **Load Tests** | Treat pre-production load tests like production.
    Use tools like Grafana or New Relic to monitor API performance, memory, and throughput
    to identify bottlenecks early. | - Early issue detection- Reduced cost of failure
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Set Alerts in** **Lower Environments** | Implement alerts for critical
    issues (e.g., rising error rates, abnormal latency) in testing phases to address
    issues before they hit production. | - Faster time to resolution- Reduced cost
    of failure |'
  prefs: []
  type: TYPE_TB
- en: '| **Use** **Distributed Tracing** | Apply distributed tracing in non-production
    environments to identify inefficient paths and bottlenecks, providing developers
    insights for fixing issues before they escalate. | - Faster time to resolution-
    Improved developer ownership |'
  prefs: []
  type: TYPE_TB
- en: Table 10.6 - Starting with Instrumentation
  prefs: []
  type: TYPE_NORMAL
- en: In summary, shifting left in observability transforms it from a reactive, production-focused
    practice into a proactive, holistic approach that safeguards the entire lifecycle
    of cloud native applications. By investing in observability early, you significantly
    reduce the likelihood of surprises in production, ensuring that your cloud native
    architecture can scale and perform reliably under any conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Alert Fatigue & Incident Response Maturity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In cloud native environments, alert fatigue is the silent enemy, creeping in
    when teams are bombarded with endless notifications, many of which signal minor
    issues or false alarms. This constant noise desensitizes even the most vigilant
    engineers, causing critical alerts to be overlooked or delayed. In the worst cases,
    teams may become so accustomed to low-priority alerts that they miss the ones
    that matter most. Incident response maturity, on the other hand, is the antidote,
    a reflection of a team’s ability to manage alerts efficiently, triage effectively,
    and resolve issues with precision and speed.
  prefs: []
  type: TYPE_NORMAL
- en: But how do you avoid drowning in a sea of alerts? And more importantly, how
    do you transform alert chaos into a streamlined, mature incident response process?
  prefs: []
  type: TYPE_NORMAL
- en: How to Combat Alert Fatigue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Prioritize Critical Alerts**: Focus only on the alerts that genuinely matter,
    those that directly impact system performance and customer experience. If an alert
    doesn’t require immediate action, it’s just noise. Keep the signal clear.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Set Thoughtful Thresholds**: Alerts should trigger only when critical thresholds
    are crossed. A minor fluctuation in CPU usage shouldn’t send your team into a
    frenzy. Design alerting rules that capture significant changes and filter out
    the minor blips.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Group Related Alerts**: When multiple alerts fire for the same underlying
    issue, it’s easy to get overwhelmed. By grouping related alerts into a single,
    actionable notification, you reduce the clutter and give your team a clearer picture
    of the problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tune and Retune**: Alerting rules aren’t “set it and forget it.” Regularly
    revisit and adjust them to keep pace with evolving system behavior. What was important
    six months ago may not be relevant now. Prune the dead weight.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automate the Repetitive**: For known, recurring issues, automate the resolution.
    If a problem can be fixed by a script, there’s no need to wake a human at 3 a.m.
    Automation reduces the volume of alerts and keeps your team focused on what really
    matters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Post-Incident Review**: After an incident, analyze which alerts helped and
    which ones added noise. Use these lessons to refine your system further, ensuring
    that next time, your alerts are sharper and more accurate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By focusing on what truly matters and continuously refining your alerting processes,
    you shift from reactive firefighting to proactive, thoughtful incident management.
    This is the path to incident response maturity: where every alert has a purpose,
    every response is swift, and the system becomes resilient. As alert fatigue fades,
    what’s left is a finely tuned machine, one that runs smoothly, efficiently, and
    with the confidence that when something does go wrong, you’ll know about it, and
    you’ll know exactly how to fix it.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve peeled back the layers of common cloud native anti-patterns—logging
    everything indiscriminately, overlooking the potential of ML and AI, neglecting
    the importance of distributed tracing, and stumbling through immature alert and
    incident processes. Each of these missteps chips away at the stability and efficiency
    of a cloud native architecture, leaving teams grappling with noise, blind spots,
    and unnecessary firefighting. However, by refining our approach—using targeted
    log aggregation, harnessing AI-driven insights, embracing distributed tracing
    for visibility, and maturing our incident response processes—we lay the groundwork
    for a more resilient, agile system. As we sidestep these anti-patterns, we transition
    from reactive crisis management to proactive operational excellence.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve tackled the hidden pitfalls, it’s time to ensure the system runs
    smoothly under pressure. In the next chapter, we’ll delve into strategies for
    maintaining stability and performance as cloud-native workloads scale and evolve.
  prefs: []
  type: TYPE_NORMAL
