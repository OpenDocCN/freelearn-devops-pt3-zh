- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Observing Our Architecture
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 观察我们的架构
- en: 'In the cloud native landscape, observability and incident management are often
    treated as secondary concerns until they no longer are. All too often, organizations
    only realize the importance of proper monitoring and response processes when an
    unexpected outage or performance issue brings everything to a halt. The damage
    is usually already done by that point: trust is shaken, financial losses accrue,
    and teams are left scrambling to repair systems and reputations. This chapter
    delves into the common pitfalls, or anti-patterns, that cloud native organizations
    encounter when scaling their architectures without giving observability the attention
    it demands.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在云原生领域，可观测性和事件管理通常被视为次要问题，直到它们变得不再如此。组织常常只有在意外的停机或性能问题使一切停顿时，才意识到正确的监控和响应流程的重要性。到那时，损害通常已经发生：信任受到动摇，财务损失累积，团队则忙于修复系统和声誉。本章深入探讨云原生组织在扩展架构时，忽视可观测性所面临的常见陷阱或反模式。
- en: Knowing something is wrong in today’s complex ecosystems is not enough. We need
    to know where it’s going wrong, how it’s affecting our services, and potential
    downstream impacts. Moreover, observability can no longer be purely reactive;
    with the advent of advanced services powered by machine learning (ML) and artificial
    intelligence (AI), organizations can now predict incidents before they happen
    and identify anomalies before they evolve into critical issues. This proactive
    approach is essential for organizations navigating the ever-increasing complexity
    of hybrid workloads, microservices architectures, and multi-cloud environments.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天复杂的生态系统中，仅仅知道哪里出了问题是不够的。我们需要了解问题发生的位置，它如何影响我们的服务，以及可能的下游影响。此外，可观测性不再仅仅是被动响应；随着由机器学习（ML）和人工智能（AI）驱动的先进服务的出现，组织现在可以在事件发生前预测它们，并在问题演变成严重问题之前识别异常。对于那些在混合工作负载、微服务架构和多云环境中航行的组织来说，这种主动方法至关重要。
- en: 'This chapter will explore several anti-patterns in cloud native observability
    and incident management and the practical remediations that can help overcome
    these challenges. These include:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨云原生可观测性和事件管理中的几个反模式，以及可以帮助克服这些挑战的实际补救措施。这些挑战包括：
- en: Incomplete Observability Coverage for Distributed Tracing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式追踪的可观测性覆盖不完整
- en: Lack of Real-Time Monitoring
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺乏实时监控
- en: Ignoring Out-of-the-Box ML Features
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忽视开箱即用的机器学习功能
- en: Failure to Implement Holistic Views for Distributed Tracing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未能为分布式追踪实施全面视图
- en: Not Splitting Impact and Diagnostic Metrics
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未区分影响和诊断指标
- en: In the following sections, we will detail these anti-patterns, providing actionable
    strategies and remediations to help organizations develop a robust observability
    framework. Addressing these common pitfalls will give you the clarity needed to
    maintain operational excellence and avoid potential issues in even the most complex
    cloud native environments.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将详细介绍这些反模式，提供可操作的策略和补救措施，帮助组织建立一个强健的可观测性框架。解决这些常见的陷阱将为您提供保持运营卓越的清晰思路，并避免在即使是最复杂的云原生环境中出现潜在问题。
- en: Let’s Capture Everything in the Log Aggregator
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们在日志聚合器中捕获所有内容
- en: This section begins by exploring the risks of Incomplete Observability Coverage
    for Distributed Tracing, followed by the critical need for Real-Time Monitoring
    to ensure timely detection and response. The first instinct for many organizations
    new to cloud native architectures is to collect as much data as possible.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本节首先探讨分布式追踪的可观测性覆盖不完整的风险，接着是实时监控的关键需求，以确保及时检测和响应。对于许多新接触云原生架构的组织来说，首要的本能反应是收集尽可能多的数据。
- en: “*Let’s capture everything*,” they say as if the sheer volume of logs will magically
    make everything clear. Unfortunately, this mindset often leads to operational
    chaos rather than clarity. When capturing logs, Log aggregation tools can be powerful
    allies, but only when used with purpose. Capturing every log entry from every
    system, service, and application into a single, all-encompassing aggregator may
    sound ideal, but it quickly becomes unmanageable. What begins as a noble attempt
    to enhance visibility is a quagmire of irrelevant data, burying critical signals
    needed to troubleshoot issues under a mountain of logs with no practical value.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: “*让我们记录一切*，”他们说，好像日志的庞大数量能神奇地让一切变得清晰。然而，不幸的是，这种思维方式通常会导致操作混乱，而非清晰。在捕获日志时，日志聚合工具可以是强大的助手，但前提是要有目的地使用它们。将每个系统、服务和应用的每一条日志都捕获到一个统一的、全面的聚合器中，听起来可能是理想的，但很快就会变得难以管理。最初为增强可视性而进行的高尚尝试，最终却演变成一堆无关紧要的数据，在一山无实际价值的日志下埋藏了需要解决问题的关键信号。
- en: Take for example a fluentbit or fluentd, great tools to capture logs but without
    filtering said logs, the thousands upon million logs that can be present, are
    impossible to discphier.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以fluentbit或fluentd为例，这些都是很棒的日志捕获工具，但如果不对日志进行过滤，那么成千上万甚至百万条日志将变得难以理解。
- en: Why Indiscriminate Logging Fails
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么不加区分的日志记录会失败
- en: '**Indiscriminate logging** assumes all data is equally important, but not all
    logs are created equal. Some data is essential for diagnosing system health or
    debugging issues, while other logs are merely noise. Logging every heartbeat of
    a service might seem helpful, but wading through thousands of heartbeat logs during
    an issue diagnosis is counterproductive.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**不加区分的日志记录**假设所有数据都同等重要，但并非所有日志都是平等的。有些数据对诊断系统健康或调试问题至关重要，而其他日志则只是噪音。在服务的每次心跳都记录下来似乎有帮助，但在问题诊断过程中要浏览成千上万的心跳日志却适得其反。'
- en: Take for example logging an OK status vs WARN/FAIL status in an app. The sheer
    amount of OK may be considered noise to some, costing more than it’s worth in
    cloud storage. This approach inflates operational costs, as cloud storage and
    processing are not free, and logging everything can quickly become a financial
    burden. More data means more processing power is required to analyze it, leading
    to escalating costs and diminishing returns.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以在应用中记录OK状态与WARN/FAIL状态为例。大量的OK状态对于某些人来说可能被视为噪音，它在云存储上的成本可能超过其实际价值。这种做法抬高了运营成本，因为云存储和处理不是免费的，记录一切很快就会成为财务负担。更多的数据意味着需要更多的处理能力来分析它，导致成本上升并且回报递减。
- en: 'Smart Logging: A More Effective Approach'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 智能日志记录：一种更有效的方法
- en: Organizations need to be deliberate about what they log instead of capturing
    everything. Log retention should be front and center; however, the key is to focus
    on actionable data logs relevant to business-critical operations or that provide
    insight into system health. Setting log levels (e.g., DEBUG, INFO, WARN, ERROR)
    appropriately helps filter out unnecessary data, ensuring only meaningful information
    is captured.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 组织需要有意识地决定记录哪些内容，而不是捕获一切。日志保留应该是重中之重；然而，关键是专注于与业务关键操作相关的、具有实际价值的可操作数据日志，或者提供系统健康状况的见解。适当设置日志级别（如DEBUG、INFO、WARN、ERROR）有助于过滤掉不必要的数据，确保仅捕获有意义的信息。
- en: 'Context is also key: logs should be structured to trace issues across different
    services and environments. Capturing metadata such as request IDs, user sessions,
    or transaction IDs helps stitch logs into a coherent narrative. Tools like AWS
    CloudWatch Logs Insights or Datadog can be used for centralized log management
    and visualization, reducing noise and prioritizing critical issues. This allows
    organizations to maintain operational efficiency and quickly resolve incidents.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文同样至关重要：日志应当结构化，以便追踪不同服务和环境中的问题。捕获请求ID、用户会话或事务ID等元数据有助于将日志串联成一条连贯的叙事线。像AWS
    CloudWatch Logs Insights或Datadog这样的工具可以用于集中日志管理和可视化，减少噪音并优先处理关键问题。这使得组织能够保持运营效率并快速解决事件。
- en: 'Smart Logging: An Example'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 智能日志记录：一个例子
- en: 'Consider a cloud native e-commerce application hosted on AWS. The system consists
    of several microservices: a user service for handling authentication, a product
    service for managing inventory, a payment service for processing transactions,
    and a delivery service. The application handles millions of daily requests, so
    effective logging is essential to maintain performance and troubleshoot issues
    quickly. Here is how smart logging can be applied:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一个托管在AWS上的云原生电商应用。系统由多个微服务组成：一个用于处理认证的用户服务，一个用于管理库存的产品服务，一个用于处理交易的支付服务，以及一个配送服务。该应用每天处理数百万个请求，因此有效的日志记录对保持性能和快速排查问题至关重要。以下是如何应用智能日志记录：
- en: '`DEBUG`: During development, the engineering team enables `DEBUG` logs to capture
    detailed information about user authentication, such as API calls to AWS Cognito.
    In production, `DEBUG` logs are disabled to avoid unnecessary clutter.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DEBUG`：在开发过程中，工程团队启用`DEBUG`日志以捕获用户认证的详细信息，例如调用AWS Cognito的API。在生产环境中，`DEBUG`日志被禁用，以避免不必要的杂乱信息。'
- en: '`INFO`: In the product service, `INFO` logs capture successful actions like
    a user adding an item to their cart or completing an order. For instance:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`INFO`：在产品服务中，`INFO`日志记录诸如用户将商品添加到购物车或完成订单等成功操作。例如：'
- en: '[PRE0]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'WARN: When a transient issue arises, such as a timeout during a payment request
    to AWS RDS, a WARN log is generated:'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'WARN: 当出现临时问题时，例如在向AWS RDS发起支付请求时发生超时，将生成WARN日志：'
- en: '[PRE1]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'user_id`, `session_id`, `transaction_id`, and `request_id`. For example, the
    payment service logs may include:'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: user_id`, `session_id`, `transaction_id`, 和 `request_id`。例如，支付服务的日志可能包括：
- en: '[PRE2]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'ERROR: Delivery failed - transaction_id=txn001, delivery_id=delv789, error_code=DELIV_ERRORte,'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ERROR: 投递失败 - transaction_id=txn001, delivery_id=delv789, error_code=DELIV_ERRORte,'
- en: '[PRE3]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: from opentelemetry import metrics
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: from opentelemetry import metrics
- en: from opentelemetry.sdk.metrics import MeterProvider
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: from opentelemetry.sdk.metrics import MeterProvider
- en: from opentelemetry.sdk.metrics.export import ConsoleMetricExporter, PeriodicExportingMetricReader
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: from opentelemetry.sdk.metrics.export import ConsoleMetricExporter, PeriodicExportingMetricReader
- en: Set up MeterProvider and Metric Exporter
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 MeterProvider 和 Metric Exporter
- en: provider = MeterProvider()
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: provider = MeterProvider()
- en: metrics.set_meter_provider(provider)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: metrics.set_meter_provider(provider)
- en: exporter = ConsoleMetricExporter()
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: exporter = ConsoleMetricExporter()
- en: reader = PeriodicExportingMetricReader(exporter)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: reader = PeriodicExportingMetricReader(exporter)
- en: provider.add_metric_reader(reader)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: provider.add_metric_reader(reader)
- en: Create a meter for recording metrics
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建用于记录指标的计量器
- en: meter = metrics.get_meter(__name__)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: meter = metrics.get_meter(__name__)
- en: 'Define metrics: Impact and Diagnostic'
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义指标：影响和诊断
- en: impact_latency = meter.create_histogram("impact_latency", unit="ms", description="Request
    Latency")
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: impact_latency = meter.create_histogram("impact_latency", unit="ms", description="请求延迟")
- en: diagnostic_cpu_usage = meter.create_observable_gauge("diagnostic_cpu_usage",
    description="CPU Usage")
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: diagnostic_cpu_usage = meter.create_observable_gauge("diagnostic_cpu_usage",
    description="CPU 使用率")
- en: Function to simulate recording of impact metrics
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模拟记录影响指标的函数
- en: 'def record_impact_metrics(latency_value):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 'def record_impact_metrics(latency_value):'
- en: impact_latency.record(latency_value)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: impact_latency.record(latency_value)
- en: 'print(f"Recorded impact latency: {latency_value}ms")'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 'print(f"记录的影响延迟: {latency_value}ms")'
- en: Function to observe diagnostic metrics
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 观察诊断指标的函数
- en: 'def observe_diagnostic_metrics():'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 'def observe_diagnostic_metrics():'
- en: import psutil
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: import psutil
- en: return psutil.cpu_percent(interval=None)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: return psutil.cpu_percent(interval=None)
- en: Register diagnostic metric callback
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注册诊断指标回调
- en: meter.create_observable_gauge("diagnostic_cpu_usage", callbacks=[observe_diagnostic_metrics])
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: meter.create_observable_gauge("diagnostic_cpu_usage", callbacks=[observe_diagnostic_metrics])
- en: Simulating metric recording
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模拟指标记录
- en: record_impact_metrics(120)  # Simulating a latency of 120ms
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 'record_impact_metrics(120)  # 模拟120ms的延迟'
- en: '```'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '```'
- en: 'Key Points to observe in the code are:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中需要观察的关键点有：
- en: '`impact_latency histogram` tracks the length of a request, a key metric for
    user experience. In this example, we record a latency of 120ms.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`impact_latency histogram` 追踪请求的时长，这是用户体验的一个关键指标。在这个例子中，我们记录了120ms的延迟。'
- en: '`diagnostic_cpu_usage` observable gauge monitors CPU usage. We use the ``psutil``
    library to gather CPU statistics, a proper diagnostic metric to understand system
    resource usage during incidents.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`diagnostic_cpu_usage` 可观察计量器监控CPU使用率。我们使用``psutil``库来收集CPU统计信息，这是一种在事故发生时了解系统资源使用情况的适当诊断指标。'
- en: By collecting and analyzing impact and diagnostic metrics, teams can quickly
    detect performance issues while gathering the information necessary to diagnose
    and resolve root causes. This combined approach ensures that cloud native systems
    remain resilient and performant, even under pressure.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过收集和分析影响和诊断指标，团队可以迅速检测性能问题，并收集必要的信息以诊断和解决根本原因。这种综合方法确保了云原生系统在高压下仍能保持弹性和性能。
- en: Proper metrics dictate the relevance of results; the next section will go into
    a real-world scenario of what happens when we neglect distributed tracing.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的指标决定了结果的相关性；下一部分将进入一个真实场景，讨论当我们忽视分布式追踪时会发生什么。
- en: Real World Consequences
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 真实世界的后果
- en: During a recent engagement with a leading e-commerce retailer, we were called
    in to address significant performance issues that emerged during a high-traffic
    sale. The retailer’s microservices architecture managed critical operations like
    inventory management and payment processing, but their observability was fragmented.
    Relying solely on logs and metrics from individual services, they couldn’t trace
    transactions end-to-end, making it impossible to quickly identify the source of
    latency when checkout times began to slow down under the increased load. Hours
    into the incident, we implemented distributed tracing, immediately revealing a
    third-party payment API as the bottleneck causing the delays.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在与一家领先的电子商务零售商进行的最近一次合作中，我们受邀解决在高流量促销期间出现的重大性能问题。该零售商的微服务架构管理着库存管理和支付处理等关键操作，但其可观察性是碎片化的。仅依赖单个服务的日志和指标，他们无法追踪端到端的事务，导致在结账时间开始因负载增加而变慢时，无法迅速识别延迟的来源。事故发生几个小时后，我们实施了分布式追踪，立即揭示了一个第三方支付API作为导致延迟的瓶颈。
- en: Introducingm distributed tracing gave the retailer real-time visibility into
    the entire transaction flow across all microservices. The integration of this
    tracing allowed the operations team to pinpoint and resolve issues much faster,
    avoiding prolonged outages that could have been identified within minutes instead
    of hours. Our intervention reduced downtime and restored customer trust by ensuring
    that future peak traffic periods would be handled with better performance monitoring
    and faster response times. The image below shows a simple Kubernetes microservice
    followed by a list of issue we had discovered
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 引入分布式追踪为零售商提供了实时的整个事务流的可视化，跨所有微服务。此追踪的集成使得运营团队能够更快地定位并解决问题，避免了本可以在几分钟内识别出来的长期停机。我们的干预减少了停机时间，并通过确保未来的高峰流量期间能更好地进行性能监控和更快的响应时间，恢复了客户的信任。下图展示了一个简单的Kubernetes微服务，后面列出了我们发现的问题。
- en: '![](img/B22364_10_2.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22364_10_2.jpg)'
- en: Figure 10.2 - Simple Kubernetes Microservice
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 - 简单的Kubernetes微服务
- en: 'Key issues we identified included:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们识别出的关键问题包括：
- en: '**Lack of end-to-end transaction visibility**: The system couldn’t trace the
    customer journey through microservices.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏端到端的事务可视化**：系统无法追踪客户在微服务中的整个旅程。'
- en: '**Isolated logs and metrics**: Metrics were limited to individual services
    and offered no insight into how they interacted in a transaction.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日志和指标孤立**：指标仅限于单个服务，无法洞察它们在事务中如何交互。'
- en: '**Unmonitored third-party dependencies**: The third-party API causing the delays
    was not adequately monitored, making it invisible until distributed tracing was
    implemented.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未监控的第三方依赖**：导致延迟的第三方API没有得到充分监控，直到实施了分布式追踪才被发现。'
- en: '**Slow incident response times**: Without distributed tracing, the team relied
    on manual troubleshooting, prolonging the resolution of critical issues.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**慢响应的事故处理时间**：没有分布式追踪，团队依赖手动排查故障，延长了关键问题的解决时间。'
- en: '**To address these challenges, organizations need a comprehensive observability
    strategy that integrates distributed tracing, centralized logging, and robust
    monitoring across all system components. The list below provides** **more context:**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**为了解决这些挑战，组织需要一个全面的可观察性策略，整合分布式追踪、集中日志记录和强大的监控，涵盖所有系统组件。以下列表提供了** **更多的背景信息：**'
- en: '**Implemented Distributed Tracing**: We deployed OpenTelemetry as our tracing
    solution, providing end-to-end visibility of transactions across the microservices
    architecture. This allowed the team to detect bottlenecks immediately, including
    the slow third-party API causing checkout delays. The key to consistency was to
    package OpenTelemtry SDK (in this case, python) with all code.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实施分布式追踪**：我们部署了 OpenTelemetry 作为我们的追踪解决方案，为微服务架构中的事务提供了端到端的可视化。这使团队能够立即检测到瓶颈，包括导致结账延迟的第三方
    API。保持一致性的关键是将 OpenTelemetry SDK（在此案例中为 Python）与所有代码打包在一起。'
- en: '**Enhanced Log Aggregation**: We integrated the tracing system with CloudWatch
    for centralized log aggregation, ensuring all logs from the various microservices
    were collected and searchable in one place. This improved the team’s ability to
    correlate logs with tracing data, making incident detection and diagnosis faster
    and more accurate.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强日志聚合**：我们将追踪系统与 CloudWatch 集成，进行集中式日志聚合，确保来自不同微服务的所有日志都能集中收集并进行检索。这提高了团队将日志与追踪数据关联的能力，使得事故检测和诊断更快速、更准确。'
- en: '**Real-Time Monitoring and Alerts**: We set up monitoring dashboards and real-time
    alerts using Prometheus and Grafana, configuring them to display system health,
    performance metrics, and traces. This gave the operations team proactive insights
    into system performance, reducing their reliance on reactive troubleshooting during
    high-traffic events.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时监控与警报**：我们使用 Prometheus 和 Grafana 设置了监控仪表板和实时警报，配置它们以显示系统健康状况、性能指标和追踪信息。这为运维团队提供了系统性能的主动洞察，减少了他们在高流量事件中对被动故障排除的依赖。'
- en: '**Monitoring of Third-Party Dependencies**: We established specific monitors
    for third-party APIs, which included setting latency thresholds and failure rate
    alerts. This ensured that external dependencies were continuously tracked for
    performance degradation, preventing them from silently becoming bottlenecks in
    the future.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第三方依赖监控**：我们为第三方 API 建立了特定的监控系统，包括设置延迟阈值和故障率警报。这确保了外部依赖持续被监控，以防它们在未来默默成为瓶颈。'
- en: By leveraging these tools and strategies, we improved the retailer’s system
    visibility and reduced their response times to critical incidents. These solutions
    can be replicated using equivalent services in the major cloud providers, such
    as AWS XRay + CloudWatch, Azure Application Insights, and Google Cloud Operations
    Suite.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用这些工具和策略，我们提高了零售商系统的可视化，并减少了他们对关键事件响应的时间。这些解决方案可以通过主要云服务提供商的等效服务复制，例如 AWS
    XRay + CloudWatch、Azure Application Insights 和 Google Cloud Operations Suite。
- en: Next, we will discuss best practices and what to consider when building distributed
    tracing to tie together all we have learned up to this point.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论最佳实践以及在构建分布式追踪时需要考虑的因素，以便将我们迄今所学的知识结合起来。
- en: Best Practices for Resolving Tracing Neglect
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决追踪忽视的最佳实践
- en: Neglecting distributed tracing is like trying to navigate a complex city with
    incomplete maps; inevitably, you’ll get lost. End-to-end tracing acts as your
    GPS, connecting the dots between microservices, identifying bottlenecks, and illuminating
    paths that might otherwise remain hidden in the shadows of fragmented logs. Distributed
    tracing must be treated as a foundational practice, not an afterthought, to ensure
    a cloud native system remains agile, scalable, and responsive.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 忽视分布式追踪就像在没有完整地图的情况下尝试在复杂的城市中导航；最终，你会迷路。端到端追踪就像你的 GPS，连接微服务之间的点，识别瓶颈，并照亮那些可能隐藏在分散日志阴影中的路径。分布式追踪必须作为基础实践，而不是事后的思考，以确保云原生系统保持敏捷、可扩展和响应迅速。
- en: 'Consider the following when applying building tracing:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用构建追踪时，请考虑以下因素：
- en: '**Start Early and Be Consistent**: One of the most critical best practices
    is implementing tracing from the beginning. Retroactively adding tracing to an
    existing architecture is akin to patching holes in a sinking ship; it is inefficient
    and prone to leaving gaps. Consistency is key; every microservice, dependency,
    and path that requests take through your system should be covered by tracing.
    This ensures that no part of the system is left in the dark.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**尽早开始并保持一致性**：最关键的最佳实践之一是从一开始就实施追踪。事后为现有架构添加追踪就像为一艘正在下沉的船打补丁；效率低下且容易留下漏洞。一致性至关重要；每个微服务、依赖项和请求在系统中路径的每个环节都应覆盖追踪。这确保系统的任何部分都不会置于黑暗中。'
- en: '**Trace the Entire Journey**: Successful tracing doesn’t stop at the surface.
    It needs to span the entire request lifecycle, from the user-facing frontend through
    the backend services and to external dependencies like third-party APIs. This
    complete visibility ensures that any latency or failure can be tracked down to
    its root cause, whether it is inside your system or external to it. The result
    is faster diagnostics, quicker resolutions, and fewer headaches when incidents
    arise.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跟踪整个过程**：成功的跟踪不仅仅停留在表面。它需要覆盖整个请求生命周期，从面向用户的前端到后台服务，再到外部依赖，如第三方API。这种全面的可见性确保了任何延迟或故障都可以追溯到根本原因，无论它是发生在系统内部还是外部。结果是更快的诊断、更快的解决方案，以及在事件发生时更少的头疼。'
- en: '**Integrate with Metrics and Logs**: Tracing is robust, but it becomes a force
    multiplier when combined with metrics and logs. Use impact metrics to identify
    when user experience is degrading and diagnostic metrics to figure out why. Aggregating
    all three pillars of observability, tracing, metrics, and logs creates a comprehensive
    view of your system, empowering teams with the insights they need to stay proactive
    rather than reactive.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与指标和日志集成**：跟踪功能强大，但与指标和日志结合时，它的作用更为显著。使用影响指标来识别用户体验何时下降，使用诊断指标来找出原因。将可观察性的三大支柱——跟踪、指标和日志进行整合，可以创建一个全面的系统视图，使团队能够获得他们所需的洞察，保持主动而非被动。'
- en: '**Automate and Set Guardrails**: Manually instrumenting every service for tracing
    can be daunting, especially in dynamic, cloud native environments. Automation
    tools and frameworks like OpenTelemetry simplify this process, ensuring that tracing
    is baked into every new microservice or deployment by default. Setting automated
    guardrails for latency thresholds, error rates, and critical service performance
    can trigger alerts before incidents spiral out of control.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化并设置保护措施**：手动为每个服务进行跟踪仪器化可能令人望而生畏，尤其是在动态的云原生环境中。像OpenTelemetry这样的自动化工具和框架简化了这个过程，确保每个新的微服务或部署默认都内置了跟踪功能。设置自动化保护措施，如延迟阈值、错误率和关键服务性能，可以在事件失控之前触发警报。'
- en: '**Monitor What Matters Most**: Not every trace, log, or metric needs to be
    treated equally. Prioritize tracing for the critical paths within your system,
    such as user transactions, payment flows, or any service that directly impacts
    customer experience. This focus ensures that your team’s attention is on what
    matters most while still keeping an eye on the broader system’s health.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控最重要的内容**：并非每个跟踪、日志或指标都需要同等对待。优先跟踪系统中的关键路径，例如用户交易、支付流程，或任何直接影响客户体验的服务。这种聚焦确保团队的注意力集中在最重要的内容上，同时仍然关注整个系统的健康状况。'
- en: Adhering to these best practices can help traditional organizations transform
    distributed tracing from a reactive tool into a proactive asset when shifting
    to cloud native. When tracing is holistic and integrated, the result is a cloud
    native architecture that is resilient, transparent, and able to meet the demands
    of modern applications. While tracing forms the backbone of a well-functioning
    cloud native system, the actual test of resilience lies in how an organization
    responds when things go wrong. Even with the best tracing practices, the system’s
    ability to recover and maintain stability is compromised without a mature process
    for handling alerts and incidents.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循这些最佳实践可以帮助传统组织在向云原生转型时，将分布式跟踪从一个反应式工具转变为一个主动的资产。当跟踪是全面且集成时，结果是一个云原生架构，它具有弹性、透明性，并能够满足现代应用的需求。虽然跟踪是一个运作良好的云原生系统的支柱，但真正的弹性考验在于组织在问题发生时的应对能力。即使采用最佳的跟踪实践，系统的恢复能力和维持稳定性也会受到影响，如果没有成熟的警报和事件处理流程。
- en: In the final section of this chapter, we will explore how immature processes
    for alerts and incidents can undermine even the most robust architectures and
    how addressing these shortcomings is essential for sustaining operational excellence
    in cloud native environments.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最后一节，我们将探讨警报和事件处理流程不成熟如何破坏即使是最强大的架构，以及如何解决这些不足是维持云原生环境中运营卓越的关键。
- en: Immature processes for alerts & incidents
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 警报和事件处理流程不成熟
- en: 'While offering agility and scalability, cloud native organizations can often
    suffer from immature processes when handling alerts and incidents. In environments
    that manage thousands of microservices, the noise from redundant alerts, incomplete
    observability setups, and ineffective incident response protocols can overwhelm
    teams. As organizations modernize their infrastructure, they often forget a fundamental
    truth: **alerting and incident management are not about gathering all available
    metrics but focusing on the right metrics, responding to the right signals, and
    ensuring these processes function smoothly across all environments, not just**
    **in production**.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然提供了灵活性和可扩展性，但云原生组织在处理警报和事件时往往会遭遇不成熟的流程。在管理数千个微服务的环境中，冗余警报、观察性设置不完整以及无效的事件响应协议所带来的噪声可能会让团队不堪重负。当组织现代化其基础设施时，他们往往会忽略一个基本真理：**警报和事件管理的关键不在于收集所有可用的指标，而是专注于正确的指标，响应正确的信号，并确保这些过程在所有环境中顺畅运行，而不仅仅是在生产环境中**。
- en: At the heart of many cloud native failures is a “*collect everything*” mindset,
    gathering every possible metric and sending alerts for every anomaly. This approach
    often leads to chaos, leaving engineering and operations teams drowning in data
    without actionable insights. The issue is not a lack of metrics; it’s the absence
    of purposeful, well-aligned metrics and alerts. By understanding the dangers of
    metric dumping, we can become more cautious and aware, ensuring that every alert
    has a clear purpose, and each metric gathered should address a specific use case
    tied to both business and technical objectives.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 许多云原生失败的核心问题是“*收集一切*”的心态，收集所有可能的指标，并为每一个异常发送警报。这种做法通常会导致混乱，使工程和运维团队淹没在没有可操作洞察的数据中。问题不是指标的缺乏，而是缺乏有目的且良好对齐的指标和警报。通过理解指标倾倒的危险，我们可以变得更加谨慎和警觉，确保每个警报都有明确的目的，每个收集的指标都应该解决一个与业务和技术目标相关的具体用例。
- en: This section will provide a guide through building a mature, effective system
    for alerting and incident response, highlighting common pitfalls and strategies
    for overcoming them.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将指导如何构建一个成熟、有效的警报和事件响应系统，重点介绍常见的陷阱以及克服这些陷阱的策略。
- en: 'In this section, we will cover:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将涵盖：
- en: Purpose-driven metrics and alerts
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标驱动的指标和警报
- en: The trap of metric dumping
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标倾倒的陷阱
- en: Shifting left in observability
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在可观察性上向左移动
- en: Alert fatigue
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 警报疲劳
- en: Incident response maturity
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件响应成熟度
- en: These topics will help develop a resilient, proactive approach to alerting and
    incident management, enabling teams to respond swiftly and intelligently when
    challenges arise.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些主题将帮助我们培养一种韧性强、主动的警报和事件管理方法，使团队在挑战出现时能够迅速且智能地响应。
- en: Purpose-driven Metrics and Alerts
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标驱动的指标和警报
- en: The sheer volume of metrics available can be overwhelming. However, not every
    metric is useful, and collecting everything without a clear purpose, leads to
    noise, alert fatigue, and inefficiency. Purpose-driven metrics focus on gathering
    data that aligns directly with specific business or technical objectives, ensuring
    that every metric serves a tangible purpose.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的指标数量庞大，可能令人不知所措。然而，并不是每个指标都是有用的，收集所有数据而没有明确目标，会导致噪声、警报疲劳和低效。目标驱动的指标专注于收集与特定业务或技术目标直接相关的数据，确保每个指标都有实际目的。
- en: 'Metrics should be chosen based on their ability to provide actionable insights,
    not just because they’re easy to collect. For example, instead of gathering CPU
    usage for all instances, consider: *“What are we trying to achieve by monitoring
    this? Are we looking to understand performance under load? Predict infrastructure
    scaling needs? Optimize resource utilization?*” Once the goal is clear, we can
    design metrics and alerts that align with it.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 指标应该根据它们提供可操作洞察的能力来选择，而不仅仅是因为它们易于收集。例如，除了收集所有实例的CPU使用率外，考虑：*“我们通过监控这个想要实现什么目标？我们是在了解负载下的性能吗？预测基础设施扩展需求吗？优化资源利用吗？*”
    一旦目标明确，我们可以设计与之对齐的指标和警报。
- en: 'Purpose-driven Metrics and Alerts: An Example'
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标驱动的指标和警报：一个示例
- en: 'For instance, let’s consider a microservices-based e-commerce platform. One
    critical business objective is ensuring a seamless customer checkout experience.
    In this case, purpose-driven metrics could include:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们考虑一个基于微服务的电子商务平台。一个关键的业务目标是确保顺畅的客户结账体验。在这种情况下，目标驱动的指标可能包括：
- en: '**Latency of the checkout service**: This metric tracks the time it takes for
    the checkout process to complete, directly impacting user experience. The business
    goal is to ensure customers aren’t abandoning their carts due to slow performance.
    The associated alert could trigger when latency exceeds a defined threshold, prompting
    engineers to investigate slowdowns before they impact many users.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结账服务的延迟**：这个指标跟踪完成结账过程所需的时间，直接影响用户体验。业务目标是确保客户不会因性能问题而放弃购物车。相关警报可能会在延迟超过定义的阈值时触发，提醒工程师在影响大量用户之前调查性能下降问题。'
- en: '**Error rate during payment processing**: Monitoring this metric helps ensure
    that failed payment transactions don’t disrupt sales. Instead of sending alerts
    for every failed transaction, an alert might be triggered when the error rate
    surpasses a certain percentage over a specific time window, allowing teams to
    differentiate between minor blips and real issues.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支付处理中的错误率**：监控这个指标有助于确保支付失败的交易不会干扰销售。不是针对每一笔失败的交易都发送警报，而是当错误率在特定时间窗口内超过某个百分比时，触发警报，让团队能够区分小的波动和真正的问题。'
- en: '**Inventory update delays**: If inventory updates after sales are delayed,
    this can result in overselling, impacting revenue and customer satisfaction. A
    targeted metric around the time it takes to sync inventory after an order is placed
    serves a business goal of operational accuracy. Alerts are triggered if these
    delays exceed acceptable limits.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**库存更新延迟**：如果销售后库存更新延迟，可能会导致超卖，影响收入和客户满意度。围绕订单完成后库存同步所需时间的目标指标，服务于业务目标的运营准确性。如果这些延迟超过可接受的限制，则触发警报。'
- en: By defining metrics like these, we do not just collect data for the sake of
    it. Instead, we ensure that each metric serves a well-defined purpose, allowing
    teams to focus on what truly matters to the business. Now that we’ve seen the
    value of purpose-driven metrics, we must avoid the opposite approach, which we
    call the “metric dumping” trap.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通过定义像这样的指标，我们并不是仅仅为了收集数据而收集数据。相反，我们确保每个指标都有一个明确的目的，使团队能够专注于对业务真正重要的事项。现在我们已经看到了目标驱动的指标的价值，我们必须避免相反的方法，这就是我们所说的“指标倾倒”陷阱。
- en: Next, we will examine the trap of metric dumping and how it can derail even
    the best-intentioned cloud native monitoring strategies.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨指标倾倒的陷阱，以及它是如何破坏即便是最有意图的云原生监控策略的。
- en: The Trap of Metric Dumping
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指标倾倒的陷阱
- en: In the rush to embrace monitoring tools and gather insights, many organizations
    fall into the trap of metric dumping. This occurs when every possible metric is
    collected without considering its value or purpose. On the surface, this might
    seem like a way to guarantee complete visibility. However, it leads to data overload,
    alert fatigue, and reduced system performance, making it harder for teams to respond
    to critical issues promptly. **Metric dumping** is the process of collecting every
    available metric, whether it’s CPU usage, memory, network latency, or disk I/O,
    without considering how these metrics will be used or whether they contribute
    to achieving business goals. Teams may believe that collecting more data gives
    them more control and insight.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在急于采用监控工具和收集洞察的过程中，许多组织陷入了指标倾倒的陷阱。这种情况发生在收集所有可能的指标时，没有考虑其价值或目的。表面上看，这似乎是一种确保完全可视化的方法。然而，这会导致数据过载、警报疲劳和系统性能下降，使得团队更难及时应对关键问题。**指标倾倒**是指收集所有可用的指标，无论是CPU使用率、内存、网络延迟还是磁盘I/O，都没有考虑这些指标将如何使用，或者它们是否有助于实现业务目标。团队可能认为，收集更多的数据能给他们更多的控制权和洞察力。
- en: 'Metric Dumping: An Example'
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指标倾倒：一个例子
- en: For example, imagine an organization that monitors the CPU usage of every instance
    across hundreds of microservices, regardless of whether CPU usage is relevant
    to the service’s performance. They collect this data at a highly granular level
    (every second), even though the service has no history of CPU-related performance
    issues. Over time, this approach generates vast amounts of data that clog dashboards,
    increase storage costs, and create an alert system constantly firing off non-critical
    warnings. This is a classic case of metric dumping, collecting more data than
    is necessary or actionable.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设一个组织监控每个实例的CPU使用率，跨越数百个微服务，无论CPU使用率是否与服务性能相关。他们以非常精细的粒度（每秒）收集这些数据，即使该服务没有历史的与CPU相关的性能问题。随着时间的推移，这种方法会产生大量的数据，堵塞仪表板，增加存储成本，并创建一个警报系统，持续触发无关紧要的警告。这是典型的指标倾倒，收集了超出必要或可操作的数据。
- en: 'Metric dumping creates two significant problems that slow down operations:
    operational inefficiency and alert fatigue. Here’s how these issues manifest:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 指标倾倒会带来两个显著的问题，导致操作变慢：操作低效性和警报疲劳。以下是这些问题的体现方式：
- en: '**Operational Inefficiency**:'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作低效性**：'
- en: The flood of unnecessary metrics slows down monitoring systems, as every metric
    must be processed, stored, and analyzed.
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不必要的指标洪流减慢了监控系统，因为每个指标都必须被处理、存储和分析。
- en: This results in massive data pipelines that consume more resources than needed.
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这会导致庞大的数据管道，消耗超过所需的资源。
- en: Engineers and operations teams must sift through irrelevant data, making it
    harder to find important insights.
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工程师和运维团队必须从无关的数据中筛选信息，这使得找到重要见解变得更加困难。
- en: Critical alerts may be delayed, buried, or missed, potentially allowing issues
    to escalate into outages.
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键警报可能会被延迟、淹没或遗漏，从而可能导致问题升级为停机故障。
- en: '**Alert Fatigue**:'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**警报疲劳**：'
- en: Monitoring too many metrics leads to excessive alerts, overwhelming teams with
    a stream of non-critical notifications.
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控过多的指标会导致过多的警报，压倒团队，造成非关键通知的流入。
- en: Over time, teams become desensitized, increasing the risk of missing or ignoring
    truly important alerts.
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着时间的推移，团队变得麻木，增加了错过或忽略真正重要警报的风险。
- en: This reduces team effectiveness and increases the likelihood of system downtime,
    longer recovery times, and a negative impact on customer experience.
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这降低了团队的效率，增加了系统停机、恢复时间延长以及对客户体验产生负面影响的可能性。
- en: 'Here’s how you can move away from metric dumping and toward a more focused,
    efficient monitoring strategy:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何摆脱指标倾倒，朝着更专注、更高效的监控策略转变的方法：
- en: '| **Action** | **Description** |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| **行动** | **描述** |'
- en: '| **Define Clear Business and** **Technical Goals** | Ensure every metric has
    a well-defined purpose. Start by asking, “What business or operational problem
    are we trying to solve?” Collect metrics supporting these objectives. |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| **定义明确的业务和** **技术目标** | 确保每个指标都有明确的目的。首先要问：“我们要解决的业务或操作问题是什么？”收集支持这些目标的指标。
    |'
- en: '| **Prioritize** **Actionable Metrics** | Focus on metrics that provide actionable
    insights. Avoid collecting data just because it’s available. Ensure metrics help
    the team make decisions or take action. |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| **优先关注** **可操作指标** | 聚焦于那些能提供可操作见解的指标。避免仅仅因为数据可用就收集它。确保这些指标能帮助团队做出决策或采取行动。
    |'
- en: '| **Regularly Review and** **Prune Metrics** | Periodically audit the metrics
    being collected. Retire those that are no longer relevant, reducing noise and
    keeping the monitoring system efficient. |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| **定期审查和** **修剪指标** | 定期审计正在收集的指标。淘汰那些不再相关的指标，减少噪声，保持监控系统的高效性。 |'
- en: '| **Create** **Threshold-based Alerts** | Design alerts that trigger only when
    critical thresholds are crossed. This reduces unnecessary alerts and helps teams
    focus on the most important issues. |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| **创建** **基于阈值的警报** | 设计仅在关键阈值被突破时触发的警报。这减少了不必要的警报，帮助团队专注于最重要的问题。 |'
- en: '| **Use** **Aggregated Metrics** | Aggregate metrics to get a high-level view,
    avoiding excessive granularity. Monitor averages over time to identify meaningful
    patterns and reduce noise. |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| **使用** **聚合指标** | 聚合指标以获得高层次的视图，避免过度细化。监控时间段内的平均值，以识别有意义的模式并减少噪声。 |'
- en: '| **Focus on Key Performance** **Indicators (KPIs)** | Align metrics with KPis
    that measure the health and performance of critical systems, like user experience,
    transaction success rates, and service latency. |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| **专注于关键绩效** **指标 (KPI)** | 将指标与衡量关键系统健康状况和性能的KPI对齐，如用户体验、交易成功率和服务延迟。 |'
- en: Table 10.5 - Metric dumping resolution table
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.5 - 指标倾倒解决表
- en: By following these steps, you can eliminate the inefficiencies of metric dumping
    and create a streamlined monitoring system that delivers clear, actionable insights.
    This will improve response times, reduce alert fatigue, and enable teams to focus
    on the most critical aspects of your cloud native environment. The next section
    will dive into the observability side by addressing *Shifting left* *in Observability*.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循这些步骤，你可以消除指标倾倒的低效，创建一个简化的监控系统，提供清晰、可操作的洞察力。这将改善响应时间，减少警报疲劳，并使团队能够集中精力处理云原生环境中最关键的方面。接下来的部分将通过讨论*在可观察性中向左转*来深入探讨。
- en: Shifting Left in Observability
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在可观察性中向左转
- en: One of the most overlooked aspects of cloud native observability is the failure
    to extend monitoring and alerting into the early stages of the software development
    lifecycle(SDLC). This oversight leads to an anti-pattern where full observability
    is treated as a production-only concern. In cloud native environments, where microservices
    sprawl and deployment velocity are high, waiting until production to catch issues
    is akin to letting the fuse burn down on a bomb. This is where the practice of
    shifting left comes into play.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 云原生可观察性中最常被忽视的一个方面是未能将监控和警报延伸到软件开发生命周期（SDLC）的早期阶段。这种忽视导致了一种反模式，将完整的可观察性仅视为生产环境的关注点。在云原生环境中，微服务分布广泛且部署速度快，等到生产环境才发现问题就像是让炸弹的引线慢慢燃烧一样。这就是“向左转”实践的关键所在。
- en: Shifting left in observability means embedding monitoring, alerting, and diagnostics
    into earlier environments, such as development, testing, and **UAT** (**User Acceptance
    Testing**), instead of waiting until the code reaches production. By doing so,
    organizations can catch performance bottlenecks, scaling issues, or misconfigurations
    earlier, long before they disrupt production services or, worse, customers.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在可观察性中向左转意味着将监控、警报和诊断嵌入到开发、测试和**UAT**（**用户验收测试**）等早期环境中，而不是等到代码达到生产环境才进行监控。通过这种方式，组织可以更早发现性能瓶颈、扩展问题或配置错误，远在它们破坏生产服务或更糟的情况——影响客户之前。
- en: 'Imagine a scenario in which a cloud native e-commerce application is being
    deployed. In production, the company uses a robust observability platform like
    Prometheus and Grafana for monitoring and alerting system health. However, in
    pre-production environments, like staging or UAT, there’s only a basic setup:
    maybe some logs or simple uptime monitoring.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个场景，一个云原生电商应用正在部署。在生产环境中，公司使用像Prometheus和Grafana这样的强大可观察性平台来监控和警报系统健康。然而，在预生产环境中，如临时环境或UAT，只有基本的设置：可能只有一些日志或简单的正常运行时间监控。
- en: This means that while the application undergoes various stages of testing, critical
    performance metrics such as API latency or resource saturation are not being monitored.
    The development team is unaware that under load, a particular microservice starts
    exhibiting high latency after a specific number of concurrent users. This issue
    only surfaces once the application is live in production, where latency spikes
    impact real users, leading to a scramble to mitigate the issue under the stress
    of a live incident.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着虽然应用程序经历了各种测试阶段，但像API延迟或资源饱和度等关键性能指标并没有被监控。开发团队不知道，在负载下，某个特定的微服务在特定数量的并发用户下会开始表现出高延迟。这个问题只在应用程序上线后在生产环境中暴露出来，延迟的激增影响了真实用户，导致在实际事件压力下急忙处理问题。
- en: Had observability been shifted left, this problem could have been identified
    much earlier. With the right metrics in place, developers would have seen that
    API latency gradually degrades with increasing load during the load-testing phase,
    allowing them to resolve the issue before production deployment.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可观察性向左转，这个问题本可以更早地被识别出来。通过正确的指标，开发人员会发现，在负载测试阶段，随着负载的增加，API延迟逐渐增加，这使得他们能够在生产部署前解决该问题。
- en: 'The key to shifting left in observability is realizing that monitoring is crucial
    in all environments, not just production. Here’s how to start:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在可观察性中向左转的关键是意识到监控在所有环境中都至关重要，而不仅仅是生产环境。以下是如何开始的方法：
- en: '| **Action** | **Description** | **Benefits of** **Shifting Left** |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| **行动** | **描述** | **向左转的好处** |'
- en: '| **Instrument Early** | Add monitoring and tracing from the start, ensuring
    every feature or service has observability baked in during development across
    all environments (development, Cl, staging). | - Early issue detection- Improved
    developer ownership |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| **提前部署监控** | 从一开始就加入监控和追踪，确保每个功能或服务在开发过程中（开发、CI、预发布）都有可观察性。 | - 早期问题检测 -
    提升开发者责任感 |'
- en: '| **Monitor** **Load Tests** | Treat pre-production load tests like production.
    Use tools like Grafana or New Relic to monitor API performance, memory, and throughput
    to identify bottlenecks early. | - Early issue detection- Reduced cost of failure
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| **监控** **负载测试** | 将预生产环境的负载测试视作生产环境一样对待。使用像 Grafana 或 New Relic 这样的工具监控 API
    性能、内存和吞吐量，以便尽早发现瓶颈。 | - 早期问题检测 - 降低失败成本 |'
- en: '| **Set Alerts in** **Lower Environments** | Implement alerts for critical
    issues (e.g., rising error rates, abnormal latency) in testing phases to address
    issues before they hit production. | - Faster time to resolution- Reduced cost
    of failure |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| **在** **低环境中设置警报** | 在测试阶段实施关键问题的警报（例如：错误率上升、延迟异常），以便在问题进入生产之前解决它们。 | - 更快的解决时间
    - 降低失败成本 |'
- en: '| **Use** **Distributed Tracing** | Apply distributed tracing in non-production
    environments to identify inefficient paths and bottlenecks, providing developers
    insights for fixing issues before they escalate. | - Faster time to resolution-
    Improved developer ownership |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| **使用** **分布式追踪** | 在非生产环境中应用分布式追踪，以识别低效路径和瓶颈，为开发者提供修复问题的见解，防止问题升级。 | - 更快的解决时间
    - 提升开发者责任感 |'
- en: Table 10.6 - Starting with Instrumentation
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.6 - 从监控开始
- en: In summary, shifting left in observability transforms it from a reactive, production-focused
    practice into a proactive, holistic approach that safeguards the entire lifecycle
    of cloud native applications. By investing in observability early, you significantly
    reduce the likelihood of surprises in production, ensuring that your cloud native
    architecture can scale and perform reliably under any conditions.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，将可观察性向左推进，将其从一种被动、以生产为中心的实践转变为一种主动、全面的方式，保护整个云原生应用生命周期。通过提前投资于可观察性，你可以显著减少生产环境中出现意外情况的可能性，确保你的云原生架构能够在任何条件下可靠地扩展和运行。
- en: Alert Fatigue & Incident Response Maturity
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 警报疲劳与事件响应成熟度
- en: In cloud native environments, alert fatigue is the silent enemy, creeping in
    when teams are bombarded with endless notifications, many of which signal minor
    issues or false alarms. This constant noise desensitizes even the most vigilant
    engineers, causing critical alerts to be overlooked or delayed. In the worst cases,
    teams may become so accustomed to low-priority alerts that they miss the ones
    that matter most. Incident response maturity, on the other hand, is the antidote,
    a reflection of a team’s ability to manage alerts efficiently, triage effectively,
    and resolve issues with precision and speed.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在云原生环境中，警报疲劳是潜在的敌人，当团队被源源不断的通知淹没时，这种情况悄然发生，其中许多警报只是小问题或虚假警报。持续的噪声让即使是最警觉的工程师也会产生麻木感，导致关键警报被忽视或延迟。在最糟糕的情况下，团队可能会对低优先级的警报习以为常，以至于错过了最重要的警报。另一方面，事件响应成熟度则是解药，反映了团队高效管理警报、有效分类并精准迅速解决问题的能力。
- en: But how do you avoid drowning in a sea of alerts? And more importantly, how
    do you transform alert chaos into a streamlined, mature incident response process?
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 但如何避免淹没在无休止的警报中呢？更重要的是，如何将警报混乱转化为一个精简且成熟的事件响应过程？
- en: How to Combat Alert Fatigue
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何应对警报疲劳
- en: '**Prioritize Critical Alerts**: Focus only on the alerts that genuinely matter,
    those that directly impact system performance and customer experience. If an alert
    doesn’t require immediate action, it’s just noise. Keep the signal clear.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优先处理关键警报**：只关注真正重要的警报，即那些直接影响系统性能和用户体验的警报。如果某个警报不需要立即处理，那它只是噪声。保持信号的清晰。'
- en: '**Set Thoughtful Thresholds**: Alerts should trigger only when critical thresholds
    are crossed. A minor fluctuation in CPU usage shouldn’t send your team into a
    frenzy. Design alerting rules that capture significant changes and filter out
    the minor blips.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设定合理阈值**：警报应该只在超过关键阈值时触发。CPU 使用的小幅波动不应该让你的团队陷入慌乱。设计触发规则时，应该捕捉到显著变化，并过滤掉微小波动。'
- en: '**Group Related Alerts**: When multiple alerts fire for the same underlying
    issue, it’s easy to get overwhelmed. By grouping related alerts into a single,
    actionable notification, you reduce the clutter and give your team a clearer picture
    of the problem.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关联警报**：当多个警报触发同一个潜在问题时，很容易让人感到不知所措。通过将相关警报组合成一个可操作的通知，你可以减少杂乱无章的干扰，为团队提供更清晰的问题概貌。'
- en: '**Tune and Retune**: Alerting rules aren’t “set it and forget it.” Regularly
    revisit and adjust them to keep pace with evolving system behavior. What was important
    six months ago may not be relevant now. Prune the dead weight.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调整和再调整**：警报规则不是“一劳永逸”的。定期回顾并调整它们，以跟上系统行为的变化。六个月前重要的事情，现在可能已经不再相关。去除无效的部分。'
- en: '**Automate the Repetitive**: For known, recurring issues, automate the resolution.
    If a problem can be fixed by a script, there’s no need to wake a human at 3 a.m.
    Automation reduces the volume of alerts and keeps your team focused on what really
    matters.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化重复性工作**：对于已知的、重复出现的问题，进行自动化解决。如果问题可以通过脚本修复，就没有必要在凌晨三点把人叫醒。自动化可以减少警报量，并让团队专注于真正重要的事项。'
- en: '**Post-Incident Review**: After an incident, analyze which alerts helped and
    which ones added noise. Use these lessons to refine your system further, ensuring
    that next time, your alerts are sharper and more accurate.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事后回顾**：事件发生后，分析哪些警报有帮助，哪些警报增加了噪音。利用这些经验教训进一步优化你的系统，确保下一次警报更加精准和高效。'
- en: 'By focusing on what truly matters and continuously refining your alerting processes,
    you shift from reactive firefighting to proactive, thoughtful incident management.
    This is the path to incident response maturity: where every alert has a purpose,
    every response is swift, and the system becomes resilient. As alert fatigue fades,
    what’s left is a finely tuned machine, one that runs smoothly, efficiently, and
    with the confidence that when something does go wrong, you’ll know about it, and
    you’ll know exactly how to fix it.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 通过聚焦于真正重要的事情，并持续优化警报流程，你可以从反应式的应急处理转变为主动且深思熟虑的事件管理。这是事件响应成熟度的路径：每个警报都有其目的，每次响应都迅速，系统也变得更加韧性。当警报疲劳逐渐消失时，剩下的是一台精细调优的机器，它平稳、高效地运行，并且在发生问题时，你能够及时了解并知道如何修复。
- en: Summary
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we’ve peeled back the layers of common cloud native anti-patterns—logging
    everything indiscriminately, overlooking the potential of ML and AI, neglecting
    the importance of distributed tracing, and stumbling through immature alert and
    incident processes. Each of these missteps chips away at the stability and efficiency
    of a cloud native architecture, leaving teams grappling with noise, blind spots,
    and unnecessary firefighting. However, by refining our approach—using targeted
    log aggregation, harnessing AI-driven insights, embracing distributed tracing
    for visibility, and maturing our incident response processes—we lay the groundwork
    for a more resilient, agile system. As we sidestep these anti-patterns, we transition
    from reactive crisis management to proactive operational excellence.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们剖析了常见的云原生反模式——无差别地记录所有日志、忽视机器学习和人工智能的潜力、忽略分布式追踪的重要性，以及在不成熟的警报和事件响应流程中磕磕绊绊。每一个这些错误都会削弱云原生架构的稳定性和效率，使团队在噪音、盲点和不必要的火灾救援中挣扎。然而，通过优化我们的做法——使用有针对性的日志聚合、利用人工智能驱动的洞察、拥抱分布式追踪以提高可视性，并且完善我们的事件响应流程——我们为更具韧性和灵活性的系统奠定了基础。通过规避这些反模式，我们从反应式的危机管理过渡到主动的运营卓越。
- en: Now that we’ve tackled the hidden pitfalls, it’s time to ensure the system runs
    smoothly under pressure. In the next chapter, we’ll delve into strategies for
    maintaining stability and performance as cloud-native workloads scale and evolve.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经解决了潜在的隐患，是时候确保系统在压力下运行平稳了。在下一章中，我们将深入探讨随着云原生工作负载的扩展和演变，如何保持稳定性和性能。
