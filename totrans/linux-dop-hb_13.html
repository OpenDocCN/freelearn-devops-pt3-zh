<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer038">
<h1 class="chapter-number" id="_idParaDest-273"><a id="_idTextAnchor412"/><a id="_idTextAnchor413"/>13</h1>
<h1 id="_idParaDest-274"><a id="_idTextAnchor414"/>CI/CD with Terraform, GitHub, and Atlantis</h1>
<p>In this chapter, we are going to build on the previous chapters in this book by introducing pipelines for <strong class="bold">continuous integration</strong> (<strong class="bold">CI</strong>) and <strong class="bold">continuous deployment</strong> (<strong class="bold">CD</strong>). There are many CI and CD tools available for you, both open source and closed source, as well as self-hosted and <strong class="bold">Software-as-a-Service</strong> (<strong class="bold">SaaS</strong>). We are going to demonstrate an example pipeline, starting from committing source to the repository where we store <strong class="bold">Terraform</strong> code to applying changes in your infrastructure. We will do this automatically but with reviews from <span class="No-Break">your team.</span></p>
<p>In this chapter, we are going to cover the <span class="No-Break">following topics:</span></p>
<ul>
<li>What <span class="No-Break">is CI/CD?</span></li>
<li>Continuously integrating and deploying <span class="No-Break">your infrastructure</span></li>
<li>CI/CD <span class="No-Break">with Atlantis</span></li>
</ul>
<h1 id="_idParaDest-275"><a id="_idTextAnchor415"/>Technical requirements</h1>
<p>For this chapter, you will need <span class="No-Break">the following:</span></p>
<ul>
<li>A <span class="No-Break">Linux box</span></li>
<li>A free account on GitHub or similar platform (GitLab <span class="No-Break">or Bitbucket)</span></li>
<li>The latest version <span class="No-Break">of Terraform</span></li>
<li>The <span class="No-Break"><strong class="bold">AWS CLI</strong></span></li>
<li><span class="No-Break">Git</span></li>
</ul>
<h1 id="_idParaDest-276"><a id="_idTextAnchor416"/>What is CI/CD?</h1>
<p>CI/CD is a<a id="_idIndexMarker1208"/> set of practices, tools, and processes that allow software development teams to automate the building, testing, and deployment of their applications, enabling them to release software more frequently and with greater confidence in <span class="No-Break">its quality.</span></p>
<p><strong class="bold">Continuous integration</strong> (<strong class="bold">CI</strong>) is a <a id="_idIndexMarker1209"/>practice where developers regularly integrate their code changes into a repository, and each integration triggers an automated build and test process. This helps catch errors early and ensures that the application can be built and <span class="No-Break">tested reliably.</span></p>
<p>For example, using <strong class="bold">Docker</strong>, a <a id="_idIndexMarker1210"/>developer can set<a id="_idIndexMarker1211"/> up a <strong class="bold">CI pipeline</strong> that automatically builds and tests their application whenever changes are pushed to the code repository. The pipeline can include steps to build a Docker image, run automated tests, and publish the image to a <span class="No-Break">Docker registry.</span></p>
<p>Continuous delivery is<a id="_idIndexMarker1212"/> the practice of getting software to be available for deployment after the successful integration process. For example, with a Docker image, delivery would be pushing the image to the Docker registry from where it could be picked up by the <span class="No-Break">deployment pipeline.</span></p>
<p>Finally, <strong class="bold">continuous deployment</strong> (<strong class="bold">CD</strong>) is <a id="_idIndexMarker1213"/>the practice of automatically deploying continuous delivery process code artifacts (Docker images, Java JAR files, ZIP archives, and so on) to production as soon as they are tested and validated. This eliminates the need for manual intervention in the <span class="No-Break">deployment process.</span></p>
<p>Let’s look at some common <span class="No-Break">deployment strategies:</span></p>
<ul>
<li><strong class="bold">Rolling deployment</strong>: This <a id="_idIndexMarker1214"/>strategy <a id="_idIndexMarker1215"/>involves deploying changes to a subset of servers at a time, gradually rolling out the changes to the entire infrastructure. This allows teams to monitor the changes and quickly roll back if any <span class="No-Break">issues arise.</span></li>
<li><strong class="bold">Blue-green deployment</strong>: In<a id="_idIndexMarker1216"/> this <a id="_idIndexMarker1217"/>strategy, two identical production environments are set up, one active (blue) and the other inactive (green). Code changes are deployed to the inactive environment and tested before switching the traffic to the new environment. This allows for zero <span class="No-Break">downtime deployment.</span></li>
<li><strong class="bold">Canary deployment</strong>: This <a id="_idIndexMarker1218"/>strategy<a id="_idIndexMarker1219"/> involves deploying changes to a small subset of users while keeping the majority of the users on the current version. This allows teams to monitor the changes and gather feedback before rolling out the changes to the entire <span class="No-Break">user base.</span></li>
<li><strong class="bold">Feature toggles/feature switches</strong>: With<a id="_idIndexMarker1220"/> this strategy, changes are deployed to production but hidden behind a<a id="_idIndexMarker1221"/> feature toggle. This toggle is then gradually turned on for<a id="_idIndexMarker1222"/> select users or environments, allowing teams to control the rollout of new features and gather feedback before making them available <span class="No-Break">to everyone.</span></li>
</ul>
<p>All of <a id="_idIndexMarker1223"/>these<a id="_idIndexMarker1224"/> strategies <a id="_idIndexMarker1225"/>can <a id="_idIndexMarker1226"/>be <a id="_idIndexMarker1227"/>automated using <strong class="bold">CD tools</strong>, such as <strong class="bold">Jenkins</strong>, <strong class="bold">CircleCI</strong>, <strong class="bold">GitHub</strong> and <strong class="bold">GitLab Actions</strong>, <strong class="bold">Travis CI</strong>, and <span class="No-Break">many others.</span></p>
<p>While we’re talking about the deployment of applications, we need to at least mention <strong class="bold">GitOps</strong>. GitOps is a <a id="_idIndexMarker1228"/>new approach to infrastructure and application deployment that <a id="_idIndexMarker1229"/>uses <strong class="bold">Git</strong> as the single source of truth for declarative infrastructure and application specifications. The idea is to define the desired state of the infrastructure and applications in Git repositories and use a GitOps tool to automatically apply those changes to the <span class="No-Break">target environment.</span></p>
<p>In GitOps, every change to the infrastructure or application is made via a Git commit, which triggers a pipeline that applies the changes to the target environment. This provides a complete audit trail of all changes and ensures that the infrastructure is always in the <span class="No-Break">desired state.</span></p>
<p>Some of the tools that help with enabling GitOps include <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="bold">FluxCD</strong>: This is a<a id="_idIndexMarker1230"/> popular <a id="_idIndexMarker1231"/>GitOps tool that can automate the deployment and scaling of applications and infrastructure, using Git as the single source of truth. It integrates<a id="_idIndexMarker1232"/> with <strong class="bold">Kubernetes</strong>, <strong class="bold">Helm</strong>, and <a id="_idIndexMarker1233"/>other tools to provide a complete <span class="No-Break">GitOps workflow.</span></li>
<li><strong class="bold">ArgoCD</strong>: This <a id="_idIndexMarker1234"/>is another popular GitOps tool that can deploy and <a id="_idIndexMarker1235"/>manage applications and infrastructure, using Git as the source of truth. It provides a web-based UI and <strong class="bold">CLI</strong> to manage the GitOps pipeline and integrates with Kubernetes, Helm, and <span class="No-Break">other tools.</span></li>
<li><strong class="bold">Jenkins X</strong>: This is a <a id="_idIndexMarker1236"/>CI/CD platform that includes GitOps <a id="_idIndexMarker1237"/>workflows for building, testing, and deploying applications to Kubernetes clusters. It uses GitOps to manage the entire pipeline, from source code to <span class="No-Break">production deployment.</span></li>
</ul>
<p>Now that we know what CI/CD is, we can look into various tools that could be used to build such pipelines. In the next section, we will provide you with some examples of a pipeline for cloning the latest version of your repository, building a Docker i<a id="_idTextAnchor417"/>mage, and running <span class="No-Break">some tests.</span></p>
<h2 id="_idParaDest-277"><a id="_idTextAnchor418"/>An example of CI/CD pipelines</h2>
<p>Let’s look at a few examples of automation pipelines that will apply our Terraform changes to the different CD tools <span class="No-Break">out there.</span></p>
<h3>Jenkins</h3>
<p>Jenkins<a id="_idIndexMarker1238"/> is the most popular open source CI/CD tools out<a id="_idIndexMarker1239"/> there. It transformed from the clicked-through configuration to the <strong class="bold">CaaC</strong> approach. Its go-to<a id="_idIndexMarker1240"/> language for defining pipelines<a id="_idIndexMarker1241"/> is <strong class="bold">Groovy</strong> and <strong class="bold">Jenkinsfile</strong>. The<a id="_idIndexMarker1242"/> following is an example of Jenkinsfile that will run a plan, ask the user for the input, and run<strong class="source-inline"> apply</strong> if the user approves <span class="No-Break">the change:</span></p>
<pre class="source-code">
pipeline {
  agent any
  environment {
    AWS_ACCESS_KEY_ID = credentials('aws-key-id')
    AWS_SECRET_ACCESS_KEY = credentials('aws-secret-key')
  }</pre>
<p>The preceding code opens a new pipeline definition. It’s defined to run in any available Jenkins agent. Next, we’re setting up environment variables to be used within this pipeline. The <a id="_idIndexMarker1243"/>environment variables text is being retrieved from the <strong class="source-inline">aws-key-id</strong> and <strong class="source-inline">aws-secret-key</strong> Jenkins credentials. These need to <a id="_idIndexMarker1244"/>be defined before we can run <span class="No-Break">this pipeline.</span></p>
<p>Next, we’ll define that each step pipeline will run inside the <span class="No-Break"><strong class="source-inline">stages</strong></span><span class="No-Break"> block:</span></p>
<pre class="source-code">
  stages {
    stage('Checkout') {
        steps {
            checkout scm
        }
    }</pre>
<p>First, we’ll clone our Git repository; the step that does this is <strong class="source-inline">checkout scm</strong>. The URL will be configured in the Jenkins <span class="No-Break">UI directly:</span></p>
<pre class="source-code">
    stage('TF Plan') {
        steps {
            dir('terraform') {
                sh 'terraform init'
                sh 'terraform plan -out terraform.tfplan'
            }
       }
     }</pre>
<p>Next, we’ll run <strong class="source-inline">terraform init</strong> to initialize the Terraform environment. Here, we’re running the plan and saving the output to the <strong class="source-inline">terraform.tfplan</strong> file, which we will run <strong class="source-inline">apply</strong> with in the <span class="No-Break">final step:</span></p>
<pre class="source-code">
    stage('Approval') {
        steps {
            script {
                def userInput = input(id: 'confirm', message: 'Run apply?', parameters: [ [$class: 'BooleanParameterDefinition', defaultValue: false, description: 'Running apply', name: 'confirm'] ])
            }
        }
    }</pre>
<p>This step <a id="_idIndexMarker1245"/>defines user input. We’ll need to confirm this <a id="_idIndexMarker1246"/>by running <strong class="source-inline">apply</strong> after we review the plan’s output. We’re defining the default as <strong class="source-inline">false</strong>. The pipeline will wait for your input on <span class="No-Break">this step:</span></p>
<pre class="source-code">
    stage('TF Apply') {
        steps {
            dir('terraform') {
                sh 'terraform apply -auto-approve -input=false terraform.tfplan'
                sh 'rm -f terraform.tfplan'
            }
        }
    }
  }
}</pre>
<p>Finally, if you’ve confirmed that the pipeline will run <strong class="source-inline">apply</strong> without asking for further user input (<strong class="source-inline">-input=false option</strong>) and if <strong class="source-inline">apply</strong> ran without any errors, it will remove the <strong class="source-inline">terraform.tfplan</strong> file that was created in the <span class="No-Break">plan step.</span></p>
<h3>GitHub Actions basics</h3>
<p>It’s possible<a id="_idIndexMarker1247"/> to create a similar pipeline with <strong class="bold">GitHub Actions</strong> that<a id="_idIndexMarker1248"/> corresponds to the preceding Jenkinsfile, but unfortunately, GitHub Actions does not support user input while an action is running. We could use the <strong class="source-inline">workflow_dispatch</strong> option, but it will ask the user for input before the Action runs (see the official documentation for a reference: <a href="https://github.blog/changelog/2021-11-10-github-actions-input-types-for-manual-workflows/">https://github.blog/changelog/2021-11-10-github-actions-input-types-for-manual-workflows/</a>). So, instead, let’s create an action that will run <strong class="source-inline">plan</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">apply</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
name: Terraform Apply
on:
  push:
    branches: [ main ]</pre>
<p>The preceding code defines that the GitHub Action will only be triggered on changes to the <span class="No-Break">main branch:</span></p>
<pre class="source-code">
env:
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}</pre>
<p>Here, we’re defining environment variables similarly to the Jenkins pipeline. AWS access and secret keys are coming from secrets stored in GitHub that we will need to add beforehand. This won’t be required if our GitHub runner is running inside the AWS environment or we are using GitHub OpenID Connect. You can read about the latter by checking out the GitHub <span class="No-Break">documentation: </span><a href="https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/configuring-openid-connect-in-amazon-web-services"><span class="No-Break">https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/configuring-openid-connect-in-amazon-web-services</span></a><span class="No-Break">.</span></p>
<p>Next, we can define the GitHub Action steps within the <span class="No-Break"><strong class="source-inline">jobs</strong></span><span class="No-Break"> block:</span></p>
<pre class="source-code">
jobs:
  terraform_apply:
    runs-on: ubuntu-latest</pre>
<p>Here, we’re defining a job named <strong class="source-inline">terraform_apply</strong> that will run on the latest version of the Ubuntu <a id="_idIndexMarker1249"/>runner that’s available in <span class="No-Break">Github Actions:</span></p>
<pre class="source-code">
    steps:
    - name: Checkout code
      uses: actions/checkout@v2</pre>
<p>This step <a id="_idIndexMarker1250"/>checks out the code. We’re using a predefined Action available in GitHub instead of creating a script that will run the Git command line. The exact code it will run is available <span class="No-Break">at </span><a href="https://github.com/actions/checkout"><span class="No-Break">https://github.com/actions/checkout</span></a><span class="No-Break">:</span></p>
<pre class="source-code">
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v1</pre>
<p>The <strong class="source-inline">Setup Terraform</strong> step will download Terraform for us. By default, it’ll download the latest available version of Terraform, but we can pin an exact version if we need to. The code for the step is available <span class="No-Break">at </span><a href="https://github.com/hashicorp/setup-terraform"><span class="No-Break">https://github.com/hashicorp/setup-terraform</span></a><span class="No-Break">:</span></p>
<pre class="source-code">
    - name: Terraform Plan
      working-directory: terraform/
      run: |
        terraform init
        terraform plan -out terraform.tfplan</pre>
<p>In the <strong class="source-inline">Terraform Plan</strong> step, we’re initializing Terraform and running the plan in the same way we did for the <span class="No-Break">Jenkins pipeline:</span></p>
<pre class="source-code">
    - name: Terraform Apply
      working-directory: terraform/
      run: |
        terraform apply -auto-approve –input=false terraform.tfplan
        rm -f terraform.tfplan</pre>
<p>Finally, the <strong class="source-inline">Terraform Apply</strong> step is applying infrastructure changes from the previously saved Terraform plan file, <strong class="source-inline">terraform.tfplan</strong>, and removing the <span class="No-Break">plan file.</span></p>
<p>If you want <a id="_idIndexMarker1251"/>to create a more robust piece of code <a id="_idIndexMarker1252"/>that will work in every CI/CD tool out there, it’s possible to create<a id="_idIndexMarker1253"/> a <strong class="bold">Bash script</strong> to do the heavy lifting. With Bash scripting, it’s also much easier to embed some testing before even running the plan. Here’s a sample Bash script that will run the Terraform plan and apply it <span class="No-Break">for you:</span></p>
<pre class="source-code">
#!/usr/bin/env bash
set -u
set -e</pre>
<p>Here, we’re setting Bash as a default shell for running this script. In the next few lines, we’re modifying the default settings for the script so that it stops executing when it encounters any unbound variables <span class="No-Break">or errors:</span></p>
<pre class="source-code">
# Check if Terraform binary is in PATH
if command -v terraform &amp;&gt; /dev/null; then
  TERRAFORM_BIN="$(command -v terraform)"
else
  echo "Terraform not installed?"
  exit 1
fi</pre>
<p>This code block checks whether Terraform is available in the system and saves the full path to it inside the <strong class="source-inline">TERRAFORM_BIN</strong> variable, which we will be <span class="No-Break">using later:</span></p>
<pre class="source-code">
# Init terraform backend
$TERRAFORM_BIN init -input=false</pre>
<p>Initialize the Terraform environment before running <span class="No-Break">the plan:</span></p>
<pre class="source-code">
# Plan changes
echo "Running plan..."
$TERRAFORM_BIN plan -input=false -out=./terraform.tfplan</pre>
<p>Run <strong class="source-inline">plan</strong> and <a id="_idIndexMarker1254"/>save it to a file for <span class="No-Break">later use:</span></p>
<pre class="source-code">
echo "Running Terraform now"
if $TERRAFORM_BIN apply -input=false ./terraform.tfplan; then
    echo "Terraform finished successfully"
    RETCODE=0
else
    echo "Failed!"    fi
fi</pre>
<p>The <a id="_idIndexMarker1255"/>preceding code block executes <strong class="source-inline">Terraform apply</strong> and checks the return code of the command. It also displays <span class="No-Break">appropriate information.</span></p>
<p>Other major CI/CD solutions use a very similar approach. The biggest difference is between Jenkins, corporate tools, and open source solutions where <strong class="bold">YAML</strong> configuration is<a id="_idIndexMarker1256"/> the most common. In the next section, we will dig a bit deeper into each stage of the pipeline, focusing on the integration testing for Terraf<a id="_idTextAnchor419"/>orm and deploying changes to <span class="No-Break">the infrastructure.</span></p>
<h1 id="_idParaDest-278">Continuously integrating and deploying your infrastructure</h1>
<p>Testing application <a id="_idIndexMarker1257"/>code is now a de facto standard, especially since the adoption of <strong class="bold">test-driven development</strong> (<strong class="bold">TDD</strong>). TDD is <a id="_idIndexMarker1258"/>a software development process in which developers write automated tests before <span class="No-Break">writing code.</span></p>
<p>These tests are designed to fail initially, and developers then write code to make them pass. The code is continuously refactored to ensure it is efficient and maintainable while passing all tests. This approach helps reduce the number of bugs and increase the reliability of <span class="No-Break">the software.</span></p>
<p>Testing<a id="_idIndexMarker1259"/> infrastructure is not as<a id="_idIndexMarker1260"/> easy as that as it’s hard to check whether Amazon <strong class="bold">Elastic Compute Cloud</strong> (<strong class="bold">EC2</strong>) will be successfully started without actually starting the instance. It’s possible to mock <strong class="bold">API</strong> calls to AWS, but it won’t guarantee that the actual API will return the same results as your testing code. With AWS, it would also mean that testing will be slow (we will need to wait for this EC2 instance to come up) and probably generate additional <span class="No-Break">cloud costs.</span></p>
<p>There are multiple infrastructure testing tools, both integrated with Terraform and third-par<a id="_idTextAnchor420"/>ty software (which is also an open <span class="No-Break">source software).</span></p>
<h2 id="_idParaDest-279"><a id="_idTextAnchor421"/>Integration testing</h2>
<p>There are<a id="_idIndexMarker1261"/> multiple basic tests we could run in our CI pipeline. We<a id="_idIndexMarker1262"/> can detect drift between our actual code and what’s running in the cloud, we can lint our code according to the recommended format, and we can test whether the code is to the letter of our compliance policies. We can also estimate AWS costs from the Terraform code. Harder and more time-consuming processes include unit testing and end-to-end testing of our code. Let’s take a look at the available tools we could use in <span class="No-Break">our pipeline.</span></p>
<p>Most basic tests we could be running from the start involve simply running <strong class="source-inline">terraform validate</strong> and <strong class="source-inline">terraform fmt</strong>. The former will check whether the syntax of the Terraform code is valid (meaning there are no typos in resources and/or variables, all required variables are present, and so on). The <strong class="source-inline">fmt</strong> check will update the formatting of the code according to the Terraform standards, which means that all excessive white spaces will be removed, or some white spaces may be added to align the <strong class="source-inline">=</strong> sign for readability. This may be sufficient for simpler infrastructure and we recommend adding those tests from the start as it’s pretty straightforward to do. You can reuse parts of the code we provided ea<a id="_idTextAnchor422"/>rlier to bootstrap the process for your <span class="No-Break">existing code.</span></p>
<h3>Infrastructure costs</h3>
<p>Infrastructure costs<a id="_idIndexMarker1263"/> are not a functional, static, or unit test you could be running in your testing pipeline. Although it’s very useful to monitor that aspect of your infrastructure, your manager will also be happy to know when the AWS budget is checking out <span class="No-Break">with predictions.</span></p>
<p><strong class="bold">Infracost.io</strong> is a <a id="_idIndexMarker1264"/>cloud cost estimation tool that allows you to monitor and manage infrastructure costs by providing real-time cost analysis for cloud resources. With Infracost.io, you can estimate the cost of infrastructure changes and avoid any unexpected costs by providing cost feedback at every stage of the <span class="No-Break">development cycle.</span></p>
<p>Integrating Infracost.io <a id="_idIndexMarker1265"/>into GitHub Actions is a straightforward process that involves creating an Infracost.io account and generating an API key that will allow you to access the cost <span class="No-Break">estimation data.</span></p>
<p>Next, you need to install the Infracost.io CLI on your local machine. The CLI is a command-line tool that allows you to calculate and compare <span class="No-Break">infrastructure costs.</span></p>
<p>After installing the CLI, you can add an Infracost.io action to your GitHub workflow by creating a new action file – for <span class="No-Break">example, </span><span class="No-Break"><strong class="source-inline">.github/workflows/infracost.yml</strong></span><span class="No-Break">.</span></p>
<p>In the Infracost.io action file, you need to specify the Infracost.io API key and the path to your Terraform <span class="No-Break">configuration files:</span></p>
<pre class="source-code">
name: Infracost
on:
  push:
    branches:
      - main
jobs:
  infracost:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v2
      - name: Run Infracost
        uses: infracost/infracost-action@v1
        with:
          api_key: ${{ secrets.INFRACOST_API_KEY }}
          terraform_dir: ./terraform</pre>
<p>Finally, commit <a id="_idIndexMarker1266"/>and push the changes to your GitHub repository. Whenever a new Terraform configuration file is pushed to the repository, the Infracost.io action will automatically calculate the cost estimation and provide feedback on the GitHub <span class="No-Break">Actions page.</span></p>
<p>Infracost is free for open source projects, but it’s also possible to create your own service for monitoring cloud costs. A GitHub repository for Infracost can be found <span class="No-Break">at </span><a href="https://github.com/infracost/infracost."><span class="No-Break">https://github.com/infracost/infracost.</span></a></p>
<p>By integrating it into your CI pipeline, you can proactively monitor and manage your infrastructure costs and make informed decisions on infrast<a id="_idTextAnchor423"/>ructure changes before deploying it to your <span class="No-Break">cloud account.</span></p>
<h3>Drift testing</h3>
<p>One of the <a id="_idIndexMarker1267"/>challenges of managing infrastructure with Terraform is ensuring that the actual state of the infrastructure matches the desired state defined in the Terraform configuration files. This is where the concept of <em class="italic">drift</em> <span class="No-Break">comes in.</span></p>
<p><strong class="bold">Drift</strong> occurs <a id="_idIndexMarker1268"/>when there is a difference between the desired state of the infrastructure and its actual state. For example, if a resource that was created using Terraform is manually modified outside of Terraform, the actual state of the infrastructure will differ from the desired state defined in Terraform configuration files. This can cause inconsistencies in the infrastructure and may lead to <span class="No-Break">operational issues.</span></p>
<p>To detect drift in the infrastructure, Terraform provides a command called <strong class="source-inline">terraform plan</strong>. When this command is run, Terraform compares the desired state defined in the Terraform configuration files with the actual state of the infrastructure and generates a plan of the changes that need to be made to bring the infrastructure back into the desired state. If there are any differences between the desired and actual states, Terraform will show them in the <span class="No-Break">plan output.</span></p>
<p>It’s also possible to use third-party tools that will extend this feature of Terraform. One of these tools <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">Driftctl</strong></span><span class="No-Break">.</span></p>
<p><strong class="source-inline">Driftctl</strong> is<a id="_idIndexMarker1269"/> an open <a id="_idIndexMarker1270"/>source tool that helps detect drift in cloud infrastructure managed by Terraform. It scans the actual state of the infrastructure resources and compares them against the desired state defined in Terraform configuration files to identify any <a id="_idIndexMarker1271"/>discrepancies or differences. <strong class="source-inline">Driftctl</strong><strong class="source-inline"><a id="_idIndexMarker1272"/></strong> supports a wide range of cloud providers, including AWS, <strong class="bold">Google Cloud</strong>, <strong class="bold">Microsoft Azure</strong>, <span class="No-Break">and</span><span class="No-Break"><a id="_idIndexMarker1273"/></span><span class="No-Break"> Kubernetes.</span></p>
<p><strong class="source-inline">Driftctl</strong> can be used in various ways to detect drift in infrastructure. It can be integrated with a CI/CD pipeline to automatically detect drift and trigger corrective actions. It can also be run manually to check for drift <span class="No-Break">on demand.</span></p>
<p>Here’s an example GitHub pipeline that utilizes <strong class="source-inline">Driftctl</strong> to detect <span class="No-Break">infrastructure drift:</span></p>
<pre class="source-code">
name: Detect Infrastructure Drift
on:
  push:
    branches:
      - main</pre>
<p>The preceding code indicates that this pipeline will run only on the <span class="No-Break"><strong class="source-inline">main</strong></span><span class="No-Break"> branch.</span></p>
<p>Here, we’re defining a job called <strong class="source-inline">detect-drift</strong> that will run on the latest Ubuntu Linux <span class="No-Break">runner available:</span></p>
<pre class="source-code">
jobs:
  detect-drift:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v2</pre>
<p>Next, we’re starting to define what we’re going to do in each step of the pipeline – first, we will use a predefined action that will run <strong class="source-inline">git clone</strong> on <span class="No-Break">the runner:</span></p>
<pre class="source-code">
    - name: Install Terraform
      run: |
        sudo apt-get update
        sudo apt-get install -y unzip
        curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -
        sudo apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"
        sudo apt-get update &amp;&amp; sudo apt-get install terraform</pre>
<p>The next step<a id="_idIndexMarker1274"/> we’re defining is a shell script that will install, unzip, and download Terraform from the public repository published <span class="No-Break">by HashiCorp:</span></p>
<pre class="source-code">
    - name: Install Driftctl
      run: |
        curl https://github.com/cloudskiff/driftctl/releases/download/v0.8.2/driftctl_0.8.2_linux_amd64.tar.gz -sSLo driftctl.tar.gz
        tar -xzf driftctl.tar.gz
        sudo mv driftctl /usr/local/bin/</pre>
<p>In this step, we will install the <strong class="source-inline">Driftctl</strong> tool by downloading an archive from the public release on GitHub. We will extract it and move the binary to the <strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">usr/local/bin</strong></span><span class="No-Break"> directory:</span></p>
<pre class="source-code">
    - name: Initialize Terraform
      run: terraform init
    - name: Check Terraform Configuration
      run: terraform validate</pre>
<p>The preceding <a id="_idIndexMarker1275"/>steps simply involve running <strong class="source-inline">terraform init</strong> and <strong class="source-inline">terraform validate</strong> to verify whether we can access the Terraform backend and whether the code we intend to check in the next few steps is valid from a <span class="No-Break">syntax perspective:</span></p>
<pre class="source-code">
    - name: Detect Drift with Driftctl
      run: |
        driftctl scan –from tfstate://./terraform.tfstate –output json &gt; drift.json
    - name: Upload Drift Report to GitHub
      uses: actions/upload-artifact@v2
      with:
        name: drift-report
        path: drift.json</pre>
<p>The final two steps are running the <strong class="source-inline">Driftctl</strong> tool and saving their findings inside the <strong class="source-inline">driftctl.json</strong> file, which is uploaded to GitHub artifacts with the <span class="No-Break">name </span><span class="No-Break"><strong class="source-inline">drift-report</strong></span><span class="No-Break">.</span></p>
<p>To summarize, this pipeline runs on the <strong class="source-inline">main</strong> branch and performs the <span class="No-Break">following steps:</span></p>
<ul>
<li>Checks out the code from the <span class="No-Break">GitHub repository.</span></li>
<li>Installs <strong class="source-inline">terraform</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">driftctl</strong></span><span class="No-Break">.</span></li>
<li>Initializes Terraform and validates the Terraform <span class="No-Break">configuration files.</span></li>
<li>Uses <strong class="source-inline">driftctl</strong> to scan the actual state of the infrastructure resources and compares them against the desired state defined in the Terraform configuration files to detect any drift. The output of this scan is saved to a JSON file <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">drift.json</strong></span><span class="No-Break">.</span></li>
<li>Uploads the <strong class="source-inline">drift.json</strong> file as an artifact to GitHub, making it available for <span class="No-Break">further analysis.</span></li>
</ul>
<p>Additionally, this <a id="_idIndexMarker1276"/>pipeline can be customized to meet specific requirements, such as integrating with a CI/CD pipeline or running on a schedule to regularly check for drift in <span class="No-Break">the infrastructure.</span></p>
<p>The need for installing <strong class="source-inline">driftctl</strong> and <strong class="source-inline">terraform</strong> on every pipeline run is not desired, so we recommend that you prepare your own Docker image with preinstalled proper versions of those tools and use that instead. It will also increase your security along <span class="No-Break">the way.</span></p>
<p>You can find more in<a id="_idTextAnchor424"/>formation about the project <a id="_idIndexMarker1277"/>on the <span class="No-Break">website: </span><a href="https://driftctl.com/"><span class="No-Break">https://driftctl.com/</span></a><span class="No-Break">.</span></p>
<h3>Security testing</h3>
<p>Testing infrastructure security <a id="_idIndexMarker1278"/>is an essential aspect of maintaining secure and stable systems. As modern infrastructure is often defined and managed as code, it is necessary to test it just like any other code. <strong class="bold">Infrastructure as Code</strong> (<strong class="bold">IaC</strong>) testing helps identify security <a id="_idIndexMarker1279"/>vulnerabilities, configuration issues, and other flaws that may lead to <span class="No-Break">system compromise.</span></p>
<p>There are several automated tools available that can aid in infrastructure security testing. These tools can help identify potential security issues, such as misconfigured security groups, unused security rules, and unsecured <span class="No-Break">sensitive data.</span></p>
<p>There are several tools we can use to test security as a separate process from the <span class="No-Break">CI pipeline:</span></p>
<ul>
<li><strong class="bold">Prowler</strong>: This <a id="_idIndexMarker1280"/>is an open source tool that scans AWS infrastructure for security vulnerabilities. It can <a id="_idIndexMarker1281"/>check for issues such as AWS <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) misconfigurations, open security groups, and <strong class="bold">S3 bucket</strong> <span class="No-Break">permission</span><span class="No-Break"><a id="_idIndexMarker1282"/></span><span class="No-Break"> issues.</span></li>
<li><strong class="bold">CloudFormation Guard</strong>: This<a id="_idIndexMarker1283"/> is a tool that validates <strong class="bold">AWS CloudFormation</strong> templates <a id="_idIndexMarker1284"/>against a set of predefined security rules. It can help identify issues such as open security groups, unused IAM policies, and non-encrypted <span class="No-Break">S3 buckets.</span></li>
<li><strong class="bold">OpenSCAP</strong>: This<a id="_idIndexMarker1285"/> is a tool that provides automated security compliance testing for Linux-based infrastructure. It can scan the system for compliance with various security standards, such as the <strong class="bold">Payment Card Industry Data Security Standard</strong> (<strong class="bold">PCI DSS</strong>) or <strong class="bold">National Institute of Standards and Technology Special Publication 800-53</strong> (<strong class="bold">NIST </strong><span class="No-Break"><strong class="bold">SP 800-53</strong></span><span class="No-Break">).</span></li>
<li><strong class="bold">InSpec</strong>: This<a id="_idIndexMarker1286"/> is another open source testing framework that can be used for testing infrastructure compliance and security. It has built-in support for various platforms and can be used to test against different security standards, such as the <strong class="bold">Health Insurance Portability and Accountability Act</strong> (<strong class="bold">HIPAA</strong>) and <strong class="bold">Center for Internet </strong><span class="No-Break"><strong class="bold">Security</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">CIS</strong></span><span class="No-Break">).</span></li>
</ul>
<p>Here, we’re <a id="_idIndexMarker1287"/>focusing on integrating some security testing within the CI. The tools we could integrate here are <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="bold">tfsec</strong> is an <a id="_idIndexMarker1288"/>open source static analysis tool for Terraform code. It scans Terraform configurations to detect potential security issues and provides suggestions for remediation. It has built-in support for various cloud providers and can help identify issues such as weak authentication, insecure network configurations, and unencrypted data storage. Its GitHub repository can be found <span class="No-Break">at </span><a href="https://github.com/aquasecurity/tfsec"><span class="No-Break">https://github.com/aquasecurity/tfsec</span></a><span class="No-Break">.</span></li>
<li><strong class="bold">Terrascan</strong> is an <a id="_idIndexMarker1289"/>open source tool for static code analysis of IaC files. It supports various IaC file formats, including Terraform, Kubernetes YAML, and Helm charts, and scans them for security vulnerabilities, compliance violations, and other issues. Terrascan can be integrated into a CI/CD pipeline and helps ensure that infrastructure deployments are secure and compliant with industry standards. Its GitHub repository can be found <span class="No-Break">at </span><a href="https://github.com/tenable/terrascan"><span class="No-Break">https://github.com/tenable/terrascan</span></a><span class="No-Break">.</span></li>
<li><strong class="bold">CloudQuery</strong> is an open <a id="_idIndexMarker1290"/>source tool that enables users to test security policies and compliance across different cloud platforms, including AWS, <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>), and Microsoft Azure. It provides a unified query language and interface to access cloud resources, allowing users to analyze configurations and detect potential security vulnerabilities. CloudQuery integrates with various CI/CD pipelines, making it easy to automate testing for security policies and compliance. Users can also customize queries and rules based on their specific needs and standards. You can read more about this topic in their blog <span class="No-Break">post: </span><a href="https://www.cloudquery.io/how-to-guides/open-source-cspm"><span class="No-Break">https://www.cloudquery.io/how-to-guides/open-source-cspm</span></a><span class="No-Break">.</span></li>
</ul>
<p>Let’s look at the<a id="_idIndexMarker1291"/> example GitHub pipeline that’s integrating the <span class="No-Break"><strong class="source-inline">terrascan</strong></span><span class="No-Break"> tool:</span></p>
<pre class="source-code">
name: Terrascan Scan
on: [push]
jobs:
  scan:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v2
    - name: Install Terrascan
      run: |
        wget https://github.com/accurics/terrascan/releases/latest/download/terrascan_linux_amd64.zip
        unzip terrascan_linux_amd64.zip
        rm -f terrascan_linux_amd64.zip
        sudo mv terrascan /usr/local/bin/
    - name: Run Terrascan
      run: |
        terrascan scan -f ./path/to/infrastructure/code</pre>
<p>In this<a id="_idIndexMarker1292"/> workflow, the <strong class="source-inline">on: [push]</strong> line specifies that the workflow should be triggered whenever changes are pushed to <span class="No-Break">the repository.</span></p>
<p>The <strong class="source-inline">jobs</strong> section contains a single job called <strong class="source-inline">scan</strong>. The <strong class="source-inline">runs-on</strong> key specifies that the job should run on an <span class="No-Break">Ubuntu machine.</span></p>
<p>The <strong class="source-inline">steps</strong> section contains <span class="No-Break">three steps:</span></p>
<ol>
<li>It checks out the code from the repository using the <span class="No-Break"><strong class="source-inline">actions/checkout</strong></span><span class="No-Break"> action.</span></li>
<li>It downloads and installs Terrascan on the machine using the <strong class="source-inline">wget</strong> and <strong class="source-inline">unzip</strong> commands. Note that this step assumes that you’re running the workflow on a <span class="No-Break">Linux machine.</span></li>
<li>It runs Terrascan to scan the infrastructure code. You’ll need to replace <strong class="source-inline">./path/to/infrastructure/code</strong> with the actual path to your <span class="No-Break">infrastructure code.</span></li>
</ol>
<p>Once you’ve created this workflow and pushed it to your GitHub repository, GitHub Actions will automatically run the workflow whenever changes are pushed to the repository. You can view the results of the Terrascan scan in the <span class="No-Break">workflow logs.</span></p>
<p>Let’s move on to <strong class="bold">infrastructure unit testing</strong>. There <a id="_idIndexMarker1293"/>are generally two options at the moment: <strong class="bold">Terratest</strong> when testing <strong class="bold">HCL</strong> code directly or <strong class="bold">CDKT<a id="_idTextAnchor425"/>F</strong>/<strong class="bold">Pulumi</strong> if you want to use more advanced programming languages to maintain <span class="No-Break">your IaC.</span></p>
<h3>Testing with Terratest</h3>
<p>Terratest is <a id="_idIndexMarker1294"/>an open source testing framework<a id="_idIndexMarker1295"/> for infrastructure code, including HCL code for Terraform. It was first released in 2017 by <strong class="bold">Gruntwork</strong>, a company that specializes in infrastructure automation and offers a set of pre-built infrastructure modules called the Gruntwork <span class="No-Break">IaC Library.</span></p>
<p>Terratest is designed to simplify testing infrastructure code by providing a suite of helper functions and libraries that allow users to write automated tests <a id="_idIndexMarker1296"/>written in <strong class="bold">Go</strong>, a popular programming language for infrastructure automation. Terratest can be used to test<a id="_idIndexMarker1297"/> not only Terraform code but also infrastructure <a id="_idIndexMarker1298"/>built with other tools <a id="_idIndexMarker1299"/>such as <strong class="bold">Ansible</strong>, <strong class="bold">Packer</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="bold">Docker</strong></span><span class="No-Break">.</span></p>
<p>One of the key benefits of Terratest is that it allows developers to test their infrastructure code in a production-like environment, without the need for a dedicated test environment. This can be achieved by using tools such as Docker and Terraform to spin up temporary infrastructure resources for <span class="No-Break">testing purposes.</span></p>
<p>Terratest also provides a range of <a id="_idIndexMarker1300"/>test types, including <strong class="bold">unit tests</strong>, <strong class="bold">integration tests</strong>, and <strong class="bold">end-to-end tests</strong>, allowing <a id="_idIndexMarker1301"/>users to test their<a id="_idIndexMarker1302"/> infrastructure code at different levels of abstraction. This helps ensure that code changes are thoroughly tested before they are deployed to production, reducing the risk of downtime or <span class="No-Break">other issues.</span></p>
<p>An example of testing an <strong class="source-inline">aws_instance</strong> resource we created in a module in the previous chapter would look <span class="No-Break">like this:</span></p>
<pre class="source-code">
package test
import (
  "testing"
  "github.com/gruntwork-io/terratest/modules/aws"
  "github.com/gruntwork-io/terratest/modules/terraform"
  "github.com/stretchr/testify/assert"
)
func TestAwsInstance(t *testing.T) {
  terraformOptions := &amp;terraform.Options{
    TerraformDir: "../path/to/terraform/module",
  }
  defer terraform.Destroy(t, terraformOptions)
  terraform.InitAndApply(t, terraformOptions)
  instanceID := terraform.Output(t, terraformOptions, "aws_instance_id")
  instance := aws.GetEc2Instance(t, "us-west-2", instanceID)
  assert.Equal(t, "t3.micro", instance.InstanceType)
  assert.Equal(t, "TestInstance", instance.Tags["Name"])
}</pre>
<p>In this <a id="_idIndexMarker1303"/>example, we first define a test function called <strong class="source-inline">TestAwsInstance</strong> using the <a id="_idIndexMarker1304"/>standard <strong class="bold">Go</strong> testing library. Inside the function, we create a <strong class="source-inline">terraformOptions</strong> object that specifies the directory of our <span class="No-Break">Terraform module.</span></p>
<p>Then, we use the <strong class="source-inline">terraform.InitAndApply</strong> function to initialize and apply the Terraform configuration, creating <a id="_idIndexMarker1305"/>the <strong class="bold">AWS EC2</strong> <span class="No-Break">instance resource.</span></p>
<p>Next, we use the <strong class="source-inline">aws.GetEc2Instance</strong> function from the <strong class="bold">Terratest AWS module</strong> to <a id="_idIndexMarker1306"/>retrieve information about the instance that was created, using its ID <span class="No-Break">as input.</span></p>
<p>Finally, we <a id="_idIndexMarker1307"/>use the <strong class="source-inline">assert</strong> library from the <strong class="source-inline">testify</strong> package to write assertions that validate the properties of the instance, such as its instance type and tags. If any of the assertions fail, the test <span class="No-Break">will fail.</span></p>
<p>To run this example, you will need to ensure that you have ins<a id="_idTextAnchor426"/>talled the Terratest and AWS Go modules, and have valid AWS credentials set up in <span class="No-Break">your environment.</span></p>
<h3>Unit testing with CDKTF</h3>
<p>AWS <strong class="bold">Cloud Development Kit for Terraform</strong> (<strong class="bold">CDKTF</strong>) is an open source framework <a id="_idIndexMarker1308"/>for Terraform that allows<a id="_idIndexMarker1309"/> developers to define the infrastructure and services inside any cloud solution Terraform supports<a id="_idIndexMarker1310"/> using programming languages<a id="_idIndexMarker1311"/> such as <strong class="bold">TypeScript</strong>, <strong class="bold">Python</strong>, and <strong class="bold">C#</strong>. It <a id="_idIndexMarker1312"/>enables the creation of IaC using high-level object-oriented abstractions, reducing the complexity of writing and maintaining <span class="No-Break">infrastructure code.</span></p>
<p>CDKTF was <a id="_idIndexMarker1313"/>initially released in March 2020, and it is a collaboration between AWS and HashiCorp, the company behind Terraform. CDKTF leverages the best of both worlds: the familiarity and expressiveness of modern programming languages, and the declarative, multi-cloud capabilities <span class="No-Break">of Terraform.</span></p>
<p>TypeScript is the most popular language used with CDKTF, and it provides a type-safe development experience with features such as static type-checking, code completion, <span class="No-Break">and refactoring.</span></p>
<p>As an example, let’s reuse the Terraform code from <a href="B18197_12.xhtml#_idTextAnchor365"><span class="No-Break"><em class="italic">Chapter 12</em></span></a><span class="No-Break">:</span></p>
<pre class="source-code">
resource "aws_instance" "vm_example" {
  ami           = "ami-830c94e3"
  instance_type = "t2.micro"
  tags = {
    Name = "DevOpsGuideTerraformExample"
}</pre>
<p>The <a id="_idIndexMarker1314"/>equivalent code in CDKTF in Python would look <span class="No-Break">like this:</span></p>
<pre class="source-code">
#!/usr/bin/env python
from constructs import Construct
from cdktf import App, TerraformStack
from cdktf_cdktf_provider_aws.provider import AwsProvider
from cdktf_cdktf_provider_aws.instance import Instance
from cdktf_cdktf_provider_aws.data_aws_ami import DataAwsAmi, DataAwsAmiFilter</pre>
<p>Here, we’re <a id="_idIndexMarker1315"/>importing all required modules. The first line of the script says that the default interpreter for it should be a Python interpreter that’s available in <span class="No-Break">the system:</span></p>
<pre class="source-code">
class MyStack(TerraformStack):
    def __init__(self, scope: Construct, id: str):
        super().__init__(scope, id)
        AwsProvider(self, "AWS", region="eu-central-1")</pre>
<p>In the preceding line, we’re configuring a default provider to use the <strong class="source-inline">eu-central-1</strong> region. Let’s see <span class="No-Break">what’s next:</span></p>
<pre class="source-code">
        ec2_instance = Instance(
            self,
            id_="ec2instanceName",
            instance_type="t2.micro"
            ami="ami-830c94e3",
        )
app = App()
MyStack(app, "ec2instance")
app.synth()</pre>
<p>The<a id="_idIndexMarker1316"/> following is an example unit test for the <a id="_idIndexMarker1317"/>preceding code that uses the <strong class="source-inline">unittest</strong> Python module and the usual syntax for TDD <span class="No-Break">in Python:</span></p>
<pre class="source-code">
 import unittest
from your_cdk_module import MyStack  # Replace 'your_cdk_module' with the actual module name containing the MyStack class
class TestMyStack(unittest.TestCase):
    def test_ec2_instance(self):
        app = App()
        stack = MyStack(app, "test-stack")
        # Get the EC2 instance resource created in the stack
        ec2_instance = stack.node.try_find_child("ec2instance")
        # Assert EC2 instance properties
        self.assertEqual(ec2_instance.ami, "ubuntuAMI")
        self.assertEqual(ec2_instance.instance_type, "t3.micro")
        self.assertEqual(ec2_instance.key_name, "admin_key")
        self.assertEqual(ec2_instance.subnet_id, "subnet-1234567890")
if __name__ == '__main__':
    unittest.main()</pre>
<p>As stated <a id="_idIndexMarker1318"/>previously, unfortunately, the instance <a id="_idIndexMarker1319"/>must be running in AWS for us to test whether we have the desired tags and other properties available through the CDKTF. The instance is being created by the <strong class="source-inline">setUp()</strong> function and terminated with the <strong class="source-inline">tearDown()</strong> function. Here, w<a id="_idTextAnchor427"/>e’re using a small instance that is <strong class="bold">Free Tier eligible</strong>, but <a id="_idIndexMarker1320"/>with bigger instances, it will generate <span class="No-Break">some costs.</span></p>
<h3>Experimental Terraform testing module</h3>
<p>The<a id="_idIndexMarker1321"/> final but very interesting option is using the <a id="_idIndexMarker1322"/>testing Terraform module. This module allows you to write Terraform (HCL) code tests, also in the same language. This will potentially make it much easier to write tests as the current options we’ve already gone through cover writing tests in Golang or by <span class="No-Break">using CTKTF.</span></p>
<p>At the time of writing, the module is considered highly experimental, but it’s worth keeping an eye on how it will develop in <span class="No-Break">the future.</span></p>
<p>T<a id="_idTextAnchor428"/>he module’s website can be found <span class="No-Break">at </span><a href="https://developer.hashicorp.com/terraform/language/modules/testing-experiment"><span class="No-Break">https://developer.hashicorp.com/terraform/language/modules/testing-experiment</span></a><span class="No-Break">.</span></p>
<h3>Other integration tools worth mentioning</h3>
<p>There is a galaxy of other testing tools out there, and more are being developed as you read this. The following is a short list of tools worth mentioning that we didn’t have enough space to <span class="No-Break">describe properly:</span></p>
<ul>
<li><strong class="bold">Checkov</strong> (https://www.checkov.io/): This is an<a id="_idIndexMarker1323"/> open source IaC <a id="_idIndexMarker1324"/>static analysis tool that helps developers and <strong class="bold">DevOps</strong> teams identify and fix security and compliance issues early in the development <span class="No-Break">life cycle.</span></li>
<li><strong class="bold">Super-linter</strong> (https://github.com/github/super-linter): This is an<a id="_idIndexMarker1325"/> open source code linting <a id="_idIndexMarker1326"/>tool that can automatically detect and flag issues in various programming languages and help maintain consistent <span class="No-Break">code quality.</span></li>
<li><strong class="bold">Trivy</strong> (https://github.com/aquasecurity/trivy): This is a vulnerability scanner for<a id="_idIndexMarker1327"/> container<a id="_idIndexMarker1328"/> images that helps developers and DevOps teams identify and fix vulnerabilities in their <span class="No-Break">containerized applications.</span></li>
<li><strong class="bold">Kitchen-Terraform</strong> (https://github.com/newcontext-oss/kitchen-terraform): This<a id="_idIndexMarker1329"/> tool is part of the Test Kitchen plugin<a id="_idIndexMarker1330"/> collection and allows systems to utilize <strong class="bold">Test Kitchen</strong> to <a id="_idIndexMarker1331"/>apply and validate Terraform configurations <a id="_idIndexMarker1332"/>with <span class="No-Break"><strong class="bold">InSpec</strong></span><span class="No-Break"> controls.</span></li>
<li><strong class="bold">RSpec-Terraform</strong> (https://github.com/bsnape/rspec-terraform): This tool provides RSpec<a id="_idIndexMarker1333"/> tests for<a id="_idIndexMarker1334"/> Terraform modules. RSpec is a <strong class="bold">behavior-driven development</strong> (<strong class="bold">BDD</strong>) testing<a id="_idIndexMarker1335"/> framework for <strong class="bold">Ruby</strong> that allows <a id="_idIndexMarker1336"/>developers to <a id="_idIndexMarker1337"/>write expressive and readable tests in a <strong class="bold">domain-specific </strong><span class="No-Break"><strong class="bold">language</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">DSL</strong></span><span class="No-Break">).</span></li>
<li><strong class="bold">Terraform-Compliance</strong> (https://github.com/terraform-compliance/cli): This is a BDD<a id="_idIndexMarker1338"/> testing<a id="_idIndexMarker1339"/> tool designed for <span class="No-Break">Terraform files.</span></li>
<li><strong class="bold">Clarity</strong> (https://github.com/xchapter7x/clarity): This<a id="_idIndexMarker1340"/> is a <a id="_idIndexMarker1341"/>declarative Terraform test framework that specializes in <span class="No-Break">unit testing.</span></li>
</ul>
<p>In conclusion, we have a lot of testing options available. This could be overwhelming for some. Testing the Terraform code is still in development, similar to the other IaC solutions we’ve mentioned. When implementing CI pipelines, it’s best to focus on the low-hanging fruits at the beginning (format checking, running <strong class="source-inline">terraform plan</strong>, and so on) and add more testing during development. We realize it’s a hard thing to accomplish, but we believe that investing in this will enable us to make infrastructure changes with more confide<a id="_idTextAnchor429"/>nce and protect us from <span class="No-Break">unintentional outages.</span></p>
<p>In the next section, we will focus on various CD solutions, both SaaS <span class="No-Break">and self-hosted.</span></p>
<h2 id="_idParaDest-280"><a id="_idTextAnchor430"/>Deployment</h2>
<p><strong class="bold">Continuous deployment</strong> in<a id="_idIndexMarker1342"/> terms of Terraform comes down to running <strong class="source-inline">terraform apply</strong>. This <a id="_idIndexMarker1343"/>can be done automatically with a simple Bash script, but the hard part is to integrate this into the CD tool to ensure we won’t remove any data by mistake with confidence. Assuming we have done a great job with our integration testing and we’re confident enough to do it without any further user interaction, we can enable it to <span class="No-Break">run automatically.</span></p>
<p>Previously, we’ve shown examples of using Jenkins, GitHub Actions, and even a Bash script that we could embed into the process to automate it. We can successfully use these solutions to deploy our changes to the infrastructure; however, there are already dedicated solu<a id="_idTextAnchor431"/>tions for that. Let’s take a look at the most popular ones, starting with SaaS offerings from the company behind Terraform – <span class="No-Break">HashiCorp.</span></p>
<h3>HashiCorp Cloud and Terraform Cloud</h3>
<p>HashiCorp is a<a id="_idIndexMarker1344"/> software company that provides various tools for infrastructure automation and management. Two of the most popular products offered by HashiCorp are <strong class="bold">HashiCorp Cloud</strong> and <span class="No-Break"><strong class="bold">Terraform Cloud</strong></span><span class="No-Break">.</span></p>
<p>HashiCorp Cloud is a cloud-based service that provides a suite of tools for infrastructure automation and management. It includes HashiCorp’s popular tools such as Terraform, <strong class="bold">Vault</strong>, <strong class="bold">Consul</strong>, and <strong class="bold">Nomad</strong>. With <a id="_idIndexMarker1345"/>HashiCorp Cloud, users can create and manage their infrastructure <a id="_idIndexMarker1346"/>using <a id="_idIndexMarker1347"/>the same tools that they <span class="No-Break">use locally.</span></p>
<p>On the other hand, Terraform Cloud <a id="_idIndexMarker1348"/>is a specific offering from HashiCorp that focuses solely on the IaC tool Terraform. Terraform Cloud provides a central place for teams to collaborate on infrastructure code, store configuration state, and automate infrastructure workflows. It offers several features, such as workspace management, version control, and collaboration tools, that make it easier for teams to work together on large-scale <span class="No-Break">infrastructure projects.</span></p>
<p>One of the key differences between HashiCorp Cloud and Terraform Cloud is t<a id="_idTextAnchor432"/>hat the former offers a suite of tools for infrastructure automation and management, while the latter is specifically focused <span class="No-Break">on Terraform.</span></p>
<h3>Scalr</h3>
<p><strong class="bold">Scalr</strong> is a<a id="_idIndexMarker1349"/> cloud management platform that provides enterprise-level solutions for cloud infrastructure automation and management. It was founded in 2007 by Sebastian Stadil to simplify the process of managing multiple <span class="No-Break">cloud environments.</span></p>
<p>Scalr is a multi-cloud management platform. It comes in two flavors: commercial, which is hosted by the parent company as a SaaS solution, and open source, which you can deploy yourself. It can run your Terraform code, but it has more features, such as cost analysis, which will present the estimated bill from the cloud provider for the infrastructure you are deploying. It comes with a web UI, abstracting a lot of work that needs to be done when working with IaC. As we mentioned previously, it is a multi-cloud solution and it comes with a<a id="_idIndexMarker1350"/> centralized <strong class="bold">single sign-on</strong> (<strong class="bold">SSO</strong>) that lets you view and manage all your cloud environments from one pl<a id="_idTextAnchor433"/>ace. It comes with roles, a modules registry, and so on. It is a good solution if you are looking for more than just a centralized <span class="No-Break">IaC tool.</span></p>
<h3>Spacelift</h3>
<p><strong class="bold">Spacelift</strong> is a <a id="_idIndexMarker1351"/>cloud-native IaC platform that helps development teams automate and manage their infrastructure with Terraform, Pulimi, or <strong class="bold">CloudFormation</strong>. It <a id="_idIndexMarker1352"/>also<a id="_idIndexMarker1353"/> supports automation of Kubernetes<a id="_idIndexMarker1354"/> using <strong class="bold">kubectl</strong> and <span class="No-Break"><strong class="bold">Ansible CaaC</strong></span><span class="No-Break">.</span></p>
<p>The platform offers a range of features, such as version control, automated testing, and continuous delivery, allowing teams to accelerate their infrastructure deployment cycles and reduce the risk of errors. Spacelift also provides real-time monitoring and alerts, making it easy to identify and resolve issues before they cause downtime or affect <span class="No-Break">user experience.</span></p>
<p>Spacelift was <a id="_idIndexMarker1355"/>founded in 2020 by a team of experienced DevOps and infrastructure engineers who recognized the need for a better way to manage IaC. The company has since grown rapidly, attracting c<a id="_idTextAnchor434"/>ustomers from a wide range of industries, including healthcare, finance, <span class="No-Break">and e-commerce.</span></p>
<p>The official website <a id="_idIndexMarker1356"/>can be found <span class="No-Break">at </span><span class="No-Break">https://spacelift.com</span><span class="No-Break">.</span></p>
<h3>Env0</h3>
<p><strong class="bold">Env0</strong> is a<a id="_idIndexMarker1357"/> SaaS platform that enables teams to automate their infrastructure and application delivery workflows with Terraform. It was founded in 2018 by a team of experienced DevOps engineers who recognized the need for a streamlined and easy-to-use solution for <span class="No-Break">managing IaC.</span></p>
<p>Env0 offers a variety of features and integrations to help teams manage their Terraform environments, including automated environment provisioning, integration with popular CI/CD tools such as Jenkins<a id="_idIndexMarker1358"/> and <strong class="bold">CircleCI</strong>, and support for multiple cloud providers, including AWS, Azure, and <span class="No-Break">Google Cloud.</span></p>
<p>As a private company, Env0 does not publicly disclose its financial information. However, they have received significant funding from venture capital firms, including a $3.5 million seed round in 2020 led by Boldstart Ventures and <span class="No-Break">Grove Ventures.</span></p>
<p>Env0 has quickly established itself as a leading SaaS provider for managing Terraform environments and streaml<a id="_idTextAnchor435"/>ining DevOps workflows and looks like a very interesting option to us in <span class="No-Break">your environment.</span></p>
<p>The official <a id="_idIndexMarker1359"/>website can be found <span class="No-Break">at </span><a href="https://www.env0.com/"><span class="No-Break">https://www.env0.com/</span></a><span class="No-Break">.</span></p>
<h3>Atlantis</h3>
<p><strong class="bold">Atlantis</strong> is an<a id="_idIndexMarker1360"/> open source project that aims to simplify the management of Terraform infrastructure code by providing a streamlined workflow for creating, reviewing, and merging pull requests. The first release of Atlantis was made in 2018 and it has since gained popularity among developers and DevOps teams who use Terraform as their <span class="No-Break">IaC tool.</span></p>
<p>Atlantis works by integrating with your existing version control system, such as GitHub or GitLab, and continuously monitors for pull requests that contain Terraform code changes. When<a id="_idIndexMarker1361"/> a new pull request is opened, Atlantis automatically creates a new environment for the changes and posts a comment in the pull request with a link to the environment. This allows reviewers to quickly and easily see the changes in a live environment and provide feedback. Once the changes have been reviewed and approved, Atlantis can automatically merge the pull request and apply the changes to the <span class="No-Break">target infrastructure.</span></p>
<p>This open source tool is the one we will dive deeper into. Since its source code is available for free, you will be able to download it yourself an<a id="_idTextAnchor436"/>d deploy it on your local environment or inside the public cloud. Let’s deploy Atlantis in AWS and configure a simple infrastructure to be managed <span class="No-Break">by it.</span></p>
<h1 id="_idParaDest-281">CI/CD with Atlantis</h1>
<p>Armed <a id="_idIndexMarker1362"/>with the knowledge about tooling and principles around CI/CD (both delivery and deployment), we will create a CI/CD pipeline with the use of <a id="_idTextAnchor437"/>Git and the open source tool Atlantis. We will automatically test and deploy changes to our AWS infrastructure with it and do basic testing along <span class="No-Break">the way.</span></p>
<h2 id="_idParaDest-282"><a id="_idTextAnchor438"/>Deploying Atlantis to AWS</h2>
<p>We will use the <a id="_idIndexMarker1363"/>Terraform module created by Anton Bobenko from the <strong class="source-inline">terraform-aws-modules</strong> project on GitHub. Here is the Terraform Registry link to the <span class="No-Break">module: </span><a href="https://registry.terraform.io/modules/terraform-aws-modules/atlantis/aws/latest"><span class="No-Break">https://registry.terraform.io/modules/terraform-aws-modules/atlantis/aws/latest</span></a><span class="No-Break">.</span></p>
<p>You can use this module in two ways. First, which is natural, is using it in your existing Terraform code to deploy it in AWS. The second, which we will use for this demonstration, is using the module <a id="_idIndexMarker1364"/>as a standalone project. The module will also create a new <strong class="bold">Virtual Private Cloud</strong> (<strong class="bold">VPC</strong>) for you in the <strong class="bold">eu-west</strong> AWS zone<a id="_idIndexMarker1365"/> and Atlantis will be running inside the AWS ECS service. This will generate some <span class="No-Break">infrastructure costs.</span></p>
<p>To do so, we need to clone the <span class="No-Break">GitHub repository:</span></p>
<pre class="console">
git clone git@github.com:terraform-aws-modules/terraform-aws-atlantis.git
Cloning into 'terraform-aws-atlantis'...
Host key fingerprint is SHA256:+Aze234876JhhddE
remote: Enumerating objects: 1401, done.
remote: Counting objects: 100% (110/110), done.
remote: Compressing objects: 100% (101/101), done.
remote: Total 1400 (delta 71), reused 81 (delta 52), pack-reused 1282
Receiving objects: 100% (1401/1401), 433.19 KiB | 1.12 MiB/s, done.
Resolving deltas: 100% (899/899), done.</pre>
<p>Next, we <a id="_idIndexMarker1366"/>will have to create a Terraform variables file. We have some boilerplate in the repository in the <strong class="source-inline">terraform.tfvars.sample</strong> file. Let’s <span class="No-Break">copy it:</span></p>
<pre class="console">
cp terraform.tfvars.sample terraform.tfvars</pre>
<p>Before proceeding, make sure you’ve created a GitHub repository that will hold all your Terraform code. We will be creating a Webhook for this repository when Atlantis is deployed, but you will need to add it to the <strong class="source-inline">terraform.tfvars</strong> file before <span class="No-Break">applying it.</span></p>
<p>Let’s take a look at the variables in the <strong class="source-inline">terraform.tfvars</strong> file we will be able <span class="No-Break">to change:</span></p>
<pre class="source-code">
cidr = "10.10.0.0/16"
azs = ["eu-west-1a", "eu-west-1b"]
private_subnets = ["10.10.1.0/24", "10.10.2.0/24"]
public_subnets = ["10.10.11.0/24", "10.10.12.0/24"]
route53_zone_name = "example.com"
ecs_service_assign_public_ip = true
atlantis_repo_allowlist = ["github.com/terraform-aws-modules/*"]
atlantis_github_user = ""
atlantis_github_user_token = ""
tags = {
  Name = "atlantis"
}</pre>
<p><strong class="source-inline">atlantis_repo_allowlist</strong> is the first one you would need to update to match the repositories you’d like Atlantis to be able to use. Make sure it’s pointing to your repository. <strong class="source-inline">route53_zone_name</strong> should be changed as well to something similar, such as <strong class="source-inline">automation.yourorganisation.tld</strong>. Note that it needs to be a public domain – GitHub will use it to send webhooks over to Atlantis to trigger builds. You will need to <a id="_idIndexMarker1367"/>create<a id="_idIndexMarker1368"/> the <strong class="bold">Route53</strong> hosted DNS zone in your Terraform code or use the <span class="No-Break">web console.</span></p>
<p>Two more variables you will need to update are <strong class="source-inline">atlantis_github_user</strong> and <strong class="source-inline">atlantis_github_user_token</strong>. The first one is self-explanatory, but for the second, you will need to visit the GitHub website and generate your <strong class="bold">personal access token</strong> (<strong class="bold">PAT</strong>). This<a id="_idIndexMarker1369"/> will allow Atlantis to access the repository you want to use. To do that, you will need to follow the guidelines on the GitHub documentation <span class="No-Break">pages: </span><a href="https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token"><span class="No-Break">https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token</span></a><span class="No-Break">.</span></p>
<p>After updating the <strong class="source-inline">terraform.tfvars</strong> file, we’re ready to run <strong class="source-inline">terraform init</strong> and <span class="No-Break"><strong class="source-inline">terraform plan</strong></span><span class="No-Break">:</span></p>
<pre class="console">
admin@myhome~/aws$ terraform init
Initializing modules...
# output truncated for readability- Installed hashicorp/random v3.4.3 (signed by HashiCorp)
Terraform has been successfully initialized!</pre>
<p>Terraform has created a lock file called <strong class="source-inline">.terraform.lock.hcl</strong> to record the provider selections it made. Include this file in your version control repository so that Terraform will make the same selections by default when you run <strong class="source-inline">terraform init</strong> in <span class="No-Break">the future.</span></p>
<p>Now, we <a id="_idIndexMarker1370"/>can run the following <strong class="source-inline">terraform </strong><span class="No-Break"><strong class="source-inline">plan</strong></span><span class="No-Break"> command:</span></p>
<pre class="console">
admin@myhome~/aws$ terraform plan
Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
  + create
 &lt;= read (data resources)
Terraform will perform the following actions:
# Output truncated for readability
  # aws_cloudwatch_log_group.atlantis will be created
  + resource "aws_cloudwatch_log_group" "atlantis" {
      + arn               = (known after apply)
      + id                = (known after apply)
      + name              = "atlantis"
      + name_prefix       = (known after apply)
      + retention_in_days = 7
      + skip_destroy      = false
      + tags              = {
          + "Name" = "atlantis"
        }
      + tags_all          = {
          + "Name" = "atlantis"
        }
    }
# Removed some output for readability
Plan: 49 to add, 0 to change, 0 to destroy.</pre>
<p>If you <a id="_idIndexMarker1371"/>see a similar output, you can apply it. The module will also return a lot of information about the created resources. It’s worth paying attention <span class="No-Break">to them.</span></p>
<p>After running <strong class="source-inline">terraform apply</strong> (it will take a couple of minutes), you will see an output similar to <span class="No-Break">the following:</span></p>
<pre class="console">
Apply complete! Resources: 49 added, 0 changed, 0 destroyed.
Outputs:
alb_arn = "arn:aws:elasticloadbalancing:eu-central-1:673522028003:loadbalancer/app/atlantis/8e6a5c314c2936bb"
# Output truncated for readabilityatlantis_url = "https://atlantis.atlantis.devopsfury.com"
atlantis_url_events = "https://atlantis.atlantis.devopsfury.com/events"
public_subnet_ids = [
  "subnet-08a96bf6a15a65a20",
  "subnet-0bb98459f42567bdb",
]
webhook_secret = &lt;sensitive&gt;</pre>
<p>If you’ve done everything correctly, you should be able to access the Atlantis website under the <a href="http://atlantis.automation.yourorganisation.tld">atlantis.automation.yourorganisation.tld</a> domain we created previously. The module added all the necessary records to the Route53 zone <span class="No-Break">for us.</span></p>
<p>If everything <a id="_idIndexMarker1372"/>has gone well up to this point, when you visit <a href="https://atlantis.automation.yourorganisation.tld">https://atlantis.automation.yourorganisation.tld</a>, you will see the following <span class="No-Break">Atlantis panel:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer033">
<img alt="Figure 13.1 – Atlantis website after successfully deploying it using the Terraform module" height="722" src="image/B18197_13_01.jpg" width="1328"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.1 – Atlantis website after successfully deploying it using the Terraform module</p>
<p>The <strong class="source-inline">webhook_secret</strong> output that’s marked as sensitive in the preceding output will be used to set up a webhook on the GitHub repository side. To view it, you will need to run the <span class="No-Break">following command:</span></p>
<pre class="console">
admin@myhome:~/aws$ terraform output webhook_secret
"bf3b20b285c91c741eeff34621215ce241cb62594298a4cec44a19ac3c70ad3333cc97d9e8b24c06909003e5a879683e4d07d29efa750c47cdbeef3779b3eaef"</pre>
<p>We can also automate it with Terraform by using the module available in the same repository as the <span class="No-Break">Atlantis one.</span></p>
<p>Here’s the full URL to the <span class="No-Break">module: </span><a href="https://github.com/terraform-aws-modules/terraform-aws-atlantis/tree/master/examples/github-repository-webhook"><span class="No-Break">https://github.com/terraform-aws-modules/terraform-aws-atlantis/tree/master/examples/github-repository-webhook</span></a><span class="No-Break">.</span></p>
<p>Alternatively, you can create a webhook manually for testing by going to the GitHub website and following the <span class="No-Break">documentation: </span><a href="https://docs.github.com/en/webhooks-and-events/webhooks/creating-webhooks"><span class="No-Break">https://docs.github.com/en/webhooks-and-events/webhooks/creating-webhooks</span></a><span class="No-Break">.</span></p>
<p>Remember <a id="_idIndexMarker1373"/>to use the secret generated automatically by Terraform in the output variable – that <span class="No-Break">is, </span><span class="No-Break"><strong class="source-inline">webhook_secret</strong></span><span class="No-Break">.</span></p>
<p>Creating webhook documentation is also well described in the Atlantis <span class="No-Break">documentation: </span><a href="https://www.runatlantis.io/docs/configuring-webhooks.xhtml#github-github-enterprise"><span class="No-Break">https://www.runatlantis.io/docs/configuring-webhooks.xhtml#github-github-enterprise</span></a><span class="No-Break">.</span></p>
<p>You may encounter that Atlantis won’t come up as expected and you will see an <strong class="source-inline">HTTP 500 error</strong> issue when accessing the web panel. To track down any issues with this service, such as Atlantis is still unavailable or responding with errors to the GitHub webhook, you can go to the AWS console and find the ECS service. From there, you should see a cluster named <strong class="bold">atlantis</strong>. If you click on it, you’ll see the configuration and status of the cluster, as shown in the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer034">
<img alt="Figure 13.2 – Amazon Elastic Container Service (ECS) Atlantis cluster information" height="960" src="image/B18197_13_02.jpg" width="1581"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.2 – Amazon Elastic Container Service (ECS) Atlantis cluster information</p>
<p>If you go<a id="_idIndexMarker1374"/> to the <strong class="bold">Tasks</strong> tab (visible in the preceding screenshot) and click on the task ID (for example, <strong class="bold">8ecf5f9ced3246e5b2bf16d7485e981c</strong>), you will see the <span class="No-Break">following information:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer035">
<img alt="Figure 13.3 – Details of the task inside the Atlantis ECS cluster" height="793" src="image/B18197_13_03.jpg" width="1601"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.3 – Details of the task inside the Atlantis ECS cluster</p>
<p>The <strong class="bold">Logs</strong> tab will show you all <span class="No-Break">recent events.</span></p>
<p>You can view more detailed log information in the <strong class="bold">CloudWatch</strong> service when you go to the <strong class="bold">Logs</strong> | <strong class="bold">Log groups</strong> section and find the <strong class="bold">atlantis</strong> log group. Inside, you will be able to see log streams containing all the logs from your task. If you already have multiple streams, it’s possible to quickly track down the correct stream by its <span class="No-Break"><strong class="bold">Task ID</strong></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer036">
<img alt="Figure 13.4 – CloudWatch log streams containing logs from ECS tasks running Atlantis" height="513" src="image/B18197_13_04.jpg" width="1338"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.4 – CloudWatch log streams containing logs from ECS tasks r<a id="_idTextAnchor439"/>unning Atlantis</p>
<p>If everything has <a id="_idIndexMarker1375"/>worked fine so far, we’re ready to test whether Atlantis can run <strong class="source-inline">terraform plan</strong> and <strong class="source-inline">terraform apply</strong>. Let’s get back to <span class="No-Break">our code.</span></p>
<h2 id="_idParaDest-283"><a id="_idTextAnchor440"/>Running Terraform using Atlantis</h2>
<p>To <a id="_idIndexMarker1376"/>execute <strong class="source-inline">terraform plan</strong>, we will have to <a id="_idIndexMarker1377"/>create a <a id="_idIndexMarker1378"/>new <strong class="bold">pull request</strong> on our GitHub repository. Let’s create a module that will <a id="_idIndexMarker1379"/>create an <strong class="bold">S3 bucket</strong> and <strong class="bold">DynamoDB</strong> table<a id="_idIndexMarker1380"/> for us. The <strong class="source-inline">main.tf</strong> file will look <span class="No-Break">like this:</span></p>
<pre class="source-code">
# Configure the AWS Provider
provider "aws" {
  region = var.region
}</pre>
<p>The preceding code configures the AWS provider to use the region specified inside the <span class="No-Break"><strong class="source-inline">region</strong></span><span class="No-Break"> variable.</span></p>
<p>This <a id="_idIndexMarker1381"/>Terraform code block configures a required Terraform <a id="_idIndexMarker1382"/>version and where the Terraform state file is located. In this example, we’re using local storage, but in a production environment, we should use a remote <span class="No-Break">state location.</span></p>
<pre class="source-code">
terraform {
  required_version = "&gt;=1.0"
  backend "local" {
    path = "tfstate/terraform.local-tfstate"
  }
}
resource "aws_s3_bucket" "terraform_state" {
  bucket        = "terraform-states"
  acl           = "private"
  force_destroy = false
  versioning {
    enabled = true
  }</pre>
<p>The preceding block defines an S3 bucket where we intend to store a Terraform state. It is a private S3 bucket with enabled versioning. This is the recommended setup for storing <span class="No-Break">state files.</span></p>
<pre class="source-code">
  server_side_encryption_configuration {
    rule {
      apply_server_side_encryption_by_default {
        sse_algorithm = "AES256"
      }
    }
  }
}</pre>
<p>The preceding <a id="_idIndexMarker1383"/>code also <a id="_idIndexMarker1384"/>configures <strong class="bold">server-side encryption</strong> (<strong class="bold">SSE</strong>) for<a id="_idIndexMarker1385"/> the <span class="No-Break">S3 bucket.</span></p>
<pre class="source-code">
resource "aws_dynamodb_table" "dynamodb-terraform-state-lock" {
  name           = "terraform-state-lock"
  hash_key       = "LockID"
  read_capacity  = 1
  write_capacity = 1
  attribute {
    name = "LockID"
    type = "S"
  }
  tags = {
    Name = "DynamoDB Terraform State Lock Table"
  }
}</pre>
<p>The preceding code block defines a DynamoDB table that’s used for Terraform state locking. The <strong class="source-inline">variables.tf</strong> file will only contain <span class="No-Break">one variable:</span></p>
<pre class="source-code">
variable "region" {
  type    = string
  default = "eu-central-1"
}</pre>
<p>After adding these files to Git and committing the changes and creating a new pull request on GitHub, you will be able to ask Atlantis to run a plan and apply for you. If you run a plan, Atlantis will lock the module you’ve modified and no one else will be able to apply any changes to it unless you unlock or apply <span class="No-Break">your changes.</span></p>
<p>To trigger <a id="_idIndexMarker1386"/>a plan for your new pull request, just add a comment<a id="_idIndexMarker1387"/> to your pull request stating <strong class="source-inline">atlantis plan</strong>. After a while, depending on how big the module is, you will get a plan output similar to what’s shown in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer037">
<img alt="Figure 13.5 – Interaction with Atlantis on GitHub" height="983" src="image/B18197_13_05.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.5 – Interaction with Atlantis on GitHub</p>
<p>At the time of writing, Atlantis doesn’t support automatically applying the changes. However, it’s possible to automate at the CI level. For example, when using GitHub, you could create a GitHub Action that would, after a successful test, add the <strong class="source-inline">atlantis apply</strong> comment, which would trigger Atlantis to apply changes and report back with <span class="No-Break">the status.</span></p>
<p>At this point, we can give developers the power to modify our infrastructure without directly allowing them to run Terraform on their local machines. At the same time, we’re removing the possibility of applying changes by many users at the same time, which, without a distributed locking mechanism, can be very destructive. Furthermore, working with<a id="_idIndexMarker1388"/> Terraform will be easier as no one will have to install it on local machines, no one will have to have direct access to our AWS account, and we will gain more visibility of changes to <span class="No-Break">our infrastructure.</span></p>
<p>Building CI/CD with<a id="_idIndexMarker1389"/> Terraform still has a long way to go. IaC is still behin<a id="_idTextAnchor441"/>d testing features available in other programming languages, but many developers are working on it. We’re looking forward to making testing infrastructure easier <span class="No-Break">for everybody.</span></p>
<h1 id="_idParaDest-284">Summary</h1>
<p>In this chapter, we explored the benefits of using Terraform for IaC and discussed the importance of incorporating CI/CD processes in Terraform workflows. We covered testing infrastructure and various tools for <span class="No-Break">automating deployment.</span></p>
<p>In the final section, we explained how to deploy Atlantis, an open source tool for automated Terraform pull request previews, to AWS and configure GitHub to trigger <strong class="source-inline">terraform plan</strong> and <strong class="source-inline">terraform apply</strong>. With Atlantis, Terraform users can collaborate on infrastructure changes through GitHub pull requests, allowing for infrastructure changes to be reviewed and approved before they are applied to production. By incorporating Atlantis into your Terraform workflow, you can improve collaboration, reduce errors, and achie<a id="_idTextAnchor442"/>ve faster and more secure <span class="No-Break">infrastructure changes.</span></p>
<p>In the final chapter, we will slow down a little and talk about DevOps misconceptions and antipatterns, and how to <span class="No-Break">avoid them.</span></p>
<h1 id="_idParaDest-285">Exercises</h1>
<p>Try out the following exercises to test your knowledge of <span class="No-Break">this chapter:</span></p>
<ol>
<li>Try to deploy Atlantis locally by following the documentation found <span class="No-Break">at </span><a href="https://www.runatlantis.io/guide/testing-locally.xhtml"><span class="No-Break">https://www.runatlantis.io/guide/testing-locally.xhtml</span></a><span class="No-Break">.</span></li>
<li>Create a repository and configure the webhook and PAT for yourself. Run a plan for your new repository (hint: instead of AWS resources, you can use a <strong class="bold">null resource</strong> <span class="No-Break">for testing).</span></li>
<li>Create an account on one of the CD solution websites and try to run a plan using this SaaS. There’s usually a free plan for <span class="No-Break">public repositories.</span></li>
</ol>
</div>
</div></body></html>