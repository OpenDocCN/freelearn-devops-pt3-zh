- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Managing Advanced Kubernetes Resources
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理高级 Kubernetes 资源
- en: In the previous chapter, we covered Kubernetes and why we need it and then discussed
    bootstrapping a Kubernetes cluster using MiniKube and KinD. We then looked at
    the `Pod` resource and discussed how to create and manage pods, how to troubleshoot
    them, and how to ensure your application’s reliability using probes, along with
    multi-container design patterns to appreciate why Kubernetes uses pods in the
    first place instead of containers. We also looked at `Secrets` and `ConfigMaps`.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了 Kubernetes 及其需求，并讨论了如何使用 MiniKube 和 KinD 启动 Kubernetes 集群。接着，我们查看了
    `Pod` 资源，讨论了如何创建和管理 Pods，如何排查问题，并如何通过探针确保应用程序的可靠性，还探讨了多容器设计模式，以理解 Kubernetes 为什么使用
    Pods 而不是容器。我们还了解了 `Secrets` 和 `ConfigMaps`。
- en: Now, we will dive deep into the advanced aspects of Kubernetes and Kubernetes
    command-line best practices.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将深入探讨 Kubernetes 的高级部分以及 Kubernetes 命令行最佳实践。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主要内容：
- en: The need for advanced Kubernetes resources
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级 Kubernetes 资源的需求
- en: Kubernetes Deployments
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 部署
- en: Kubernetes Services and Ingresses
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 服务和入口
- en: Horizontal pod autoscaling
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平 Pod 自动扩展
- en: Managing stateful applications
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理有状态的应用程序
- en: Kubernetes command-line best practices, tips, and tricks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 命令行最佳实践、技巧和窍门
- en: So, let’s dive in!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们开始吧！
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For this chapter, we will spin up a cloud-based Kubernetes cluster, **Google
    Kubernetes Engine** (**GKE**), for the exercises. That is because you will not
    be able to spin up load balancers and PersistentVolumes within your local system,
    and therefore, we cannot use KinD and MiniKube in this chapter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将为练习启动一个基于云的 Kubernetes 集群，**Google Kubernetes Engine**（**GKE**）。因为你将无法在本地系统中启动负载均衡器和
    PersistentVolumes，所以在本章中我们不能使用 KinD 和 MiniKube。
- en: Currently, **Google Cloud Platform** (**GCP**) provides a free $300 trial for
    90 days, so you can go ahead and sign up for one at [https://cloud.google.com/free](https://cloud.google.com/free).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，**Google Cloud Platform**（**GCP**）提供免费的 $300 试用，期限为 90 天，所以你可以前往 [https://cloud.google.com/free](https://cloud.google.com/free)
    注册。
- en: Spinning up GKE
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动 GKE
- en: Once you’ve signed up and logged in to your console, you can open the Google
    Cloud Shell CLI to run the commands.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注册并登录到控制台后，你可以打开 Google Cloud Shell CLI 来运行命令。
- en: 'You need to enable the GKE API first using the following command:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要先启用 GKE API，使用以下命令：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To create a three-node GKE cluster, run the following command:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个三节点 GKE 集群，运行以下命令：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: And that’s it! The cluster is up and running.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！集群已成功启动并运行。
- en: 'You will also need to clone the following GitHub repository for some exercises:
    [https://github.com/PacktPublishing/Modern-DevOps-Practices-2e](https://github.com/PacktPublishing/Modern-DevOps-Practices-2e).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要克隆以下 GitHub 仓库来进行一些练习：[https://github.com/PacktPublishing/Modern-DevOps-Practices-2e](https://github.com/PacktPublishing/Modern-DevOps-Practices-2e)。
- en: 'Run the following command to clone the repository into your home directory
    and `cd` into the `ch6` directory to access the required resources:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令将仓库克隆到你的主目录，并使用 `cd` 进入 `ch6` 目录以访问所需的资源：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now, let’s understand why we need advanced Kubernetes resources.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解一下为什么我们需要高级 Kubernetes 资源。
- en: The need for advanced Kubernetes resources
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级 Kubernetes 资源的需求
- en: In the last chapter, we looked at pods, the basic building blocks of Kubernetes
    that provide everything for your containers to run within a Kubernetes environment.
    However, pods on their own are not that effective. The reason is that while they
    define a container application and its specification, they do not replicate, auto-heal,
    or maintain a particular state. When you delete a pod, the pod is gone. You cannot
    maintain multiple versions of your code or roll out and roll back releases using
    a pod. You also cannot autoscale your application with traffic with pods alone.
    Pods do not allow you to expose your containers to the outside world, and they
    do not provide traffic management capabilities such as load balancing, content
    and path-based routing, storing persistent data to externally attached storage,
    and so on. To solve these problems, Kubernetes provides us with specific advanced
    resources, such as Deployments, Services, Ingresses, PersistentVolumes and claims,
    and StatefulSets. Let’s start with Kubernetes Deployments in the next section.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了 pod，这是 Kubernetes 的基本构建块，为您的容器在 Kubernetes 环境中提供一切必需的内容。然而，单独的 pod
    并不是那么有效。原因在于，尽管它们定义了容器应用及其规范，但它们不会复制、自动修复或维护特定状态。删除 pod 后，该 pod 就不存在了。您不能使用 pod
    维护代码的多个版本，也不能使用 pod 进行发布和回滚。使用仅有的 pod 也不能根据流量自动缩放您的应用程序。Pod 无法让您将容器暴露给外部世界，也不提供负载均衡、基于内容和路径的路由、将持久数据存储到外部附加存储等流量管理功能。为了解决这些问题，Kubernetes
    为我们提供了特定的高级资源，如 Deployments、Services、Ingresses、PersistentVolumes 和 claims，以及 StatefulSets。让我们在下一节中从
    Kubernetes 部署开始。
- en: Kubernetes Deployments
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 部署
- en: Let’s understand Kubernetes Deployments using a simple analogy.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的类比来理解 Kubernetes 部署。
- en: Imagine you’re a chef preparing a particular dish in your kitchen. You want
    to make sure that it is consistently perfect every time you serve it, and you
    want to be able to change the recipe without causing a chaotic mess.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，您是一位在厨房里准备特定菜肴的厨师。您希望确保每次上菜时都能保持一致的完美，同时希望能够更改食谱而不引起混乱。
- en: In the world of Kubernetes, a “Deployment” is like your sous chef. It helps
    you create and manage copies of your pods effortlessly.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 的世界中，“Deployment” 就像是您的副厨。它帮助您轻松创建和管理您的 pod 的副本。
- en: 'Here’s how it works:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 它是如何工作的：
- en: '**Creating consistency**: You want to serve your dish to many guests. Therefore,
    instead of cooking each plate separately, you prepare a bunch of them at once.
    All of them should taste the same and strictly as intended. A Kubernetes Deployment
    does the same for your pod. It creates multiple identical copies of your pod,
    ensuring they all have the same setup.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建一致性**：您希望将您的菜肴服务给许多客人。因此，与其分别烹饪每份盘子，不如一次性准备一堆。所有的盘子都应该味道相同，完全按照预期来。Kubernetes
    部署为您的 pod 做到了这一点，创建多个相同的 pod 副本，确保它们都具有相同的设置。'
- en: '`Deployment` resource slowly and carefully replaces old copies with new ones
    individually, so your app is always available, and your guests (or users) don’t
    notice any hiccups.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Deployment` 资源会逐个缓慢而谨慎地替换旧副本，以确保您的应用始终可用，您的访客（或用户）不会注意到任何中断。'
- en: '**Rolling back gracefully**: Sometimes, experiments don’t go as planned, and
    you must revert to the original recipe. Just as in your kitchen, Kubernetes lets
    you roll back to the previous version of your pod if things don’t work out with
    the new one.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优雅回滚**：有时，实验并不如预期那样顺利，您可能需要回到原始食谱。就像在厨房里一样，Kubernetes 允许您将 pod 的版本回滚到之前的版本，如果新版本出现问题的话。'
- en: '**Scaling easily**: Imagine your restaurant suddenly gets a rush of customers,
    and you need more plates for your special dish. A Kubernetes Deployment helps
    with that, too. It can quickly create more copies of your pod to handle the increased
    demand and remove them when things quieten down.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轻松扩展**：想象一下您的餐厅突然迎来了一波顾客，您需要更多的盘子来盛放您的特色菜肴。Kubernetes 部署也能帮助您做到这一点。它可以快速创建更多的
    pod 副本来处理增加的需求，并在事态平静下来时移除它们。'
- en: '**Managing multiple kitchens**: If you have multiple restaurants, you’d want
    your signature dish to taste the same in all of them. Similarly, if you’re using
    Kubernetes across different environments such as testing, development, and production,
    Deployments help keep things consistent.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理多个厨房**：如果您有多家餐馆，您希望您的招牌菜在所有餐馆中味道一致。类似地，如果您在测试、开发和生产等不同环境中使用 Kubernetes，部署可以帮助保持一致性。'
- en: In essence, Kubernetes Deployments help manage your pod, like a sous chef manages
    the dishes served from a kitchen. They ensure consistency, safety, and flexibility,
    ensuring your application runs smoothly and can be updated without causing a mess
    in your *software kitchen*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，Kubernetes 的 Deployments 帮助管理你的 pod，就像副厨师管理厨房里做的菜肴一样。它们确保一致性、安全性和灵活性，确保你的应用顺利运行，并且可以在不打乱你
    *软件厨房* 的情况下进行更新。
- en: Container application Deployments within Kubernetes are done through `Deployment`
    resources. `Deployment` resources employ `ReplicaSet` resources behind the scenes,
    so it would be good to look at `ReplicaSet` resources before we move on to understand
    `Deployment` resources.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中的容器应用部署是通过 `Deployment` 资源来完成的。`Deployment` 资源背后使用了 `ReplicaSet`
    资源，因此在继续学习 `Deployment` 资源之前，了解 `ReplicaSet` 资源会很有帮助。
- en: ReplicaSet resources
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`ReplicaSet` 资源'
- en: '`ReplicaSet` resources are Kubernetes controllers that help you run multiple
    pod replicas at a given time. They provide horizontal scaling for your container
    workloads, forming the basic building block of a horizontal scale set for your
    containers, a group of similar containers tied together to run as a unit.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReplicaSet` 资源是 Kubernetes 控制器，帮助你在给定时间运行多个 pod 副本。它们为容器工作负载提供水平扩展，形成容器的水平扩展集的基本构建块，一组相似的容器组合在一起作为一个单元运行。'
- en: '`ReplicaSet` resources define the number of pod replicas to run at a given
    time. The Kubernetes controller then tries to maintain the replicas and recreates
    a pod if it goes down.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReplicaSet` 资源定义了在给定时间运行的 pod 副本数目。Kubernetes 控制器随后会尝试维持副本，并在 pod 宕机时重新创建它。'
- en: You should never use `ReplicaSet` resources on their own, but instead, they
    should act as a backend to a `Deployment` resource.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你不应单独使用 `ReplicaSet` 资源，而应将其作为 `Deployment` 资源的后端。
- en: 'For understanding, however, let’s look at an example. To access the resources
    for this section, `cd` into the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好理解，我们来看一个例子。要访问本节的资源，`cd` 到以下目录：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `ReplicaSet` resource manifest, `nginx-replica-set.yaml`, looks like this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReplicaSet` 资源清单 `nginx-replica-set.yaml` 看起来是这样的：'
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The resource manifest includes `apiVersion` and `kind`, as with any other resource.
    It also contains a `metadata` section that defines the resource’s `name` and `labels`
    attributes, similar to any other Kubernetes resource.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 资源清单包括 `apiVersion` 和 `kind`，就像其他任何资源一样。它还包含一个 `metadata` 部分，用于定义资源的 `name`
    和 `labels` 属性，类似于其他任何 Kubernetes 资源。
- en: 'The `spec` section contains the following attributes:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`spec` 部分包含以下属性：'
- en: '`replicas`: This defines the number of pod replicas matched by the selector
    to run at a given time.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`replicas`：这定义了通过选择器匹配的 pod 副本数目，在给定时间内运行。'
- en: '`selector`: This defines the basis on which the `ReplicaSet` resource will
    include pods.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`selector`：这定义了 `ReplicaSet` 资源将包含 pod 的基础。'
- en: '`selector.matchLabels`: This defines labels and their values to select pods.
    Therefore, the `ReplicaSet` resource will select any pod with the `app:` `nginx`
    label.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`selector.matchLabels`：这定义了选择 pod 的标签及其值。因此，`ReplicaSet` 资源将选择任何具有 `app:` `nginx`
    标签的 pod。'
- en: '`template`: This is an optional section that you can use to define the pod
    template. This section’s contents are very similar to defining a pod, except it
    lacks the `name` attribute, as the `ReplicaSet` resource will generate dynamic
    names for pods. If you don’t include this section, the `ReplicaSet` resource will
    still try to acquire existing pods with matching labels. However, it cannot create
    new pods because of the missing template. Therefore, it is best practice to specify
    a template for a `ReplicaSet` resource.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`template`：这是一个可选部分，你可以使用它来定义 pod 模板。这个部分的内容非常类似于定义一个 pod，唯一不同的是它没有 `name`
    属性，因为 `ReplicaSet` 资源会为 pod 生成动态名称。如果不包含这个部分，`ReplicaSet` 资源仍会尝试获取具有匹配标签的现有 pod。但由于缺少模板，它不能创建新的
    pod。因此，最佳实践是为 `ReplicaSet` 资源指定一个模板。'
- en: 'Let’s go ahead and apply this manifest to see what we get:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续应用这个清单，看看会得到什么：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, let’s run the following command to list the `ReplicaSet` resources:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们运行以下命令列出 `ReplicaSet` 资源：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Right—so, we see that there are three desired replicas. Currently, `3` replicas
    are running, but `0` are ready. Let’s wait for a while and then rerun the following
    command:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对的——我们看到有三个期望的副本。目前，`3` 个副本正在运行，但 `0` 个副本准备好。让我们等一会儿，然后重新运行以下命令：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And now, we see `3` ready pods that are awaiting a connection. Now, let’s list
    the pods and see what the `ReplicaSet` resource has done behind the scenes using
    the following command:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们看到 `3` 个已就绪的 pod 正在等待连接。接下来，让我们列出这些 pod，看看 `ReplicaSet` 资源在幕后做了什么，使用以下命令：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: There are three `nginx` pods, each with a name that starts with `nginx` but
    ends with a random hash. The `ReplicaSet` resource has appended a random hash
    to generate unique pods at the end of the `ReplicaSet` resource name. Yes—the
    name of every resource of a particular kind in Kubernetes should be unique.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个 `nginx` pod，每个名称以 `nginx` 开头，但以随机哈希值结尾。`ReplicaSet` 资源会在 `ReplicaSet` 资源名称的末尾附加一个随机哈希，以生成唯一的
    pod。是的——Kubernetes 中每种资源的名称都应该是唯一的。
- en: 'Let’s go ahead and use the following command to delete one of the pods from
    the `ReplicaSet` resource and see what we get:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续使用以下命令删除 `ReplicaSet` 资源中的一个 pod，并查看结果：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We see that even though we deleted the `nginx-9kvkj` pod, the `ReplicaSet` resource
    has replaced it with a new pod, `nginx-9xbdf`. That is how `ReplicaSet` resources
    work.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，尽管我们删除了 `nginx-9kvkj` pod，但 `ReplicaSet` 资源已经用一个新 pod `nginx-9xbdf` 替代了它。这就是
    `ReplicaSet` 资源的工作方式。
- en: You can delete a `ReplicaSet` resource just like any other Kubernetes resource.
    You can run the `kubectl delete replicaset <ReplicaSet name>` command for an imperative
    approach or use `kubectl delete -f <manifest_file>` for a declarative approach.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像删除其他 Kubernetes 资源一样删除 `ReplicaSet` 资源。你可以运行命令 `kubectl delete replicaset
    <ReplicaSet 名称>` 来进行命令式删除，或者使用 `kubectl delete -f <清单文件>` 进行声明式删除。
- en: 'Let’s use the former approach and delete the `ReplicaSet` resource by using
    the following command:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用之前的方法，通过以下命令删除 `ReplicaSet` 资源：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s check whether the `ReplicaSet` resource has been deleted by running the
    following command:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行以下命令检查 `ReplicaSet` 资源是否已被删除：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We don’t have anything in the `default` namespace. This means that the `ReplicaSet`
    resource is deleted.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `default` 命名空间中没有任何资源。这意味着 `ReplicaSet` 资源已被删除。
- en: As we discussed, `ReplicaSet` resources should not be used on their own but
    should instead be the backend of `Deployment` resources. Let’s now look at Kubernetes
    `Deployment` resources.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论的，`ReplicaSet` 资源不应单独使用，而应作为 `Deployment` 资源的后台。接下来我们来看一下 Kubernetes `Deployment`
    资源。
- en: Deployment resources
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Deployment 资源
- en: Kubernetes `Deployment` resources help to manage deployments for container applications.
    They are typically used for managing stateless workloads. You can still use them
    to manage stateful applications, but the recommended approach for stateful applications
    is to use `StatefulSet` resources.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes `Deployment` 资源有助于管理容器应用程序的部署。它们通常用于管理无状态工作负载。尽管你仍然可以使用它们来管理有状态应用程序，但推荐的有状态应用程序处理方式是使用
    `StatefulSet` 资源。
- en: 'Kubernetes Deployments use `ReplicaSet` resources as a backend, and the chain
    of resources looks like what’s shown in the following diagram:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes Deployments 使用 `ReplicaSet` 资源作为后台，资源链条如下图所示：
- en: '![Figure 6.1 – Deployment chain](img/B19877_Figure_6.01.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – Deployment 链接](img/B19877_Figure_6.01.jpg)'
- en: Figure 6.1 – Deployment chain
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – Deployment 链接
- en: 'Let’s take the preceding example and create an nginx `Deployment` resource
    manifest—`nginx-deployment.yaml`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以之前的示例为基础，创建一个 nginx `Deployment` 资源清单——`nginx-deployment.yaml`：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The manifest is very similar to the `ReplicaSet` resource, except for the `kind`
    attribute—`Deployment`, in this case.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 该清单与 `ReplicaSet` 资源非常相似，唯一不同的是 `kind` 属性——在这种情况下是 `Deployment`。
- en: 'Let’s apply the manifest by using the following command:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下命令应用该清单：
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'So, as the `Deployment` resource has been created, let’s look at the chain
    of resources it created. Let’s run `kubectl get` to list the `Deployment` resources
    using the following command:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，`Deployment` 资源已经创建，让我们看看它创建的资源链条。通过以下命令运行 `kubectl get` 来列出 `Deployment`
    资源：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: And we see there is one `Deployment` resource called `nginx`, with `3/3` ready
    pods and `3` up-to-date pods. As `Deployment` resources manage multiple versions,
    `UP-TO-DATE` signifies whether the latest `Deployment` resource has rolled out
    successfully. We will look into the details of this in the subsequent sections.
    It also shows `3` available pods at that time.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到有一个名为 `nginx` 的 `Deployment` 资源，包含 `3/3` 个已就绪 pod 和 `3` 个最新的 pod。由于 `Deployment`
    资源管理多个版本，`UP-TO-DATE` 表示最新的 `Deployment` 资源是否已成功滚动发布。我们将在后续部分详细讨论这一点。同时，它显示当前有
    `3` 个可用的 pod。
- en: 'As we know `Deployment` resources create `ReplicaSet` resources in the background,
    let’s get the `ReplicaSet` resources using the following command:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道`Deployment`资源会在后台创建`ReplicaSet`资源，我们可以使用以下命令查看`ReplicaSet`资源：
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: And we see that the `Deployment` resource has created a `ReplicaSet` resource,
    which starts with `nginx` and ends with a random hash. That is required as a `Deployment`
    resource might contain one or more `ReplicaSet` resources. We will look at how
    in the subsequent sections.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，`Deployment`资源创建了一个`ReplicaSet`资源，其名称以`nginx`开头，并以一个随机哈希值结尾。这是必要的，因为一个`Deployment`资源可能包含一个或多个`ReplicaSet`资源。我们将在后续部分了解这一点。
- en: 'Next in the chain are pods, so let’s get the pods using the following command
    to see for ourselves:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是 pods，我们可以使用以下命令查看这些 pods：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: And, as expected, we have three pods. Each begins with the `ReplicaSet` resource
    name and ends with a random hash. That’s why you see two hashes in the pod name.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们有三个 pod。每个 pod 的名称都以`ReplicaSet`资源名称开始，并以一个随机哈希值结束。这就是为什么你会在 pod 名称中看到两个哈希值。
- en: 'Let’s assume you have a new release and want to deploy a new version of your
    container image. So, let’s update the `Deployment` resource with a new image using
    the following command:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个新版本并希望部署一个新的容器镜像。那么，我们可以使用以下命令更新`Deployment`资源并使用新的镜像：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To check the deployment status, run the following command:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查部署状态，可以运行以下命令：
- en: '[PRE18]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Imperative commands such as those just shown are normally not used in production
    environments because they lack the audit trail you would get with declarative
    manifests using Git to version them. However, if you do choose to use imperative
    commands, you can always record the change cause of the previous rollout using
    the following command:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 像刚才显示的这些命令，通常不会在生产环境中使用，因为它们缺乏使用 Git 版本控制的声明性清单所提供的审计追踪。然而，如果你选择使用这些命令，你始终可以使用以下命令记录上次发布的更改原因：
- en: '[PRE19]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To check the Deployment history, run the following command:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看部署历史，可以运行以下命令：
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As we can see, there are two revisions in the Deployment history. Revision `1`
    was the initial Deployment, and revision `2` was because of the `kubectl set image`
    command we ran, as evident from the `CHANGE-CAUSE` column.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，部署历史中有两个修订版本。修订版本`1`是最初的部署，修订版本`2`是由于我们运行了`kubectl set image`命令，正如`CHANGE-CAUSE`列中所显示的那样。
- en: 'Let’s say you find an issue after Deployment and want to roll it back to the
    previous version. To do so and also recheck the status of the Deployment, run
    the following command:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在部署后发现了问题，并希望回滚到先前的版本。为了这样做，并重新检查部署的状态，可以运行以下命令：
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'And finally, let’s recheck the deployment history using the following command:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用以下命令重新检查部署历史：
- en: '[PRE22]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: And we get revision `3` with a `CHANGE-CAUSE` value of `<none>`. In this case,
    we did not annotate the rollback as in the last command.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到修订版本`3`，其`CHANGE-CAUSE`值为`<none>`。在这种情况下，我们没有像上一个命令那样注释回滚。
- en: Tip
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Always annotate Deployment updates as it becomes easier to peek into the history
    to see what got deployed.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 始终注释部署更新，因为这样可以更容易查看历史记录，了解部署了什么内容。
- en: Now, let’s look at some common Kubernetes Deployment strategies to understand
    how to use Deployments effectively.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看一些常见的 Kubernetes 部署策略，以便了解如何有效地使用部署。
- en: Kubernetes Deployment strategies
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 部署策略
- en: 'Updating an existing Deployment requires you to specify a new container image.
    That is why we version container images in the first place so that you can roll
    out, and roll back, application changes as required. As we run everything in containers—and
    containers, by definition, are ephemeral—this enables a host of different deployment
    strategies that we can implement. There are several deployment strategies, and
    some of these are set out as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 更新现有的部署需要指定一个新的容器镜像。这就是为什么我们首先对容器镜像进行版本控制，以便可以根据需要滚动发布和回滚应用程序更改的原因。由于我们所有的应用都运行在容器中——容器本质上是短暂的——这使得我们可以实现多种不同的部署策略。这里有几种部署策略，其中一些如下所示：
- en: '**Recreate**: This is the simplest of all. Delete the old pod and deploy a
    new one.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重新创建**：这是所有方法中最简单的一种。删除旧的 pod 并部署一个新的。'
- en: '**Rolling update**: Slowly roll out the pods of the new version while still
    running the old version, and slowly remove the old pods as the new pods get ready.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滚动更新**：在运行旧版本的同时，逐步推出新版本的 pods，并随着新 pods 的准备好，逐步删除旧的 pods。'
- en: '**Blue/green**: This is a derived deployment strategy where we keep both versions
    running simultaneously and switch the traffic to the newer version when we want.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蓝绿发布**：这是一种派生的部署策略，我们同时运行两个版本，当需要时将流量切换到新版。'
- en: '**Canary**: This applies to Blue/Green Deployments where we switch a percentage
    of traffic to the newer version of the application before fully rolling out the
    release.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金丝雀发布**：这适用于蓝绿部署（Blue/Green Deployments），其中我们在完全推出版本之前，将一定比例的流量切换到应用程序的新版。'
- en: '**A/B testing**: A/B testing is more of a technique to apply to Blue/Green
    Deployments. This is when you want to roll out the newer version to a subset of
    willing users and study the usage patterns before completely rolling out the newer
    version. You do not get A/B testing out of the box with Kubernetes but instead
    should rely on service mesh technologies that plug in well with Kubernetes, such
    as **Istio**, **Linkerd**, and **Traefik**.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A/B 测试**：A/B 测试更多是一种应用于蓝绿部署的技术。这是当你希望将新版推出给一部分愿意的用户，并在完全推出新版之前研究他们的使用模式。Kubernetes
    并没有开箱即用的 A/B 测试功能，但你可以依赖与 Kubernetes 配合良好的服务网格技术，如 **Istio**、**Linkerd** 和 **Traefik**。'
- en: Kubernetes provides two deployment strategies out of the box—`Recreate` and
    `RollingUpdate`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了两种开箱即用的部署策略——`Recreate` 和 `RollingUpdate`。
- en: Recreate
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重建
- en: 'The `Recreate` strategy is the most straightforward deployment strategy. When
    you update the `Deployment` resource with the `Recreate` strategy, Kubernetes
    immediately spins down the old `ReplicaSet` resource and creates a new one with
    the required number of replicas along the lines of the following diagram:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`Recreate` 策略是最简单直接的部署策略。当你使用 `Recreate` 策略更新 `Deployment` 资源时，Kubernetes 会立即停止旧的
    `ReplicaSet` 资源，并根据以下图示创建一个新的 `ReplicaSet` 资源，且具有所需数量的副本：'
- en: '![Figure 6.2 – Recreate strategy](img/B19877_Figure_6.02.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – Recreate 策略](img/B19877_Figure_6.02.jpg)'
- en: Figure 6.2 – Recreate strategy
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – Recreate 策略
- en: Kubernetes does not delete the old `ReplicaSet` resource but instead sets `replicas`
    to `0`. That is required to roll back to the old version quickly. This approach
    results in downtime and is something you want to use only in case of a constraint.
    Thus, this strategy isn’t the default deployment strategy in Kubernetes.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 不会删除旧的 `ReplicaSet` 资源，而是将 `replicas` 设置为 `0`。这是为了能够快速回滚到旧版本。这种方法会导致停机时间，因此你只应在有约束的情况下使用。因此，这种策略不是
    Kubernetes 的默认部署策略。
- en: Tip
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: You can use the `Recreate` strategy if your application does not support multiple
    replicas, if it does not support more than a certain number of replicas (such
    as applications that need to maintain a quorum), or if it does not support multiple
    versions at once.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的应用程序不支持多个副本，或者它不支持超过一定数量的副本（例如需要维持法定人数的应用程序），或者它不支持同时运行多个版本，那么你可以使用 `Recreate`
    策略。
- en: 'Let’s update `nginx-deployment` with the `Recreate` strategy. Let’s look at
    the `nginx-recreate.yaml` file:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `Recreate` 策略更新 `nginx-deployment`。让我们来看一下 `nginx-recreate.yaml` 文件：
- en: '[PRE23]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The YAML file now contains a `strategy` section with a `Recreate` type. Now,
    let’s apply the `nginx-recreate.yaml` file and watch the `ReplicaSet` resources
    using the following command:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: YAML 文件现在包含一个 `strategy` 部分，并且设置为 `Recreate` 类型。现在，让我们应用 `nginx-recreate.yaml`
    文件，并使用以下命令查看 `ReplicaSet` 资源：
- en: '[PRE24]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `Deployment` resource creates a new `ReplicaSet` resource—`nginx-6799fc88d8`—with
    `0` desired replicas. It then sets `0` desired replicas to the old `ReplicaSet`
    resource and waits for the old `ReplicaSet` resource to be completely evicted.
    It then starts automatically rolling out the new `ReplicaSet` resource to the
    desired images.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`Deployment` 资源会创建一个新的 `ReplicaSet` 资源——`nginx-6799fc88d8`——其期望副本数为 `0`。然后，它将旧的
    `ReplicaSet` 资源的期望副本数设置为 `0`，并等待旧的 `ReplicaSet` 资源完全驱逐。接着，它会自动开始推出新的 `ReplicaSet`
    资源，以达到期望的镜像。'
- en: RollingUpdate
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 滚动更新
- en: 'When you update the Deployment with a `RollingUpdate` strategy, Kubernetes
    creates a new `ReplicaSet` resource, and it simultaneously spins up the required
    number of pods on the new `ReplicaSet` resource while slowly spinning down the
    old `ReplicaSet` resource, as evident from the following diagram:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用 `RollingUpdate` 策略更新 Deployment 时，Kubernetes 会创建一个新的 `ReplicaSet` 资源，同时在新的
    `ReplicaSet` 资源上启动所需数量的 pod，并逐渐关闭旧的 `ReplicaSet` 资源，正如以下图示所示：
- en: '![Figure 6.3 – RollingUpdate strategy](img/B19877_Figure_6.03.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – RollingUpdate 策略](img/B19877_Figure_6.03.jpg)'
- en: Figure 6.3 – RollingUpdate strategy
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – RollingUpdate 策略
- en: '`RollingUpdate` is the default deployment strategy. You can use the `RollingUpdate`
    strategy in most applications, apart from ones that cannot tolerate more than
    one version of the application at a given time.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`RollingUpdate`是默认的部署策略。除了那些无法容忍在给定时间内存在多个版本的应用外，大多数应用都可以使用`RollingUpdate`策略。'
- en: 'Let’s update the `nginx` `Deployment` resource using the `RollingUpdate` strategy.
    We will reuse the standard `nginx-deployment.yaml` file that we used before. Use
    the following command and see what happens to the `ReplicaSet` resources:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`RollingUpdate`策略更新`nginx`的`Deployment`资源。我们将重新使用之前使用的标准`nginx-deployment.yaml`文件。使用以下命令并查看`ReplicaSet`资源会发生什么：
- en: '[PRE25]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As we see, the old `ReplicaSet` resource—`nginx-6799fc88d8`—is being rolled
    down, and the new `ReplicaSet` resource—`nginx-6889dfccd5`—is being rolled out
    simultaneously.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，旧的`ReplicaSet`资源—`nginx-6799fc88d8`—正在被下线，而新的`ReplicaSet`资源—`nginx-6889dfccd5`—正在被同时上线。
- en: The `RollingUpdate` strategy also has two options—`maxUnavailable` and `maxSurge`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`RollingUpdate`策略还有两个选项—`maxUnavailable`和`maxSurge`。'
- en: While `maxSurge` defines the maximum number of additional pods we can have at
    a given time, `maxUnavailable` defines the maximum number of unavailable pods
    we can have at a given time.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当`maxSurge`定义了在给定时间内我们可以拥有的额外Pod的最大数量时，`maxUnavailable`定义了在给定时间内我们可以拥有的不可用Pod的最大数量。
- en: Tip
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Set `maxSurge` to `0` if your application cannot tolerate more than a certain
    number of replicas. Set `maxUnavailable` to `0` if you want to maintain reliability
    and your application can tolerate more than the set replicas. You cannot specify
    `0` for both parameters, as that will make any rollout attempts impossible. While
    setting `maxSurge`, ensure your cluster has spare capacity to spin up additional
    pods, or the rollout will fail.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的应用无法容忍超过某个数量的副本，请将`maxSurge`设置为`0`。如果你希望保持可靠性，并且应用可以容忍超过设定的副本数，请将`maxUnavailable`设置为`0`。你不能将这两个参数都设置为`0`，因为那样会使任何滚动尝试变得不可能。在设置`maxSurge`时，确保你的集群有足够的空闲容量来启动额外的Pod，否则滚动更新将失败。
- en: Using these settings, we can create different kinds of custom rollout strategies—some
    popular ones are discussed in the following sections.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些设置，我们可以创建不同类型的自定义滚动策略—接下来的部分将讨论一些流行的策略。
- en: Ramped slow rollout
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 渐进式慢滚动
- en: If you have numerous replicas but want to roll out the release slowly, observe
    the application for any issues, and roll back your deployment if needed, you should
    use this strategy.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有多个副本，但希望缓慢推出发布，观察应用是否有问题，并在需要时回滚部署，你应该使用这个策略。
- en: 'Let’s create an `nginx` deployment, `nginx-ramped-slow-rollout.yaml`, using
    the **ramped slow** **rollout** strategy:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用**渐进式慢滚动**策略创建一个`nginx`部署，`nginx-ramped-slow-rollout.yaml`：
- en: '[PRE26]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The manifest is very similar to the generic Deployment, except that it contains
    `10` replicas and a `strategy` section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 该清单与通用的部署非常相似，只不过它包含了`10`个副本和一个`strategy`部分。
- en: 'The `strategy` section contains the following:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`strategy`部分包含以下内容：'
- en: '`type`: `RollingUpdate`'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`type`: `RollingUpdate`'
- en: '`rollingUpdate`: The section describing rolling update attributes –`maxSurge`
    and `maxUnavailable`'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rollingUpdate`: 描述滚动更新属性的部分—`maxSurge`和`maxUnavailable`'
- en: 'Now, let’s apply the YAML file and wait for the deployment to completely roll
    out to `10` replicas using the following commands:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们应用YAML文件，并使用以下命令等待部署完全滚动到`10`个副本：
- en: '[PRE27]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As we see, the pods have rolled out completely. Let’s now update the `Deployment`
    resource with a different `nginx` image version and see what we get using the
    following command:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，Pod已经完全滚动完成。现在，让我们使用以下命令更新`Deployment`资源，换用不同的`nginx`镜像版本，看看会发生什么：
- en: '[PRE28]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: So, we see two `ReplicaSet` resources here—`nginx-6799fc88d8` and `nginx-6889dfccd5`.
    While the `nginx-6799fc88d8` pod is slowly rolling down from `10` pods to `0`,
    one at a time, simultaneously, the `nginx-6889dfccd5` pod is slowly rolling up
    from `0` pods to `10`. At any given time, the number of pods never exceeds `11`.
    That is because `maxSurge` is set to `1`, and `maxUnavailable` is `0`. This is
    a slow rollout in action.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们在这里看到两个`ReplicaSet`资源—`nginx-6799fc88d8`和`nginx-6889dfccd5`。当`nginx-6799fc88d8`的Pod正在从`10`个Pod慢慢减少到`0`时，一次只减少一个，`nginx-6889dfccd5`的Pod则同时从`0`个Pod逐步增加到`10`个Pod。在任何时刻，Pod的数量都不会超过`11`。这是因为`maxSurge`设置为`1`，而`maxUnavailable`设置为`0`。这就是慢滚动的实际效果。
- en: Tip
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Ramped slow rollout is useful when we want to be cautious before we impact many
    users, but this strategy is extremely slow and may only suit some applications.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 渐进式慢滚动在我们希望在影响许多用户之前保持谨慎时非常有用，但这种策略非常慢，可能只适用于某些应用。
- en: Let’s look at the best-effort controlled rollout strategy for a faster rollout
    without compromising application availability.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看更快推出的最佳努力控制策略，而不影响应用程序的可用性。
- en: Best-effort controlled rollout
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最佳努力控制的推出
- en: '**Best-effort controlled rollout** helps you roll out your deployment on a
    best-effort basis, and you can use it to roll out your release faster and ensure
    that your application is available. It can also help with applications that do
    not tolerate more than a certain number of replicas at a given point.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**Best-effort controlled rollout** 帮助你以最佳努力的方式推出部署，你可以使用它来更快地推出你的发布，并确保你的应用程序可用。它还可以帮助处理那些在特定时间点不允许超过一定数量副本的应用程序。'
- en: We will set `maxSurge` to `0` and `maxUnavailable` to any percentage we find
    suitable for remaining unavailable at a given time to implement this. It can be
    specified using the number of pods or as a percentage.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 `maxSurge` 设置为 `0`，并将 `maxUnavailable` 设置为适当的百分比，以确保在任何给定时间内仍然不可用。可以按照 pod
    数量或百分比来指定。
- en: Tip
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Using a percentage is a better option since, with this, you don’t have to recalculate
    your `maxUnavailable` parameter if the replicas change.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 使用百分比是更好的选择，因为这样做的话，如果副本数量发生变化，你就不需要重新计算 `maxUnavailable` 参数了。
- en: 'Let’s look at the manifest—`nginx-best-effort-controlled-rollout.yaml`:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看清单— `nginx-best-effort-controlled-rollout.yaml`：
- en: '[PRE29]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let’s now apply the YAML file and see what we get:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们应用 YAML 文件并看看我们得到了什么：
- en: '[PRE30]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: So, we see the `ReplicaSet` resource rolling out such that the total pods are
    at most `10` at any point, and the total unavailable pods are never more than
    `25%`. You may also notice that instead of creating a new `ReplicaSet` resource,
    the `Deployment` resource uses an old `ReplicaSet` resource containing the `nginx:latest`
    image. Remember when I said the old `ReplicaSet` resource is not deleted when
    you update a `Deployment` resource?
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们看到 `ReplicaSet` 资源进行滚动式推出，以便任何时候总体 pod 数量最多为 `10`，总体不可用 pod 数量永远不超过 `25%`。你还可以注意到，更新
    `Deployment` 资源时并不会创建新的 `ReplicaSet` 资源，而是使用包含 `nginx:latest` 镜像的旧 `ReplicaSet`
    资源。还记得我说过更新 `Deployment` 资源时不会删除旧 `ReplicaSet` 资源吗？
- en: '`Deployment` resources on their own are great ways of scheduling and managing
    pods. However, we have overlooked an essential part of running containers in Kubernetes—exposing
    them to the internal or external world. Kubernetes provides several resources
    to help expose your workloads appropriately—primarily, `Service` and `Ingress`
    resources. Let’s have a look at these in the next section.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`Deployment` 资源本身是调度和管理 pod 的好方式。然而，我们忽视了 Kubernetes 中运行容器的一个重要部分——将它们暴露给内部或外部世界。Kubernetes
    提供了几种资源来帮助适当地暴露你的工作负载——主要是 `Service` 和 `Ingress` 资源。让我们在下一节中看看这些。'
- en: Kubernetes Services and Ingresses
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes Services 和 Ingresses
- en: It’s story time! Let’s simplify Kubernetes Services.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 故事时间到！让我们简化 Kubernetes Services。
- en: Imagine you have a group of friends who love to order food from your restaurant.
    Instead of delivering each order to their houses separately, you set up a central
    delivery point in their neighborhood. This delivery point (or hub) is your “service.”
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你有一群喜欢从你的餐馆订餐的朋友。与其分别将每个订单送到他们家里，不如在他们的社区建立一个中央交付点。这个交付点（或中心）就是你的“服务”。
- en: In Kubernetes, a Service is like that central hub. It’s a way for the different
    parts of your application (such as your website, database, or other things) to
    talk to each other, even if they’re in separate containers or machines. It gives
    them easy-to-remember addresses to find each other without getting lost.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，一个 **Service** 就像是那个中央枢纽。它是你应用程序不同部分（比如你的网站、数据库或其他组件）之间进行通信的一种方式，即使它们在不同的容器或机器上也可以。它为它们提供了易于记忆的地址，以便彼此找到而不至于迷失。
- en: The `Service` resource helps expose Kubernetes workloads to the internal or
    external world. As we know, pods are ephemeral resources—they can come and go.
    Every pod is allocated a unique IP address and hostname, but once a pod is gone,
    the pod’s IP address and hostname change. Consider a scenario where one of your
    pods wants to interact with another. However, because of its transient nature,
    you cannot configure a proper endpoint. If you use the IP address or the hostname
    as the endpoint of a pod and the pod is destroyed, you will no longer be able
    to connect to it. Therefore, exposing a pod on its own is not a great idea.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`Service` 资源有助于将 Kubernetes 工作负载暴露到内部或外部世界。如我们所知，pods 是临时性的资源——它们可以出现也可以消失。每个
    pod 都会分配一个独特的 IP 地址和主机名，但一旦 pod 被销毁，pod 的 IP 地址和主机名也会发生变化。假设你的一个 pod 想与另一个 pod
    交互，但是由于 pod 的短暂特性，你无法配置一个合适的端点。如果你使用 IP 地址或主机名作为 pod 的端点，而该 pod 被销毁后，你将无法再连接到它。因此，单独暴露一个
    pod 并不是一个好主意。'
- en: Kubernetes provides the `Service` resource to provide a static IP address to
    a group of pods. Apart from exposing the pods on a single static IP address, it
    also provides load balancing of traffic between pods in a round-robin configuration.
    It helps distribute traffic equally between the pods and is the default method
    of exposing your workloads.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了 `Service` 资源，以便为一组 pod 提供静态 IP 地址。除了通过单一静态 IP 地址暴露这些 pod 外，它还在轮询配置中为
    pod 之间的流量提供负载均衡。它帮助均匀地分配流量到各个 pod，是暴露工作负载的默认方法。
- en: '`Service` resources are also allocated a static `Service` resource FQDN instead
    of the IP address within your cluster to make your endpoints fail-safe.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`Service` 资源还会分配一个静态的 `Service` 资源 FQDN，而不是集群内的 IP 地址，以确保端点的容错性。'
- en: 'Now, coming back to `Service` resources, there are multiple `Service` resource
    types— `ClusterIP`, `NodePort`, and `LoadBalancer`, each having its own respective
    use case:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回到 `Service` 资源，存在多种 `Service` 资源类型——`ClusterIP`、`NodePort` 和 `LoadBalancer`，每种类型都有各自的使用场景：
- en: '![Figure 6.4 – Kubernetes Services](img/B19877_Figure_6.04.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – Kubernetes 服务](img/B19877_Figure_6.04.jpg)'
- en: Figure 6.4 – Kubernetes Services
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – Kubernetes 服务
- en: Let’s understand each of these with the help of examples.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过示例来理解这些内容。
- en: ClusterIP Service resources
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ClusterIP 服务资源
- en: '`ClusterIP` `Service` resources are the default `Service` resource type that
    exposes pods within the Kubernetes cluster. It is not possible to access `ClusterIP`
    `Service` resources outside the cluster; therefore, they are never used to expose
    your pods to the external world. `ClusterIP` `Service` resources generally expose
    backend apps such as data stores and databases—the business and data layers—in
    a three-tier architecture.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClusterIP` `Service` 资源是默认的 `Service` 资源类型，用于在 Kubernetes 集群内部暴露 pods。无法从集群外部访问
    `ClusterIP` `Service` 资源；因此，它们永远不会用于将 pods 暴露到外部世界。`ClusterIP` `Service` 资源通常用于暴露后端应用程序，如数据存储和数据库——即三层架构中的业务和数据层。'
- en: Tip
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: When choosing between `Service` resource types, as a general rule of thumb,
    always start with the `ClusterIP` `Service` resource and change it if needed.
    This will ensure that only the required Services are exposed externally.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择 `Service` 资源类型时，一般的经验法则是，始终从 `ClusterIP` `Service` 资源开始，必要时再进行修改。这将确保只有需要的服务暴露到外部。
- en: 'To understand `ClusterIP` `Service` resources better, let’s create a `redis
    Deployment` resource first using the imperative method with the following command:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解 `ClusterIP` `Service` 资源，让我们首先使用以下命令通过命令式方法创建一个 `redis Deployment` 资源：
- en: '[PRE31]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let’s try exposing the `redis` deployment pods using a `ClusterIP` `Service`
    resource. To access the resources for this section, `cd` into the following:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用 `ClusterIP` `Service` 资源暴露 `redis` 部署的 pods。要访问此部分的资源，请 `cd` 到以下目录：
- en: '[PRE32]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let’s look at the `Service` resource manifest, `redis-clusterip.yaml`, first:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们查看 `Service` 资源清单 `redis-clusterip.yaml`：
- en: '[PRE33]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The `Service` resource manifest starts with `apiVersion` and `kind` as any other
    resource. It has a `metadata` section that contains `name` and `labels`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`Service` 资源清单从 `apiVersion` 和 `kind` 开始，和其他资源一样。它有一个 `metadata` 部分，其中包含 `name`
    和 `labels`。'
- en: 'The `spec` section contains the following:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`spec` 部分包含以下内容：'
- en: '`ports`: This section includes a list of ports that we want to expose via the
    `Service` resource:'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ports`：该部分包括我们希望通过 `Service` 资源暴露的端口列表：'
- en: 'A. `port`: The port we wish to expose.'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. `port`：我们希望暴露的端口。
- en: 'B. `protocol`: The protocol of the port we expose (TCP/UDP).'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. `protocol`：我们暴露的端口协议（TCP/UDP）。
- en: 'C. `targetPort`: The target container port where the exposed port will forward
    the connection. This allows us to have a port mapping similar to Docker.'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C. `targetPort`：目标容器端口，暴露端口将把连接转发到该端口。这使我们能够拥有类似于Docker的端口映射。
- en: '`selector`: This section contains `labels` based on which pod group is selected.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`selector`：该部分包含用于选择Pod组的`labels`。'
- en: 'Let’s apply the `Service` resource manifest using the following command and
    see what we get:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下命令应用`Service`资源清单，看看我们能得到什么：
- en: '[PRE34]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let’s run `kubectl get` to list the `Service` resource and get the cluster
    IP:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行`kubectl get`命令列出`Service`资源并获取集群IP：
- en: '[PRE35]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We see a `redis` `Service` resource running with a `ClusterIP` type. But as
    this pod is not exposed externally, the only way to access it is through a second
    pod running within the cluster.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到一个`redis`的`Service`资源正在以`ClusterIP`类型运行。但由于这个Pod没有对外暴露，访问它的唯一方式是通过集群内的第二个Pod。
- en: 'Let’s create a `busybox` pod in interactive mode to inspect the `Service` resource
    and run some tests using the following command:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个`busybox` Pod，以交互模式检查`Service`资源并使用以下命令运行一些测试：
- en: '[PRE36]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: And with this, we see a prompt. We have launched the `busybox` container and
    are currently within that. We will use the `telnet` application to check the connectivity
    between pods.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就看到了一个提示。我们已经启动了`busybox`容器，并且现在正处于其中。我们将使用`telnet`应用程序检查Pods之间的连通性。
- en: 'Let’s telnet the cluster IP and port to see whether it’s reachable using the
    following command:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下命令telnet集群IP和端口，看看是否可达：
- en: '[PRE37]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The IP/port pair is reachable from there. Kubernetes also provides an internal
    DNS to promote service discovery and connect to the `Service` resource. We can
    do a reverse `nslookup` on the cluster IP to get the `Service` resource’s FQDN
    using the following command:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里可以访问IP/端口对。Kubernetes还提供了内部DNS以促进服务发现并连接到`Service`资源。我们可以使用以下命令对集群IP进行反向`nslookup`，获取`Service`资源的FQDN：
- en: '[PRE38]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'As we can see, the IP address is accessible from the FQDN—`redis.default.svc.cluster.local`.
    We can use the entire domain or parts of it based on our location. The FQDN is
    formed of these parts: `<service_name>.<namespace>.svc.<cluster-domain>.local`.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，IP地址可以通过FQDN访问——`redis.default.svc.cluster.local`。我们可以根据我们的地理位置使用整个域名或其中的一部分。FQDN由以下部分组成：`<service_name>.<namespace>.svc.<cluster-domain>.local`。
- en: 'Kubernetes uses `default` namespace till now and will continue doing so. If
    your source pod is located in the same namespace as the `Service` resource, you
    can use `service_name` to connect to your `Service` resource—something like the
    following example:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes至今一直使用`default`命名空间，并将继续使用。如果你的源Pod位于与`Service`资源相同的命名空间内，你可以使用`service_name`来连接到你的`Service`资源——像下面的示例一样：
- en: '[PRE39]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'If you want to call a `Service` resource from a pod situated in a different
    namespace, you can use `<service_name>.<namespace>` instead—something like the
    following example:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想从位于不同命名空间的Pod调用`Service`资源，可以改用`<service_name>.<namespace>`，像下面的示例一样：
- en: '[PRE40]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Some service meshes, such as Istio, allow multi-cluster communication. In that
    situation, you can also use the cluster name for connecting to the `Service` resource,
    but as this is an advanced topic, it is beyond the scope of this discussion.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 一些服务网格（例如Istio）支持多集群通信。在这种情况下，您还可以使用集群名称来连接到`Service`资源，但由于这是一个高级主题，超出了本讨论的范围。
- en: Tip
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Always use the shortest domain name possible for endpoints, as it allows for
    more flexibility in moving your Kubernetes resources across your environments.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 始终使用尽可能短的域名来表示端点，因为它允许在不同环境中更灵活地移动Kubernetes资源。
- en: '`ClusterIP` Services work very well for exposing internal pods, but what if
    we want to expose our pods to the external world? Kubernetes offers various `Service`
    resource types for that; let’s look at the `NodePort` `Service` resource type
    first.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClusterIP`类型的服务非常适合暴露内部Pod，但如果我们想将Pod暴露给外部世界呢？Kubernetes提供了多种`Service`资源类型；首先让我们看看`NodePort`类型的`Service`资源。'
- en: NodePort Service resources
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NodePort Service资源
- en: '`NodePort` `Service` resources are used to expose your pods to the external
    world. Creating a `NodePort` `Service` resource spins up a `ClusterIP` `Service`
    resource and maps the `ClusterIP` port to a random high port number (default:
    `30000`-`32767`) on all cluster nodes. You can also specify a static `NodePort`
    number if you so desire. So, with a `NodePort` `Service` resource, you can access
    your pods using the IP address of any node within your cluster and the `NodePort`
    of the service.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '`NodePort` `Service`资源用于将你的Pods暴露到外部。创建一个`NodePort` `Service`资源会启动一个`ClusterIP`类型的`Service`资源，并将`ClusterIP`端口映射到所有集群节点的随机高端口号（默认范围：`30000`-`32767`）。你也可以根据需要指定一个静态的`NodePort`端口号。因此，使用`NodePort`
    `Service`资源，你可以通过集群中任意节点的IP地址和该服务的`NodePort`来访问你的Pods。'
- en: Tip
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Though it is possible to specify a static `NodePort` number, you should avoid
    using it. That is because you might end up in port conflicts with other `Service`
    resources and put a high dependency on config and change management. Instead,
    keep things simple and use dynamic ports.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可以指定静态的`NodePort`端口号，但你应该避免使用它。这是因为你可能会与其他`Service`资源发生端口冲突，并且对配置和变更管理产生较高依赖。相反，保持简单，使用动态端口。
- en: Going by the Flask application example, let’s create a `flask-app` pod with
    the `redis` `Service` resource we created before, acting as its backend, and then
    we will expose the pod on `NodePort`.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 以Flask应用为例，创建一个`flask-app` Pod，并使用之前创建的`redis` `Service`资源作为其后端，然后我们将通过`NodePort`暴露该Pod。
- en: 'Use the following command to create a pod imperatively:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令命令式地创建一个Pod：
- en: '[PRE41]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, as we’ve created the `flask-app` pod, let’s check its status using the
    following command:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然我们已经创建了`flask-app` Pod，使用以下命令查看它的状态：
- en: '[PRE42]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The `flask-app` pod is running successfully and is ready to accept requests.
    It’s time to understand the resource manifest for the `NodePort` `Service` resource,
    `flask-nodeport.yaml`:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`flask-app` Pod已经成功运行，并准备好接受请求。现在是时候理解`NodePort`类型的`Service`资源清单`flask-nodeport.yaml`了：'
- en: '[PRE43]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The manifest is similar to the `ClusterIP` manifest but contains a `type` attribute
    specifying the `Service` resource type—`NodePort`.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 清单类似于`ClusterIP`清单，但包含一个`type`属性，用于指定`Service`资源类型——`NodePort`。
- en: 'Let’s apply this manifest to see what we get using the following command:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用这个清单，看看通过以下命令可以得到什么：
- en: '[PRE44]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, let’s list the `Service` resource to get the `NodePort` Service using
    the following command:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们列出`Service`资源并获取`NodePort`类型的Service，使用以下命令：
- en: '[PRE45]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: And we see that the type is now `NodePort`, and the container port `5000` is
    mapped to node port `32618`.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到类型现在是`NodePort`，容器端口`5000`映射到了节点端口`32618`。
- en: If you are logged in to any Kubernetes node, you can access the `Service` resource
    using `localhost:32618`. But as we are using Google Cloud Shell, we need to SSH
    into a node to access the `Service` resource.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经登录到任何Kubernetes节点，可以通过`localhost:32618`访问`Service`资源。但由于我们正在使用Google Cloud
    Shell，我们需要SSH进入节点才能访问`Service`资源。
- en: 'Let’s list the nodes first using the following command:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先列出节点，使用以下命令：
- en: '[PRE46]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'And as we can see, we have three nodes. Let’s SSH into the `gke-node-1dhh`
    node using the following command:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们有三台节点。让我们通过以下命令SSH进入`gke-node-1dhh`节点：
- en: '[PRE47]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, as we are within the `gke-node-1dhh` node, let’s curl `localhost:32618`
    using the following command:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然我们处在`gke-node-1dhh`节点，使用以下命令curl访问`localhost:32618`：
- en: '[PRE48]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: And we get a response back! You can SSH into any node and curl the endpoint
    and should get a similar response.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们收到了一个响应！你可以SSH进入任意节点并使用curl访问该端点，应该能得到类似的响应。
- en: 'To exit from the node and get back to the Cloud Shell prompt, run the following
    command:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 要退出节点并返回到Cloud Shell提示符，运行以下命令：
- en: '[PRE49]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: And you are back at the Cloud Shell prompt.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 你已回到Cloud Shell提示符。
- en: Tip
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: A `NodePort` `Service` resource is an intermediate kind of resource. This means
    that while it forms an essential building block of providing external services,
    it is not used on its own most of the time. When you are running on the cloud,
    you can use `LoadBalancer` `Service` resources instead. Even for an on-premises
    setup, it makes sense not to use `NodePort` for every `Service` resource and instead
    use `Ingress` resources.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`NodePort`类型的`Service`资源是一个中介类型的资源。这意味着虽然它是提供外部服务的重要组成部分，但大多数时候它并不会单独使用。当你在云环境中运行时，你可以使用`LoadBalancer`类型的`Service`资源。即使是在本地部署环境下，也不建议为每个`Service`资源使用`NodePort`，而应该使用`Ingress`资源。'
- en: Now, let’s look at the `LoadBalancer` `Service` resource used extensively to
    expose your Kubernetes workloads externally.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看广泛用于将你的 Kubernetes 工作负载暴露到外部的 `LoadBalancer` `Service` 资源。
- en: LoadBalancer Service resources
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LoadBalancer 服务资源
- en: '`LoadBalancer` `Service` resources help expose your pods on a single load-balanced
    endpoint. These `Service` resources can only be used within cloud platforms and
    platforms that provide Kubernetes controllers with access to spin up external
    network resources. A `LoadBalancer` Service practically spins up a `NodePort`
    `Service` resource and then requests the Cloud API to spin up a load balancer
    in front of the node ports. That way, it provides a single endpoint to access
    your `Service` resource from the external world.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`LoadBalancer` `Service` 资源帮助在单一的负载均衡端点上暴露你的 Pod。这些 `Service` 资源只能在云平台及提供 Kubernetes
    控制器来访问外部网络资源的平台上使用。`LoadBalancer` 服务实际上会启动一个 `NodePort` `Service` 资源，然后请求云 API
    在节点端口前面启动一个负载均衡器。这样，它提供了一个单一的端点，供外部世界访问你的 `Service` 资源。'
- en: Spinning up a `LoadBalancer` `Service` resource is simple—just set the type
    to `LoadBalancer`.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 启动一个 `LoadBalancer` `Service` 资源很简单——只需将类型设置为 `LoadBalancer`。
- en: 'Let’s expose the Flask application as a load balancer using the following manifest—`flask-loadbalancer.yaml`:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下清单将 Flask 应用程序暴露为负载均衡器——`flask-loadbalancer.yaml`：
- en: '[PRE50]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now, let’s apply the manifest using the following command:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下命令应用清单：
- en: '[PRE51]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let’s get the `Service` resource to notice the changes using the following
    command:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下命令让 `Service` 资源注意到这些更改：
- en: '[PRE52]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The `Service` resource type is now `LoadBalancer`. As you can see, it now contains
    an external IP along with the cluster IP.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '`Service` 资源类型现在是 `LoadBalancer`。如你所见，它现在包含了一个外部 IP 以及集群 IP。'
- en: 'You can then curl on the external IP on port `5000` using the following command:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以使用以下命令在外部 IP 的端口 `5000` 上执行 curl：
- en: '[PRE53]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: And you get the same response as before. Your `Service` resource is now running
    externally.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你会得到与之前相同的响应。你的 `Service` 资源现在已在外部运行。
- en: Tip
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: '`LoadBalancer` `Service` resources tend to be expensive as every new resource
    spins up a network load balancer within your cloud provider. If you have HTTP-based
    workloads, use `Ingress` resources instead of `LoadBalancer` to save on resource
    costs and optimize traffic as they spin up an application load balancer instead.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`LoadBalancer` `Service` 资源通常比较昂贵，因为每当新建一个资源时，它会在你的云服务提供商中启动一个网络负载均衡器。如果你的工作负载基于
    HTTP，建议使用 `Ingress` 资源而不是 `LoadBalancer` 来节省资源成本并优化流量，因为它们会启动一个应用程序负载均衡器。'
- en: While Kubernetes Services form the basic building block of exposing your container
    applications internally and externally, Kubernetes also provides `Ingress` resources
    for additional fine-grained control over traffic. Let’s have a look at this in
    the next section.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Kubernetes 服务是暴露你的容器应用程序内部和外部的基本构建块，Kubernetes 还提供了 `Ingress` 资源，用于对流量进行更精细的控制。让我们在下一部分中看看这一点。
- en: Ingress resources
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ingress 资源
- en: Imagine you have a beautiful front entrance to your restaurant where customers
    come in. They walk through this main entrance to reach different parts of your
    restaurant, such as the dining area or the bar. This entrance is like your “ingress.”
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你的餐厅有一个漂亮的前门，顾客通过这个门进入。顾客通过这个主要入口进入餐厅后，可以到达餐厅的不同区域，比如就餐区或酒吧。这个入口就像你的“入口”。
- en: In Kubernetes, an Ingress is like that front entrance. It helps manage external
    access to the Services inside your cluster. Instead of exposing each Service individually,
    you can use an Ingress to decide how people from the outside can reach different
    parts of your application.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，Ingress 就像那个前门。它帮助管理外部访问你集群内的服务。你不需要为每个服务单独暴露，你可以使用 Ingress 来决定外部用户如何访问你应用程序的不同部分。
- en: In simple terms, a Kubernetes Service is like a central delivery point for your
    application’s different parts, and an Ingress is like a front entrance that helps
    people from the outside find and access those parts easily.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，Kubernetes 服务就像是你应用程序不同部分的中央交付点，而 Ingress 就像是一个前门，帮助外部用户轻松找到并访问这些部分。
- en: '`Ingress` resources act as reverse proxies into Kubernetes. You don’t need
    a load balancer for every application you run within your estate, as load balancers
    normally forward traffic and don’t require high levels of computing power. Therefore,
    spinning up a load balancer for everything does not make sense.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`Ingress` 资源充当 Kubernetes 中的反向代理。你不需要为你运行的每个应用程序都创建负载均衡器，因为负载均衡器通常转发流量，并不需要很高的计算能力。因此，为每个应用程序都启动负载均衡器并不明智。'
- en: 'Therefore, Kubernetes provides a way of routing external traffic into your
    cluster via `Ingress` resources. These resources help you subdivide traffic according
    to multiple conditions. Some of these are set out here:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Kubernetes 提供了一种通过 `Ingress` 资源将外部流量路由到集群的方式。这些资源帮助你根据多个条件划分流量。以下是一些设定：
- en: Based on the URL path
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 URL 路径
- en: Based on the hostname
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于主机名
- en: A combination of the two
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两者的结合
- en: 'The following diagram illustrates how `Ingress` resources work:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了 `Ingress` 资源的工作原理：
- en: '![Figure 6.5 – Kubernetes Ingress resources](img/B19877_Figure_6.05.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5 – Kubernetes 入口资源](img/B19877_Figure_6.05.jpg)'
- en: Figure 6.5 – Kubernetes Ingress resources
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – Kubernetes 入口资源
- en: '`Ingress` resources require an ingress controller to work. While most cloud
    providers have a controller installed, you must install an ingress controller
    on-premises or in a self-managed Kubernetes cluster. For more details on installing
    an ingress controller, refer to [https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/).
    You can install more than one ingress controller, but you will have to annotate
    your manifests to denote explicitly which controller the `Ingress` resource should
    use.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '`Ingress` 资源需要一个入口控制器才能正常工作。虽然大多数云服务提供商已经安装了控制器，但你必须在本地或自管理的 Kubernetes 集群中安装入口控制器。有关安装入口控制器的详细信息，请参考
    [https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/)。你可以安装多个入口控制器，但你需要在清单中注解，明确指定
    `Ingress` 资源应该使用哪个控制器。'
- en: For this chapter, we will use the `Ingress` resources before and do an exact
    like-for-like migration.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将在前面使用 `Ingress` 资源，并进行精确的逐一迁移。
- en: 'To understand how the nginx ingress controller works on GKE (or any other cloud),
    let’s look at the following diagram:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解 nginx 入口控制器在 GKE（或其他云）上的工作原理，让我们看看以下图示：
- en: '![Figure 6.6 – nginx ingress controller on GKE](img/B19877_Figure_6.06.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6 – GKE 上的 nginx 入口控制器](img/B19877_Figure_6.06.jpg)'
- en: Figure 6.6 – nginx ingress controller on GKE
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – GKE 上的 nginx 入口控制器
- en: The client connects to the `Ingress` resource via an ingress-managed load balancer,
    and the traffic moves to the ingress controllers that act as the load balancer’s
    backend. The ingress controllers then route the traffic to the correct `Service`
    resource via routing rules defined on the `Ingress` resource.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端通过一个由入口管理的负载均衡器连接到 `Ingress` 资源，流量会转发到充当负载均衡器后端的入口控制器。然后，入口控制器根据在 `Ingress`
    资源上定义的路由规则将流量路由到正确的 `Service` 资源。
- en: 'Now, let’s go ahead and install the `nginx` ingress controller using the following
    command:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下命令安装 `nginx` 入口控制器：
- en: '[PRE54]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: This will boot up several resources under the `ingress-nginx` namespace. Most
    notable is the `ingress-nginx-controller` `Deployment`, which is exposed via the
    `ingress-nginx-controller` `LoadBalancer` `Service`.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动几个资源，位于 `ingress-nginx` 命名空间下。最显著的是 `ingress-nginx-controller` `Deployment`，它通过
    `ingress-nginx-controller` `LoadBalancer` `Service` 进行暴露。
- en: 'Let’s now expose the `flask-app` `Service` via an `Ingress` resource, but before
    we do that, we will have to expose the `flask-app` `Service` on a `ClusterIP`
    instead, so let’s apply the relevant manifest using the following command:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过 `Ingress` 资源暴露 `flask-app` `Service`，但在此之前，我们需要先将 `flask-app` `Service`
    暴露为 `ClusterIP`，所以让我们使用以下命令应用相关清单：
- en: '[PRE55]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The next step is to define an `Ingress` resource. Remember that as GKE is running
    on a public cloud, it has the ingress controllers installed and running. So, we
    can simply go and create an ingress manifest—`flask-basic-ingress.yaml`:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是定义一个 `Ingress` 资源。记住，由于 GKE 运行在公共云中，因此已经安装并运行了入口控制器。所以，我们可以简单地创建一个入口清单—`flask-basic-ingress.yaml`：
- en: '[PRE56]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: This resource defines a default backend that passes all traffic to the `flask-app`
    pod, so it is counter-productive, but let’s look at it for simplicity.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 该资源定义了一个默认的后端，将所有流量传递到 `flask-app` pod，因此它是低效的，但为了简便起见，我们先看一下它。
- en: 'Apply the manifest using the following command:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令应用清单：
- en: '[PRE57]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now, let’s list the `Ingress` resources using the following command:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用以下命令列出 `Ingress` 资源：
- en: '[PRE58]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We can see that the `flask-app` `Ingress` resource is now listed with `HOSTS
    *`. That means that this would listen on all hosts on all addresses. So, anything
    that does not match other Ingress rules will be routed here. As mentioned, we
    need the `nginx-ingress-controller` Service external IP address to invoke all
    Services exposed via Ingress. To get the external IP of the `nginx-ingress-controller`
    Service, run the following command:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 `flask-app` 的 `Ingress` 资源现在列出了 `HOSTS *`。这意味着它会监听所有主机上的所有地址。所以，任何不匹配其他
    Ingress 规则的流量都会路由到这里。如前所述，我们需要 `nginx-ingress-controller` 服务的外部 IP 地址来调用通过 Ingress
    暴露的所有服务。要获取 `nginx-ingress-controller` 服务的外部 IP 地址，请运行以下命令：
- en: '[PRE59]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We see an external IP allocated to it, which we will use further.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到为其分配了一个外部 IP 地址，接下来我们将使用它。
- en: Important note
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Remember that Ingress rules take a while to propagate across the cluster, so
    if you receive an error initially when you curl the endpoint, wait for 5 minutes,
    and you should get the response back.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，Ingress 规则传播到集群中需要一些时间，因此如果你在 `curl` 端点时最初收到错误，请等待 5 分钟，之后你应该会收到响应。
- en: 'Let’s curl this IP and see what we get using the following command:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下命令 `curl` 这个 IP 地址，看看我们能得到什么：
- en: '[PRE60]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now, let’s clean up the `Ingress` resource using the following command:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用以下命令清理 `Ingress` 资源：
- en: '[PRE61]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The simple Ingress rule is counterproductive as it routes all traffic to a single
    `Service` resource. The idea of Ingress is to use a single load balancer to route
    traffic to multiple targets. Let’s look at two ways to do this—**path-based**
    and **name-based** routing.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的 Ingress 规则是适得其反的，因为它将所有流量都路由到一个 `Service` 资源。Ingress 的目的是使用单个负载均衡器将流量路由到多个目标。让我们来看两种实现方法——**基于路径的路由**和**基于名称的路由**。
- en: Path-based routing
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于路径的路由
- en: Let’s consider an application with two versions, `v1` and `v2`, and you want
    both to co-exist on a single endpoint. You can use **path-based routing** for
    such a scenario.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含两个版本的应用程序，`v1` 和 `v2`，并希望它们都在单个端点上共存。在这种情况下，你可以使用 **基于路径的路由**。
- en: 'Let’s create the two application versions first using the imperative method
    by running the following commands:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先使用命令式方法创建这两个应用程序版本，运行以下命令：
- en: '[PRE62]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Now, expose the two pods as `ClusterIP` `Service` resources using the following
    commands:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用以下命令将这两个 Pod 暴露为 `ClusterIP` `Service` 资源：
- en: '[PRE63]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We will then create an `Ingress` resource using the following manifest file,
    `nginx-app-path-ingress.yaml`, which will expose two endpoints—`<external-ip>/v1`,
    which routes to the `v1` `Service` resource, and `<external-ip>/v2`, which routes
    to the `v2` `Service` resource:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用以下清单文件 `nginx-app-path-ingress.yaml` 创建一个 `Ingress` 资源，该资源将暴露两个端点——`<external-ip>/v1`，该端点路由到
    `v1` `Service` 资源，以及 `<external-ip>/v2`，该端点路由到 `v2` `Service` 资源：
- en: '[PRE64]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The Ingress manifest contains several rules. The `http` rule has two paths—`/v1`
    and `/v2`, having the `pathType` value set to `Prefix`. Therefore, any traffic
    arriving on a URL that starts with `/v1` is routed to the `nginx-v1` `Service`
    resource on port `80`, and traffic arriving on `/v2` is routed to the `nginx-v2`
    `Service` resource on port `80`.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '`Ingress` 清单包含多个规则。`http` 规则有两个路径—`/v1` 和 `/v2`，其 `pathType` 值设置为 `Prefix`。因此，任何以
    `/v1` 开头的 URL 流量将被路由到端口 `80` 上的 `nginx-v1` `Service` 资源，任何到达 `/v2` 的流量则被路由到端口
    `80` 上的 `nginx-v2` `Service` 资源。'
- en: 'Let’s apply the manifest by using the following command:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下命令应用该清单：
- en: '[PRE65]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now, let’s list the `Ingress` resources by running the following command:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，运行以下命令列出 `Ingress` 资源：
- en: '[PRE66]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Now that we have the external IP, we can `curl` both endpoints to see what
    we get using the following commands:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了外部 IP，可以使用以下命令 `curl` 这两个端点，看看我们能得到什么：
- en: '[PRE67]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Sometimes, a path-based route is not always feasible, as you might not want
    your users to remember the path of multiple applications. However, you can still
    run multiple applications using a single Ingress endpoint—that is, by using **name-based
    routing**.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 有时基于路径的路由并不总是可行，因为你可能不希望用户记住多个应用程序的路径。不过，你仍然可以通过单一的 Ingress 端点运行多个应用程序——也就是通过使用
    **基于名称的路由**。
- en: Name-based routing
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于名称的路由
- en: '`host` header we pass while making an HTTP request. The `Ingress` resource
    can route based on the header. For example, if we want to access the `v1` `Service`
    resource, we can use `v1.example.com`, and for the `v2` `Service` resource, we
    can use the `v2.example.com` URL.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在发起 HTTP 请求时传递的 `host` 头部。`Ingress` 资源可以根据该头部进行路由。例如，如果我们想访问 `v1` 的 `Service`
    资源，可以使用 `v1.example.com`，而访问 `v2` 的 `Service` 资源时，可以使用 `v2.example.com` URL。
- en: 'Let’s now have a look at the `nginx-app-host-ingress.yaml` manifest to understand
    this concept further:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下 `nginx-app-host-ingress.yaml` 清单，以便更深入理解这个概念：
- en: '[PRE68]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The manifest now contains multiple hosts—`v1.example.com` routing to `nginx-v1`,
    and `v2.example.com` routing to `nginx-v2`.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，清单中包含了多个主机——`v1.example.com` 路由到 `nginx-v1`，`v2.example.com` 路由到 `nginx-v2`。
- en: 'Now, let’s apply this manifest and get the Ingress using the following commands:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们应用这个清单并使用以下命令获取 Ingress：
- en: '[PRE69]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: This time, we can see that two hosts are defined, `v1.example.com` and `v2.example.com`,
    running on the same address. Before we hit those endpoints, we need to make an
    entry on the `/etc/hosts` file to allow our machine to resolve `v1.example.com`
    and `v2.example.com` to the Ingress address.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们看到两个主机被定义了，`v1.example.com` 和 `v2.example.com`，它们运行在同一个地址上。在访问这些端点之前，我们需要在
    `/etc/hosts` 文件中添加一个条目，允许我们的机器将 `v1.example.com` 和 `v2.example.com` 解析到 Ingress
    地址。
- en: 'Edit the `/etc/hosts` file and add the following entry at the end:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑 `/etc/hosts` 文件，并在末尾添加以下条目：
- en: '[PRE70]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Now, let’s `curl` both endpoints and see what we get:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们 `curl` 两个端点，看看会得到什么：
- en: '[PRE71]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: And, as we can see, the name-based routing is working correctly! You can create
    a more dynamic setup by combining multiple hosts and path-based routing.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，基于名称的路由正常工作！你可以通过结合多个主机和基于路径的路由来创建一个更动态的设置。
- en: '`Service`, `Ingress`, `Pod`, `Deployment`, and `ReplicaSet` resources help
    us to maintain a set number of replicas within Kubernetes and help to serve them
    under a single endpoint. As you may have noticed, they are linked together using
    a combination of `labels` and `matchLabels` attributes. The following diagram
    will help you visualize this:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '`Service`、`Ingress`、`Pod`、`Deployment` 和 `ReplicaSet` 资源帮助我们在 Kubernetes 中保持一定数量的副本，并帮助通过一个端点为它们提供服务。正如你可能已经注意到的，它们是通过
    `labels` 和 `matchLabels` 属性的组合来关联的。下图将帮助你更好地理解这一点：'
- en: '![Figure 6.7 – Linking Deployment, Service, and Ingress](img/B19877_Figure_6.07.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 – 连接 Deployment、Service 和 Ingress](img/B19877_Figure_6.07.jpg)'
- en: Figure 6.7 – Linking Deployment, Service, and Ingress
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – 连接 Deployment、Service 和 Ingress
- en: Till now, we have been scaling our pods manually, but a better way would be
    to autoscale the replicas based on resource utilization and traffic. Kubernetes
    provides a resource called `HorizontalPodAutoscaler` to handle that requirement.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在手动缩放 Pods，但更好的方法是根据资源使用情况和流量自动伸缩副本。Kubernetes 提供了一个名为 `HorizontalPodAutoscaler`
    的资源来处理这个需求。
- en: Horizontal Pod autoscaling
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 水平 Pod 自动伸缩
- en: Imagine you’re the manager of a snack bar at a park. On a sunny day, lots of
    people come to enjoy the park, and they all want snacks. Now, you have a few workers
    at your snack bar who make and serve the snacks.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你是一个公园小吃摊的经理。在一个阳光明媚的日子里，很多人来享受公园，他们都想要小吃。现在，你有几个工人在小吃摊制作和提供小吃。
- en: Horizontal Pod autoscaling in Kubernetes is like having magical helpers who
    adjust the number of snack makers (pods) based on how many people want snacks
    (traffic).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中的水平 Pod 自动伸缩就像是拥有一群神奇的助手，他们根据需要多少人想要小吃（流量），来调整小吃制作员（pods）的数量。
- en: 'Here’s how it works:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是它的工作原理：
- en: '**Average days**: You might only need one or two snack makers on regular days
    with fewer people. In Kubernetes terms, you have a few pods running your application.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**普通日子**：在平常日子里，只有少数人来，可能只需要一两个小吃制作员。在 Kubernetes 的术语中，你只需要几个 Pod 来运行你的应用程序。'
- en: '**Busy days**: But when it’s a sunny weekend, and everyone rushes to the park,
    more people want snacks. Your magical helpers (Horizontal Pod autoscaling) notice
    the increase in demand. They say, “*We need more snack makers!*” So, more snack
    makers (pods) are added automatically to handle the rush.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**繁忙的日子**：但是当是一个阳光明媚的周末，大家都涌向公园时，会有更多的人想要小吃。你的神奇助手（水平 Pod 自动伸缩）注意到需求的增加，他们说：“*我们需要更多的小吃制作员！*”因此，更多的小吃制作员（pods）会自动添加，以应对这股人潮。'
- en: '**Scaling down**: Once the sun sets and the crowd leaves, you don’t need as
    many snack makers anymore. Your magical helpers see the decrease in demand and
    say, “*We can have fewer snack makers now.*” So, extra snack makers (pods) are
    removed, saving resources.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缩小规模**：当太阳落山，游客离开时，你不再需要那么多小吃制作员了。你的神奇助手看到需求减少，便说：“*我们现在可以减少小吃制作员数量了。*”因此，多余的小吃制作员（pods）会被移除，从而节省资源。'
- en: '**Automatic adjustment**: These magical helpers monitor the crowd and adjust
    the number of snack makers (pods) in real time. When the demand goes up, they
    deploy more. When it goes down, they remove some.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动调整**：这些神奇的助手会实时监控人流，并调整小吃制作员（pods）的数量。当需求增加时，他们会部署更多；当需求减少时，他们会移除一些。'
- en: In the same way, Kubernetes Horizontal pod autoscaling watches how busy your
    application is. If there’s more traffic (more people wanting your app), it automatically
    adds more pods. If things quieten down, it scales down the number of pods. This
    helps your app handle varied traffic without you manually doing everything.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 以同样的方式，Kubernetes 水平 Pod 自动扩缩会监控你的应用程序有多繁忙。如果流量增加（更多人想要使用你的应用），它会自动添加更多的 Pod。如果流量减少，它会缩减
    Pod 数量。这可以帮助你的应用程序在流量波动时自动调整，而不需要你手动操作。
- en: So, Horizontal pod autoscaling is like having magical assistants that ensure
    your application has the correct number of workers (pods) to handle the crowd
    (traffic) efficiently.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，水平 Pod 自动扩缩（Horizontal pod autoscaling）就像拥有神奇的助手，确保你的应用程序有正确数量的工作者（Pods）来高效地处理流量（traffic）。
- en: '`HorizontalPodAutoscaler` is a Kubernetes resource that helps you to update
    replicas within your `ReplicaSet` resources based on defined factors, the most
    common being CPU and memory.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '`HorizontalPodAutoscaler` 是一个 Kubernetes 资源，它可以帮助你根据定义的因素（最常见的是 CPU 和内存）更新
    `ReplicaSet` 资源中的副本数。'
- en: 'To understand this better, let’s create an `nginx` Deployment, and this time,
    we will set the resource limits within the pod. Resource limits are a vital element
    that enables `HorizontalPodAutoscaler` resources to function. It relies on the
    percentage utilization of the limits to decide when to spin up a new replica.
    We will use the following `nginx-autoscale-deployment.yaml` manifest under `~/modern-devops/ch6/deployments`
    for this exercise:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这一点，让我们创建一个 `nginx` 部署（Deployment），这一次我们将在 Pod 内部设置资源限制。资源限制是使 `HorizontalPodAutoscaler`
    资源能够工作的关键因素。它依赖于限制的百分比利用率来决定何时启动新的副本。我们将使用以下 `nginx-autoscale-deployment.yaml`
    清单文件，路径为 `~/modern-devops/ch6/deployments` 来进行这个练习：
- en: '[PRE72]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Use the following command to perform a new deployment:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令执行新的部署：
- en: '[PRE73]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Let’s expose this deployment with a `LoadBalancer` `Service` resource and get
    the external IP:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `LoadBalancer` 类型的 `Service` 资源来暴露这个部署，并获取外部 IP：
- en: '[PRE74]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Now, let’s autoscale this deployment. The `Deployment` resource needs at least
    1 pod replica and can have a maximum of 5 pod replicas while maintaining an average
    CPU utilization of `25%`. Use the following command to create a `HorizontalPodAutoscaler`
    resource:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们对这个部署进行自动扩缩。`Deployment` 资源需要至少 1 个 Pod 副本，最多可以有 5 个 Pod 副本，同时保持平均 CPU
    利用率为 `25%`。使用以下命令创建一个 `HorizontalPodAutoscaler` 资源：
- en: '[PRE75]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Now that we have the `HorizontalPodAutoscaler` resource created, we can load
    test the application using the `hey` load testing utility preinstalled in Google
    Cloud Shell. But before you fire the load test, open a duplicate shell session
    and watch the `Deployment` resource using the following command:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了 `HorizontalPodAutoscaler` 资源，我们可以使用 Google Cloud Shell 中预安装的 `hey`
    负载测试工具对应用程序进行负载测试。但是，在启动负载测试之前，打开一个复制的 Shell 会话，并使用以下命令查看 `Deployment` 资源：
- en: '[PRE76]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Open another duplicate shell session and watch the `HorizontalPodAutoscaler`
    resource using the following command:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 打开另一个复制的 Shell 会话，并使用以下命令查看 `HorizontalPodAutoscaler` 资源：
- en: '[PRE77]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Now, in the original window, run the following command to fire a load test:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在原始窗口中，运行以下命令以启动负载测试：
- en: '[PRE78]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'It will start a load test for 2 minutes, with 10 concurrent users continuously
    hammering the `Service`. You will see the following output if you open the window
    where you’re watching the `HorizontalPodAutoscaler` resource. As soon as we start
    firing the load test, the average utilization reaches `46%`. The `HorizontalPodAutoscaler`
    resource waits for some time, then it increases the replicas, first to `2`, then
    to `4`, and finally to `5`. When the test is complete, the utilization drops quickly
    to `27%`, `25%`, and finally, `0%`. When the utilization goes to `0%`, the `HorizontalPodAutoscaler`
    resource spins down the replicas from `5` to `1` gradually:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 它将开始一个持续 2 分钟的负载测试，10 个并发用户持续地攻击 `Service`。如果你打开正在监控 `HorizontalPodAutoscaler`
    资源的窗口，你将看到以下输出。当我们开始执行负载测试时，平均利用率达到了 `46%`。`HorizontalPodAutoscaler` 资源等待一段时间后，增加副本数，首先是
    `2`，然后是 `4`，最后是 `5`。当测试完成时，利用率迅速下降至 `27%`，`25%`，最终降至 `0%`。当利用率降至 `0%` 时，`HorizontalPodAutoscaler`
    资源会逐渐减少副本数，从 `5` 减少到 `1`：
- en: '[PRE79]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Likewise, we will see the replicas of the `Deployment` changing when the `HorizontalPodAutoscaler`
    resource actions the changes:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们将看到当 `HorizontalPodAutoscaler` 资源进行操作时，`Deployment` 的副本数发生变化：
- en: '[PRE80]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Besides CPU and memory, you can use other parameters to scale your workloads,
    such as network traffic. You can also use external metrics such as latency and
    other factors that you can use to decide when to scale your traffic.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 CPU 和内存之外，你还可以使用其他参数来扩展你的工作负载，例如网络流量。你还可以使用外部度量指标，比如延迟以及其他因素，来决定何时扩展你的流量。
- en: Tip
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: While you should use the `HorizontalPodAutoscaler` resource with CPU and memory,
    you should also consider scaling on external metrics such as response time and
    network latency. That will ensure better reliability as they directly impact customer
    experience and are crucial to your business.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你应该使用 `HorizontalPodAutoscaler` 资源来处理 CPU 和内存，但你也应考虑基于外部度量（如响应时间和网络延迟）进行扩展。这将确保更好的可靠性，因为这些直接影响客户体验，对你的业务至关重要。
- en: Till now, we have been dealing with stateless workloads. However, pragmatically
    speaking, some applications need to save the state. Let’s look at some considerations
    for managing stateful applications.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在处理无状态的工作负载。然而，从实际的角度来看，一些应用程序需要保存状态。让我们来看一下管理有状态应用程序的一些考虑因素。
- en: Managing stateful applications
  id: totrans-371
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理有状态的应用程序
- en: Imagine you’re a librarian in a magical library. You have a bunch of enchanted
    books that store valuable knowledge. Each book has a unique story and is kept
    in a specific spot on the shelf. These books are like your “stateful applications,”
    and managing them requires extra care.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你是一个魔法图书馆的馆长。你有一堆存储着宝贵知识的魔法书。每本书都有独特的故事，并且被放在书架上的特定位置。这些书就像你的“有状态应用程序”，管理它们需要额外的细心。
- en: Managing stateful applications in the world of technology is like taking care
    of these magical books in your library.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在技术世界中管理有状态应用程序，就像在你的图书馆中照料这些魔法书一样。
- en: 'Here’s how it works:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是它的工作原理：
- en: '**Stateful books**: Some books in your library are “stateful.” This means they
    hold vital information that changes over time, such as bookmarks or notes from
    readers.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有状态的书籍**：你书库中的一些书是“有状态的”。这意味着它们保存着随着时间变化的重要信息，例如书签或读者的笔记。'
- en: '**Fixed locations**: Just as each book has a specific place on the shelf, stateful
    applications must also be in particular locations. They might need to be on certain
    machines or use specific storage to keep their data safe.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**固定位置**：就像每本书在书架上都有一个特定的位置一样，有状态的应用程序也必须位于特定的位置。它们可能需要放在某些机器上，或者使用特定的存储来确保数据的安全。'
- en: '**Maintaining inventory**: You must remember where each book is placed. Similarly,
    managing stateful applications means remembering their exact locations and configurations.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**维护库存**：你必须记住每本书的位置。同样，管理有状态应用程序意味着要记住它们的确切位置和配置。'
- en: '**Careful handling**: When someone borrows a stateful book, you must ensure
    they return it in good condition. With stateful applications, you must handle
    updates and changes carefully to avoid losing important data.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小心处理**：当有人借阅一本有状态的书时，你必须确保他们归还时书籍完好无损。同样，对于有状态的应用程序，你必须小心地处理更新和变更，以避免丢失重要数据。'
- en: '**Backup spells**: Sometimes, you cast a spell to create a copy of a book,
    just in case something happens to the original. With stateful applications, you
    back up your data to restore it if anything goes wrong.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**备份法术**：有时，你施放一个法术来创建一本书的副本，以防万一原本的书出了问题。对于有状态的应用程序，你会备份数据，以便在出现问题时恢复它。'
- en: '**Moving with caution**: If you need to rearrange the library, you move books
    one at a time so that nothing gets lost. Similarly, with stateful applications,
    if you need to move them between machines or storage, it’s done cautiously to
    avoid data loss.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**谨慎移动**：如果你需要重新安排图书馆的布局，你会一次移动一本书，以确保没有书籍丢失。同样，对于有状态的应用程序，如果你需要在机器或存储之间移动它们，必须谨慎操作，以避免数据丢失。'
- en: In the world of technology, managing stateful applications means taking extra
    care of applications that hold important data. You ensure they’re placed in the
    right spots, handle updates carefully, and create backups to keep valuable information
    safe, just like how you protect your enchanted books in the magical library!
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在技术的世界里，管理有状态应用程序意味着要特别小心那些保存重要数据的应用程序。你需要确保它们放置在正确的位置，仔细处理更新，并创建备份以确保宝贵的信息安全，就像你在神奇的图书馆中保护你的魔法书一样！
- en: '`Deployment` resources are beneficial for stateless workloads, as they do not
    need to add any state considerations while updating `ReplicaSet` resources, but
    they cannot work effectively with stateful workloads. To manage such workloads,
    you can use a `StatefulSet` resource.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '`部署`资源适用于无状态工作负载，因为它们在更新`副本集`资源时不需要考虑任何状态问题，但它们无法有效地与有状态工作负载一起工作。要管理此类工作负载，可以使用`有状态副本集`资源。'
- en: StatefulSet resources
  id: totrans-383
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有状态副本集资源
- en: '`StatefulSet` resources help manage stateful applications. They are similar
    to `Deployment` resources, but unlike a `Deployment` resource, they also keep
    track of state and require `Volume` and `Service` resources to operate. `StatefulSet`
    resources maintain a sticky identity for each pod. This means that the volume
    mounted on one pod cannot be used by the other. In a `StatefulSet` resource, Kubernetes
    orders pods by numbering them instead of generating a random hash. Pods within
    a `StatefulSet` resource are also rolled out and scaled-in in order. If a particular
    pod goes down and is recreated, the same volume is mounted to the pod.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '`有状态副本集`资源有助于管理有状态应用程序。它们类似于`部署`资源，但与`部署`资源不同，它们还会跟踪状态，并且需要`卷`和`服务`资源来运行。`有状态副本集`资源为每个
    pod 维护一个粘性标识符。这意味着一个 pod 上挂载的卷不能被另一个 pod 使用。在`有状态副本集`资源中，Kubernetes 通过为 pods 编号而不是生成随机哈希来为
    pods 排序。`有状态副本集`资源中的 pods 也按顺序进行滚动更新和缩容。如果某个 pod 崩溃并被重新创建，则相同的卷会被挂载到该 pod 上。'
- en: 'The following diagram illustrates a `StatefulSet` resource:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个`有状态副本集`资源：
- en: '![Figure 6.8 – StatefulSet resource](img/B19877_Figure_6.08.jpg)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8 – 有状态副本集资源](img/B19877_Figure_6.08.jpg)'
- en: Figure 6.8 – StatefulSet resource
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 – 有状态副本集资源
- en: A `StatefulSet` resource has a stable and unique network identifier, therefore,
    it requires a headless `Service` resource. Headless Services are `Service` resources
    that do not have a cluster IP. Instead, the Kubernetes DNS resolves the `Service`
    resource’s FQDN directly to the pods.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '`有状态副本集`资源具有稳定且唯一的网络标识符，因此，它需要一个无头`服务`资源。无头服务是没有集群 IP 的`服务`资源。相反，Kubernetes
    DNS 会将`服务`资源的 FQDN 直接解析到 pods。'
- en: As a `StatefulSet` resource is supposed to persist data, it requires Persistent
    Volumes to operate. Therefore, let’s look at how to manage volumes using Kubernetes.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`有状态副本集`资源需要持久化数据，因此它需要持久卷才能运行。因此，让我们看看如何使用 Kubernetes 管理卷。
- en: Managing Persistent Volumes
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理持久卷
- en: '**Persistent Volumes** are Kubernetes resources that deal with storage. They
    can help you manage and mount **hard disks**, **SSDs**, **filestores**, and other
    block and network storage entities. You can provision Persistent Volumes manually
    or use dynamic provisioning within Kubernetes. When you use dynamic provisioning,
    Kubernetes will request the cloud provider via the cloud controller manager to
    provide the required storage. Let’s look at both methods to understand how they
    work.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '**持久卷**是 Kubernetes 资源，用于处理存储。它们可以帮助您管理和挂载**硬盘**、**固态硬盘**、**文件存储**以及其他块存储和网络存储实体。您可以手动配置持久卷，也可以在
    Kubernetes 中使用动态配置。当使用动态配置时，Kubernetes 会通过云控制器管理器请求云提供商提供所需的存储。让我们看看两种方法，了解它们如何工作。'
- en: Static provisioning
  id: totrans-392
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 静态配置
- en: '`PersistentVolume` resource using the disk information. The developer can then
    use the `PersistentVolume` resource within their `StatefulSet` resource, as in
    the following diagram:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 使用磁盘信息创建的`持久卷`资源。然后，开发人员可以在他们的`有状态副本集`资源中使用此`持久卷`资源，如下图所示：
- en: '![Figure 6.9 – Static provisioning](img/B19877_Figure_6.09.jpg)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9 – 静态配置](img/B19877_Figure_6.09.jpg)'
- en: Figure 6.9 – Static provisioning
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 – 静态配置
- en: Let’s now look at a static provisioning example.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一个静态配置的示例。
- en: 'To access the resources for this section, `cd` into the following:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本节的资源，`cd` 到以下目录：
- en: '[PRE81]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: So, we first need to create a disk within the cloud platform. Since we’re using
    Google Cloud, let’s proceed and use the `gcloud` commands to do so.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们首先需要在云平台中创建一个磁盘。由于我们使用的是 Google Cloud，我们将继续使用 `gcloud` 命令来完成此操作。
- en: 'Use the following command to create a persistent zonal disk. Ensure that you
    use the same zone as your Kubernetes cluster. As we are using the `us-central1-a`
    zone for the Kubernetes cluster, we will use the same in the following command:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令创建持久化区域磁盘。确保使用与您的 Kubernetes 集群相同的区域。由于我们使用的是`us-central1-a`区域的 Kubernetes
    集群，接下来我们将使用相同的区域：
- en: '[PRE82]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: As the disk is now ready, we can then create a `PersistentVolume` resource from
    it.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 由于磁盘现在已经准备好，我们可以从中创建一个`持久卷`资源。
- en: 'The manifest file, `nginx-manual-pv.yaml`, looks like this:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 清单文件`nginx-manual-pv.yaml`如下所示：
- en: '[PRE83]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'The `spec` section contains `capacity`, `accessModes`, and the kind of disk
    it needs to provision. You can specify one or more access modes to a PersistentVolumes:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '`spec`部分包含`capacity`、`accessModes`以及它需要配置的磁盘类型。你可以为PersistentVolumes指定一个或多个访问模式：'
- en: '`ReadWriteOnce`: Only one pod can read and write to the disk at a time; therefore,
    you cannot mount such a volume to multiple pods'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReadWriteOnce`：每次只有一个Pod可以读取和写入磁盘；因此，你不能将这样的卷挂载到多个Pod。'
- en: '`ReadOnlyMany`: Multiple pods can read from the same volume simultaneously,
    but no pod can write to the disk'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReadOnlyMany`：多个Pod可以同时读取同一个卷，但没有Pod可以写入该磁盘。'
- en: '`ReadWriteMany`: Multiple pods can read and write to the same volume at once'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReadWriteMany`：多个Pod可以同时读取和写入同一个卷。'
- en: Tip
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Not all kinds of storage support all access modes. You need to decide the volume
    type during the initial requirement analysis and architectural assessment phase.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有类型的存储都支持所有的访问模式。你需要在初步需求分析和架构评估阶段决定卷的类型。
- en: 'OK—let’s now go and apply the manifest to provision the `PersistentVolume`
    resource using the following command:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 好的——现在我们来应用清单，通过以下命令来配置`PersistentVolume`资源：
- en: '[PRE84]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Let’s now check whether the Persistent Volume is available by using the following
    command:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下命令检查PersistentVolume是否可用：
- en: '[PRE85]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'As the Persistent Volume is now available, we must create a headless `Service`
    resource to help maintain network identity in the `StatefulSet` resource. The
    following `nginx-manual-service.yaml` manifest describes it:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 由于PersistentVolume现在已经可用，我们必须创建一个无头的`Service`资源，帮助在`StatefulSet`资源中保持网络身份。以下是描述该资源的`nginx-manual-service.yaml`清单：
- en: '[PRE86]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: It is very similar to the regular `Service` resource, except that we have specified
    `clusterIP` as `None`.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 它与常规的`Service`资源非常相似，只是我们将`clusterIP`设置为`None`。
- en: 'Now, let’s go and apply the manifest using the following command:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下命令来应用清单：
- en: '[PRE87]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'As the `Service` resource is created, we can create a `StatefulSet` resource
    that uses the created `PersistentVolume` and `Service` resources. The `StatefulSet`
    resource manifest, `nginx-manual-statefulset.yaml`, looks like this:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 随着`Service`资源的创建，我们可以创建一个使用已创建的`PersistentVolume`和`Service`资源的`StatefulSet`资源。`StatefulSet`资源清单`nginx-manual-statefulset.yaml`如下所示：
- en: '[PRE88]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: The manifest contains various sections. While most are similar to the `Deployment`
    resource manifest, this requires a `volume` definition and a separate `volumeClaimTemplates`
    section. The `volumeClaimTemplates` section consists of the `accessModes`, `resources`,
    and `selector` sections. The `selector` section defines the `matchLabels` attribute,
    which helps to select a particular `PersistentVolume` resource. In this case,
    it selects the `PersistentVolume` resource we defined previously. It also contains
    the `serviceName` attribute that defines the headless `Service` resource it will
    use.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 清单包含了多个部分。虽然大多数部分与`Deployment`资源清单类似，但它需要一个`volume`定义和一个单独的`volumeClaimTemplates`部分。`volumeClaimTemplates`部分包括`accessModes`、`resources`和`selector`部分。`selector`部分定义了`matchLabels`属性，用于选择特定的`PersistentVolume`资源。在这种情况下，它选择了我们之前定义的`PersistentVolume`资源。它还包含`serviceName`属性，定义了它将使用的无头`Service`资源。
- en: 'Now, let’s go ahead and apply the manifest using the following command:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续使用以下命令应用清单：
- en: '[PRE89]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Now, let’s inspect a few elements to see where we are. The `StatefulSet` resource
    creates a `PersistentVolumeClaim` resource to claim the `PersistentVolume` resource
    we created before.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检查一些元素，看看我们目前的进展。`StatefulSet`资源创建了一个`PersistentVolumeClaim`资源，用来声明我们之前创建的`PersistentVolume`资源。
- en: 'Get the `PersistentVolumeClaim` resource using the following command:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令获取`PersistentVolumeClaim`资源：
- en: '[PRE90]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: As we can see, the `StatefulSet` resource has created a `PersistentVolumeClaim`
    resource called `html-nginx-manual-0` that is bound to the `nginx-manual-pv` `PersistentVolume`
    resource. Therefore, manual provisioning has worked correctly.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，`StatefulSet`资源已经创建了一个名为`html-nginx-manual-0`的`PersistentVolumeClaim`资源，该资源绑定到了`nginx-manual-pv`的`PersistentVolume`资源上。因此，手动配置已正确工作。
- en: 'If we query the `PersistentVolume` resource using the following command, we
    will see that the status is now showing as `Bound`:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用以下命令查询`PersistentVolume`资源，我们将看到其状态现在显示为`Bound`：
- en: '[PRE91]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Now, let’s have a look at the pods using the following command:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下命令查看Pod的状态：
- en: '[PRE92]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: As we see, the `StatefulSet` resource has created a pod and appended it with
    a serial number instead of a random hash. It wants to maintain ordering between
    the pods and mount the same volumes to the pods they previously mounted.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，`StatefulSet`资源已经创建了一个Pod，并附加了一个序列号，而不是随机哈希值。它希望在Pod之间保持顺序，并将之前挂载的相同卷挂载到这些Pod上。
- en: 'Now, let’s open a shell into the pod and create a file within the `/usr/share/nginx/html`
    directory using the following commands:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打开Pod的Shell，并使用以下命令在`/usr/share/nginx/html`目录中创建一个文件：
- en: '[PRE93]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Great! So, let’s go ahead and delete the pod and see whether we can get the
    file in the same location again using the following commands:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！那么，让我们继续删除Pod，并使用以下命令查看是否能在相同位置再次找到文件：
- en: '[PRE94]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: And, as we can see, the file still exists, even after we deleted the pod.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，即使我们删除了Pod，文件仍然存在。
- en: Static provisioning isn’t one of the best ways of doing things, as you must
    manually keep track and provision volumes. That involves a lot of manual activities
    and may be error-prone. Some organizations that want to keep a line between Dev
    and Ops can use this technique. Kubernetes allows this provision. However, for
    more DevOps-friendly organizations, **dynamic provisioning** is a better way of
    doing it.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 静态配置并不是最好的做法，因为你必须手动跟踪和配置卷。这涉及很多手动操作，可能容易出错。一些希望在开发和运维之间保持分隔的组织可能会使用这种技术。Kubernetes
    允许这种配置。然而，对于更适合DevOps的组织来说，**动态配置**是更好的方法。
- en: Dynamic provisioning
  id: totrans-440
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动态配置
- en: Dynamic provisioning is when Kubernetes provides storage resources for you by
    interacting with the cloud provider. When we provisioned the disk manually, we
    interacted with the cloud APIs using the `gcloud` command line. What if your organization
    decides to move to some other cloud provider later? That would break many existing
    scripts, and you would have to rewrite the storage provisioning steps. Kubernetes
    is inherently portable and platform-independent. You can provision resources in
    the same way on any cloud platform.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 动态配置是指Kubernetes通过与云服务提供商交互为你提供存储资源。当我们手动配置磁盘时，我们通过`gcloud`命令行与云API进行交互。如果你们的组织后来决定迁移到其他云服务提供商，这将破坏许多现有的脚本，你将不得不重新编写存储配置步骤。Kubernetes
    本质上是可移植且平台无关的。你可以在任何云平台上以相同的方式配置资源。
- en: But then, different cloud providers have different storage offerings. How would
    Kubernetes know what kind of storage it needs to provision? Well, Kubernetes uses
    `StorageClass` resources for that. `StorageClass` resources are Kubernetes resources
    that define the type of storage they need to provide when someone uses it.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，不同的云服务提供商有不同的存储方案。那么，Kubernetes 如何知道它需要配置什么样的存储呢？其实，Kubernetes 使用`StorageClass`资源来解决这个问题。`StorageClass`资源是Kubernetes资源，定义了在有人使用它时需要提供的存储类型。
- en: 'The following diagram illustrates dynamic provisioning:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了动态配置的示意：
- en: '![Figure 6.10 – Dynamic provisioning](img/B19877_Figure_6.010.jpg)'
  id: totrans-444
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.10 – 动态配置](img/B19877_Figure_6.010.jpg)'
- en: Figure 6.10 – Dynamic provisioning
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 – 动态配置
- en: 'Let’s see an example storage class manifest, `fast-storage-class.yaml`, that
    provisions an SSD within GCP:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个存储类清单示例，`fast-storage-class.yaml`，它在 GCP 中配置了一个 SSD：
- en: '[PRE95]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: The `StorageClass` resource contains a provisioner and any parameters the provisioner
    requires. You may have noticed that I have kept the name `fast` instead of `gce-ssd`
    or similar. That is because we want to keep the names as generic as possible.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '`StorageClass`资源包含了一个供应者和该供应者所需的任何参数。你可能已经注意到，我使用了`fast`这个名称，而不是`gce-ssd`或类似的名称。原因是我们希望保持名称尽可能通用。'
- en: Tip
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Keep generic storage class names such as `fast`, `standard`, `block`, and `shared`,
    and avoid names specific to the cloud platform. Because storage class names are
    used in Persistent Volume claims, if you migrate to another cloud provider, you
    may end up changing a lot of manifests just to avoid confusion.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 保持通用的存储类名称，如`fast`、`standard`、`block`和`shared`，避免使用特定于云平台的名称。因为存储类名称会在持久化卷声明中使用，如果你迁移到另一个云服务提供商，你可能会需要修改大量清单来避免混淆。
- en: 'Let’s go ahead and apply the manifest using the following command:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续使用以下命令应用清单：
- en: '[PRE96]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: As the `StorageClass` resource is created, let’s use it to provision an `nginx`
    `StatefulSet` resource dynamically.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 当`StorageClass`资源创建完毕后，让我们使用它来动态配置一个`nginx`的`StatefulSet`资源。
- en: 'We need to create a `Service` resource manifest, `nginx-dynamic-service.yaml`,
    first:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要先创建一个`Service`资源清单，`nginx-dynamic-service.yaml`：
- en: '[PRE97]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'The manifest is very similar to the manual `Service` resource. Let’s go ahead
    and apply it using the following command:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 该清单与手动 `Service` 资源非常相似。让我们继续使用以下命令来应用它：
- en: '[PRE98]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Now, let’s look at the `StatefulSet` resource manifest, `nginx-dynamic-statefulset.yaml`:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们查看 `StatefulSet` 资源清单，`nginx-dynamic-statefulset.yaml`：
- en: '[PRE99]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'The manifest is similar to the manual one, but this one contains the `storageClassName`
    attribute in the `volumeClaimTemplates` section and lacks the `selector` section,
    as we are dynamically provisioning the storage. Use the following command to apply
    the manifest:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 该清单与手动清单类似，但在 `volumeClaimTemplates` 部分包含 `storageClassName` 属性，并且缺少 `selector`
    部分，因为我们正在动态配置存储。使用以下命令应用清单：
- en: '[PRE100]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'As the `StatefulSet` resource is created, let’s go ahead and check the `PersistentVolumeClaim`
    and `PersistentVolume` resources using the following commands:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建 `StatefulSet` 资源时，让我们继续使用以下命令检查 `PersistentVolumeClaim` 和 `PersistentVolume`
    资源：
- en: '[PRE101]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: And we can see that the claim is bound to a Persistent Volume that is dynamically
    provisioned. Now, let’s proceed and run the following command to do similar tests
    with this `StatefulSet` resource.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到声明已绑定到一个动态配置的持久卷。现在，让我们继续运行以下命令，以类似的方式测试这个 `StatefulSet` 资源。
- en: 'Let’s create a file in the `nginx-dynamic-0` pod using the following command:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下命令在 `nginx-dynamic-0` pod 中创建一个文件：
- en: '[PRE102]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Now, delete the pod and open a shell session again to check whether the file
    exists by using the following commands:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，删除 pod，并再次打开一个 shell 会话，通过以下命令检查文件是否存在：
- en: '[PRE103]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: And as we can see, the file exists in the volume, even if the pod was deleted.
    That is dynamic provisioning in action for you!
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，即使删除了 pod，文件也存在于卷中。这就是动态配置生效的方式！
- en: You will have observed that we have used the `kubectl` command multiple times
    throughout this chapter. When you perform activities throughout the day, using
    shortcuts and best practices wherever you can makes sense. Let’s look at some
    best practices while using `kubectl`.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，本章节中我们多次使用了 `kubectl` 命令。在您进行一天中的各种活动时，尽可能使用快捷方式和最佳实践是有意义的。让我们看看在使用
    `kubectl` 时的一些最佳实践。
- en: Kubernetes command-line best practices, tips, and tricks
  id: totrans-471
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 命令行最佳实践，技巧和窍门
- en: For seasoned Kubernetes developers and administrators, `kubectl` is a command
    they run most of the time. The following steps will simplify your life, save you
    a ton of time, let you focus on more essential activities, and set you apart from
    the rest.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 对于经验丰富的 Kubernetes 开发人员和管理员，`kubectl` 是他们大部分时间运行的命令。以下步骤将简化您的生活，节省大量时间，让您专注于更重要的活动，并使您脱颖而出。
- en: Using aliases
  id: totrans-473
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用别名
- en: Most system administrators use aliases for an excellent reason—they save valuable
    time. Aliases in Linux are different names for commands, and they are mostly used
    to shorten the most frequently used commands; for example, `ls -l` becomes `ll`.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数系统管理员出于一个很好的理由使用别名——它们节省宝贵的时间。Linux 中的别名是命令的不同名称，它们主要用于缩短最常用的命令；例如，`ls -l`
    变成 `ll`。
- en: You can use the following aliases with `kubectl` to make your life easier.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下别名与 `kubectl` 一起使用，使生活更轻松。
- en: k for kubectl
  id: totrans-476
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k 代表 kubectl
- en: 'Yes—that’s right. By using the following alias, you can use `k` instead of
    typing `kubectl`:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 是的——没错。通过以下别名，您可以使用 `k` 而不是输入 `kubectl`：
- en: '[PRE104]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: That will save a lot of time and hassle.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 这将节省大量时间和麻烦。
- en: Using kubectl --dry-run
  id: totrans-480
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 kubectl --dry-run
- en: '`kubectl --dry-run` helps you to generate YAML manifests from imperative commands
    and saves you a lot of typing time. You can write an imperative command to generate
    a resource and append that with a `--dry-run=client -o yaml` string to generate
    a YAML manifest from the imperative command. The command does not create the resource
    within the cluster, but instead just outputs the manifest. The following command
    will generate a `Pod` manifest using `--dry-run`:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl --dry-run` 可以帮助您从命令生成 YAML 清单，并节省大量输入时间。您可以编写一个命令来生成资源，并附加 `--dry-run=client
    -o yaml` 字符串以从该命令生成 YAML 清单。该命令不会在集群中创建资源，而是仅输出清单。以下命令将使用 `--dry-run` 生成 `Pod`
    清单：'
- en: '[PRE105]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: And you now have the skeleton YAML file that you can edit according to your
    liking.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您有一个可以根据自己喜好编辑的骨架 YAML 文件。
- en: Now, imagine typing this command multiple times during the day! At some point,
    it becomes tiring. Why not shorten it by using the following alias?
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一天中多次输入此命令！在某些时候，这变得令人疲倦。为什么不使用以下别名缩短它呢？
- en: '[PRE106]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: You can then use the alias to generate other manifests.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 您随后可以使用别名生成其他清单。
- en: 'To generate a `Deployment` resource manifest, use the following command:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成一个`Deployment`资源清单，使用以下命令：
- en: '[PRE107]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: You can use the dry run to generate almost all resources from imperative commands.
    However, some resources do not have an imperative command, such as a `DaemonSet`
    resource. You can generate a manifest for the closest resource and modify it for
    such resources. A `DaemonSet` manifest is very similar to a `Deployment` manifest,
    so you can generate a `Deployment` manifest and change it to match the `DameonSet`
    manifest.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用dry run来从命令式命令生成几乎所有资源。然而，有些资源没有命令式命令，例如`DaemonSet`资源。你可以为最接近的资源生成清单并对其进行修改，以适应此类资源。`DaemonSet`清单与`Deployment`清单非常相似，因此你可以生成一个`Deployment`清单并将其修改为适应`DaemonSet`清单。
- en: Now, let’s look at some of the most frequently used `kubectl` commands and their
    possible aliases.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看一些最常用的`kubectl`命令及其可能的别名。
- en: kubectl apply and delete aliases
  id: totrans-491
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`kubectl apply`和`delete`的别名'
- en: 'If you use manifests, you will use the `kubectl apply` and `kubectl delete`
    commands most of the time within your cluster, so it makes sense to use the following
    aliases:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用清单，你通常会在集群中使用`kubectl apply`和`kubectl delete`命令，因此使用以下别名是很有意义的：
- en: '[PRE108]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'You can then use them to apply or delete resources using the following commands:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以使用以下命令来应用或删除资源：
- en: '[PRE109]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: While troubleshooting containers, most of us use `busybox`. Let’s see how to
    optimize it.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在排查容器问题时，我们大多数人使用`busybox`。让我们来看看如何优化它。
- en: Troubleshooting containers with busybox using an alias
  id: totrans-497
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用别名通过busybox排查容器问题
- en: 'We use the following commands to open a `busybox` session:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下命令来打开`busybox`会话：
- en: '[PRE110]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: Now, opening several `busybox` sessions during the day can be tiring. How about
    minimizing the overhead by using the following alias?
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，白天打开多个`busybox`会话可能会让人感到疲劳。怎样通过使用以下别名来最小化开销呢？
- en: '[PRE111]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'We can then open a shell session to a new `busybox` pod using the following
    command:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用以下命令打开一个新的`busybox` pod的shell会话：
- en: '[PRE112]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'Now, that is much cleaner and easier. Likewise, you can also create aliases
    of other commands that you use frequently. Here’s an example:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这样更干净、更简单。同样，你还可以为其他经常使用的命令创建别名。以下是一个示例：
- en: '[PRE113]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: And so on, according to your needs. You may also be used to autocompletion within
    `bash`, where your commands autocomplete when you press *Tab* after typing a few
    words. `kubectl` also provides autocompletion of commands, but not by default.
    Let’s now look at how to enable `kubectl` autocompletion within `bash`.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 依此类推，根据你的需求。你也许已经习惯了在`bash`中使用自动补全功能，当你输入几个单词后按*Tab*键，命令会自动补全。`kubectl`也提供了命令的自动补全，但默认情况下并未启用。现在，让我们来看一下如何在`bash`中启用`kubectl`的自动补全功能。
- en: Using kubectl bash autocompletion
  id: totrans-507
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`kubectl`的bash自动补全
- en: 'To enable `kubectl` `bash` autocompletion, use the following command:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用`kubectl`的`bash`自动补全功能，使用以下命令：
- en: '[PRE114]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: The command adds the `kubectl` completion `bash` command as a source to your
    `.bashrc` file. So, the next time you log in to your shell, you should be able
    to use `kubectl` autocomplete. That will save you a ton of time when typing commands.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将`kubectl`的completion `bash`命令作为源添加到你的`.bashrc`文件中。因此，下次你登录到shell时，应该就能使用`kubectl`自动补全功能了。这样在输入命令时可以节省大量时间。
- en: Summary
  id: totrans-511
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We began this chapter by managing pods with `Deployment` and `ReplicaSet` resources
    and discussed some critical Kubernetes deployment strategies. We then looked into
    Kubernetes service discovery and models and understood why we required a separate
    entity to expose containers to the internal or external world. We then looked
    at different `Service` resources and where to use them. We talked about `Ingress`
    resources and how to use them to create reverse proxies for our container workloads.
    We then delved into Horizontal Pod autoscaling and used multiple metrics to scale
    our pods automatically.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始时，我们通过`Deployment`和`ReplicaSet`资源来管理pods，并讨论了一些关键的Kubernetes部署策略。接着，我们研究了Kubernetes服务发现和模型，并理解了为什么需要一个单独的实体来将容器暴露给内部或外部世界。然后，我们了解了不同的`Service`资源及其使用场景。我们还讨论了`Ingress`资源，并介绍了如何使用它们为容器工作负载创建反向代理。接着，我们深入研究了Horizontal
    Pod自动扩展，并使用多个指标自动扩展pods。
- en: We looked at state considerations and learned about static and dynamic storage
    provisioning using `PersistentVolume`, `PersistentVolumeClaim`, and `StorageClass`
    resources, and talked about some best practices surrounding them. We looked at
    `StatefulSet` resources as essential resources that help you schedule and manage
    stateful containers. Finally, we looked at some best practices, tips, and tricks
    surrounding the `kubectl` command line and how to use them effectively.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了状态考虑因素，并学习了如何使用 `PersistentVolume`、`PersistentVolumeClaim` 和 `StorageClass`
    资源进行静态和动态存储配置，并讨论了一些围绕它们的最佳实践。我们还研究了 `StatefulSet` 资源，这些是帮助你调度和管理有状态容器的关键资源。最后，我们还探讨了有关
    `kubectl` 命令行的一些最佳实践、技巧和窍门，以及如何有效地使用它们。
- en: The topics covered in this and the previous chapter are just the core of Kubernetes.
    Kubernetes is a vast tool with enough functionality to write an entire book, so
    these chapters only give you the gist of what it is all about. Please feel free
    to read about the resources in detail in the Kubernetes official documentation
    at [https://kubernetes.io](https://kubernetes.io).
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 本章和上一章所涵盖的内容只是 Kubernetes 的核心。Kubernetes 是一个功能庞大的工具，足够写一本完整的书籍，因此这些章节只是给你概述了它的基本内容。请随时查阅
    Kubernetes 官方文档的详细资源：[https://kubernetes.io](https://kubernetes.io)。
- en: In the next chapter, we will delve into the world of the cloud and look at **Container-as-a-Service**
    (**CaaS**) and serverless offerings for containers.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨云计算世界，了解 **容器即服务** (**CaaS**) 和无服务器容器服务。
- en: Questions
  id: totrans-516
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: A Kubernetes Deployment deletes an old `ReplicaSet` resource when the image
    is updated. (True/False)
  id: totrans-517
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes 部署在镜像更新时会删除旧的 `ReplicaSet` 资源。（正确/错误）
- en: What are the primary deployment strategies supported by Kubernetes? (Choose
    two)
  id: totrans-518
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes 支持的主要部署策略有哪些？（选择两个）
- en: A. Recreate
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. 重建
- en: B. Rolling update
  id: totrans-520
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. 滚动更新
- en: C. Ramped slow rollout
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C. 渐进式慢速发布
- en: D. Best-effort controlled rollout
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D. 最佳努力控制的滚动发布
- en: Which types of resources can you use to expose containers externally? (Choose
    three)
  id: totrans-523
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用哪些类型的资源来将容器暴露到外部？（选择三个）
- en: A. `ClusterIP Service`
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. `ClusterIP 服务`
- en: B. `NodePort Service`
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. `NodePort 服务`
- en: C. `LoadBalancer Service`
  id: totrans-526
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C. `LoadBalancer 服务`
- en: D. `Ingress`
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D. `Ingress`
- en: It is a best practice to start with a `ClusterIP` Service and change the Service
    type later if needed. (True/False)
  id: totrans-528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最好的实践是先从 `ClusterIP` 服务开始，必要时再更改服务类型。（正确/错误）
- en: '`Deployment` resources are suitable for stateful workloads. (True/False)'
  id: totrans-529
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Deployment` 资源适用于有状态工作负载。（正确/错误）'
- en: Which kinds of workloads can you run with Ingresses?
  id: totrans-530
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Ingress 时，你可以运行哪些类型的工作负载？
- en: A. HTTP
  id: totrans-531
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. HTTP
- en: B. TCP
  id: totrans-532
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. TCP
- en: C. FTP
  id: totrans-533
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C. FTP
- en: D. SMTP
  id: totrans-534
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D. SMTP
- en: Which resources would you define for dynamic volume provisioning? (Choose two)
  id: totrans-535
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会为动态卷配置定义哪些资源？（选择两个）
- en: A. `StorageClass`
  id: totrans-536
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. `StorageClass`
- en: B. `PersistentVolumeClaim`
  id: totrans-537
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. `PersistentVolumeClaim`
- en: C. `PersistentVolume`
  id: totrans-538
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C. `PersistentVolume`
- en: D. `StatefulSet`
  id: totrans-539
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D. `StatefulSet`
- en: To make your horizontal scaling more meaningful, what parameters should you
    use to scale your pods? (Choose three)
  id: totrans-540
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使你的水平扩展更具意义，你应该使用哪些参数来扩展你的 Pod？（选择三个）
- en: A. CPU
  id: totrans-541
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. CPU
- en: B. Memory
  id: totrans-542
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. 内存
- en: C. External metrics, such as response time
  id: totrans-543
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C. 外部指标，如响应时间
- en: D. Packets per second (PPS)
  id: totrans-544
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D. 每秒数据包（PPS）
- en: What are the forms of routing within an `Ingress` resource? (Choose two)
  id: totrans-545
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `Ingress` 资源中，路由形式有哪些？（选择两个）
- en: A. Simple
  id: totrans-546
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. 简单
- en: B. Path-based
  id: totrans-547
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. 基于路径的
- en: C. Name-based
  id: totrans-548
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C. 基于名称的
- en: D. Complex
  id: totrans-549
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D. 复杂
- en: Answers
  id: totrans-550
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 答案
- en: False. An image Deployment just scales the old `ReplicaSet` resource to `0`.
  id: totrans-551
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误。一个镜像部署只是将旧的 `ReplicaSet` 资源缩放到 `0`。
- en: A and B
  id: totrans-552
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A 和 B
- en: B, C, and D
  id: totrans-553
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B、C 和 D
- en: 'True'
  id: totrans-554
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确
- en: False. Use `StatefulSet` resources instead.
  id: totrans-555
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误。请改用 `StatefulSet` 资源。
- en: A
  id: totrans-556
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: A and B
  id: totrans-557
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A 和 B
- en: A, B, and C
  id: totrans-558
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A、B 和 C
- en: B and C
  id: totrans-559
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B 和 C
