- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Expressing Your Business Goals in Application Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **business logic** that makes our company technology unique and provides
    a competitive advantage is usually the business logic that we employ. Expressing
    our business rules as **application code** can drive forward automation, reduce
    cycle times, and increase productive output. However, when we move that logic
    to the cloud, we can be trapped by some **anti-patterns** that we would normally
    get away with in the old monolithic, on-premises architectures we are evolving.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Lift and shift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stateful applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tight coupling, low cohesion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The comprehensive definition of done
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other pitfalls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lift and shift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we move applications to the cloud, we need to shift our thinking from deploying
    an application as an independent unit to the application being the emergent behavior
    of the interaction of various services. In this section, we will explore the typical
    process of shifting an application to the cloud, the strategies we can use, and
    how to increase the maturity of our cloud native solution.
  prefs: []
  type: TYPE_NORMAL
- en: Building in the cloud versus building for the cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we migrate applications to the cloud, the simplest method is to package
    up the existing deployment in a VM, deploy it to the cloud, and call it cloud
    native. This thinking limits the actual usage of the cloud to simply reflect the
    existing topologies we had in our on-premises environment. But what have we achieved?
    We still have the same limitations of the system we just migrated from but without
    any of the advantages of the cloud. We’ve just moved our code from our server
    to someone else’s server. We may gain some efficiencies in maintainability, organizational
    complexity, and onboarding time. However, this is not unique to cloud hyperscalers,
    and we could achieve the same results with most other VM hosts. This **lift-and-shift**
    mindset gets us into the cloud but falls short of fully utilizing it. This mindset
    is the difference between building *in* the cloud versus building *for* the cloud.
    Once the application is in the cloud via this lift-and-shift methodology, we can
    make improvements and optimizations not only to the application itself but also
    to its surrounding infrastructure and architecture.
  prefs: []
  type: TYPE_NORMAL
- en: I previously worked for a company that had an existing on-premises solution.
    This on-premises solution was distributed to customers via remote deployment.
    The client provided a machine, and a specific onboarding team logged in to that
    machine and ran a playbook to set up the application. This lift-and-shift mindset
    persisted into the *cloud native* hosted offerings they provided. The onboarding
    team provisioned a new instance and database in the cloud, and then somebody installed
    the application, and the client accessed the cloud instance. This process was
    the company’s first iteration of providing services in the cloud. However, the
    manual processes have been persistent and difficult to shake. These processes
    are a classic example of building in the cloud versus building for the cloud.
    It can be challenging to relinquish control of these business procedures to automation.
    However, unless we utilize the advantages that the cloud provides, we fail to
    recognize the actual efficiencies of this shift. A good approach that would have
    allowed for much faster cycle times and reduced new customer entry barriers is
    shifting to self-serve, on-demand onboarding, using a cloud factory approach,
    as we will see in more detail later in the chapter. Similar techniques were adopted
    in their future cloud native applications, built from the ground up. However,
    this brings us to a new anti-pattern, having the attitude that “we’ll build it
    right this time.”
  prefs: []
  type: TYPE_NORMAL
- en: The myth of “We’ll build it right this time”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the anti-patterns we often see is software teams wanting to burn everything
    to the ground and start again to become cloud native. This all-or-nothing approach
    not only fragments your business into **legacy** (your original application) and
    **greenfield** (your brand-new application) development but also means that you
    neglect a product that customers are using to work on a product that will likely
    not have users until at least parity with your on-premises solution. The timelines
    of these projects are often wildly underestimated and require reskilling and re-resourcing
    to get the cloud native skills you need. The all-or-nothing approach frequently
    means that critical decisions around your application and its architecture are
    made upfront at the point in time when your organization likely has the least
    cloud experience on hand!
  prefs: []
  type: TYPE_NORMAL
- en: When shifting to the cloud, AWS has the 7Rs of migration strategies to use,
    which we went through in [*Chapter 2*](B22364_02.xhtml#_idTextAnchor055). To refresh
    your memory, these are refactor, replatform, repurchase, rehost, relocate, retain,
    and retire.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll notice that *rebuild* is not one of the options. To take full advantage
    of cloud native services in an existing application, we must choose an option
    that will eventually lead us down the refactor path. The easiest way to start
    is to build a cloud factory for our existing application.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud factories
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The lift-and-shift part of a cloud migration is unavoidable. Running the existing
    application in the cloud is the first step to migrating it to become cloud native.
    When deploying an on-premises application, there is a significant lead time, as
    it involves hardware provided by the customer, with customer-controlled access
    and rollouts with manual steps. As discussed in our earlier example, a common
    anti-pattern in this space reflects that process in the cloud environment. Customers
    use different **firewalls**, **hypervisors**, **hardware**, and **security** in
    an on-premises environment. The rollout process typically requires manual intervention
    to deal with the idiosyncrasies of the particular client.
  prefs: []
  type: TYPE_NORMAL
- en: 'When deploying in a cloud environment, we get to specify these options ourselves.
    We say how big our VM is, how we configure our firewall and networking, or what
    operating system we use. Instead of multiple unique customer environments, we’re
    deploying the same cloud environment multiple times, meaning all the quirks are
    identical for each implementation case. We can now automate the provisioning workflow
    with certainty, reducing onboarding from a process that might take weeks with
    multiple client contacts to a process that can run in a pipeline and might take
    30 minutes. Creating a cloud factory for your application is a crucial first step
    for migrating on-premises applications to the cloud without rearchitecting to
    a multitenant model. We will delve deeper into this in [*Chapter 12*](B22364_12.xhtml#_idTextAnchor320).
    As we start to transition our application to the cloud, the question still remains:
    how will we refactor this while retaining the end functionality? The answer is
    through the use of the strangler fig pattern.'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud native through the strangler fig pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A strangler fig is a plant that grows on a host tree. Sometimes, the host tree
    dies, leaving only the strangler fig. The **strangler fig pattern**, coined by
    Martin Fowler, is similar. It lets us take our existing applications and make
    them cloud native by slow degrees, eventually replacing our legacy solution altogether.
    Through this mechanism of action, we also allow for the deferral of system-wide
    architectural decisions until later in the process, once the cloud maturity of
    our organization has improved. The first stage of cloud migration is to take our
    existing application to the cloud – that is, rehost. You can also technically
    take this approach without the rehost phase and instead redirect traffic to our
    on-premises instance, although this requires additional networking and solid authorization
    strategies to be in place. This simple transition is depicted in *Figure 7**.1*.
    We start with an on-premises instance and replace it with a cloud instance. The
    switch is transparent to the end user.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Initial migration of an application from on-premises to the
    cloud](img/B22364_07_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Initial migration of an application from on-premises to the cloud
  prefs: []
  type: TYPE_NORMAL
- en: By completing this stage, we have already achieved some efficiencies; provisioning
    is faster by removing the dependency on physical hardware and utilizing cloud
    factories, colocation costs have disappeared, and we mitigate the operational
    overhead of disparate systems. However, we’re not cloud native; the overheads
    around OS patching and database maintenance still exist, and we’re still operating
    in a manner that matches our infrastructure topology to our customer base.
  prefs: []
  type: TYPE_NORMAL
- en: The next stage of our migration is a simple but critical phase that supports
    the future iterations of our application. We need to add an **API proxy layer**.
    All hyperscalers have a managed service that performs this function; in AWS, it
    is **API Gateway**, Azure has **API Management**, and GCP has **Apigee** and **API
    Gateway**. Some open source projects provide similar functionality for specific
    environments, such as **Kubernetes**. The key here is that we are introducing
    a layer between our end user and our application that can perform **Layer 7 routing**
    as defined in the OSI model. This model will allow us to inspect incoming traffic
    and decide actions based on HTTP request properties. In contrast to the architecture
    in *Figure 7**.1*, we now have an additional architectural element, the API proxy,
    which is once again transparent to the end user.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Addition of an API proxy to the cloud instance](img/B22364_07_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Addition of an API proxy to the cloud instance
  prefs: []
  type: TYPE_NORMAL
- en: Functionally, we have yet to start using the API layer’s capabilities to their
    full extent, but we have achieved some operational efficiencies as part of this
    change. If we were using **Transport Layer Security** (**TLS**), we would likely
    have a provisioned TLS certificate. Switching to a fully managed proxy allows
    the TLS termination to occur at the proxy layer, freeing us from the operational
    overhead of managing this with a manual or semi-automated process. The key is
    that our application is no longer bound to our deployed instance. Typically, we
    build on-premises applications using a monolithic architecture, as the deployment
    of these applications is tightly coupled to the topology of the hardware we deploy
    them on. In the cloud, these limitations no longer constrain us. It is detrimental
    to the ability of development teams to operate in this environment. Using the
    monolith architecture usually results in high internal coupling between components,
    making it difficult to predict the blast radius of a particular change without
    knowing the full scope of its use throughout the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution is to use the Layer 7 routing capabilities of the API proxy to
    decompose our application into new cloud native implementations. For example,
    many applications have a user management system so users can log in to the application.
    Traditionally, someone might achieve this by storing passwords in a database,
    ideally, hashed and salted. This approach is a definite source of risk for most
    companies. **Insecure hash algorithms**, **timing leaks**, and **password database
    security** are all things your company is directly responsible for under this
    model. By migrating this to a managed service, we significantly de-risk our operations.
    We can also make changes at this stage to make our solution more cloud native
    through replatforming some of the easier migrations, such as databases, to a compatible
    managed database service. Continuing our architectural evolution, we break down
    the monolithic application into components in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22364_07_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Beginning to decompose the monolith into domain-driven microservices
  prefs: []
  type: TYPE_NORMAL
- en: Under this new architecture, we have separated concerns for our user management
    and our newly replatformed application monolith. Our user service provides an
    abstraction behind our API proxy for performing actions such as resetting passwords,
    updating email addresses, and other user-centric functions. At the same time,
    our original monolithic application still contains all the functionality external
    to users. We’ve managed to refactor one part of our application to be truly cloud
    native and use managed services. Most importantly, we don’t need to know how the
    entire application will be architected to achieve this. We need to understand
    this particular domain and the services available to accelerate it. We’ve also
    broken any coupling that may have existed between the user service and unrelated
    parts of the application. Under this new model, changes to the user service have
    a blast radius limited to the service itself without unforeseen side effects on
    the rest of the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'In some simple cases, we may only have two targets for the API proxy: the new
    cloud native service and the old legacy service. However, as you perform this
    method of replacing or migrating functionality, it is also worth reevaluating
    your architecture and seeing whether you can reduce coupling within your application
    or increase cohesion within a specific domain by breaking out disparate services.
    Rarely, the perfect solution to a problem requiring refactoring to become cloud
    native is to build a cloud native monolith.'
  prefs: []
  type: TYPE_NORMAL
- en: Slowly, we can continue to break down the service into its emergent domains.
    We establish bounded contexts within our application, representing highly cohesive
    parts of our business context. For more information on bounded contexts and domain-driven
    design, I recommend reading *Domain Driven Design* by Eric Evans. We then decompose
    our architecture into these domains and look to utilize cloud native services
    wherever possible. As a part of this shift, if our application supports multiple
    customers, we can also build multitenancy into these services. Eventually, we
    will reach a point where we have integrated the entire application into a series
    of cloud native services backed by managed services that provide equivalent or
    improved functionality. As the final step in our architectural evolution, we have
    removed the monolith and left only the new application services. This is reflected
    in *Figure 7**.4*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22364_07_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – The original monolith is deprecated and is truly cloud native
  prefs: []
  type: TYPE_NORMAL
- en: 'By using the API proxy to slowly and methodically decompose the monolith, we
    have effectively accomplished the desired result: removing the legacy monolith
    and adopting cloud native services. At this point, it is possible to remove the
    API proxy; however, in most cases, the application proxy still provides benefits
    by acting as a central entry point to your application.'
  prefs: []
  type: TYPE_NORMAL
- en: We have examined typical anti-patterns in the initial cloud migration, including
    unproductive migration strategies such as one-and-done migration or retirement
    and rebuilding. We have also explored how the strangler fig pattern allows us
    to keep servicing our current clients while modernizing our application. Now,
    we have a path to becoming cloud native that does not require broad sweeping solutions
    all at once but can be part of a longer-term digital transformation focusing on
    client outcomes rather than technological puritanism.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have dived into the migrations of existing applications, we can
    start to look at how the applications themselves are constructed to be cloud native.
    The first stop on this journey is addressing where we store the state for our
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Stateful applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most applications are driven by a series of **stateful processes** at their
    core. These states might be **ephemeral** – that is, they might not be data with
    long-term context, such as a user session that is only active while the user is
    on a website. In other scenarios, we might persist these states for longer-term
    storage. For example, an online store might require maintaining the state of a
    shopping cart, collecting payment, and shipping the items. These are all states
    that need to be persisted in our architecture somewhere. In a single server model,
    conflating the system’s local and external state is trivial. In this section,
    we will look into the scalability and robustness of these patterns to examine
    how we can manage the state cloud natively.
  prefs: []
  type: TYPE_NORMAL
- en: The stateless server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common anti-pattern when building cloud native applications is to store state
    locally to a server. Most cloud services have options, like **session affinity**,
    to enable you to migrate applications to the cloud with a locally stored state.
    However, we should refrain from using these patterns in new or refactored cloud
    native applications. Two main patterns allow us to achieve this in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: In the **state assertion pattern**, the client presents a **verifiable state
    representation** to the backend server. We typically use this pattern for transient
    state, the quintessential example of which is replacing user session tokens, which
    we can match to the ephemeral state stored on the machine, with a user session
    assertion like a **JSON Web Token** (**JWT**) or **Security Assertion Markup Language**
    (**SAML**) response. In both cases, the client stores their state, and we can
    verify that the client’s state has not been altered through cryptographically
    secure signatures. This pattern comes with some caveats, for instance, the fact
    that these tokens (unless encrypted) are transparent to the end user, so we should
    never include secret information that we don’t want the user to see in the assertion.
    They are also prime targets for token theft, so good practices around **token
    lifetimes**, TLS, and **storage of the tokens** on the client’s device are all
    paramount with this pattern.
  prefs: []
  type: TYPE_NORMAL
- en: The second pattern is using **external state storage**. If the data we are handling
    is not transient and requires use by multiple parties, then we must persist the
    state to storage external to the server. The type of data being stored decides
    how we store it on the backend, too. The key here is to move the state out of
    our application, which provides numerous benefits in the world of the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: We typically encounter three kinds of state data. Of course, there are always
    exceptions and edge cases, but as a general rule, we can choose external state
    storage suitable for our use case.
  prefs: []
  type: TYPE_NORMAL
- en: '**Transient state data** is data that represents a substate of a system at
    a point in time, but it is inconsequential if the data gets deleted. This might
    be because the data itself is a cache of other data sources that can be reconstructed
    or because the nature of the data is transient anyway, for example, **short-lived
    session tokens**. Typically, we store this data because we require it at short
    notice. Think of it like your short-term memory. It holds values that you are
    currently actively working with but might be replaced at any point. Cloud services
    have solutions tailored toward high-performance workloads and can be leveraged
    for more cost-effective solutions. For high-performance workloads, we can use
    services like **ElastiCache** in AWS, **Memorystore** in GCP, or **Azure Cache**
    in Azure; these all mirror the concept of traditional deployed cache services.
    Other emerging solutions in the space, like **Momento**, allow for cache as a
    service. If latency is not mission-critical, other proprietary solutions might
    be more cost-effective and scalable with only minimal impact on latency, for example,
    TTLs on DynamoDB (a NoSQL service from AWS) tables or even fully SaaS solutions
    such as Momento. The critical difference from the self-managed paradigm is that
    these services are managed, and all have options to be automatically scalable,
    allowing us to focus on those parts of our application that deliver value, our
    domain logic.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Persistent state data** is data the system needs a persistent reference to
    with context in a semantic model. These might be items such as orders we want
    to keep a log of or bank accounts for which we want to maintain a balance. The
    way in which we store this data can have different modalities, such as **relational**
    versus **non-relational**, **normalized** versus **denormalized**, or **structured**
    versus **unstructured**. Typically, these representations of state can be thought
    of as records that might be akin to our long-term memory. At the time of writing,
    this is an exciting space, as there are leaps and bounds of progress being made
    in the serverless offerings for relational databases like **Aurora Serverless**
    on AWS or **Cloud Spanner** on GCP. For non-relational databases, most cloud providers
    have well-established, truly serverless offerings (truly serverless in the way
    that they scale to zero). AWS has **DynamoDB**, Azure has **Cosmos DB**, and GCP
    has **Cloud Firestore**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supporting data** is typically data that has little meaning without the context
    of persistent data. This might be data like photos, PDF documents, or other types
    of files that we want to store because it provides additional information. The
    difference between persistent and supporting data is that supporting data can
    be thought of as an object rather than a record. This distinction is also reflected
    in the way the services are named, usually referred to as blob or application
    stores. AWS has **S3**, GCP has **Cloud Storage**, and Azure has **Azure Blob
    Storage**. Once again, all of these are managed services, and their throughput
    and capacity will scale with our requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: The question is, when do we commit state to an external service? The general
    rule of thumb is that any state that requires persistence beyond one transaction
    should be committed to external state management. The local state is fine within
    the context of the transaction for processing purposes, but the external state
    is necessary for anything breaking this boundary. A parallel we can draw, which
    we have all likely suffered with in the past, is a multi-page web form, where
    every time you submit a value that is incorrect, it forgets the previous pages
    and takes you back to page one. That is the risk we run with local state that
    crosses translation boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'These data types are the most common when serving Online Transaction Processing
    (OLTP) workloads. The storage and consumption patterns are different when serving
    analytical (OLAP) workloads. When analytical functionality is required, persisting
    data to an analytical store purpose-built for your use case is usually recommended,
    such as a data warehouse. Each of the hyperscalers has slightly different approaches
    in this space: GCP has the fully managed serverless solution **BigQuery**, AWS
    has **Redshift**, and Azure has **Azure Synapse**. This area also has significant
    contenders outside of the hyperscalers, like **Snowflake** and **Databricks**.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve discussed removing the state from the local server, let’s explore
    the new possibilities for resiliency and scalability this opens for us in a cloud
    native environment.
  prefs: []
  type: TYPE_NORMAL
- en: Resiliency and scalability in a stateless server paradigm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Werner Vogels, the CTO of AWS, once mentioned that “*Everything fails, all the
    time*.” If we persist state locally to our server, then that state is only as
    durable as that single server. Large companies, such as hyperscalers, employ legions
    of engineers to ensure their applications are durable, available, and bug-free.
    Most people embarking on a cloud native transformation won’t have access to the
    same level of resourcing that these large companies do. This is where the **stateless
    cloud paradigm** allows us to trade on margin by using managed services to store
    our state. These managed services do have legions of engineers behind them. If
    we persist state external to our application, suddenly, the fault tolerance of
    our application becomes less consequential.
  prefs: []
  type: TYPE_NORMAL
- en: Server died? Start another one and investigate the cause. Our state was off
    the server, so it doesn’t matter whether the server went down. Our new server
    will pick right up where the old one left off. Even better, run multiple stateless
    instances of your server in a self-healing group. Cloud services also allow us
    to automate this part of our system. AWS uses **Auto Scaling groups** and **Elastic
    Load Balancing**. GCP has managed instance groups for VMs or **Cloud Run/Google
    Kubernetes Engine** for containers, as well as **load balancers** to distribute
    traffic. Azure uses **Virtual Machine Scale Sets** and **Azure App Service** to
    a similar effect. All these services allow us to mitigate the risk of single-point
    failures in our system for the parts of the cloud that are our responsibility
    and typically contain the most bugs. It’s important to note that managing the
    state does not even need to be a process we entrust to our own code; we can go
    even further and use a fully managed state as a service.
  prefs: []
  type: TYPE_NORMAL
- en: State as a service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Typically, we build state machines to replicate business processes. For example,
    we might onboard a new tenant in a distributed microservice architecture system.
    In the past, I have seen people build complex code in poorly documented and typically
    fragile ways. For example, a central tenant service called out to each of the
    microservices to orchestrate them, but this tenant service was touched by every
    team that needed onboarding actions to be performed. The result was unbound states
    and error-prone onboarding that resulted in a wide array of edge cases, with no
    one easily able to grasp the full complexity of the system.
  prefs: []
  type: TYPE_NORMAL
- en: We want a state machine that tells us if the requested action has been completed.
    Here is where managed services can also be of benefit. Solutions such as **AWS
    Step Functions**, **Google Workflows**, or **Azure Logic Apps** allow us to outsource
    the maintenance of the state to the cloud itself. This is an excellent solution
    for when centralized orchestration is required. In our previous example, we want
    to onboard a tenant, so we make a state machine that creates a new tenant in the
    tenant service, provisions a new user as the admin in the user service, and sends
    an email to that user to log in. Once the user has accepted the invitation, there
    may be more stages, such as provisioning new data for the tenant, prompting the
    admin to add other users, or setting up retention policies on user files.
  prefs: []
  type: TYPE_NORMAL
- en: We could do this in a distributed way with eventing and service-specific state,
    but typically, that results in unbound and undocumented behavior without appropriate
    oversight. The state machine as a service approach also allows us a single pane
    of glass to view our state machine structure and how various instances of state
    are progressing through it. When the tenant onboarding system breaks, we can immediately
    see where the error is by viewing our well-defined state machine.
  prefs: []
  type: TYPE_NORMAL
- en: The anti-pattern we typically see in this system is people using state machines
    for systems that do not cross bounded contexts (i.e., they don’t require orchestration).
    In these scenarios, we should instead rely on state representation internal to
    the bounded context, such as updating an order item from “ordered” to “packed”
    and then to “shipped.” The state transitions in this scenario are simple, linear,
    and within a bounded context. Hence, external state orchestration is not required.
    The final piece of the state puzzle is configuring our applications.
  prefs: []
  type: TYPE_NORMAL
- en: Application configuration as state
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fundamentally, our application behavior is an emergent property of our application
    state filtered through our business logic. The anti-pattern here is defining application
    configuration in the same code we use to define our business logic. Application
    configuration is just another form of state, one that typically differs between
    deployed environments. Our code should be agnostic of the environment it is deployed
    in, instead, configuration should be managed through deployment itself. There
    are two places we typically store application configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: Externally in a key-value store or secret manager. We touched on this approach
    in [*Chapter 5*](B22364_05.xhtml#_idTextAnchor136) for feature flags.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Internally in the template used to create new instances of our application,
    like through environment variables. This is typically for bootstrapping values,
    such as service discovery endpoints or database connection strings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The difference between the local state in the configuration domain and the
    local state in the transaction domain is that the state in the configuration domain
    must satisfy two criteria to be effective:'
  prefs: []
  type: TYPE_NORMAL
- en: It must be immutable; the configuration must not change due to external factors
    except for the service’s redeployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It must be universal; all copies of the application must be provisioned with
    identical copies of local state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These two paradigms ensure that our transactions are agnostic of the actual
    backend service completing the request. In the external case, we have a little
    more flexibility but need to be careful of the effects of rotation and cache invalidation.
  prefs: []
  type: TYPE_NORMAL
- en: State allows our application to provide meaning through the lens of our business
    logic. However, improperly handled state can cause issues with resilience and
    scalability. Luckily, in the cloud landscape, there are many battle-tested tools
    that provide ways for us to store our application state. We can even shift our
    state machines entirely to the cloud with cloud native offerings while also reducing
    operational complexity to a minimum. While state is the lifeblood of our application,
    the health and malleability of our code are normally measured through two other
    properties; coupling and cohesion.
  prefs: []
  type: TYPE_NORMAL
- en: Tight coupling, low cohesion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In software design, two measures of interrelatedness are often used as a litmus
    test for sound system design. These are **coupling** and **cohesion**. Coupling
    refers to disparate services calling each other to accomplish a task. High coupling
    implies that the services are heavily interdependent and are challenging to operate
    in isolation without worrying about dependencies or side effects. Cohesion is
    the opposite. Coupling measures the relationships between services, and cohesion
    focuses on the relationships inside the service. If a service has low cohesion,
    it tries to do many disparate things simultaneously. We commonly see low cohesion
    and high coupling as an anti-pattern in cloud native software development. In
    this section, we will explore how these anti-patterns tend to be reflected in
    cloud environments and how to avoid them.
  prefs: []
  type: TYPE_NORMAL
- en: The Lambdalith versus single-purpose functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common anti-pattern we see is low cohesion in deployed infrastructure. Typically,
    this anti-pattern gets introduced through siloed infrastructure teams; for information
    on why this might be a lousy idea, see [*Chapter 5*](B22364_05.xhtml#_idTextAnchor136).
    Let’s assume we have a serverless function on AWS, a **Lambda function**, and
    every time we want a new one, we need a sign-off from the infrastructure team
    to create a new function for us rather than being empowered to create a new Lambda
    function ourselves. Then, we get a feature that should only take a day to implement
    but should really be a serverless function. Rather than wait for the infrastructure
    team to deal with their backlog of tickets and provide us with our function, we
    see a tantalizing preexisting Lambda function that, if we just added some extra
    routing, could also handle this other functionality. Compound this effect over
    many features, and suddenly, we end up with a significant monolithic serverless
    function. Hence the moniker, the **Lambdalith**. The problem is that these serverless
    functions have low cohesion. This means that by modifying our function, we have
    a large blast radius that could impact utterly unrelated functionality simply
    due to process inefficiencies and siloed ownership.
  prefs: []
  type: TYPE_NORMAL
- en: I previously worked with an organization that had an architecture team separate
    from the infrastructure and development teams. Creating a service required the
    interaction of three teams and was aligned to a monthly cadence. This particular
    organization had teams aligned to business domains; each business domain typically
    had a few services they managed. While feature development was rapid, the event
    of a new service being added to support those features was exceedingly rare. These
    containers grew to significant complexity with low cohesion between application
    parts. **Conway’s law** was alive and well, and the architecture closely followed
    the team topologies to a fault.
  prefs: []
  type: TYPE_NORMAL
- en: In any process, be it online sales or provisioning new infrastructure, the more
    difficult this process is, the less likely it will be completed. Typically, people
    ask how much friction is suitable to ensure we still produce secure, deployable
    artifacts. The answer almost always is as little as humanly possible. We should
    enable teams to take ownership of their own output by providing them with a safe
    and secure platform in which they can achieve their goals. Infrastructure and
    architectural resources should be available to support them at all points. However,
    if the development team cannot drive the process, you will find that the process
    will be woefully underutilized.
  prefs: []
  type: TYPE_NORMAL
- en: The truly cloud native antithesis of the Lambdalith is the single-purpose serverless
    function. In this pattern, each function does exactly one thing and does it well.
    For example, a `POST` method on a specific API endpoint. This does not mean it
    cannot share code with other single-purpose functions. Typically, grouping these
    functions into pseudoservices with high internal cohesion makes sense. However,
    each deployed function should be completely agnostic of its peers in the pseudoservice
    group. This grouping might be performed by having several single-purpose functions
    deployed from the same repo (or parent folder if using a monorepo). This pattern
    provides us with high cohesion in our deployed units. Each unit is only concerned
    with satisfying the requirements for a single type of request. There is a limit
    to the level of atomicity to which we should break these units down. Namely, they
    should never be so atomic that we must chain multiple together.
  prefs: []
  type: TYPE_NORMAL
- en: Chaining serverless functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another anti-pattern we commonly see is the chaining of serverless functions
    in the call stack. This form of coupling can have an extremely negative effect
    on your solution’s performance and cost-effectiveness. For example, consider a
    serverless function that uses a typical synchronous **backend for frontend** (**BFF**)
    approach to call some business logic in another serverless function that queries
    a database. This situation is illustrated in the following figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22364_07_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Chained invocations of serverless functions
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the figure, each preceding call runs whilst waiting for the
    subsequent call to complete. With this invocation pattern, we are doubling our
    running compute. In a containerized or VM-level environment, this is not an issue,
    as our compute resource can serve other requests while we wait for the chained
    call to finish. However, in a serverless function environment, our function can
    only serve one invocation at a time. This means that while we wait for the second
    serverless function in the chain to complete, our first lambda function cannot
    serve any other requests. Therefore, we are doubling our computing costs and resource
    consumption without any tangible benefit. Some cloud providers, such as GCP, are
    building platforms that allow this unused computing power to be better utilized.
    However, most default implementations are limited to completing a single request
    at a time. Chained functions are a prime example of coupling that can be converted
    to high cohesion internally in a single function. We more often need to perform
    the reverse operation and decouple coupled services.
  prefs: []
  type: TYPE_NORMAL
- en: Decoupling coupled services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we call services as dependencies from another service, we increase the
    blast radius of changes to the service being depended on to include our dependent
    service. This is a form of tight coupling that can be very detrimental to the
    performance of our application. The more services we chain together, the less
    reliable our service becomes, as we are now dealing with the product of the reliabilities
    of each service in the chain. Let’s say each service has a 95% reliability rate.
    If we combine 4 services in a single call, our reliability decreases to 81.4%
    (0.95^4). Typically, this problem arises as it fits our mental model of services
    very well. As programmers, when we need to perform some work internal to our application,
    we call a function and await the results. Extending this model to a multiservice
    architecture, we call another service and await the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, cloud providers have a cloud native way to solve this tight coupling
    problem. It requires two changes in thinking to implement correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to break the idea of synchronous feedback. Sending an HTTP `202` return
    code and performing work asynchronously is just as, if not more, valid than a
    synchronous response with an HTTP `200` response all in one call.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to stop thinking of each of the services that need to work as dependents
    and start thinking of them as isolated units of work that need to be completed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key to implementing these solutions in a cloud native environment is to
    decouple these services by putting a managed service in the middle. AWS has **Simple
    Queue Service** and **EventBridge**, GCP has **Google Pub/Sub**, and Azure has
    **Azure Event Grid** and **Azure** **Service Bus**.
  prefs: []
  type: TYPE_NORMAL
- en: These managed services all provide similar functionality. They act as a message
    broker between our services so that our services do not need to talk to one another
    synchronously to pass information between them. They differ slightly in how they
    operate. Some are **simple message queues**, and others are **complete event bus
    implementations** with publish and subscribe functionality.
  prefs: []
  type: TYPE_NORMAL
- en: The result of using any of these services is similar. Instead of our reliability
    now being the result of a series product, we have decoupled the services to concern
    themselves with the reliability of the managed service. Let’s take our four unreliable
    services and attach them to our managed service, allowing for asynchronous execution.
    Assuming our managed service has four 9s of uptime (99.99% uptime), our result
    is four services, each with 95.98% reliability. If any of our services goes down,
    the other services will still operate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing **dead letter queues** (**DLQs**) can further improve the reliability
    of these services. If one of our services cannot process messages, we can send
    the backlog of messages to be processed to the DLQ. Once we have fixed our service
    and everything is operational, we can automatically replay the events from our
    DLQ and complete the outstanding work. This means that instead of a single service
    failure impacting all systems, the blast radius of a single system is limited
    to the system itself. The system will eventually be consistent once all unprocessed
    messages have been replayed. When we need to eventually trace these events through
    our system, perhaps to troubleshoot why they ended up in our DLQ, we need to correlate
    their path, which brings us to an essential part of distributed systems: telemetry
    and event correlation.'
  prefs: []
  type: TYPE_NORMAL
- en: Telemetry and event correlation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can’t improve what you cannot measure. Understanding precisely the degree
    of coupling within a deployed application can be challenging. Typically, we come
    across an anti-pattern using traditional logging systems with distributed systems.
    Traditional logging systems do not provide the granularity (level of detail) and
    traceability (correlation with other messages) required to debug and improve distributed
    systems. Typically, when we debug a distributed system, we are trying to piece
    together the result of an action across multiple deployed units. This is where
    robust **telemetry** comes into play. We can tag all of our requests, messages,
    and invocations with a **correlation ID** on entry into our distributed system,
    and then use this correlation ID to trace the effect of that action across all
    of our deployed units and managed services. We will go into more detail on telemetry
    systems in [*Chapter 10*](B22364_10.xhtml#_idTextAnchor270). However, we can utilize
    the correlation aspect of modern telemetry systems to assist us in decoupling
    applications. By following our traces, we can reveal dependencies between systems
    that previously would have required us to look into the source code or environmental
    configuration to find. Once we identify the dependencies within our application,
    we can slowly move from tightly coupled dependencies (one service calling another)
    to loosely coupled dependencies (two or more services joined by a shared, managed
    message bus or queue).
  prefs: []
  type: TYPE_NORMAL
- en: '**Tight coupling** and **low cohesion** are anti-patterns we are typically
    shielded from in an on-premises environment. In the cloud, these patterns become
    dysfunctional, leading to poorly performing applications and unexpected side effects.
    The key to rectifying these anti-patterns is, firstly, to be able to measure the
    coupling and cohesion, and, secondly, to work to decouple tightly coupled services
    while increasing internal cohesion. Typically, modeling cohesion and coupling
    should be part of the architectural planning for a feature and form part of the
    definition of done. Let’s explore some common pitfalls and address the comprehensive
    definition of done.'
  prefs: []
  type: TYPE_NORMAL
- en: The comprehensive definition of done
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When creating software in **siloed release models**, as discussed in [*Chapter
    5*](B22364_05.xhtml#_idTextAnchor136), we looked at empowering teams to own the
    delivery of their outputs from conception to deployment and beyond into operations.
    However, this requires the development team to also take ownership (with support
    from other teams) of the functionality and responsibilities that the siloed release
    pipeline previously hid from the team on the path to production. Hence, we need
    to revisit the definition of *done* (and, in some cases, the definition of *ready*)
    for our software teams. Previously we have visited the cultural and business shift
    required to make this happen, but in this section, we will discuss building these
    requirements intrinsically into your definition of done.
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring security
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Security** is a critical factor in the delivery pipeline. Neglecting sound
    security practices can lead to a gradual accumulation of risk for the company,
    often unnoticed until a breach occurs. This omission can result in a blame game
    and severe consequences. To develop secure applications, it’s crucial to integrate
    several security practices into the **software delivery life cycle** (**SDLC**).
    These practices should be part of the definition of done for any work, and their
    review should be as rigorous as code review before deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring **open source** or **external dependencies** is an anti-pattern. In
    the software world, many open source packages provide base functionality on which
    we build our business logic. However, each package we pull from an external source
    represents a possible vector for malicious code to be added to our application.
    Maintaining and alerting on a **software bill of materials** (**SBoM**) gives
    you an indication of the health of your project. Many tools exist to help you
    manage the versions of software packages used. A typical pattern for managing
    dependencies at a language level is to use a read-through private artifact repository
    for your language, populating this artifact registry with internal packages to
    be used and allowing it to pull and cache upstream packages. This repository will
    enable you to have a single pane of glass containing all dependencies and versions
    of your application, GCP, AWS, Azure, and many niche players, all of which can
    export and monitor SBoMs from their respective artifact repository services. Pull
    requests should be instrumented to ensure that the packages they add do not add
    any new vulnerabilities, and maintenance should be done regularly, informed by
    the SBoM, to address any new vulnerabilities that have been found.
  prefs: []
  type: TYPE_NORMAL
- en: Not having a **threat model** for your application or building one and ignoring
    it is an anti-pattern. Typically, when we see the shift from a dedicated security
    team to a supported and empowered development team, the development team uses
    the security team to produce a threat model but fails to address it throughout
    the SDLC. The preliminary threat model should form part of the definition of ready
    for the team. The threat model should be fundamental in deciding how to tackle
    a problem and must be verified to ensure the built solution correctly mitigates
    the identified risks. Thus, the threat model should be a living document as a
    change is implemented, providing details on how risks are mitigated so that the
    changes can be merged confidently. Once in production, the application should
    be monitored through a **cloud native application protection platform** (**CNAPP**)
    to catch any risks or misconfigurations that might not be addressed by the threat
    model. The key to effective threat modeling is to choose the correct level of
    granularity. If you are a stock market making changes to the settlement engine,
    then the proper level of granularity might be every merge. Other lower-risk environments
    might only require threat modeling on a less granular level. The idea is to find
    the correct amount of friction that mitigates the risk to the proper level without
    compromising on the necessary level of security for your application.
  prefs: []
  type: TYPE_NORMAL
- en: The final *ignoring* security anti-pattern to address is born out of the increased
    flexibility the cloud gives us, and that is the failure to address defense in
    depth. In an on-premises environment, the delineation between what is inside the
    network and what is outside the network is evident. You have a physical cable
    going to a firewall that serves as the ingress point for all your traffic. Your
    solution might have some software-defined networking downstream, but there is
    a clear separation. In the cloud environment, all of a sudden, different services
    run in **virtual private clouds** (**VPCs**) and outside VPCs. Endpoints can be
    addressable over the internet or through endpoint projections into your network.
    Some services exist in **cloud provider-managed networks** and require additional
    networking. All of this means that it is less clear where traffic is flowing.
    There is tooling to help with this but, fundamentally, the highly configurable
    nature of cloud environments means that misconfigurations can present a larger
    risk surface. Managed cloud services already have strong **identity and access
    management** (**IAM**) tooling. This should be complemented with robust, **zero-trust
    authentication and authorization** tooling in your code that is validated at every
    application level. Many organizations are still early in their journey of implementing
    zero-trust architecture. Hence, it should be considered a North Star principle
    rather than an absolute requirement. The key is asking yourself, “*What happens
    if we accidentally expose this service to the internet directly?*” This limits
    the blast radius of cloud misconfigurations and ensures that if an internal service
    is accidentally exposed to the public, it still authorizes incoming traffic. This
    blast radius consideration also needs to be considered from a CI/CD perspective.
    One client I worked with had a single repository and project for all infrastructure.
    This resulted in highly privileged CI/CD accounts with enormous blast radii spanning
    multiple disparate systems. Having a robust defense-in-depth strategy means that
    as application architecture shifts to more of a self-serve model, the platform
    that our developers are building on top of is secure enough to tolerate failures
    at each level. Just as we must ensure our developers are building secure platforms,
    we must also ensure we are building observable ones.
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring observability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the monolith, logging stages to the console was enough to debug our application.
    This worked because the application was a simple arrangement (infrastructure)
    of complex objects (our code). In the cloud native world, we shift much of that
    complexity into the infrastructure, giving us a complex arrangement (infrastructure)
    of simple objects (our code). This requires much more robust **logging** and **telemetry
    practices** than logging into a console. We will dive into this topic in significantly
    more detail in [*Chapter 10*](B22364_10.xhtml#_idTextAnchor270). However, we will
    go through some aspects in this section that should form the basis of the definition
    of done.
  prefs: []
  type: TYPE_NORMAL
- en: The first anti-pattern is ignoring spans and only using logging. Logging provides
    us with point-in-time information about the state of our application. Spans are
    different. They provide us with context for a period of execution in our application.
    As part of our definition of done, we should include the addition of spans that
    provide meaningful information about executing subsections of our code. Throughout
    the execution of the span, we should also ensure that we are adding enough enriching
    data to make the diagnosis of issues easier through our observability platform.
    For any deployment that exceeds the scope of a single instance, we must also consider
    correlation to allow us to group spans together and trace their path through our
    distributed application. Trying to piece together the execution context of a request
    from a series of log entries across multiple services is significantly more difficult
    than reading a correlated span flame graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second anti-pattern is collecting metrics with no functional output. We
    quite often see a company collecting many metrics but no alerting or deviation
    monitoring. We have the data to check whether our application is performing as
    intended. However, we are missing that crucial step that actually tells us when
    it isn’t. With comprehensive monitoring, alerting, and rectification procedures,
    we can ensure that our system’s non-functional requirements, such as latency and
    error percentage, do not fall outside of acceptable margins. Therefore, as part
    of our definition of done, we should ensure two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we must ensure that the changes being made have monitoring set up.
    This might be through synthetic traffic, **application performance monitoring**
    (**APM**), observability tooling, and traditional logging.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, we must ensure that the right people are notified when this tooling
    detects a problem. This might be done by automatically creating a ticket in your
    ticketing system, notifying users on a messaging channel, or other means. The
    important thing is that regressions are identified, and people know rectification
    must occur.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By including these two items in our definition of done, we can provide certainty
    that as we add new functionality or modify existing functionality, we don’t breach
    the non-functional requirements of the system. This level of observability also
    gives us insight into which parts of our applications are candidates for optimization
    as part of our continuous improvement process. Previously, for clients where users
    complained about the slowness of the application, we filtered our metrics to rank
    requests by two factors: how often they were called and how long the typical transaction
    took. We found that three endpoints were consistently called and consistently
    slow. With some query optimization, we reduced the response time by two orders
    of magnitude. The change took about three days in total, and the end users were
    significantly happier. Without collecting these metrics and utilizing their outputs,
    we would have needed significant testing in a production environment to get the
    same level of insight. Observability is great for finding the cause of an incident
    (i.e. when something goes wrong) but what about stopping incidents from occurring
    in the first place?'
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring reliability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final part of this section discusses ignoring **reliability**. This is an
    anti-pattern that we see all too often in cloud migrations. Teams care about having
    their features work without considering their continued operation. This is where
    the mentality of *You build it, you run it* can be beneficial. Development teams
    that also own the operation of their output are more likely to consider reliability
    because they are invested and want to avoid call-outs at nighttime or during weekends.
    Cloud native services provide significant tooling to ensure reliability and continuity
    of service. However, utilizing these services can mean the difference between
    an outage of seconds and an outage of days. Any company that wishes to conform
    to internal or external **service-level objectives** (**SLOs**) or has a contractual
    **service-level agreement** (**SLA**) must ensure that they treat reliability
    as a critical aspect of their definition of done.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first anti-pattern we will address is an aspect of the deployment process.
    As we discussed in [*Chapter 5*](B22364_05.xhtml#_idTextAnchor136), development
    teams should own the deployment and operation of their changes. The anti-pattern
    we often see in this space utilizes the same deployment strategy across our environments.
    In a development or test environment, it is typical for us to use **all-or-nothing**
    deployment strategies. This strategy is sound when we want to guarantee that the
    version of the code we are calling is the latest version and maintain fast feedback
    loops between the deployment and testing cycles. Applying this same methodology
    to a production environment means that if our change breaks functionality, the
    change either breaks everything or nothing. We might even have avoidable downtime
    on a successful deployment as the new services might take time to come online.
    For production systems, we care about two things: **early feedback** on a problem
    and **quick rectification** of a problem. Many cloud native deployment approaches
    will allow us to make incremental or quickly revertable changes to preserve our
    system’s operation, especially when using highly managed services such as API
    gateways or functions as a service. These strategies usually come at the cost
    of additional time to deploy or additional resources provision. They also normally
    require external state management, as any internal state will be lost on deployment.
    Some of the methods we can use are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rolling deployments**: These deployments take a set of resources running
    the same application (say, a set of three containers that might all be running
    our user service) and then incrementally update each one in series until all services
    are running the new version, waiting for each service to become healthy before
    starting to deploy the next. This allows us to mitigate the avoidable downtime
    that comes with waiting for services to become ready-to-serve traffic in an all-or-nothing
    approach but does not provide us robust options for returning the application
    to a good state in the event of a failure that is only present at runtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blue-green deployment**: In this strategy, you have two separate groups of
    resources. One set of resources serves your production traffic using the latest
    known production stable version of the application (blue) while the other is deployed
    (green). After you have ensured that the newly deployed system is working, you
    cut across to the new system, which might be through **aliases** or **internal
    DNS**. You can then decommission the old blue target resources. In the event of
    a failure, it is trivial to point the references, DNS or otherwise, back to the
    old deployment of the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Canary deployment**: In this strategy, you follow the same deployment methodology
    as the blue-green deployment strategy. The critical difference is cutting over
    from the blue to green resources. Instead of an instantaneous cutover, we slowly
    redirect some traffic to our new instances. This becomes our canary in the coal
    mine; we can test the services with a subset of production data, and if something
    goes wrong, we will only impact a small subset of requests instead of all requests.
    Otherwise, if all is well, we progress to all traffic heading to the new resources,
    and the old resources can be decommissioned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These methodologies do not need to be applied only to your application code;
    this pattern can be used anywhere you have an expensive rollout process. One client
    I worked with had a database that had to be updated once a month. Each month,
    the data used to build the database was either modified or appended. Ingestion
    of the new data and verifying that it was correct took 15 minutes, and the client
    could not tolerate 15 minutes of downtime. Hence, we created two tables: one for
    the most recent data and one for last month’s data. Each time new data needed
    to be ingested, we would populate whichever table contained the oldest data with
    the latest data. We would then check this table against the current table in use.
    If all was well, we would update the view consumed by the end users to point to
    the table containing the new data. This allowed a seamless transition between
    datasets without taking the system offline and allowed quick fallbacks to the
    last known good configuration if there was an issue. Understanding which deployment
    strategy suits your purposes is essential, and selecting an appropriate deployment
    strategy needs to form part of the definition of done.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second reliability anti-pattern we will examine is the failure to address
    **disaster recovery** correctly. Cloud services have sensible defaults to prevent
    data loss events, such as storing objects in multiple regions or automating database
    backup processes. This process is usually tunable to meet your **recovery point
    objective** (**RPO**) – that is, how much data we can tolerate the loss of. Despite
    how protective cloud services are against data loss events, protection against
    service loss events is usually heavily dependent on your architecture. The critical
    metric data loss prevention does not address is the **recovery time objective**
    (**RTO**). Restoring a database from a backup may take a significant amount of
    time. Likewise, standing up a new instance of your infrastructure may not be a
    short process. If your application catastrophically fails, then having a plan
    in place to restore service to your end users is extremely valuable. The first
    mistake teams generally make in this space is creating one copy of their infrastructure,
    calling it a day, and then moving on with new features. In this scenario, disaster
    recovery has been completely ignored. In the event of a catastrophic failure,
    not only will the team be scrambling to recreate their service but there’s no
    defined process to do so. The second scenario we commonly see is people having
    a theoretical disaster recovery strategy. They have a list of steps to take in
    case of a failure, but if the strategy is theoretical, so are the chances of it
    actually working. An untested strategy is a waste of keystrokes. Any disaster
    recovery strategy needs to be simulated regularly. The time to test it for the
    first time (and likely the first time much of the team sees the strategy) should
    not be when there is a critical outage. Typically, disaster recovery has a few
    options; the key is that all options must be tested. The possibilities we typically
    look at for recovery are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cold recovery**: This strategy is for non-critical services. A cold recovery
    assumes you are starting from nothing, provisioning a new instance of your application,
    and restoring from backups to restore service. It is important to note that not
    having a disaster recovery plan is not the same as having a cold recovery plan.
    Like all plans, cold recovery must be documented and tested regularly to ensure
    the process meets your RPOs and RTOs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Warm recovery**: This strategy involves having a second (or more) minimal
    copy of your application running in a different location that can be quickly scaled
    up to take over from the service if it fails. Ideally, this **failover** and **scale-up**
    would be automated, but while automation is being built, it is perfectly acceptable
    to manually fail-over. An alternative architecture to warm standby that uses the
    same principles involves keeping the supporting structures of your application
    running, however, only starting your application when failover is required. This
    variation on the strategy is commonly referred to as the **pilot** **light strategy**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hot recovery**: This strategy involves running your application in a multi-active
    architecture. Much like we can run multiple servers to ensure that we can tolerate
    the failure of any single server, this pattern takes the same approach but with
    your entire architecture. The failure of any active deployment means that traffic
    can be redirected to the healthy region.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concept of **chaos engineering** is important to illustrate here. Remember
    the quote by Werner Vogels, “*Everything fails, all the time*.” Chaos engineering
    reinforces this by purposely introducing failures into your system, ensuring that
    your system is fault-tolerant. Another good strategy to use is the concept of
    game days, especially for manual processes. These simulated events run through
    the disaster recovery strategy with the responsible team to ensure that everyone
    is familiar with the process. Therefore, as each feature or service is completed,
    the disaster recovery strategy must be updated to include the requirements of
    the new changes and needs to form part of the definition of done.
  prefs: []
  type: TYPE_NORMAL
- en: Security, observability, and reliability are intrinsic parts of changes to our
    system that are often ignored. By addressing these intrinsics as part of our definition
    of done, we ensure that our development teams are not just building applications
    that are built to exhibit the features they are creating but also providing a
    platform that our end users can trust. These parts of our system form a fundamental
    baseline of cloud native operability, but there are many other pitfalls we can
    fall victim to.
  prefs: []
  type: TYPE_NORMAL
- en: Other pitfalls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several anti-patterns that commonly manifest in cloud native application
    development. This section will dissect some of these anti-patterns, their lineage
    from traditional software development, how to identify them, and the proactive
    mindset shifts required to evade them. In our scenario, cloud native applications
    have the capability to scale to any size we choose, sparking fascinating interactions
    between our software and the potential solutions to our problems. By understanding
    these anti-patterns and adopting a proactive mindset, we can empower ourselves
    to make informed decisions and avoid potential pitfalls.
  prefs: []
  type: TYPE_NORMAL
- en: Solving cloud native problems without cloud native experience
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I was working with a customer trying to migrate their existing data structures
    into an **OpenSearch** cluster. We had well-defined schemas into which the data
    had to be marshaled. The problem, however, was that the client attempted to copy
    their relational data structures directly across to OpenSearch with no denormalization
    in between. This meant that to marshal the data, we needed to perform multiple
    lookups to fetch related data structures. These lookups created a situation in
    which a single request for a model could balloon out to thousands of downstream
    requests for all of its associated data. Despite our continued protests that the
    data structures needed to be denormalized or migrated to a high-performance, read-only
    copy of the relational database, the client wanted to preserve the system’s *flexibility*
    by retaining the original relational shape in a non-relational datastore. We implemented
    many improvements to push the model as far as possible, including batching requests
    and local caching for repeated values. However, some requests were simply too
    deeply nested to optimize. The solution initially proposed by the client was to
    scale the cluster, so the client scaled the cluster until more performance bottlenecks
    were hit, and then the client scaled the cluster again. We had an interesting
    call with the cloud provider. They informed the client that they were provisioning
    more infrastructure than the cloud provider had provisioned for some subsidiary
    services. This is the first anti-pattern we would like to address. The easy access
    to virtually unlimited cloud resources comes with the temptation to solve performance
    problems by throwing more resources at it, and the resulting cloud bill will scale
    equally as quickly. We should often look inward at our application instead of
    outwardly at the infrastructure it is running on to solve problems around application
    performance. Scaling our infrastructure vertically to solve performance issues
    will only take us so far. This indicates that an alternative specialized solution
    may be required, your service has low cohesion, or your application is poorly
    optimized.
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings us to the second anti-pattern, which can also result in the first
    anti-pattern. This pattern typically starts with someone responsible for a cloud
    native service coming across a staged architecture online with many pretty icons
    and boxes and then trying to shoehorn that architecture into their use case. Our
    architecture should be informed by the requirements of the application code we
    need to write rather than the code we write conforming to some architecture. The
    cause of this can be multifaceted. A common driver for this anti-pattern is what
    we typically refer to as resume-driven development. This occurs when someone is
    more concerned about getting experience with a particular technology than about
    that technology’s potential to solve the problem. Staged architectures can form
    a good starting point for potential solutions and often illustrate best practices.
    However, we must temper these architectures, considering their suitability across
    various factors. Typically, before adopting a staged architecture verbatim, we
    should ask ourselves some questions like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Do we operate at the scale for which this staged architecture solves a problem?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we have the internal skill set to implement and maintain it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does this model follow our standard architecture practices or will it be unique?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can we make any changes to ensure this architecture more accurately solves our
    problem?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third anti-pattern we will address is manually changing deployed infrastructure
    or code bases outside our CI/CD pipeline. A typical example might be that our
    application runs a query that takes a little while to complete in production.
    So, the developer logs into production and quickly adds an index to the lookup
    column, and the problem is solved. Despite the compounding of errors that need
    to occur to allow the developer to make this change, fundamentally, we are introducing
    instability into our application. This concept is known as **environmental drift**.
    Our code and deployment pipelines define a model that does not correlate with
    what is deployed. In our example, we looked at the developer making changes to
    production, which means the first time that all of our subsequent changes are
    tested with this environmental drift is when those changes hit our production
    environment. It also causes an issue when we need to recreate our infrastructure;
    by circumventing our source model, we will create the same issue whenever we try
    to create a new instance of our infrastructure. The solution to this problem is
    relatively simple; development teams should not be able to change a non-ephemeral
    environment without following their CI/CD process. If they want to prototype a
    fix or conduct a technical spike that would be accelerated by having write access
    to the environment, then create a sandbox that can be destroyed once the work
    is done. This way, you prevent the accumulation of tests and quick fixes in any
    environments on the path to production. Ideally, these lower environments should
    be as close to the production environment as possible. On the topic of production
    environments, we must be careful about how we scale our code in reaction to real-world
    events.
  prefs: []
  type: TYPE_NORMAL
- en: Suffering from success – the downside of limitless scale
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have an upper bound for our application’s throughput when working with on-premises
    infrastructure. Eventually, we will run out of system resources to serve requests.
    In a cloud environment, we often see the same thinking come into play – an anti-pattern
    where rate limits and service limits are ignored. The consequences of neglecting
    rate limits, service limits, or throttling are significantly higher in the cloud.
    Rather than being capped by our infrastructure, we have a virtually unlimited
    pool of resources to scale into. Suppose we combine this lack of physical limits
    with stateless servers that can interchangeably serve any request, irrespective
    of any service-level partitioning that we might have. In that case, we can scale
    to meet our customer’s needs very rapidly and virtually limitlessly. In this scenario,
    we must set artificial caps on using our service. How these limits are partitioned
    (i.e., by user, tenant, customer, etc.) is up to the implementer. We set rational
    limits for using our theoretically limitless service to control runaway costs
    and ensure that we don’t impact services for any other clients. Many cloud native
    managed services already have built-in functionality that we can use to perform
    rate-limiting, usage monitoring, and licensing applications. Commonly, this is
    applied at the **API aggregation layer**, such as in AWS **API Gateway**, Azure
    **APIM**, or GCP **API Gateway**. Luckily, these same API keys can be used as
    part of our authentication strategy, for example, tying a request to a client
    to enable defense in depth checking that our API key matches the tenant we’re
    calling. As the complexity of our application grows, we might require custom authorization
    and rate-limiting logic on our APIs. For example, AWS allows you to add custom
    authorization to API Gateway through Lambda functions. Other niche API proxy players
    like **Apigee** (now acquired by Google) and **Kong** allow for complex logic
    through a comprehensive policy language.
  prefs: []
  type: TYPE_NORMAL
- en: In the on-premises monolith, things tended to fail together. Was our server
    overloaded or not? It’s a question with a relatively simple answer. In the cloud
    native world, where we have services built up of many components, things tend
    to fail piecemeal. We need to be tolerant of these faults, but we also need to
    be aware that the scales the cloud lets us operate at can lead to some interesting
    behaviors. The next anti-pattern we will address is using bad timeout and retry
    practices, especially in the context of concurrent executions. Let’s assume we
    have a process that needs to load CSV files into a database and a service that
    processes a single file from these buckets as they arrive. Let’s assume our clients
    upstream, who deliver our files into the S3 bucket for us to consume, realize
    that they had an error in their system and haven’t uploaded files for the last
    three days. That’s fine; they have added all the files. Let’s assume we have a
    naive architecture that sends a request to an HTTP endpoint to pull the file for
    processing using S3 events and SNS. If we’ve ignored the consequences of concurrent
    execution, we could suddenly begin ingesting a large amount of data simultaneously.
    This puts an enormous load on the database we are loading the files into. If we
    don’t have timeouts configured for these processes, we could end up completely
    overloading our database. Therefore, all calls in our application code must have
    a timeout, and the expiration of those timeouts must be handled gracefully, cleaning
    up any work in progress that they have requested.
  prefs: []
  type: TYPE_NORMAL
- en: So, if a timeout fails, then what next? A naive response might be that we simply
    need to retry the request. If the failure results from factors other than an overloaded
    system, and these errors are rare, then we can probably get away with this approach.
    However, it’s important to note that retries are compounding the issue; we are
    requesting more server time to solve our problem. If the system is already overloaded,
    then this just compounds the effect as old requests being retried are now also
    competing with new requests. A common tactic here is an exponential backoff algorithm,
    although it is advisable to cap your maximum retry period and the total number
    of retries. This can work; however, once your server gets overloaded, a whole
    bunch of calls are going to fail, and if all these calls are retried using the
    same algorithm, then all we’ve done is kick the can down the road, and we will
    overload the server on the next retry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important aspect of retry behavior is the concept of **jitter**. We
    introduce **randomness** into our retry behavior to prevent a stampeding herd
    situation. We also need to be aware of the multiplicative effect of retries. Suppose
    our service makes calls that go three layers deep, and each service retries five
    times. In that case, the downstream system will receive 53 retries or 125 requests,
    which is the opposite of the behavior we want when downstream services are overloaded.
    Luckily, there are three effortless ways to avoid this situation:'
  prefs: []
  type: TYPE_NORMAL
- en: Decouple spiky and expensive traffic with message queues, then scale your downstream
    services based on queue depth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use cloud provider SDKs where possible, as these will already have retry behaviors
    built into them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use managed services, as these typically scale easier and have built-in retry
    and rate-limiting functionality you don’t need to build yourself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This brings us to our last anti-pattern, using implicit properties of ephemeral
    resources for hardcoded dependency mapping.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding implicit ephemeral specification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When writing code, especially **infrastructure as code** (**IaC**), we can easily
    fall into the anti-pattern of using direct specifications for partially ephemeral
    resources. An ephemeral specification, for example, would be applying an IaC configuration
    that outputs the IP address of an instance, then referring to the first configuration
    instance by directly using that IP address in another IaC configuration. If we
    change the first configuration, the IP address might change, but our ephemeral
    specification has created a hard dependency between them. Instead, we should use
    resources that aren’t ephemeral, such as DNS entries that can be updated. This
    is the simplest form of service discovery. There are robust, full-featured service
    discovery platforms that extend this functionality for various cloud providers
    and deployment configurations. Ideally, any dependencies between our infrastructure
    should be explicit rather than implicit through hardcoded values to make our deployments
    truly agnostic of the state of the deployed environment.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have now explored some common anti-patterns we see when shifting our application
    logic to the cloud. Our application code is typically the value differentiator
    or competitive advantage in our business, so we can move it to the cloud and,
    by doing so, increase its availability, resilience, and performance. Now that
    we understand the implications of running our application code in the cloud, how
    can we store all our data? This is what we will dive into in the next chapter.
  prefs: []
  type: TYPE_NORMAL
