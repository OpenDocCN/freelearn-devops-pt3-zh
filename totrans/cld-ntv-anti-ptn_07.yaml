- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Expressing Your Business Goals in Application Code
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将业务目标表达为应用代码
- en: The **business logic** that makes our company technology unique and provides
    a competitive advantage is usually the business logic that we employ. Expressing
    our business rules as **application code** can drive forward automation, reduce
    cycle times, and increase productive output. However, when we move that logic
    to the cloud, we can be trapped by some **anti-patterns** that we would normally
    get away with in the old monolithic, on-premises architectures we are evolving.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 使我们公司技术独特并提供竞争优势的**业务逻辑**通常是我们采用的业务逻辑。将我们的业务规则表现为**应用代码**可以推动自动化、减少周期时间并提高生产效率。然而，当我们将这些逻辑迁移到云端时，我们可能会被一些**反模式**所困扰，这些反模式在我们过去的单体化本地架构中通常能被忽略。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Lift and shift
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搬迁转移
- en: Stateful applications
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有状态应用程序
- en: Tight coupling, low cohesion
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 紧密耦合，低内聚性
- en: The comprehensive definition of done
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整的“完成”定义
- en: Other pitfalls
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他陷阱
- en: Lift and shift
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 搬迁转移
- en: When we move applications to the cloud, we need to shift our thinking from deploying
    an application as an independent unit to the application being the emergent behavior
    of the interaction of various services. In this section, we will explore the typical
    process of shifting an application to the cloud, the strategies we can use, and
    how to increase the maturity of our cloud native solution.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将应用程序迁移到云端时，我们需要转变思维方式，从将应用程序作为独立单元部署转变为将应用程序视为各种服务交互的涌现行为。在本节中，我们将探讨将应用程序迁移到云端的典型过程、我们可以使用的策略，以及如何提高我们云原生解决方案的成熟度。
- en: Building in the cloud versus building for the cloud
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在云中构建与为云构建
- en: When we migrate applications to the cloud, the simplest method is to package
    up the existing deployment in a VM, deploy it to the cloud, and call it cloud
    native. This thinking limits the actual usage of the cloud to simply reflect the
    existing topologies we had in our on-premises environment. But what have we achieved?
    We still have the same limitations of the system we just migrated from but without
    any of the advantages of the cloud. We’ve just moved our code from our server
    to someone else’s server. We may gain some efficiencies in maintainability, organizational
    complexity, and onboarding time. However, this is not unique to cloud hyperscalers,
    and we could achieve the same results with most other VM hosts. This **lift-and-shift**
    mindset gets us into the cloud but falls short of fully utilizing it. This mindset
    is the difference between building *in* the cloud versus building *for* the cloud.
    Once the application is in the cloud via this lift-and-shift methodology, we can
    make improvements and optimizations not only to the application itself but also
    to its surrounding infrastructure and architecture.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将应用程序迁移到云端时，最简单的方法是将现有的部署打包成一个虚拟机（VM），然后将其部署到云端，并称之为云原生。这种思维方式将云的实际使用局限于简单地反映我们在本地环境中已有的拓扑结构。但我们实现了什么呢？我们仍然面临着刚刚迁移过来的系统的相同限制，但却没有享受到云的任何优势。我们只不过是把代码从我们的服务器转移到别人的服务器。我们可能在可维护性、组织复杂性和入职时间上获得了一些效率。然而，这种做法并非云大规模服务商所特有的，我们也可以通过大多数其他虚拟机托管平台实现相同的结果。这个**搬迁转移**思维方式使我们进入了云，但未能充分利用云的潜力。这种思维方式的区别在于：在云中构建与为云构建。一旦通过这种搬迁转移方法将应用程序迁移到云端，我们不仅可以对应用程序本身进行改进和优化，还可以优化其周围的基础设施和架构。
- en: I previously worked for a company that had an existing on-premises solution.
    This on-premises solution was distributed to customers via remote deployment.
    The client provided a machine, and a specific onboarding team logged in to that
    machine and ran a playbook to set up the application. This lift-and-shift mindset
    persisted into the *cloud native* hosted offerings they provided. The onboarding
    team provisioned a new instance and database in the cloud, and then somebody installed
    the application, and the client accessed the cloud instance. This process was
    the company’s first iteration of providing services in the cloud. However, the
    manual processes have been persistent and difficult to shake. These processes
    are a classic example of building in the cloud versus building for the cloud.
    It can be challenging to relinquish control of these business procedures to automation.
    However, unless we utilize the advantages that the cloud provides, we fail to
    recognize the actual efficiencies of this shift. A good approach that would have
    allowed for much faster cycle times and reduced new customer entry barriers is
    shifting to self-serve, on-demand onboarding, using a cloud factory approach,
    as we will see in more detail later in the chapter. Similar techniques were adopted
    in their future cloud native applications, built from the ground up. However,
    this brings us to a new anti-pattern, having the attitude that “we’ll build it
    right this time.”
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾经在一家公司工作，该公司有一个现有的本地解决方案。这个本地解决方案通过远程部署分发给客户。客户提供一台机器，特定的入驻团队登录到该机器上，并运行一个脚本来设置应用程序。这样的迁移思维一直延续到了他们提供的*云原生*托管产品中。入驻团队在云中配置了一个新的实例和数据库，然后有人安装了应用程序，客户访问云实例。这一过程是该公司在云中提供服务的首次尝试。然而，手动流程一直存在，且难以改变。这些流程是经典的“在云中构建”与“为云构建”之间的区别。放弃对这些业务流程的控制，交给自动化，可能是一个挑战。然而，除非我们利用云所提供的优势，否则就无法真正认识到这一转变的实际效率。一个能够更快速地推进周期并降低新客户进入门槛的好方法是转向自助、按需入驻，采用云工厂方法，正如我们在本章稍后的部分会详细介绍的那样。在他们未来构建的云原生应用中，也采纳了类似的技术，且这些应用是从零开始构建的。然而，这也引出了一个新的反模式，即持有“这次我们要把它做对”的心态。
- en: The myth of “We’ll build it right this time”
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “这次我们要把它做对”的迷思
- en: One of the anti-patterns we often see is software teams wanting to burn everything
    to the ground and start again to become cloud native. This all-or-nothing approach
    not only fragments your business into **legacy** (your original application) and
    **greenfield** (your brand-new application) development but also means that you
    neglect a product that customers are using to work on a product that will likely
    not have users until at least parity with your on-premises solution. The timelines
    of these projects are often wildly underestimated and require reskilling and re-resourcing
    to get the cloud native skills you need. The all-or-nothing approach frequently
    means that critical decisions around your application and its architecture are
    made upfront at the point in time when your organization likely has the least
    cloud experience on hand!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常看到的一个反模式是软件团队希望摧毁一切并从头开始，以便变得云原生。这种非黑即白的方法不仅将你的业务分割成**遗留系统**（你的原始应用）和**全新领域**（你全新的应用）开发，还意味着你忽视了客户正在使用的产品，而投入到一个可能在至少达到与本地解决方案相当的功能前，不会有用户的产品上。这些项目的时间表通常被严重低估，并且需要重新培训和重新配备资源，以获得你所需的云原生技能。这种非黑即白的方法通常意味着在你组织最缺乏云经验的时刻，围绕你的应用程序及其架构的关键决策会在前期做出！
- en: When shifting to the cloud, AWS has the 7Rs of migration strategies to use,
    which we went through in [*Chapter 2*](B22364_02.xhtml#_idTextAnchor055). To refresh
    your memory, these are refactor, replatform, repurchase, rehost, relocate, retain,
    and retire.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在转向云时，AWS 提供了7种迁移策略，如我们在[*第2章*](B22364_02.xhtml#_idTextAnchor055)中讨论的那样。为了帮助你回忆，这些策略包括：重构、重新平台、重新购买、重新托管、重新迁移、保留和退休。
- en: You’ll notice that *rebuild* is not one of the options. To take full advantage
    of cloud native services in an existing application, we must choose an option
    that will eventually lead us down the refactor path. The easiest way to start
    is to build a cloud factory for our existing application.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，*重建*并不是其中的一个选项。为了在现有应用中充分利用云原生服务，我们必须选择一个最终将我们引导到重构路径的选项。开始的最简单方法是为我们的现有应用构建一个云工厂。
- en: Cloud factories
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云工厂
- en: The lift-and-shift part of a cloud migration is unavoidable. Running the existing
    application in the cloud is the first step to migrating it to become cloud native.
    When deploying an on-premises application, there is a significant lead time, as
    it involves hardware provided by the customer, with customer-controlled access
    and rollouts with manual steps. As discussed in our earlier example, a common
    anti-pattern in this space reflects that process in the cloud environment. Customers
    use different **firewalls**, **hypervisors**, **hardware**, and **security** in
    an on-premises environment. The rollout process typically requires manual intervention
    to deal with the idiosyncrasies of the particular client.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 云迁移的搬迁和移位是不可避免的。在云中运行现有应用是将其迁移到云原生的第一步。在部署本地应用程序时，由于涉及客户提供的硬件，客户控制的访问以及手动步骤的部署，这会有显著的前置时间。正如我们之前的例子中所讨论的，这个领域中的一个常见反模式反映了云环境中的这一过程。客户在本地环境中使用不同的**防火墙**、**虚拟化程序**、**硬件**和**安全**。部署过程通常需要手动干预，以处理特定客户的独特性。
- en: 'When deploying in a cloud environment, we get to specify these options ourselves.
    We say how big our VM is, how we configure our firewall and networking, or what
    operating system we use. Instead of multiple unique customer environments, we’re
    deploying the same cloud environment multiple times, meaning all the quirks are
    identical for each implementation case. We can now automate the provisioning workflow
    with certainty, reducing onboarding from a process that might take weeks with
    multiple client contacts to a process that can run in a pipeline and might take
    30 minutes. Creating a cloud factory for your application is a crucial first step
    for migrating on-premises applications to the cloud without rearchitecting to
    a multitenant model. We will delve deeper into this in [*Chapter 12*](B22364_12.xhtml#_idTextAnchor320).
    As we start to transition our application to the cloud, the question still remains:
    how will we refactor this while retaining the end functionality? The answer is
    through the use of the strangler fig pattern.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在云环境中部署时，我们可以自行指定这些选项。我们可以决定我们的虚拟机的大小、如何配置防火墙和网络，或者使用哪种操作系统。与多个独特的客户环境不同，我们多次部署相同的云环境，这意味着所有的怪癖对每个实施案例都是相同的。现在我们可以确信地自动化提供工作流程，从可能需要多个客户联系几周的过程，减少到可以在管道中运行并可能需要30分钟的过程。为您的应用程序创建一个云工厂是将本地应用程序迁移到云端的关键第一步，而无需重新设计为多租户模型。我们将在[*第12章*](B22364_12.xhtml#_idTextAnchor320)中深入探讨这一点。当我们开始将应用程序转移到云上时，一个问题仍然存在：我们如何在保留最终功能的同时重构它？答案是通过使用攀缘榕模式。
- en: Cloud native through the strangler fig pattern
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过吸收枯死树上的养分，攀援榕树的云原生
- en: A strangler fig is a plant that grows on a host tree. Sometimes, the host tree
    dies, leaving only the strangler fig. The **strangler fig pattern**, coined by
    Martin Fowler, is similar. It lets us take our existing applications and make
    them cloud native by slow degrees, eventually replacing our legacy solution altogether.
    Through this mechanism of action, we also allow for the deferral of system-wide
    architectural decisions until later in the process, once the cloud maturity of
    our organization has improved. The first stage of cloud migration is to take our
    existing application to the cloud – that is, rehost. You can also technically
    take this approach without the rehost phase and instead redirect traffic to our
    on-premises instance, although this requires additional networking and solid authorization
    strategies to be in place. This simple transition is depicted in *Figure 7**.1*.
    We start with an on-premises instance and replace it with a cloud instance. The
    switch is transparent to the end user.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 攀缘榕是一种生长在宿主树上的植物。有时，宿主树会死去，只留下攀缘榕。由马丁·福勒提出的**攀缘榕模式**类似。它让我们逐步将现有的应用程序变成云原生，最终完全替代我们的传统解决方案。通过这种行动机制，我们还允许将系统范围的架构决策推迟到后续阶段，一旦我们组织的云成熟度得到改善。云迁移的第一阶段是将现有应用程序带入云端
    - 也就是说，重新托管。您也可以在没有重新托管阶段的情况下技术上采取这种方法，而是将流量重定向到我们的本地实例，尽管这需要额外的网络设置和可靠的授权策略。这种简单的过渡在*图7**.1*中有所体现。我们从一个本地实例开始，然后用云实例替换它。这个切换对最终用户是透明的。
- en: '![Figure 7.1 – Initial migration of an application from on-premises to the
    cloud](img/B22364_07_1.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – 从本地到云的初始应用迁移](img/B22364_07_1.jpg)'
- en: Figure 7.1 – Initial migration of an application from on-premises to the cloud
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 从本地到云的初始应用迁移
- en: By completing this stage, we have already achieved some efficiencies; provisioning
    is faster by removing the dependency on physical hardware and utilizing cloud
    factories, colocation costs have disappeared, and we mitigate the operational
    overhead of disparate systems. However, we’re not cloud native; the overheads
    around OS patching and database maintenance still exist, and we’re still operating
    in a manner that matches our infrastructure topology to our customer base.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过完成这个阶段，我们已经实现了一些效率；通过消除对物理硬件的依赖并利用云工厂，部署速度变得更快，合租成本消失了，我们也减少了分散系统的操作负担。然而，我们并非云原生；操作系统补丁和数据库维护等负担仍然存在，我们仍在以一种将基础设施拓扑与客户群体匹配的方式进行操作。
- en: The next stage of our migration is a simple but critical phase that supports
    the future iterations of our application. We need to add an **API proxy layer**.
    All hyperscalers have a managed service that performs this function; in AWS, it
    is **API Gateway**, Azure has **API Management**, and GCP has **Apigee** and **API
    Gateway**. Some open source projects provide similar functionality for specific
    environments, such as **Kubernetes**. The key here is that we are introducing
    a layer between our end user and our application that can perform **Layer 7 routing**
    as defined in the OSI model. This model will allow us to inspect incoming traffic
    and decide actions based on HTTP request properties. In contrast to the architecture
    in *Figure 7**.1*, we now have an additional architectural element, the API proxy,
    which is once again transparent to the end user.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迁移的下一个阶段是一个简单但关键的阶段，支持我们应用程序的未来版本。我们需要添加一个**API 代理层**。所有的大型云服务商都有提供这种功能的托管服务；在
    AWS 中，它是**API Gateway**，Azure 有**API Management**，而 GCP 有**Apigee**和**API Gateway**。一些开源项目也为特定环境提供类似的功能，例如**Kubernetes**。这里的关键是，我们在终端用户和我们的应用程序之间引入了一个层，它可以执行
    OSI 模型中定义的**第七层路由**。该模型将允许我们检查传入的流量，并根据 HTTP 请求属性决定操作。与*图 7.1*中的架构相比，我们现在增加了一个架构元素，即
    API 代理，它对终端用户仍然是透明的。
- en: '![Figure 7.2 – Addition of an API proxy to the cloud instance](img/B22364_07_2.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 向云实例添加 API 代理](img/B22364_07_2.jpg)'
- en: Figure 7.2 – Addition of an API proxy to the cloud instance
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 向云实例添加 API 代理
- en: Functionally, we have yet to start using the API layer’s capabilities to their
    full extent, but we have achieved some operational efficiencies as part of this
    change. If we were using **Transport Layer Security** (**TLS**), we would likely
    have a provisioned TLS certificate. Switching to a fully managed proxy allows
    the TLS termination to occur at the proxy layer, freeing us from the operational
    overhead of managing this with a manual or semi-automated process. The key is
    that our application is no longer bound to our deployed instance. Typically, we
    build on-premises applications using a monolithic architecture, as the deployment
    of these applications is tightly coupled to the topology of the hardware we deploy
    them on. In the cloud, these limitations no longer constrain us. It is detrimental
    to the ability of development teams to operate in this environment. Using the
    monolith architecture usually results in high internal coupling between components,
    making it difficult to predict the blast radius of a particular change without
    knowing the full scope of its use throughout the application.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从功能上讲，我们还没有充分利用 API 层的能力，但我们已经通过这次变更实现了一些操作效率。如果我们使用**传输层安全性**（**TLS**），我们可能会有一个预配的
    TLS 证书。切换到完全托管的代理允许 TLS 终止在代理层进行，从而使我们摆脱了通过手动或半自动化过程管理 TLS 的操作负担。关键在于我们的应用程序不再与已部署的实例绑定。通常，我们使用单体架构构建本地应用程序，因为这些应用程序的部署与我们部署它们的硬件拓扑密切耦合。在云环境中，这些限制不再约束我们。这对开发团队在这种环境中的操作能力是有害的。使用单体架构通常会导致组件之间的高内聚，使得在不了解其在整个应用中的使用范围的情况下，很难预测某个变更的影响范围。
- en: 'The solution is to use the Layer 7 routing capabilities of the API proxy to
    decompose our application into new cloud native implementations. For example,
    many applications have a user management system so users can log in to the application.
    Traditionally, someone might achieve this by storing passwords in a database,
    ideally, hashed and salted. This approach is a definite source of risk for most
    companies. **Insecure hash algorithms**, **timing leaks**, and **password database
    security** are all things your company is directly responsible for under this
    model. By migrating this to a managed service, we significantly de-risk our operations.
    We can also make changes at this stage to make our solution more cloud native
    through replatforming some of the easier migrations, such as databases, to a compatible
    managed database service. Continuing our architectural evolution, we break down
    the monolithic application into components in the following figure:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是使用 API 代理的第七层路由功能，将我们的应用解构为新的云原生实现。例如，许多应用都拥有一个用户管理系统，供用户登录应用。传统上，有人可能通过将密码存储在数据库中来实现这一点，理想情况下是经过哈希处理和加盐的。这种方法对大多数公司来说是一个明确的风险来源。**不安全的哈希算法**、**时序泄漏**和**密码数据库安全**，在这种模型下，都是贵公司直接负责的事项。通过将其迁移到托管服务，我们显著降低了运营风险。我们还可以在此阶段进行更改，通过将一些较为简单的迁移（例如数据库）重新平台化到兼容的托管数据库服务中，从而使我们的解决方案更加云原生。继续我们的架构演进，我们将单体应用拆解为下图所示的组件：
- en: '![](img/B22364_07_3.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22364_07_3.jpg)'
- en: Figure 7.3 – Beginning to decompose the monolith into domain-driven microservices
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 开始将单体应用解构为领域驱动的微服务
- en: Under this new architecture, we have separated concerns for our user management
    and our newly replatformed application monolith. Our user service provides an
    abstraction behind our API proxy for performing actions such as resetting passwords,
    updating email addresses, and other user-centric functions. At the same time,
    our original monolithic application still contains all the functionality external
    to users. We’ve managed to refactor one part of our application to be truly cloud
    native and use managed services. Most importantly, we don’t need to know how the
    entire application will be architected to achieve this. We need to understand
    this particular domain and the services available to accelerate it. We’ve also
    broken any coupling that may have existed between the user service and unrelated
    parts of the application. Under this new model, changes to the user service have
    a blast radius limited to the service itself without unforeseen side effects on
    the rest of the application.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个新架构下，我们将用户管理和我们重新平台化的应用单体进行了分离。我们的用户服务通过 API 代理提供一个抽象层，用于执行如重置密码、更新电子邮件地址以及其他以用户为中心的功能。同时，我们原本的单体应用仍然包含所有用户外部的功能。我们已经成功地将应用的一个部分重构为真正的云原生，并使用了托管服务。最重要的是，我们不需要了解整个应用的架构设计就能实现这一点。我们只需要理解这个特定领域以及可用的服务来加速它的实现。我们还打破了用户服务和应用中不相关部分之间可能存在的耦合。在这种新模型下，对用户服务的变更只会影响服务本身，而不会对应用的其余部分产生意外副作用。
- en: 'In some simple cases, we may only have two targets for the API proxy: the new
    cloud native service and the old legacy service. However, as you perform this
    method of replacing or migrating functionality, it is also worth reevaluating
    your architecture and seeing whether you can reduce coupling within your application
    or increase cohesion within a specific domain by breaking out disparate services.
    Rarely, the perfect solution to a problem requiring refactoring to become cloud
    native is to build a cloud native monolith.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些简单的情况下，我们可能只有两个 API 代理目标：新的云原生服务和旧的遗留服务。然而，在执行这种替代或迁移功能的方法时，同样值得重新评估您的架构，看看您是否可以减少应用内的耦合，或通过拆分不同的服务来增加特定领域的内聚性。很少情况下，要求进行重构以成为云原生的完美解决方案是构建一个云原生的单体应用。
- en: Slowly, we can continue to break down the service into its emergent domains.
    We establish bounded contexts within our application, representing highly cohesive
    parts of our business context. For more information on bounded contexts and domain-driven
    design, I recommend reading *Domain Driven Design* by Eric Evans. We then decompose
    our architecture into these domains and look to utilize cloud native services
    wherever possible. As a part of this shift, if our application supports multiple
    customers, we can also build multitenancy into these services. Eventually, we
    will reach a point where we have integrated the entire application into a series
    of cloud native services backed by managed services that provide equivalent or
    improved functionality. As the final step in our architectural evolution, we have
    removed the monolith and left only the new application services. This is reflected
    in *Figure 7**.4*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 渐进地，我们可以继续将服务分解为其新兴领域。我们在应用程序内建立了有界上下文，代表了我们业务上高度内聚的部分。关于有界上下文和领域驱动设计的更多信息，我推荐阅读*《领域驱动设计》*，作者是埃里克·埃文斯。然后我们将我们的架构分解为这些领域，并尽可能利用云原生服务。作为这一转变的一部分，如果我们的应用程序支持多个客户，我们还可以将多租户性能构建到这些服务中。最终，我们将达到一个整合了整个应用程序的一系列云原生服务的阶段，这些服务由提供等效或改进功能的托管服务支持。作为我们架构演变的最后一步，我们已经移除了单体，并且只留下了新的应用程序服务。这在*图
    7**.4*中有所体现。
- en: '![](img/B22364_07_4.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22364_07_4.jpg)'
- en: Figure 7.4 – The original monolith is deprecated and is truly cloud native
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – 原始的单体应用程序已被弃用，真正实现了云原生
- en: 'By using the API proxy to slowly and methodically decompose the monolith, we
    have effectively accomplished the desired result: removing the legacy monolith
    and adopting cloud native services. At this point, it is possible to remove the
    API proxy; however, in most cases, the application proxy still provides benefits
    by acting as a central entry point to your application.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用API代理来缓慢而系统地分解单体，我们有效地实现了期望的结果：移除遗留的单体并采用云原生服务。在这一点上，可以移除API代理；然而，在大多数情况下，应用程序代理仍然通过作为应用程序的中心入口点提供好处。
- en: We have examined typical anti-patterns in the initial cloud migration, including
    unproductive migration strategies such as one-and-done migration or retirement
    and rebuilding. We have also explored how the strangler fig pattern allows us
    to keep servicing our current clients while modernizing our application. Now,
    we have a path to becoming cloud native that does not require broad sweeping solutions
    all at once but can be part of a longer-term digital transformation focusing on
    client outcomes rather than technological puritanism.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经审查了初始云迁移中的典型反模式，包括一次性迁移或退役重建等低效迁移策略。我们还探讨了榕树模式如何允许我们在现代化应用程序的同时继续为当前客户提供服务。现在，我们有了一条通往云原生的路径，不需要一次性广泛的解决方案，而是可以作为长期数字转型的一部分，重点放在客户结果上，而不是技术上的纯主义。
- en: Now that we have dived into the migrations of existing applications, we can
    start to look at how the applications themselves are constructed to be cloud native.
    The first stop on this journey is addressing where we store the state for our
    applications.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经深入研究了现有应用程序的迁移，我们可以开始看看应用程序本身如何构建成为云原生。这个旅程的第一站是解决我们应用程序状态存储的位置。
- en: Stateful applications
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有状态应用程序
- en: Most applications are driven by a series of **stateful processes** at their
    core. These states might be **ephemeral** – that is, they might not be data with
    long-term context, such as a user session that is only active while the user is
    on a website. In other scenarios, we might persist these states for longer-term
    storage. For example, an online store might require maintaining the state of a
    shopping cart, collecting payment, and shipping the items. These are all states
    that need to be persisted in our architecture somewhere. In a single server model,
    conflating the system’s local and external state is trivial. In this section,
    we will look into the scalability and robustness of these patterns to examine
    how we can manage the state cloud natively.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数应用程序的核心是一系列**有状态进程**。这些状态可能是**短暂的** – 也就是说，它们可能不是具有长期上下文的数据，例如用户会话仅在用户访问网站时处于活动状态。在其他情况下，我们可能会将这些状态持久化以供长期存储。例如，网上商店可能需要维护购物车状态，收集付款并发货商品的状态。这些都是需要在我们的架构中某处持久化的状态。在单服务器模型中，混合系统的本地和外部状态是微不足道的。在本节中，我们将研究这些模式的可伸缩性和健壮性，以查看如何以云原生方式管理状态。
- en: The stateless server
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无状态服务器
- en: A common anti-pattern when building cloud native applications is to store state
    locally to a server. Most cloud services have options, like **session affinity**,
    to enable you to migrate applications to the cloud with a locally stored state.
    However, we should refrain from using these patterns in new or refactored cloud
    native applications. Two main patterns allow us to achieve this in the cloud.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 构建云原生应用程序时，一个常见的反模式是将状态存储在服务器本地。大多数云服务提供选项，如**会话亲和性**，使您可以将应用程序迁移到云中，并在本地存储状态。然而，我们应该避免在新的或重构的云原生应用程序中使用这些模式。两种主要模式可以帮助我们在云中实现这一目标。
- en: In the **state assertion pattern**, the client presents a **verifiable state
    representation** to the backend server. We typically use this pattern for transient
    state, the quintessential example of which is replacing user session tokens, which
    we can match to the ephemeral state stored on the machine, with a user session
    assertion like a **JSON Web Token** (**JWT**) or **Security Assertion Markup Language**
    (**SAML**) response. In both cases, the client stores their state, and we can
    verify that the client’s state has not been altered through cryptographically
    secure signatures. This pattern comes with some caveats, for instance, the fact
    that these tokens (unless encrypted) are transparent to the end user, so we should
    never include secret information that we don’t want the user to see in the assertion.
    They are also prime targets for token theft, so good practices around **token
    lifetimes**, TLS, and **storage of the tokens** on the client’s device are all
    paramount with this pattern.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在**状态声明模式**中，客户端向后端服务器呈现**可验证的状态表示**。我们通常使用这种模式来处理短暂状态，其典型示例是替换用户会话令牌，我们可以将其与存储在机器上的短暂状态匹配，并使用用户会话声明，如**JSON
    Web Token**（**JWT**）或**安全声明标记语言**（**SAML**）响应。在这两种情况下，客户端存储他们的状态，并且我们可以通过加密安全的签名来验证客户端的状态未被篡改。此模式有一些注意事项，例如，除非加密，否则这些令牌对最终用户是透明的，因此我们绝不应在声明中包含我们不希望用户看到的秘密信息。它们也是令牌窃取的主要目标，因此在使用此模式时，**令牌生命周期**、TLS和**令牌存储**在客户端设备上的良好实践至关重要。
- en: The second pattern is using **external state storage**. If the data we are handling
    is not transient and requires use by multiple parties, then we must persist the
    state to storage external to the server. The type of data being stored decides
    how we store it on the backend, too. The key here is to move the state out of
    our application, which provides numerous benefits in the world of the cloud.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种模式是使用**外部状态存储**。如果我们处理的数据不是短暂的，并且需要由多个方使用，那么我们必须将状态持久化到服务器外部的存储中。存储的数据类型决定了我们如何在后端存储它。关键在于将状态移出我们的应用程序，这在云计算世界中提供了诸多好处。
- en: We typically encounter three kinds of state data. Of course, there are always
    exceptions and edge cases, but as a general rule, we can choose external state
    storage suitable for our use case.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常会遇到三种类型的状态数据。当然，总会有例外和边界情况，但作为一般规则，我们可以选择适合我们用例的外部状态存储。
- en: '**Transient state data** is data that represents a substate of a system at
    a point in time, but it is inconsequential if the data gets deleted. This might
    be because the data itself is a cache of other data sources that can be reconstructed
    or because the nature of the data is transient anyway, for example, **short-lived
    session tokens**. Typically, we store this data because we require it at short
    notice. Think of it like your short-term memory. It holds values that you are
    currently actively working with but might be replaced at any point. Cloud services
    have solutions tailored toward high-performance workloads and can be leveraged
    for more cost-effective solutions. For high-performance workloads, we can use
    services like **ElastiCache** in AWS, **Memorystore** in GCP, or **Azure Cache**
    in Azure; these all mirror the concept of traditional deployed cache services.
    Other emerging solutions in the space, like **Momento**, allow for cache as a
    service. If latency is not mission-critical, other proprietary solutions might
    be more cost-effective and scalable with only minimal impact on latency, for example,
    TTLs on DynamoDB (a NoSQL service from AWS) tables or even fully SaaS solutions
    such as Momento. The critical difference from the self-managed paradigm is that
    these services are managed, and all have options to be automatically scalable,
    allowing us to focus on those parts of our application that deliver value, our
    domain logic.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**瞬态状态数据**是指在某一时刻表示系统子状态的数据，但如果数据被删除，它不会带来不良后果。这可能是因为数据本身是其他数据源的缓存，可以重新构建，或者因为数据本身的性质是短暂的，例如**短生命周期会话令牌**。通常，我们存储这些数据是因为我们需要它在短时间内可用。可以把它想象成你的短期记忆，它存储着你当前正在积极使用的值，但随时可能被替换。云服务提供了专门针对高性能工作负载的解决方案，可以用于更具成本效益的方案。对于高性能工作负载，我们可以使用
    AWS 的 **ElastiCache**、GCP 的 **Memorystore** 或 Azure 的 **Azure Cache** 等服务；这些服务都体现了传统部署缓存服务的概念。该领域的其他新兴解决方案，如
    **Momento**，提供了缓存即服务。如果延迟不是关键因素，其他专有解决方案可能在成本效益和可扩展性方面更具优势，仅对延迟造成最小影响，例如 DynamoDB（AWS
    的 NoSQL 服务）表中的 TTL，甚至像 Momento 这样的完全 SaaS 解决方案。与自管理范式的关键区别在于，这些服务是受管的，并且都提供自动可扩展的选项，使我们可以专注于应用程序中带来价值的部分，即我们的领域逻辑。'
- en: '**Persistent state data** is data the system needs a persistent reference to
    with context in a semantic model. These might be items such as orders we want
    to keep a log of or bank accounts for which we want to maintain a balance. The
    way in which we store this data can have different modalities, such as **relational**
    versus **non-relational**, **normalized** versus **denormalized**, or **structured**
    versus **unstructured**. Typically, these representations of state can be thought
    of as records that might be akin to our long-term memory. At the time of writing,
    this is an exciting space, as there are leaps and bounds of progress being made
    in the serverless offerings for relational databases like **Aurora Serverless**
    on AWS or **Cloud Spanner** on GCP. For non-relational databases, most cloud providers
    have well-established, truly serverless offerings (truly serverless in the way
    that they scale to zero). AWS has **DynamoDB**, Azure has **Cosmos DB**, and GCP
    has **Cloud Firestore**.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**持久状态数据**是指系统需要在语义模型的上下文中持久化引用的数据。这些数据可能是我们希望记录的订单，或者是我们希望保持余额的银行账户。我们存储这些数据的方式可能有不同的模式，比如**关系型**与**非关系型**、**规范化**与**反规范化**，或者**结构化**与**非结构化**。通常，这些状态的表示可以看作是类似于我们长期记忆的记录。在写作本文时，这是一个令人兴奋的领域，因为关系数据库的无服务器服务，如
    AWS 的 **Aurora Serverless** 或 GCP 的 **Cloud Spanner**，正在取得巨大进展。对于非关系型数据库，大多数云提供商都拥有真正的无服务器服务（在它们能够扩展到零的方式上）。AWS
    提供了 **DynamoDB**，Azure 提供了 **Cosmos DB**，而 GCP 提供了 **Cloud Firestore**。'
- en: '**Supporting data** is typically data that has little meaning without the context
    of persistent data. This might be data like photos, PDF documents, or other types
    of files that we want to store because it provides additional information. The
    difference between persistent and supporting data is that supporting data can
    be thought of as an object rather than a record. This distinction is also reflected
    in the way the services are named, usually referred to as blob or application
    stores. AWS has **S3**, GCP has **Cloud Storage**, and Azure has **Azure Blob
    Storage**. Once again, all of these are managed services, and their throughput
    and capacity will scale with our requirements.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持数据**通常是指没有持久化数据上下文的情况下几乎没有意义的数据。这些可能是像照片、PDF文档或其他类型的文件，我们希望存储它们，因为它们提供了附加的信息。持久化数据和支持数据的区别在于，支持数据可以被视为对象，而不是记录。这一区别也反映在服务命名的方式上，通常称为Blob或应用程序存储。AWS有**S3**，GCP有**Cloud
    Storage**，Azure有**Azure Blob Storage**。再一次，这些都是托管服务，它们的吞吐量和容量会随着我们的需求进行扩展。'
- en: The question is, when do we commit state to an external service? The general
    rule of thumb is that any state that requires persistence beyond one transaction
    should be committed to external state management. The local state is fine within
    the context of the transaction for processing purposes, but the external state
    is necessary for anything breaking this boundary. A parallel we can draw, which
    we have all likely suffered with in the past, is a multi-page web form, where
    every time you submit a value that is incorrect, it forgets the previous pages
    and takes you back to page one. That is the risk we run with local state that
    crosses translation boundaries.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是，我们什么时候需要将状态提交到外部服务？一般的经验法则是，任何需要在一次事务之外持久化的状态都应该提交到外部状态管理系统。局部状态在事务处理过程中是可以接受的，但对于任何突破这一边界的情况，外部状态是必需的。我们可以举一个大家可能都曾遇到过的例子，那就是多页网页表单，每次提交一个错误的值时，它会忘记之前的页面并把你带回第一页。这就是我们在使用跨越转换边界的本地状态时所面临的风险。
- en: 'These data types are the most common when serving Online Transaction Processing
    (OLTP) workloads. The storage and consumption patterns are different when serving
    analytical (OLAP) workloads. When analytical functionality is required, persisting
    data to an analytical store purpose-built for your use case is usually recommended,
    such as a data warehouse. Each of the hyperscalers has slightly different approaches
    in this space: GCP has the fully managed serverless solution **BigQuery**, AWS
    has **Redshift**, and Azure has **Azure Synapse**. This area also has significant
    contenders outside of the hyperscalers, like **Snowflake** and **Databricks**.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据类型在处理在线事务处理（OLTP）工作负载时最为常见。在处理分析型（OLAP）工作负载时，存储和消费模式有所不同。当需要分析功能时，通常建议将数据持久化到一个专为你的使用场景构建的分析存储中，例如数据仓库。每个超大规模云服务商在这一领域有稍微不同的方法：GCP有完全托管的无服务器解决方案**BigQuery**，AWS有**Redshift**，Azure有**Azure
    Synapse**。在这些超大规模云服务商之外，也有一些重要的竞争者，如**Snowflake**和**Databricks**。
- en: Now that we’ve discussed removing the state from the local server, let’s explore
    the new possibilities for resiliency and scalability this opens for us in a cloud
    native environment.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了如何从本地服务器中移除状态，接下来让我们探索在云原生环境中，这种变化为我们带来的弹性和可扩展性的全新可能性。
- en: Resiliency and scalability in a stateless server paradigm
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无状态服务器范式中的弹性和可扩展性
- en: Werner Vogels, the CTO of AWS, once mentioned that “*Everything fails, all the
    time*.” If we persist state locally to our server, then that state is only as
    durable as that single server. Large companies, such as hyperscalers, employ legions
    of engineers to ensure their applications are durable, available, and bug-free.
    Most people embarking on a cloud native transformation won’t have access to the
    same level of resourcing that these large companies do. This is where the **stateless
    cloud paradigm** allows us to trade on margin by using managed services to store
    our state. These managed services do have legions of engineers behind them. If
    we persist state external to our application, suddenly, the fault tolerance of
    our application becomes less consequential.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: AWS的首席技术官**沃纳·福格尔斯**曾提到过“*一切都会失败，永远都会失败*。”如果我们将状态保存在本地服务器上，那么该状态的持久性就与单一服务器的可靠性一样。像超大规模公司这样的企业，雇佣了大量工程师来确保他们的应用程序具备持久性、可用性，并且没有漏洞。而大多数进行云原生转型的人无法获得这些大公司所拥有的资源。这就是**无状态云范式**发挥作用的地方，它使我们能够通过使用托管服务来存储状态，从而在边际上进行交易。这些托管服务背后有大量工程师。如果我们将状态持久化到应用程序外部，突然之间，应用程序的容错能力变得不那么重要了。
- en: Server died? Start another one and investigate the cause. Our state was off
    the server, so it doesn’t matter whether the server went down. Our new server
    will pick right up where the old one left off. Even better, run multiple stateless
    instances of your server in a self-healing group. Cloud services also allow us
    to automate this part of our system. AWS uses **Auto Scaling groups** and **Elastic
    Load Balancing**. GCP has managed instance groups for VMs or **Cloud Run/Google
    Kubernetes Engine** for containers, as well as **load balancers** to distribute
    traffic. Azure uses **Virtual Machine Scale Sets** and **Azure App Service** to
    a similar effect. All these services allow us to mitigate the risk of single-point
    failures in our system for the parts of the cloud that are our responsibility
    and typically contain the most bugs. It’s important to note that managing the
    state does not even need to be a process we entrust to our own code; we can go
    even further and use a fully managed state as a service.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器崩溃了？启动另一个服务器并调查原因。我们的状态不在服务器上，所以服务器是否宕机不再重要。我们的新服务器会从上一个服务器停止的地方继续运行。更好的是，在自我修复组中运行多个无状态实例的服务器。云服务还允许我们自动化这部分系统。AWS使用**自动扩展组**和**弹性负载均衡**。GCP为虚拟机提供了托管实例组，或者为容器提供了**Cloud
    Run/Google Kubernetes Engine**，以及**负载均衡器**来分配流量。Azure使用**虚拟机规模集**和**Azure应用服务**来实现类似效果。所有这些服务都使我们能够减轻单点故障的风险，尤其是在我们负责的云端部分，这些部分通常包含最多的漏洞。需要注意的是，管理状态甚至不需要是我们自己代码中的一个过程；我们可以更进一步，使用完全托管的状态作为服务。
- en: State as a service
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 状态即服务
- en: Typically, we build state machines to replicate business processes. For example,
    we might onboard a new tenant in a distributed microservice architecture system.
    In the past, I have seen people build complex code in poorly documented and typically
    fragile ways. For example, a central tenant service called out to each of the
    microservices to orchestrate them, but this tenant service was touched by every
    team that needed onboarding actions to be performed. The result was unbound states
    and error-prone onboarding that resulted in a wide array of edge cases, with no
    one easily able to grasp the full complexity of the system.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们构建状态机来复制业务流程。例如，我们可能会在分布式微服务架构系统中为新租户进行注册。过去，我曾见过一些人以复杂且通常脆弱的方式编写代码，且文档不足。例如，一个名为中央租户服务的服务调用每个微服务进行编排，但这个租户服务会被每个需要执行入驻操作的团队触及。结果是无穷无尽的状态和容易出错的入驻流程，导致出现大量边缘案例，没有人能够轻松掌握系统的全部复杂性。
- en: We want a state machine that tells us if the requested action has been completed.
    Here is where managed services can also be of benefit. Solutions such as **AWS
    Step Functions**, **Google Workflows**, or **Azure Logic Apps** allow us to outsource
    the maintenance of the state to the cloud itself. This is an excellent solution
    for when centralized orchestration is required. In our previous example, we want
    to onboard a tenant, so we make a state machine that creates a new tenant in the
    tenant service, provisions a new user as the admin in the user service, and sends
    an email to that user to log in. Once the user has accepted the invitation, there
    may be more stages, such as provisioning new data for the tenant, prompting the
    admin to add other users, or setting up retention policies on user files.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望有一个状态机，告诉我们请求的操作是否已完成。这里，托管服务也可以带来好处。像**AWS Step Functions**、**Google Workflows**或**Azure
    Logic Apps**这样的解决方案可以将状态的维护外包给云端自身。这是当需要集中式编排时的一个优秀解决方案。在我们之前的示例中，我们希望为一个租户进行入驻，因此我们创建一个状态机，在租户服务中创建一个新租户，在用户服务中为该租户创建一个新的管理员用户，并向该用户发送一封电子邮件，邀请其登录。一旦用户接受邀请，可能会有更多的阶段，例如为租户配置新的数据，提示管理员添加其他用户，或为用户文件设置保留策略。
- en: We could do this in a distributed way with eventing and service-specific state,
    but typically, that results in unbound and undocumented behavior without appropriate
    oversight. The state machine as a service approach also allows us a single pane
    of glass to view our state machine structure and how various instances of state
    are progressing through it. When the tenant onboarding system breaks, we can immediately
    see where the error is by viewing our well-defined state machine.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过事件和特定服务的状态以分布式方式进行管理，但通常这会导致没有适当监管的无边界、未记录的行为。将状态机作为服务的方法还可以让我们在一个单一的界面中查看状态机结构，以及各个状态实例如何在其中进展。当租户入驻系统出现故障时，我们可以通过查看定义良好的状态机立即看到错误发生的位置。
- en: The anti-pattern we typically see in this system is people using state machines
    for systems that do not cross bounded contexts (i.e., they don’t require orchestration).
    In these scenarios, we should instead rely on state representation internal to
    the bounded context, such as updating an order item from “ordered” to “packed”
    and then to “shipped.” The state transitions in this scenario are simple, linear,
    and within a bounded context. Hence, external state orchestration is not required.
    The final piece of the state puzzle is configuring our applications.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个系统中通常看到的反模式是人们将状态机用于那些不跨越边界上下文的系统（即它们不需要编排）。在这些情况下，我们应该依赖于边界上下文内部的状态表示，例如将订单项从“已订购”更新为“已打包”，然后到“已发货”。在这种情况下，状态转换简单、线性且位于边界上下文内。因此，不需要外部状态编排。状态谜题的最后一部分是配置我们的应用程序。
- en: Application configuration as state
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将应用配置视为状态
- en: 'Fundamentally, our application behavior is an emergent property of our application
    state filtered through our business logic. The anti-pattern here is defining application
    configuration in the same code we use to define our business logic. Application
    configuration is just another form of state, one that typically differs between
    deployed environments. Our code should be agnostic of the environment it is deployed
    in, instead, configuration should be managed through deployment itself. There
    are two places we typically store application configuration:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，我们的应用行为是应用状态通过业务逻辑过滤后的一个自发属性。这里的反模式是将应用配置定义在我们用来定义业务逻辑的相同代码中。应用配置只是另一种形式的状态，通常在不同的部署环境中有所不同。我们的代码应该对其部署的环境保持中立，相反，配置应通过部署本身进行管理。我们通常会将应用配置存储在两个地方：
- en: Externally in a key-value store or secret manager. We touched on this approach
    in [*Chapter 5*](B22364_05.xhtml#_idTextAnchor136) for feature flags.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在外部的键值存储或密钥管理器中。我们在[*第5章*](B22364_05.xhtml#_idTextAnchor136)中提到了这种方法，用于特性标志。
- en: Internally in the template used to create new instances of our application,
    like through environment variables. This is typically for bootstrapping values,
    such as service discovery endpoints or database connection strings.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在用于创建我们应用程序新实例的模板内部，如通过环境变量。这通常用于引导值，比如服务发现端点或数据库连接字符串。
- en: 'The difference between the local state in the configuration domain and the
    local state in the transaction domain is that the state in the configuration domain
    must satisfy two criteria to be effective:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 配置域中的本地状态与事务域中的本地状态之间的区别在于，配置域中的状态必须满足两个标准才能生效：
- en: It must be immutable; the configuration must not change due to external factors
    except for the service’s redeployment
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须是不可变的；配置不能由于外部因素变化，除非是服务的重新部署。
- en: It must be universal; all copies of the application must be provisioned with
    identical copies of local state
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它必须是通用的；所有应用的副本必须提供相同的本地状态副本。
- en: These two paradigms ensure that our transactions are agnostic of the actual
    backend service completing the request. In the external case, we have a little
    more flexibility but need to be careful of the effects of rotation and cache invalidation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种范式确保我们的事务与完成请求的实际后端服务无关。在外部情况下，我们有更多的灵活性，但需要小心轮换和缓存失效的影响。
- en: State allows our application to provide meaning through the lens of our business
    logic. However, improperly handled state can cause issues with resilience and
    scalability. Luckily, in the cloud landscape, there are many battle-tested tools
    that provide ways for us to store our application state. We can even shift our
    state machines entirely to the cloud with cloud native offerings while also reducing
    operational complexity to a minimum. While state is the lifeblood of our application,
    the health and malleability of our code are normally measured through two other
    properties; coupling and cohesion.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 状态允许我们的应用通过业务逻辑的视角提供意义。然而，不当处理的状态可能会导致弹性和可扩展性问题。幸运的是，在云环境中，有许多经过验证的工具可以帮助我们存储应用状态。我们甚至可以将我们的状态机完全迁移到云端，利用云原生服务，同时将运营复杂度降到最低。尽管状态是我们应用的命脉，但我们代码的健康性和可变性通常通过另外两个属性来衡量：耦合性和内聚性。
- en: Tight coupling, low cohesion
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 紧密耦合，低内聚
- en: In software design, two measures of interrelatedness are often used as a litmus
    test for sound system design. These are **coupling** and **cohesion**. Coupling
    refers to disparate services calling each other to accomplish a task. High coupling
    implies that the services are heavily interdependent and are challenging to operate
    in isolation without worrying about dependencies or side effects. Cohesion is
    the opposite. Coupling measures the relationships between services, and cohesion
    focuses on the relationships inside the service. If a service has low cohesion,
    it tries to do many disparate things simultaneously. We commonly see low cohesion
    and high coupling as an anti-pattern in cloud native software development. In
    this section, we will explore how these anti-patterns tend to be reflected in
    cloud environments and how to avoid them.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件设计中，两个常用的衡量相互关联性的指标常作为健全系统设计的试金石。这些是**耦合**和**内聚**。耦合指的是不同服务之间互相调用以完成任务。高耦合意味着服务之间高度依赖，且在没有担心依赖关系或副作用的情况下，单独操作这些服务会变得很困难。内聚正好相反。耦合衡量服务之间的关系，而内聚关注的是服务内部的关系。如果一个服务的内聚性差，它会试图同时做许多不同的事情。我们常常看到在云原生软件开发中，低内聚和高耦合作为一种反模式。在本节中，我们将探讨这些反模式如何在云环境中体现，并讨论如何避免它们。
- en: The Lambdalith versus single-purpose functions
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lambdalith与单一功能函数
- en: A common anti-pattern we see is low cohesion in deployed infrastructure. Typically,
    this anti-pattern gets introduced through siloed infrastructure teams; for information
    on why this might be a lousy idea, see [*Chapter 5*](B22364_05.xhtml#_idTextAnchor136).
    Let’s assume we have a serverless function on AWS, a **Lambda function**, and
    every time we want a new one, we need a sign-off from the infrastructure team
    to create a new function for us rather than being empowered to create a new Lambda
    function ourselves. Then, we get a feature that should only take a day to implement
    but should really be a serverless function. Rather than wait for the infrastructure
    team to deal with their backlog of tickets and provide us with our function, we
    see a tantalizing preexisting Lambda function that, if we just added some extra
    routing, could also handle this other functionality. Compound this effect over
    many features, and suddenly, we end up with a significant monolithic serverless
    function. Hence the moniker, the **Lambdalith**. The problem is that these serverless
    functions have low cohesion. This means that by modifying our function, we have
    a large blast radius that could impact utterly unrelated functionality simply
    due to process inefficiencies and siloed ownership.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: I previously worked with an organization that had an architecture team separate
    from the infrastructure and development teams. Creating a service required the
    interaction of three teams and was aligned to a monthly cadence. This particular
    organization had teams aligned to business domains; each business domain typically
    had a few services they managed. While feature development was rapid, the event
    of a new service being added to support those features was exceedingly rare. These
    containers grew to significant complexity with low cohesion between application
    parts. **Conway’s law** was alive and well, and the architecture closely followed
    the team topologies to a fault.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: In any process, be it online sales or provisioning new infrastructure, the more
    difficult this process is, the less likely it will be completed. Typically, people
    ask how much friction is suitable to ensure we still produce secure, deployable
    artifacts. The answer almost always is as little as humanly possible. We should
    enable teams to take ownership of their own output by providing them with a safe
    and secure platform in which they can achieve their goals. Infrastructure and
    architectural resources should be available to support them at all points. However,
    if the development team cannot drive the process, you will find that the process
    will be woefully underutilized.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: The truly cloud native antithesis of the Lambdalith is the single-purpose serverless
    function. In this pattern, each function does exactly one thing and does it well.
    For example, a `POST` method on a specific API endpoint. This does not mean it
    cannot share code with other single-purpose functions. Typically, grouping these
    functions into pseudoservices with high internal cohesion makes sense. However,
    each deployed function should be completely agnostic of its peers in the pseudoservice
    group. This grouping might be performed by having several single-purpose functions
    deployed from the same repo (or parent folder if using a monorepo). This pattern
    provides us with high cohesion in our deployed units. Each unit is only concerned
    with satisfying the requirements for a single type of request. There is a limit
    to the level of atomicity to which we should break these units down. Namely, they
    should never be so atomic that we must chain multiple together.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Chaining serverless functions
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another anti-pattern we commonly see is the chaining of serverless functions
    in the call stack. This form of coupling can have an extremely negative effect
    on your solution’s performance and cost-effectiveness. For example, consider a
    serverless function that uses a typical synchronous **backend for frontend** (**BFF**)
    approach to call some business logic in another serverless function that queries
    a database. This situation is illustrated in the following figure.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22364_07_5.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Chained invocations of serverless functions
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the figure, each preceding call runs whilst waiting for the
    subsequent call to complete. With this invocation pattern, we are doubling our
    running compute. In a containerized or VM-level environment, this is not an issue,
    as our compute resource can serve other requests while we wait for the chained
    call to finish. However, in a serverless function environment, our function can
    only serve one invocation at a time. This means that while we wait for the second
    serverless function in the chain to complete, our first lambda function cannot
    serve any other requests. Therefore, we are doubling our computing costs and resource
    consumption without any tangible benefit. Some cloud providers, such as GCP, are
    building platforms that allow this unused computing power to be better utilized.
    However, most default implementations are limited to completing a single request
    at a time. Chained functions are a prime example of coupling that can be converted
    to high cohesion internally in a single function. We more often need to perform
    the reverse operation and decouple coupled services.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Decoupling coupled services
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we call services as dependencies from another service, we increase the
    blast radius of changes to the service being depended on to include our dependent
    service. This is a form of tight coupling that can be very detrimental to the
    performance of our application. The more services we chain together, the less
    reliable our service becomes, as we are now dealing with the product of the reliabilities
    of each service in the chain. Let’s say each service has a 95% reliability rate.
    If we combine 4 services in a single call, our reliability decreases to 81.4%
    (0.95^4). Typically, this problem arises as it fits our mental model of services
    very well. As programmers, when we need to perform some work internal to our application,
    we call a function and await the results. Extending this model to a multiservice
    architecture, we call another service and await the results.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从另一个服务调用服务作为依赖项时，我们会增加对被依赖服务的更改爆炸范围，涵盖我们的依赖服务。这是一种紧耦合形式，可能对应用程序的性能造成很大影响。我们将服务链接在一起的数量越多，我们的服务就越不可靠，因为我们现在处理的是每个服务在链中的可靠性乘积。假设每个服务的可靠性为
    95%。如果我们将 4 个服务组合在一次调用中，我们的可靠性将下降到 81.4%（0.95^4）。通常，这个问题出现是因为它非常符合我们对服务的心理模型。作为程序员，当我们需要在应用程序内部执行某些工作时，我们调用一个函数并等待结果。将这个模型扩展到多服务架构时，我们调用另一个服务并等待结果。
- en: 'Luckily, cloud providers have a cloud native way to solve this tight coupling
    problem. It requires two changes in thinking to implement correctly:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，云服务提供商有一种云原生方式来解决这种紧耦合问题。正确实现这一点需要两种思维方式的转变：
- en: We need to break the idea of synchronous feedback. Sending an HTTP `202` return
    code and performing work asynchronously is just as, if not more, valid than a
    synchronous response with an HTTP `200` response all in one call.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要打破同步反馈的观念。发送一个 HTTP `202` 返回代码并异步执行工作，和同步响应 HTTP `200` 返回代码的单次调用一样有效，甚至可能更有效。
- en: We need to stop thinking of each of the services that need to work as dependents
    and start thinking of them as isolated units of work that need to be completed.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要停止将每个需要运行的服务视为依赖项，而是开始将它们看作是需要完成的独立工作单元。
- en: The key to implementing these solutions in a cloud native environment is to
    decouple these services by putting a managed service in the middle. AWS has **Simple
    Queue Service** and **EventBridge**, GCP has **Google Pub/Sub**, and Azure has
    **Azure Event Grid** and **Azure** **Service Bus**.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在云原生环境中实施这些解决方案的关键是通过在中间放置一个托管服务来解耦这些服务。AWS 提供了**简单队列服务**（Simple Queue Service）和**EventBridge**，GCP
    提供了**Google Pub/Sub**，而 Azure 则有**Azure Event Grid**和**Azure Service Bus**。
- en: These managed services all provide similar functionality. They act as a message
    broker between our services so that our services do not need to talk to one another
    synchronously to pass information between them. They differ slightly in how they
    operate. Some are **simple message queues**, and others are **complete event bus
    implementations** with publish and subscribe functionality.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这些托管服务提供了类似的功能。它们充当我们的服务之间的消息代理，这样我们的服务就不需要同步通信来传递信息。它们在操作方式上略有不同。有些是**简单消息队列**，而有些是带有发布和订阅功能的**完整事件总线实现**。
- en: The result of using any of these services is similar. Instead of our reliability
    now being the result of a series product, we have decoupled the services to concern
    themselves with the reliability of the managed service. Let’s take our four unreliable
    services and attach them to our managed service, allowing for asynchronous execution.
    Assuming our managed service has four 9s of uptime (99.99% uptime), our result
    is four services, each with 95.98% reliability. If any of our services goes down,
    the other services will still operate.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些服务的结果是类似的。我们的可靠性不再是由一系列产品的结果，而是通过解耦服务来将服务的可靠性交由托管服务负责。我们来看看四个不可靠的服务并将它们连接到托管服务上，实现异步执行。假设我们的托管服务具有四个
    9 的正常运行时间（99.99% 的正常运行时间），那么我们的结果是四个服务，每个服务具有 95.98% 的可靠性。如果其中任何一个服务出现故障，其他服务仍将继续运行。
- en: 'Implementing **dead letter queues** (**DLQs**) can further improve the reliability
    of these services. If one of our services cannot process messages, we can send
    the backlog of messages to be processed to the DLQ. Once we have fixed our service
    and everything is operational, we can automatically replay the events from our
    DLQ and complete the outstanding work. This means that instead of a single service
    failure impacting all systems, the blast radius of a single system is limited
    to the system itself. The system will eventually be consistent once all unprocessed
    messages have been replayed. When we need to eventually trace these events through
    our system, perhaps to troubleshoot why they ended up in our DLQ, we need to correlate
    their path, which brings us to an essential part of distributed systems: telemetry
    and event correlation.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Telemetry and event correlation
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can’t improve what you cannot measure. Understanding precisely the degree
    of coupling within a deployed application can be challenging. Typically, we come
    across an anti-pattern using traditional logging systems with distributed systems.
    Traditional logging systems do not provide the granularity (level of detail) and
    traceability (correlation with other messages) required to debug and improve distributed
    systems. Typically, when we debug a distributed system, we are trying to piece
    together the result of an action across multiple deployed units. This is where
    robust **telemetry** comes into play. We can tag all of our requests, messages,
    and invocations with a **correlation ID** on entry into our distributed system,
    and then use this correlation ID to trace the effect of that action across all
    of our deployed units and managed services. We will go into more detail on telemetry
    systems in [*Chapter 10*](B22364_10.xhtml#_idTextAnchor270). However, we can utilize
    the correlation aspect of modern telemetry systems to assist us in decoupling
    applications. By following our traces, we can reveal dependencies between systems
    that previously would have required us to look into the source code or environmental
    configuration to find. Once we identify the dependencies within our application,
    we can slowly move from tightly coupled dependencies (one service calling another)
    to loosely coupled dependencies (two or more services joined by a shared, managed
    message bus or queue).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '**Tight coupling** and **low cohesion** are anti-patterns we are typically
    shielded from in an on-premises environment. In the cloud, these patterns become
    dysfunctional, leading to poorly performing applications and unexpected side effects.
    The key to rectifying these anti-patterns is, firstly, to be able to measure the
    coupling and cohesion, and, secondly, to work to decouple tightly coupled services
    while increasing internal cohesion. Typically, modeling cohesion and coupling
    should be part of the architectural planning for a feature and form part of the
    definition of done. Let’s explore some common pitfalls and address the comprehensive
    definition of done.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: The comprehensive definition of done
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When creating software in **siloed release models**, as discussed in [*Chapter
    5*](B22364_05.xhtml#_idTextAnchor136), we looked at empowering teams to own the
    delivery of their outputs from conception to deployment and beyond into operations.
    However, this requires the development team to also take ownership (with support
    from other teams) of the functionality and responsibilities that the siloed release
    pipeline previously hid from the team on the path to production. Hence, we need
    to revisit the definition of *done* (and, in some cases, the definition of *ready*)
    for our software teams. Previously we have visited the cultural and business shift
    required to make this happen, but in this section, we will discuss building these
    requirements intrinsically into your definition of done.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring security
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Security** is a critical factor in the delivery pipeline. Neglecting sound
    security practices can lead to a gradual accumulation of risk for the company,
    often unnoticed until a breach occurs. This omission can result in a blame game
    and severe consequences. To develop secure applications, it’s crucial to integrate
    several security practices into the **software delivery life cycle** (**SDLC**).
    These practices should be part of the definition of done for any work, and their
    review should be as rigorous as code review before deployment.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring **open source** or **external dependencies** is an anti-pattern. In
    the software world, many open source packages provide base functionality on which
    we build our business logic. However, each package we pull from an external source
    represents a possible vector for malicious code to be added to our application.
    Maintaining and alerting on a **software bill of materials** (**SBoM**) gives
    you an indication of the health of your project. Many tools exist to help you
    manage the versions of software packages used. A typical pattern for managing
    dependencies at a language level is to use a read-through private artifact repository
    for your language, populating this artifact registry with internal packages to
    be used and allowing it to pull and cache upstream packages. This repository will
    enable you to have a single pane of glass containing all dependencies and versions
    of your application, GCP, AWS, Azure, and many niche players, all of which can
    export and monitor SBoMs from their respective artifact repository services. Pull
    requests should be instrumented to ensure that the packages they add do not add
    any new vulnerabilities, and maintenance should be done regularly, informed by
    the SBoM, to address any new vulnerabilities that have been found.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Not having a **threat model** for your application or building one and ignoring
    it is an anti-pattern. Typically, when we see the shift from a dedicated security
    team to a supported and empowered development team, the development team uses
    the security team to produce a threat model but fails to address it throughout
    the SDLC. The preliminary threat model should form part of the definition of ready
    for the team. The threat model should be fundamental in deciding how to tackle
    a problem and must be verified to ensure the built solution correctly mitigates
    the identified risks. Thus, the threat model should be a living document as a
    change is implemented, providing details on how risks are mitigated so that the
    changes can be merged confidently. Once in production, the application should
    be monitored through a **cloud native application protection platform** (**CNAPP**)
    to catch any risks or misconfigurations that might not be addressed by the threat
    model. The key to effective threat modeling is to choose the correct level of
    granularity. If you are a stock market making changes to the settlement engine,
    then the proper level of granularity might be every merge. Other lower-risk environments
    might only require threat modeling on a less granular level. The idea is to find
    the correct amount of friction that mitigates the risk to the proper level without
    compromising on the necessary level of security for your application.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: The final *ignoring* security anti-pattern to address is born out of the increased
    flexibility the cloud gives us, and that is the failure to address defense in
    depth. In an on-premises environment, the delineation between what is inside the
    network and what is outside the network is evident. You have a physical cable
    going to a firewall that serves as the ingress point for all your traffic. Your
    solution might have some software-defined networking downstream, but there is
    a clear separation. In the cloud environment, all of a sudden, different services
    run in **virtual private clouds** (**VPCs**) and outside VPCs. Endpoints can be
    addressable over the internet or through endpoint projections into your network.
    Some services exist in **cloud provider-managed networks** and require additional
    networking. All of this means that it is less clear where traffic is flowing.
    There is tooling to help with this but, fundamentally, the highly configurable
    nature of cloud environments means that misconfigurations can present a larger
    risk surface. Managed cloud services already have strong **identity and access
    management** (**IAM**) tooling. This should be complemented with robust, **zero-trust
    authentication and authorization** tooling in your code that is validated at every
    application level. Many organizations are still early in their journey of implementing
    zero-trust architecture. Hence, it should be considered a North Star principle
    rather than an absolute requirement. The key is asking yourself, “*What happens
    if we accidentally expose this service to the internet directly?*” This limits
    the blast radius of cloud misconfigurations and ensures that if an internal service
    is accidentally exposed to the public, it still authorizes incoming traffic. This
    blast radius consideration also needs to be considered from a CI/CD perspective.
    One client I worked with had a single repository and project for all infrastructure.
    This resulted in highly privileged CI/CD accounts with enormous blast radii spanning
    multiple disparate systems. Having a robust defense-in-depth strategy means that
    as application architecture shifts to more of a self-serve model, the platform
    that our developers are building on top of is secure enough to tolerate failures
    at each level. Just as we must ensure our developers are building secure platforms,
    we must also ensure we are building observable ones.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring observability
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the monolith, logging stages to the console was enough to debug our application.
    This worked because the application was a simple arrangement (infrastructure)
    of complex objects (our code). In the cloud native world, we shift much of that
    complexity into the infrastructure, giving us a complex arrangement (infrastructure)
    of simple objects (our code). This requires much more robust **logging** and **telemetry
    practices** than logging into a console. We will dive into this topic in significantly
    more detail in [*Chapter 10*](B22364_10.xhtml#_idTextAnchor270). However, we will
    go through some aspects in this section that should form the basis of the definition
    of done.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: The first anti-pattern is ignoring spans and only using logging. Logging provides
    us with point-in-time information about the state of our application. Spans are
    different. They provide us with context for a period of execution in our application.
    As part of our definition of done, we should include the addition of spans that
    provide meaningful information about executing subsections of our code. Throughout
    the execution of the span, we should also ensure that we are adding enough enriching
    data to make the diagnosis of issues easier through our observability platform.
    For any deployment that exceeds the scope of a single instance, we must also consider
    correlation to allow us to group spans together and trace their path through our
    distributed application. Trying to piece together the execution context of a request
    from a series of log entries across multiple services is significantly more difficult
    than reading a correlated span flame graph.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'The second anti-pattern is collecting metrics with no functional output. We
    quite often see a company collecting many metrics but no alerting or deviation
    monitoring. We have the data to check whether our application is performing as
    intended. However, we are missing that crucial step that actually tells us when
    it isn’t. With comprehensive monitoring, alerting, and rectification procedures,
    we can ensure that our system’s non-functional requirements, such as latency and
    error percentage, do not fall outside of acceptable margins. Therefore, as part
    of our definition of done, we should ensure two things:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we must ensure that the changes being made have monitoring set up.
    This might be through synthetic traffic, **application performance monitoring**
    (**APM**), observability tooling, and traditional logging.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, we must ensure that the right people are notified when this tooling
    detects a problem. This might be done by automatically creating a ticket in your
    ticketing system, notifying users on a messaging channel, or other means. The
    important thing is that regressions are identified, and people know rectification
    must occur.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By including these two items in our definition of done, we can provide certainty
    that as we add new functionality or modify existing functionality, we don’t breach
    the non-functional requirements of the system. This level of observability also
    gives us insight into which parts of our applications are candidates for optimization
    as part of our continuous improvement process. Previously, for clients where users
    complained about the slowness of the application, we filtered our metrics to rank
    requests by two factors: how often they were called and how long the typical transaction
    took. We found that three endpoints were consistently called and consistently
    slow. With some query optimization, we reduced the response time by two orders
    of magnitude. The change took about three days in total, and the end users were
    significantly happier. Without collecting these metrics and utilizing their outputs,
    we would have needed significant testing in a production environment to get the
    same level of insight. Observability is great for finding the cause of an incident
    (i.e. when something goes wrong) but what about stopping incidents from occurring
    in the first place?'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring reliability
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final part of this section discusses ignoring **reliability**. This is an
    anti-pattern that we see all too often in cloud migrations. Teams care about having
    their features work without considering their continued operation. This is where
    the mentality of *You build it, you run it* can be beneficial. Development teams
    that also own the operation of their output are more likely to consider reliability
    because they are invested and want to avoid call-outs at nighttime or during weekends.
    Cloud native services provide significant tooling to ensure reliability and continuity
    of service. However, utilizing these services can mean the difference between
    an outage of seconds and an outage of days. Any company that wishes to conform
    to internal or external **service-level objectives** (**SLOs**) or has a contractual
    **service-level agreement** (**SLA**) must ensure that they treat reliability
    as a critical aspect of their definition of done.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'The first anti-pattern we will address is an aspect of the deployment process.
    As we discussed in [*Chapter 5*](B22364_05.xhtml#_idTextAnchor136), development
    teams should own the deployment and operation of their changes. The anti-pattern
    we often see in this space utilizes the same deployment strategy across our environments.
    In a development or test environment, it is typical for us to use **all-or-nothing**
    deployment strategies. This strategy is sound when we want to guarantee that the
    version of the code we are calling is the latest version and maintain fast feedback
    loops between the deployment and testing cycles. Applying this same methodology
    to a production environment means that if our change breaks functionality, the
    change either breaks everything or nothing. We might even have avoidable downtime
    on a successful deployment as the new services might take time to come online.
    For production systems, we care about two things: **early feedback** on a problem
    and **quick rectification** of a problem. Many cloud native deployment approaches
    will allow us to make incremental or quickly revertable changes to preserve our
    system’s operation, especially when using highly managed services such as API
    gateways or functions as a service. These strategies usually come at the cost
    of additional time to deploy or additional resources provision. They also normally
    require external state management, as any internal state will be lost on deployment.
    Some of the methods we can use are the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '**Rolling deployments**: These deployments take a set of resources running
    the same application (say, a set of three containers that might all be running
    our user service) and then incrementally update each one in series until all services
    are running the new version, waiting for each service to become healthy before
    starting to deploy the next. This allows us to mitigate the avoidable downtime
    that comes with waiting for services to become ready-to-serve traffic in an all-or-nothing
    approach but does not provide us robust options for returning the application
    to a good state in the event of a failure that is only present at runtime.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blue-green deployment**: In this strategy, you have two separate groups of
    resources. One set of resources serves your production traffic using the latest
    known production stable version of the application (blue) while the other is deployed
    (green). After you have ensured that the newly deployed system is working, you
    cut across to the new system, which might be through **aliases** or **internal
    DNS**. You can then decommission the old blue target resources. In the event of
    a failure, it is trivial to point the references, DNS or otherwise, back to the
    old deployment of the application.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Canary deployment**: In this strategy, you follow the same deployment methodology
    as the blue-green deployment strategy. The critical difference is cutting over
    from the blue to green resources. Instead of an instantaneous cutover, we slowly
    redirect some traffic to our new instances. This becomes our canary in the coal
    mine; we can test the services with a subset of production data, and if something
    goes wrong, we will only impact a small subset of requests instead of all requests.
    Otherwise, if all is well, we progress to all traffic heading to the new resources,
    and the old resources can be decommissioned.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These methodologies do not need to be applied only to your application code;
    this pattern can be used anywhere you have an expensive rollout process. One client
    I worked with had a database that had to be updated once a month. Each month,
    the data used to build the database was either modified or appended. Ingestion
    of the new data and verifying that it was correct took 15 minutes, and the client
    could not tolerate 15 minutes of downtime. Hence, we created two tables: one for
    the most recent data and one for last month’s data. Each time new data needed
    to be ingested, we would populate whichever table contained the oldest data with
    the latest data. We would then check this table against the current table in use.
    If all was well, we would update the view consumed by the end users to point to
    the table containing the new data. This allowed a seamless transition between
    datasets without taking the system offline and allowed quick fallbacks to the
    last known good configuration if there was an issue. Understanding which deployment
    strategy suits your purposes is essential, and selecting an appropriate deployment
    strategy needs to form part of the definition of done.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'The second reliability anti-pattern we will examine is the failure to address
    **disaster recovery** correctly. Cloud services have sensible defaults to prevent
    data loss events, such as storing objects in multiple regions or automating database
    backup processes. This process is usually tunable to meet your **recovery point
    objective** (**RPO**) – that is, how much data we can tolerate the loss of. Despite
    how protective cloud services are against data loss events, protection against
    service loss events is usually heavily dependent on your architecture. The critical
    metric data loss prevention does not address is the **recovery time objective**
    (**RTO**). Restoring a database from a backup may take a significant amount of
    time. Likewise, standing up a new instance of your infrastructure may not be a
    short process. If your application catastrophically fails, then having a plan
    in place to restore service to your end users is extremely valuable. The first
    mistake teams generally make in this space is creating one copy of their infrastructure,
    calling it a day, and then moving on with new features. In this scenario, disaster
    recovery has been completely ignored. In the event of a catastrophic failure,
    not only will the team be scrambling to recreate their service but there’s no
    defined process to do so. The second scenario we commonly see is people having
    a theoretical disaster recovery strategy. They have a list of steps to take in
    case of a failure, but if the strategy is theoretical, so are the chances of it
    actually working. An untested strategy is a waste of keystrokes. Any disaster
    recovery strategy needs to be simulated regularly. The time to test it for the
    first time (and likely the first time much of the team sees the strategy) should
    not be when there is a critical outage. Typically, disaster recovery has a few
    options; the key is that all options must be tested. The possibilities we typically
    look at for recovery are as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '**Cold recovery**: This strategy is for non-critical services. A cold recovery
    assumes you are starting from nothing, provisioning a new instance of your application,
    and restoring from backups to restore service. It is important to note that not
    having a disaster recovery plan is not the same as having a cold recovery plan.
    Like all plans, cold recovery must be documented and tested regularly to ensure
    the process meets your RPOs and RTOs.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Warm recovery**: This strategy involves having a second (or more) minimal
    copy of your application running in a different location that can be quickly scaled
    up to take over from the service if it fails. Ideally, this **failover** and **scale-up**
    would be automated, but while automation is being built, it is perfectly acceptable
    to manually fail-over. An alternative architecture to warm standby that uses the
    same principles involves keeping the supporting structures of your application
    running, however, only starting your application when failover is required. This
    variation on the strategy is commonly referred to as the **pilot** **light strategy**.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hot recovery**: This strategy involves running your application in a multi-active
    architecture. Much like we can run multiple servers to ensure that we can tolerate
    the failure of any single server, this pattern takes the same approach but with
    your entire architecture. The failure of any active deployment means that traffic
    can be redirected to the healthy region.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concept of **chaos engineering** is important to illustrate here. Remember
    the quote by Werner Vogels, “*Everything fails, all the time*.” Chaos engineering
    reinforces this by purposely introducing failures into your system, ensuring that
    your system is fault-tolerant. Another good strategy to use is the concept of
    game days, especially for manual processes. These simulated events run through
    the disaster recovery strategy with the responsible team to ensure that everyone
    is familiar with the process. Therefore, as each feature or service is completed,
    the disaster recovery strategy must be updated to include the requirements of
    the new changes and needs to form part of the definition of done.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Security, observability, and reliability are intrinsic parts of changes to our
    system that are often ignored. By addressing these intrinsics as part of our definition
    of done, we ensure that our development teams are not just building applications
    that are built to exhibit the features they are creating but also providing a
    platform that our end users can trust. These parts of our system form a fundamental
    baseline of cloud native operability, but there are many other pitfalls we can
    fall victim to.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Other pitfalls
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several anti-patterns that commonly manifest in cloud native application
    development. This section will dissect some of these anti-patterns, their lineage
    from traditional software development, how to identify them, and the proactive
    mindset shifts required to evade them. In our scenario, cloud native applications
    have the capability to scale to any size we choose, sparking fascinating interactions
    between our software and the potential solutions to our problems. By understanding
    these anti-patterns and adopting a proactive mindset, we can empower ourselves
    to make informed decisions and avoid potential pitfalls.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Solving cloud native problems without cloud native experience
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I was working with a customer trying to migrate their existing data structures
    into an **OpenSearch** cluster. We had well-defined schemas into which the data
    had to be marshaled. The problem, however, was that the client attempted to copy
    their relational data structures directly across to OpenSearch with no denormalization
    in between. This meant that to marshal the data, we needed to perform multiple
    lookups to fetch related data structures. These lookups created a situation in
    which a single request for a model could balloon out to thousands of downstream
    requests for all of its associated data. Despite our continued protests that the
    data structures needed to be denormalized or migrated to a high-performance, read-only
    copy of the relational database, the client wanted to preserve the system’s *flexibility*
    by retaining the original relational shape in a non-relational datastore. We implemented
    many improvements to push the model as far as possible, including batching requests
    and local caching for repeated values. However, some requests were simply too
    deeply nested to optimize. The solution initially proposed by the client was to
    scale the cluster, so the client scaled the cluster until more performance bottlenecks
    were hit, and then the client scaled the cluster again. We had an interesting
    call with the cloud provider. They informed the client that they were provisioning
    more infrastructure than the cloud provider had provisioned for some subsidiary
    services. This is the first anti-pattern we would like to address. The easy access
    to virtually unlimited cloud resources comes with the temptation to solve performance
    problems by throwing more resources at it, and the resulting cloud bill will scale
    equally as quickly. We should often look inward at our application instead of
    outwardly at the infrastructure it is running on to solve problems around application
    performance. Scaling our infrastructure vertically to solve performance issues
    will only take us so far. This indicates that an alternative specialized solution
    may be required, your service has low cohesion, or your application is poorly
    optimized.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings us to the second anti-pattern, which can also result in the first
    anti-pattern. This pattern typically starts with someone responsible for a cloud
    native service coming across a staged architecture online with many pretty icons
    and boxes and then trying to shoehorn that architecture into their use case. Our
    architecture should be informed by the requirements of the application code we
    need to write rather than the code we write conforming to some architecture. The
    cause of this can be multifaceted. A common driver for this anti-pattern is what
    we typically refer to as resume-driven development. This occurs when someone is
    more concerned about getting experience with a particular technology than about
    that technology’s potential to solve the problem. Staged architectures can form
    a good starting point for potential solutions and often illustrate best practices.
    However, we must temper these architectures, considering their suitability across
    various factors. Typically, before adopting a staged architecture verbatim, we
    should ask ourselves some questions like the following:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Do we operate at the scale for which this staged architecture solves a problem?
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we have the internal skill set to implement and maintain it?
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does this model follow our standard architecture practices or will it be unique?
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can we make any changes to ensure this architecture more accurately solves our
    problem?
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third anti-pattern we will address is manually changing deployed infrastructure
    or code bases outside our CI/CD pipeline. A typical example might be that our
    application runs a query that takes a little while to complete in production.
    So, the developer logs into production and quickly adds an index to the lookup
    column, and the problem is solved. Despite the compounding of errors that need
    to occur to allow the developer to make this change, fundamentally, we are introducing
    instability into our application. This concept is known as **environmental drift**.
    Our code and deployment pipelines define a model that does not correlate with
    what is deployed. In our example, we looked at the developer making changes to
    production, which means the first time that all of our subsequent changes are
    tested with this environmental drift is when those changes hit our production
    environment. It also causes an issue when we need to recreate our infrastructure;
    by circumventing our source model, we will create the same issue whenever we try
    to create a new instance of our infrastructure. The solution to this problem is
    relatively simple; development teams should not be able to change a non-ephemeral
    environment without following their CI/CD process. If they want to prototype a
    fix or conduct a technical spike that would be accelerated by having write access
    to the environment, then create a sandbox that can be destroyed once the work
    is done. This way, you prevent the accumulation of tests and quick fixes in any
    environments on the path to production. Ideally, these lower environments should
    be as close to the production environment as possible. On the topic of production
    environments, we must be careful about how we scale our code in reaction to real-world
    events.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Suffering from success – the downside of limitless scale
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have an upper bound for our application’s throughput when working with on-premises
    infrastructure. Eventually, we will run out of system resources to serve requests.
    In a cloud environment, we often see the same thinking come into play – an anti-pattern
    where rate limits and service limits are ignored. The consequences of neglecting
    rate limits, service limits, or throttling are significantly higher in the cloud.
    Rather than being capped by our infrastructure, we have a virtually unlimited
    pool of resources to scale into. Suppose we combine this lack of physical limits
    with stateless servers that can interchangeably serve any request, irrespective
    of any service-level partitioning that we might have. In that case, we can scale
    to meet our customer’s needs very rapidly and virtually limitlessly. In this scenario,
    we must set artificial caps on using our service. How these limits are partitioned
    (i.e., by user, tenant, customer, etc.) is up to the implementer. We set rational
    limits for using our theoretically limitless service to control runaway costs
    and ensure that we don’t impact services for any other clients. Many cloud native
    managed services already have built-in functionality that we can use to perform
    rate-limiting, usage monitoring, and licensing applications. Commonly, this is
    applied at the **API aggregation layer**, such as in AWS **API Gateway**, Azure
    **APIM**, or GCP **API Gateway**. Luckily, these same API keys can be used as
    part of our authentication strategy, for example, tying a request to a client
    to enable defense in depth checking that our API key matches the tenant we’re
    calling. As the complexity of our application grows, we might require custom authorization
    and rate-limiting logic on our APIs. For example, AWS allows you to add custom
    authorization to API Gateway through Lambda functions. Other niche API proxy players
    like **Apigee** (now acquired by Google) and **Kong** allow for complex logic
    through a comprehensive policy language.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: In the on-premises monolith, things tended to fail together. Was our server
    overloaded or not? It’s a question with a relatively simple answer. In the cloud
    native world, where we have services built up of many components, things tend
    to fail piecemeal. We need to be tolerant of these faults, but we also need to
    be aware that the scales the cloud lets us operate at can lead to some interesting
    behaviors. The next anti-pattern we will address is using bad timeout and retry
    practices, especially in the context of concurrent executions. Let’s assume we
    have a process that needs to load CSV files into a database and a service that
    processes a single file from these buckets as they arrive. Let’s assume our clients
    upstream, who deliver our files into the S3 bucket for us to consume, realize
    that they had an error in their system and haven’t uploaded files for the last
    three days. That’s fine; they have added all the files. Let’s assume we have a
    naive architecture that sends a request to an HTTP endpoint to pull the file for
    processing using S3 events and SNS. If we’ve ignored the consequences of concurrent
    execution, we could suddenly begin ingesting a large amount of data simultaneously.
    This puts an enormous load on the database we are loading the files into. If we
    don’t have timeouts configured for these processes, we could end up completely
    overloading our database. Therefore, all calls in our application code must have
    a timeout, and the expiration of those timeouts must be handled gracefully, cleaning
    up any work in progress that they have requested.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: So, if a timeout fails, then what next? A naive response might be that we simply
    need to retry the request. If the failure results from factors other than an overloaded
    system, and these errors are rare, then we can probably get away with this approach.
    However, it’s important to note that retries are compounding the issue; we are
    requesting more server time to solve our problem. If the system is already overloaded,
    then this just compounds the effect as old requests being retried are now also
    competing with new requests. A common tactic here is an exponential backoff algorithm,
    although it is advisable to cap your maximum retry period and the total number
    of retries. This can work; however, once your server gets overloaded, a whole
    bunch of calls are going to fail, and if all these calls are retried using the
    same algorithm, then all we’ve done is kick the can down the road, and we will
    overload the server on the next retry.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important aspect of retry behavior is the concept of **jitter**. We
    introduce **randomness** into our retry behavior to prevent a stampeding herd
    situation. We also need to be aware of the multiplicative effect of retries. Suppose
    our service makes calls that go three layers deep, and each service retries five
    times. In that case, the downstream system will receive 53 retries or 125 requests,
    which is the opposite of the behavior we want when downstream services are overloaded.
    Luckily, there are three effortless ways to avoid this situation:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Decouple spiky and expensive traffic with message queues, then scale your downstream
    services based on queue depth
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use cloud provider SDKs where possible, as these will already have retry behaviors
    built into them
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use managed services, as these typically scale easier and have built-in retry
    and rate-limiting functionality you don’t need to build yourself
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This brings us to our last anti-pattern, using implicit properties of ephemeral
    resources for hardcoded dependency mapping.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding implicit ephemeral specification
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When writing code, especially **infrastructure as code** (**IaC**), we can easily
    fall into the anti-pattern of using direct specifications for partially ephemeral
    resources. An ephemeral specification, for example, would be applying an IaC configuration
    that outputs the IP address of an instance, then referring to the first configuration
    instance by directly using that IP address in another IaC configuration. If we
    change the first configuration, the IP address might change, but our ephemeral
    specification has created a hard dependency between them. Instead, we should use
    resources that aren’t ephemeral, such as DNS entries that can be updated. This
    is the simplest form of service discovery. There are robust, full-featured service
    discovery platforms that extend this functionality for various cloud providers
    and deployment configurations. Ideally, any dependencies between our infrastructure
    should be explicit rather than implicit through hardcoded values to make our deployments
    truly agnostic of the state of the deployed environment.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have now explored some common anti-patterns we see when shifting our application
    logic to the cloud. Our application code is typically the value differentiator
    or competitive advantage in our business, so we can move it to the cloud and,
    by doing so, increase its availability, resilience, and performance. Now that
    we understand the implications of running our application code in the cloud, how
    can we store all our data? This is what we will dive into in the next chapter.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
