- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding the Basics of CI/CD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Continuous integration and continuous delivery** (**CI/CD**) is a key strategy
    of DevOps release management. It automates the majority of manual human intervention
    that would traditionally be needed in order to produce a new software release
    or get new code into production. CI/CD comprises the integration tests, unit tests,
    regression tests, and the build and deploy phases. Infrastructure as code can
    be integrated into the CI/CD process too, automating the provisioning of cloud
    infrastructure, but can also include provisioning on-premises virtual infrastructure.
    With CI/CD pipelines, software development teams can make changes to code that
    are then automatically tested, pushed out for delivery, and deployed in any environment.
    As you can infer, CI/CD dramatically reduces downtime, ensuring that releases
    happen far quicker, are consistent from release to release, and occur much more
    frequently as well. Radical!'
  prefs: []
  type: TYPE_NORMAL
- en: You can tailor pipelines to accomplish all kinds of tasks, even if they have
    nothing to do with releasing software. This could include generating reports for
    the business unit, turning off unused infrastructure during off-peak hours and
    starting them again before the next workday, refreshing development databases
    with data from production, performing automated penetration tests against network
    infrastructure, automatically rotating IAM keys, SSL certificates, and more! There’s
    a lot of great information about CI/CD out there, but for the subject of this
    book, mentioning it is obligatory.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this sixth chapter, you will learn the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The ABCs of CI/CD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What **continuous integration** (**CI**) is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What **continuous delivery** (**CD**) is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What continuous testing is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DevOps transformation of Capital One
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have learned the core tenets of CI/CD,
    the philosophy that gave birth to it, and the basic strategies to implement it.
    While this chapter does not delve too deeply into the technical implementation
    of CI/CD, you will be shown the tactical strategies that will help you achieve
    success, along with some of the tools that will aid you in getting there.
  prefs: []
  type: TYPE_NORMAL
- en: The ABCs of CI/CD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CI/CD is the lifeblood of today’s software industry, powering the rapid creation
    and distribution of new programs. Tools that eliminate bottlenecks in integration
    and delivery are essential for the smooth operation of any CI/CD pipeline. Teams
    need a unified set of technologies to use in order to work collaboratively and
    efficiently on projects. Source control, testing tools, infrastructure modification,
    and monitoring tools are just some of the SDLC elements that can be unified with
    this framework.
  prefs: []
  type: TYPE_NORMAL
- en: With a well-architected CI/CD pipeline, businesses can quickly pivot to new
    trends in consumer demand and technological advancements. In contrast, it takes
    a long time for teams with traditional development strategies to implement customer-requested
    changes or to incorporate new technologies. In addition, by the time the company
    realizes it needs to pivot, consumer demand may have already shifted. This problem
    is addressed by DevOps release management because it employs continuous integration
    and continuous deployment, a slightly more advanced version of continuous delivery,
    which we will cover in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: What is a CI/CD pipeline?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A CI/CD pipeline streamlines the process of automating software or infrastructure
    as code delivery, ensuring a smooth transition from source code to production
    deployment. Think of it as a sequence of necessary steps for code to be released.
  prefs: []
  type: TYPE_NORMAL
- en: CI is an acronym for continuous integration, while CD is an acronym for continuous
    delivery or deployment. The concept of a pipeline involves automating the various
    stages of the delivery workflow, including build, test, delivery, and deployment.
    By automating and controlling each phase of the delivery process, all the advantages
    of using CI/CD pipelines are unlocked. This helps minimize human error and ensures
    consistency across each release.
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD pipelines are often configured as code, and as such are widely recognized
    by the term *pipeline as code*. In order to facilitate pipeline runs, it is common
    to use a CI server and its corresponding build agents. Depending on the product
    you are using, a build agent might be called a *runner*. Usually, build agents
    appear in the form of virtual machines and can be self-hosted and fully customized
    and require regular maintenance. Alternatively, if you are using a commercial
    SaaS product, you can use CI servers and build agents provided by the SaaS provider,
    but they may have limitations when it comes to customization and adding software
    or plugins.
  prefs: []
  type: TYPE_NORMAL
- en: Containers can also be used to facilitate the creation of consistent build environments,
    reducing the need for maintaining static build agents. In this scenario, every
    stage of the CI/CD pipeline can run independently within a container tailored
    to its specific needs. Additionally, pipelines can take advantage of the various
    benefits provided by container orchestration, including immutability and scaling,
    as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Well-architected CI/CD pipeline infrastructure should be designed to accept
    parameters that produce repeatable outcomes in any number of environments. They
    are also adaptable, considering a scenario in which a consumer need exists but
    is not being met by existing DevOps solutions. In this scenario, it is possible
    to quickly identify the solution, conduct an analysis of it, develop it, and deploy
    it to the application environment in a relatively short time – all without interrupting
    the normal development flow of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, CI/CD allows the rapid deployment of even minor changes to the
    final product, in turn allowing faster response times to user requests. It not
    only addresses user concerns but also gives them insight into the design and creation
    process. Users will notice the product improving over time as updates are rolled
    out to address bugs and add new functionality. In contrast to more conventional
    methods, such as the waterfall model, where users aren’t involved until the very
    end of the development process, DevOps release management facilitates continuous
    feedback and refinement throughout a product’s life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Different projects call for different levels of complexity and numbers of steps
    in the CI/CD pipeline. One potential pipeline might utilize a multi-stage deployment
    approach, wherein software is distributed as containers to a Kubernetes cluster
    spanning multiple cloud environments. In contrast, another pipeline may adopt
    a more straightforward approach, involving the construction, testing, and deployment
    of an app built as a `.jar` file running on a virtual machine and behind a proxy
    server. In this example, both of these pipelines share the same goal of automating
    the software delivery process.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, the establishment of well-architected CI/CD pipeline infrastructure
    is essential to fully leverage all the benefits that come with choosing DevOps
    release management. In the next section, we’ll dive deeper into the subject of
    continuous integration. Topics will include the meaning of CI, selecting the right
    CI tool for your organization, example pipeline syntax, and feature comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: What is continuous integration (CI)?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern software development would not be possible without **continuous integration**
    (**CI**). The creation of modern software typically involves the collaboration
    of numerous developers who are geographically diverse, each of whom focuses on
    a particular component, feature, or aspect of a product. In order for you to bring
    a single, comprehensive product to release, it is necessary to merge all of these
    code changes. However, manually merging all of these changes is extremely impractical,
    and a painful chore, and when developers are working on many updates concurrently,
    there will inevitably be code changes that conflict with one another. However,
    continuous integration incentivizes developers to continuously push their code
    to the same **version control system** (**VCS**), providing a brilliant synergy
    that solves this problem. With the use of CI, you can continuously commit, build,
    and test your team’s code, a vital strategy as a DevOps release manager. If your
    team tests new code often, they will catch and fix defects before they get deeply
    ingrained in the software.
  prefs: []
  type: TYPE_NORMAL
- en: While there are no hard requirements for what tools can be used in CI, many
    teams prefer using continuous integration servers such as Jenkins, GitLab CI,
    or GitHub Actions. As fresh code changes get submitted, a continuous integration
    server oversees everything and acts as the arbitrator. Each time a developer commits
    their work in the repository, the CI server will automatically run a suite of
    tests and record the outcomes. The developer who made the change to the repository
    will typically get an email with the results shortly after making the change.
    This is crucial as it allows the developer to resolve potential issues in the
    shortest amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: After changes have been subjected to automated testing, the updated code can
    receive approval for new builds to be created along with additional testing in
    QA and pre-production environments. If all quality checks pass, the code can then
    be merged into the main branch and a new release is published. Unit tests and
    integration tests are usually performed as part of the continuous integration
    process in order to guarantee that code changes won’t end up resulting in stability
    issues. Additionally, CI is a great place to integrate **static application security
    testing** (**SAST**), moving application security near the beginning of the development
    cycle. All of this test automation makes sure that any changes made to the code
    are adequately vetted before being promoted to production.
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit to increasing the commit frequency is that individual contributors
    can proactively detect and address merge conflicts at an earlier stage, either
    minimizing their occurrence or preventing them entirely. Furthermore, integrating
    smaller increments of work is an effective way to avoid committing a substantial
    number of changes all at once and encountering mysterious errors; instead, developers
    will have produced far smaller amounts of code, totaling fewer lines. This makes
    the task of identifying and resolving bugs and defects in your code significantly
    more efficient, reducing the time required from many hours to just a few minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the right CI tool for your operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are numerous choices available to you when selecting an appropriate CI/CD
    tool for your team’s operations. It is pivotal to assess your own unique requirements
    and preferences because every tool has its own set of advantages and disadvantages
    that could impact your success. Whether you prefer open source options, artificial
    intelligence capabilities, on-premises solutions, peak scalability, or extensive
    customization features, you can find the right tool for your needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'While evaluating various CI/CD tools for your team, you should consider the
    following core factors before making your final decision on which one to select:'
  prefs: []
  type: TYPE_NORMAL
- en: '**On-premises versus cloud-based**: It’s important to evaluate whether the
    tool provides cloud-based and/or on-premises (hosted) solutions and select the
    option that best suits your project requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open source versus closed source**: Consider the compatibility of the CI/CD
    tool with open source projects and how well it aligns with your project’s objectives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing integration**: It is advisable to select a CI/CD tool that has a
    user-friendly interface and a configuration that is easy to comprehend, so as
    to minimize the difficulties associated with setup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ease of setup and configuration**: You should opt for a CI/CD tool with a
    user-friendly interface and easy-to-understand configuration, reducing setup complexities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Build environment compatibility**: It’s important to consider the compatibility
    of the tool with your project’s environment and programming languages to streamline
    integration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning curve**: It is advisable to consider the learning curve that developers
    may face to facilitate the setup and configuration of their build and deployment
    workflows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Paid plan features**: To cope with project growth, it is advisable to examine
    both existing and new features offered in paid plans (if any), including allocated
    daily builds, runtime minutes, quantity of users, and number of private repositories,
    just to name a few.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Version control system compatibility**: Make sure that you verify whether
    the CI/CD tool can comfortably integrate with your preferred version control system
    or source control platform for efficient source code management and delivery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s dive deeper into the top three industry-leading CI/CD tools and help you
    assess which one is right for your enterprise. To start with, Jenkins is a well-known
    CI server that has been around for a very long time and offers many plugins with
    features that newer competitors don’t. Another robust tool that integrates with
    GitHub repositories elegantly is GitLab CI. Don’t overlook GitHub Actions, which
    provides a straightforward and easy-to-understand workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Jenkins
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Jenkins is a well-known and highly customizable open source CI/CD tool capable
    of automating almost anything. Jenkins was developed using the Java programming
    language and is open source, released under the MIT license. The software offers
    a comprehensive range of features that streamline various tasks, including building,
    testing, deploying, integrating, and releasing software. The Jenkins Server (Master)
    software is compatible with Linux, macOS, Windows, and Unix. In addition to being
    installed through native installation packages, Jenkins can be run as a standalone
    Docker container or on any machine that has **Java Runtime Environment** (**JRE**)
    installed.
  prefs: []
  type: TYPE_NORMAL
- en: The Jenkins Master supervises and coordinates the entire build process, acting
    as an arbiter. It serves as the hub for configuration settings, job definitions,
    and metadata, giving it complete control. This is where any of a diverse range
    of plugins can be installed, expanding Jenkins’ features and capabilities, such
    as integrating with *Atlassian JIRA* or *SonarSource SonarQube*. In addition,
    the Jenkins Master provides a web-based interface that is easy to use, allowing
    users to interact with Jenkins, set up jobs, and keep track of build progress.
  prefs: []
  type: TYPE_NORMAL
- en: However, any number of Slave nodes serve as the diligent workers in the system.
    They carry out assigned tasks under the direct supervision of the Master. By distributing
    tasks to multiple Slaves, the build pipeline can be completed much faster through
    parallel processing. Furthermore, Slaves can be configured on different machines,
    including various Operating Systems and environments. Thanks to this versatility,
    Jenkins can meet a diverse range of build and testing needs.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the Jenkins team has developed a sub-project called Jenkins X,
    which focuses on effortlessly running a pipeline in Kubernetes with little to
    no extra work. Jenkins X seamlessly combines Helm, Jenkins CI/CD server, Kubernetes,
    and various other tools to provide a streamlined CI/CD pipeline with pre-established
    best practices, such as employing GitOps to manage environments.
  prefs: []
  type: TYPE_NORMAL
- en: Jenkins syntax example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let’s examine an example of a Jenkins pipeline to get a practical understanding
    of its syntax and how it can be configured! In the `Jenkinsfile` file, a Docker
    container image is being built and the resulting artifact gets published to a
    designated Docker Registry:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1: Example Jenkinsfile – pipeline configured to build a Docker image](img/B21803_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Example Jenkinsfile – pipeline configured to build a Docker image'
  prefs: []
  type: TYPE_NORMAL
- en: GitLab CI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Out of all the CI/CD tools available, GitLab CI/CD stands out as the latest
    and most highly regarded option. This product is a self-hosted continuous integration
    tool, and the community edition is completely free to use.
  prefs: []
  type: TYPE_NORMAL
- en: It includes a range of features such as git repository management, issue tracking,
    code reviews, wikis, and activity feeds. Companies often choose to install GitLab
    CI/CD on-premises and connect it with their organization’s Active Directory and
    LDAP servers to ensure secure authorization and authentication. An obvious drawback
    of utilizing GitLab Community Edition is the absence of any form of customer support.
    If you encounter challenges or require assistance with a project, you are unable
    to submit tickets and request help in the same manner as you would with the other
    two versions, which are Premium and Ultimate.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading from the Community edition to either the Ultimate or Premium versions
    grants you access to customer support, along with numerous advantageous security
    features, such as two-factor authentication, advanced security scanning, and compliance
    auditing tools for your code. In addition, you will have access to various auxiliary
    tools including push rules, DORA metrics tracking, burndown charts, Security Scanning
    IDE integration, and **dynamic application security testing** (**DAST**) features.
    Moreover, you can guarantee that your projects consistently operate without incurring
    additional risk by utilizing sophisticated monitoring features, such as performance
    metrics and system health checks.
  prefs: []
  type: TYPE_NORMAL
- en: The GitLab server is responsible for detecting trigger events that initiate
    one or more pipelines. When a new pipeline begins, the GitLab server determines
    which jobs (defined in your `.gitlab-ci.yml` file) should run, skipping some and
    queuing others, if necessary. These jobs are then assigned to available runners
    in the correct sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GitLab architecture illustrated in the preceding figure is comprised of
    the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Commit**: A commit is a record of a change made in the files or code, like
    what you would find in a GitHub repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jobs**: A job is an individual task that the GitLab pipeline needs to execute,
    such as deploying an application. Every task is assigned a name and includes a
    script. Every script is executed in sequential order, ensuring that each job is
    completed before moving on to the next one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stages**: A stage serves as a clear distinction between different tasks,
    signifying the progression of a pipeline through each step. This clarifies the
    order in which tasks should be executed. As an illustration, the stages could
    include test, build, and deploy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipeline**: A pipeline is a comprehensive set of stages, with each stage
    consisting of one or more tasks. GitLab offers a variety of pipeline options,
    such as basic, merge, parent-child, and multi-project pipelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Runners**: A runner is the active component responsible for executing the
    CI/CD pipeline. You have the option to set up self-hosted GitLab runners on-premises
    or utilize the runners provided by GitLab as part of their SaaS product on GitLab.com.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GitLab server**: The GitLab server handles the hosting and management of
    your pipeline configurations. You can set up your own GitLab server instance on-premises
    or use the SaaS version which is hosted on GitLab.com.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GitLab CI syntax example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s view an example of a GitLab CI/CD pipeline to get a practical understanding
    of its syntax and how it can be configured! In the `.gitlab-ci.yml` file, a Docker
    container image is built and the resulting artifact gets published to a designated
    Docker Registry:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2: Example GitLab gitlab-ci.yml file – pipeline configured to build
    a Docker image](img/B21803_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: Example GitLab gitlab-ci.yml file – pipeline configured to build
    a Docker image'
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Actions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GitHub Actions is a tool used for continuous integration and continuous deployment
    as part of the GitHub flow. It can be utilized for integrating and deploying code
    changes to a third-party cloud application platform as well as testing, tracking,
    and managing code changes. GitHub Actions is compatible with various third-party
    CI/CD tools, the Docker container ecosystem, and other automation technologies.
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Actions seamlessly integrates automation into the software development
    life cycle on GitHub through event-driven triggers. These triggers are events
    that can be specified, ranging from creating a pull request to building a new
    branch in a repository and much more. GitHub Actions automations are managed through
    workflows that are `YAML` files located in the `.github/workflows` directory of
    a repository. These workflows define automated processes and are analogues in
    concept to a `Jenkinsfile` file in Jenkins or a `.gitlab-ci.yml` in GitLab CI/CD.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every workflow consists of several core concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Events**: An event is a defined trigger that initiates a workflow. Developers
    can configure them to search for one or multiple triggers and then adjust them
    accordingly. Additionally, they can be configured to execute on specified code
    branches within a designated repository on GitHub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jobs**: A job consists of a series of sequential tasks executed on a single
    runner. Each task operates within its own virtual machine (VM) and runs concurrently
    with other tasks, unless declared otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Steps**: A step is an independent operation that executes commands within
    a job. These can serve as either an action or a shell command. Every step in a
    job is executed on the same runner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actions**: An action refers to a command that is executed on a runner and
    serves as the fundamental component of GitHub Actions, from which it derives its
    name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Runners**: A runner functions as a server for GitHub Actions. The program
    actively monitors available tasks, executes them concurrently, and provides updates
    on the progress, logs, and outcomes. Runners can be hosted either on GitHub or
    on a localized server that is self-hosted. GitHub Hosted runners utilize Ubuntu,
    Linux, Windows, and macOS as their underlying operating systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The primary advantage of having a GitHub-native CI/CD tool is its simplicity.
    If you are already hosting a project on GitHub, you can utilize the built-in CI/CD
    tool because it fully integrates with your code repositories. CI/CD pipelines
    can be quite intricate, involving a wide array of tools for testing applications,
    integration tests, container platforms, and application platforms, among other
    components. GitHub Actions streamlines the whole process by offering frictionless
    integration with NodeJS and Docker. Notably, it enables you to quickly choose
    the desired dependency version and effortlessly connect your code to a desired
    environment and deployment platform of choice. Unlike other automation tools and
    features, GitHub Actions goes beyond the typical applications of testing, building,
    and deploying. Instead, it offers the flexibility to automate any webhook.
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Actions workflow syntax example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let’s examine an example of a GitHub Actions pipeline to get a practical
    understanding of its syntax and how it can be configured! In the GitHub Actions
    `Workflow` file, a Docker container image is being built and the resulting artifact
    gets published to a designated Docker registry:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3: Example GitHub Actions workflow – pipeline configured to build
    a Docker image](img/B21803_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: Example GitHub Actions workflow – pipeline configured to build
    a Docker image'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have established a basic understanding of the differences in syntax
    between these three tools, let’s compare the features of all three CI tools.
  prefs: []
  type: TYPE_NORMAL
- en: A side-by-side feature comparison of all three CI tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following table provides a side-by-side comparison of the features and
    benefits offered by each of these three industry-leading CI tools: Jenkins, GitLab
    CI/CD, and GitHub Actions. The information presented is intended to help you evaluate
    which tool is the best choice for your operations based on your own unique requirements
    and preferences.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Feature** | **Jenkins** | **GitLab CI/CD** | **GitHub Actions** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| On-premises (self-hosted) | Yes | Yes | Runners only |'
  prefs: []
  type: TYPE_TB
- en: '| Cloud-based | No | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Open source | Yes | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| Closed source | No | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Testing integration | Yes | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Ease of setup and configuration | Difficult | Moderate | Easy |'
  prefs: []
  type: TYPE_TB
- en: '| Build environment compatibility | Linux, Windows, macOS, Unix | Linux, Windows,
    macOS | Cloud SaaS |'
  prefs: []
  type: TYPE_TB
- en: '| Language support | Any contemporary language | C, C++, C#, Go, Java, JavaScript,
    PHP, Python, Ruby, Scala, TypeScript, and others | C, C++, C#, Java, JavaScript,
    PHP,Python, Ruby, Scala, and TypeScript |'
  prefs: []
  type: TYPE_TB
- en: '| Learning curve | Difficult | Moderate | Easy |'
  prefs: []
  type: TYPE_TB
- en: '| Paid plan features | No | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| VCS compatibility | GitMercurial (hg)Subversion (svn)Perforce (p4)ClearCaseMicrosoft
    TFS | Git | Git |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6.1: Feature comparison of Jenkins, GitLab, and GitHub Actions'
  prefs: []
  type: TYPE_NORMAL
- en: Code Integration, automated builds, and integration testing are the three pillars
    of continuous integration. The ultimate objective of the continuous integration
    process is to generate a deployable artifact. This concludes our examination of
    continuous integration and CI tools. In the next section, we’ll discuss the counterpart
    to continuous integration, continuous delivery.
  prefs: []
  type: TYPE_NORMAL
- en: What is continuous delivery (CD)?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Continuous delivery** (**CD**) refers to the process of automatically preparing
    code changes for release and deployment into a production environment. Continuous
    delivery is an essential component of DevOps release management and is often used
    in concert with continuous integration (CI).'
  prefs: []
  type: TYPE_NORMAL
- en: Even at the tail end of the **software development life cycle** (**SDLC**),
    developers can successfully deploy most product code versions with the help of
    CI/CD pipelines, along with a **version control system** (**VCS**). Continuous
    delivery enables programmers to automatically test code changes using multiple
    lenses (not just unit testing) before releasing them to customers. In this way,
    developers can have faith in the quality of the build artifacts they’re deploying,
    as they will have been subjected to rigorous testing and found to be in compliance
    with industry standards. API testing, load testing, functional and UI testing,
    integration testing, compliance testing, and others are all examples of appropriate
    types of testing that you would normally run in this phase.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, software developers are empowered to rapidly evaluate for the existence
    of bugs and defects before a new software release can be permitted access to production
    environments. It is notable to mention that continuous delivery often includes
    the execution of multi-stage deployments, whereby artifacts undergo transitions
    across different stages, including QA, staging, pre-production, and ultimately
    production. Additional testing and verification steps are usually performed at
    each stage to ensure the reliability and legitimacy of the delivered artifacts.
    Post-release validation procedures and deployment monitoring can (and should)
    be implemented to further bolster the software release’s dependability and resilience.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous delivery not only assumes the responsibilities of deploying applications,
    but also in making configuration modifications, monitoring application performance,
    and ensuring its ongoing maintenance. This is where building **disaster recovery**
    (**DR**) into the pipeline design becomes key. That is because continuous delivery
    has the potential to expand its functional scope by including operational duties
    that may involve tasks such as infrastructure management. These tasks can be achieved
    using the **infrastructure as code** (**IaC**) and **configuration as code** (**CaC**)
    tools that were made especially for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: What is infrastructure as code (IaC)?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the field of technology, the term infrastructure has typically been associated
    with physical components such as rackmount servers, networking systems, and data
    centers. However, due to the proliferation of the cloud, this infrastructure has
    evolved beyond its physical constraints, transforming into virtual services and
    environments that can be rapidly created, modified, and decommissioned. Managing
    and provisioning these dynamic resources efficiently and reliably has become a
    significant challenge in this new era. This is where the notion of IaC becomes
    relevant. IaC tools have become crucial in tackling these challenges by enabling
    the management of infrastructure through code rather than manual processes. This
    method simplifies the act of building and maintaining virtual IT infrastructure,
    improves consistency, minimizes the risk of mistakes, and enables effortless automation
    and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is for this reason that understanding the concept of idempotence is crucial
    in the context of IaC. When an IaC deployment is executed, it ensures that the
    target environment is consistently configured, regardless of its initial state.
    This is to say, idempotency can be achieved through two methods: automatically
    configuring the current target or discarding it and creating a new target environment
    from scratch.'
  prefs: []
  type: TYPE_NORMAL
- en: Idempotency
  prefs: []
  type: TYPE_NORMAL
- en: Idempotency in data pipelines refers to the ability to execute the same operation
    multiple times without changing the result beyond the initial application. This
    property ensures consistency and reliability, especially in distributed systems.
  prefs: []
  type: TYPE_NORMAL
- en: Notably, IaC has emerged as the preeminent solution to address the issue of
    configuration drift, both in release pipelines and virtualized deployment environments.
    Crucially, in the absence of IaC, teams would be required to manually manage environment
    and deployment configurations individually. When operating this way, over time,
    every environment inevitably develops its own distinct configuration that cannot
    be replicated automatically. Consequently, deployment issues can arise due to
    inconsistencies in different environments, such as dev, QA, staging, and production.
    Due to the reliance on manual processes, infrastructure administration and maintenance
    can be challenging, prone to errors, and difficult to monitor.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration drift
  prefs: []
  type: TYPE_NORMAL
- en: The gradual alteration of an IT system’s configurations over time is known as
    configuration drift. Drift most often happens unintentionally when modifications
    are made to software, hardware, or operating systems without proper documentation
    or approval. It can affect the safety and efficiency of a part or the whole of
    a system. Application failure, downtime, extended development life cycles, spikes
    in IT tickets, security vulnerabilities, audit fines, compliance failures, and
    more are all direct results of configuration drift.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, infrastructure as code leverages the advantages of the DevOps methodology
    and versioning to efficiently define and deploy various components of infrastructure.
    This includes networks, virtual machines, load balancers, DNS, serverless deployments,
    identity access management, and much more. You can think of IaC as software-defined
    infrastructure. Similarly to how the same source code consistently produces binaries
    with identical capabilities, an IaC model consistently generates the same environment
    with each deployment. IaC plays a crucial role in contemporary DevOps practices
    and is an integral part of continuous delivery. By utilizing IaC, DevOps teams
    can collaborate seamlessly using a standardized set of methods and resources to
    efficiently deploy applications and their corresponding infrastructure on a large
    scale, ensuring speed and reliability. Perhaps best of all, IaC files can be stored
    in Git and are easily auditable.
  prefs: []
  type: TYPE_NORMAL
- en: To accomplish this, IaC streamlines the configuration process and ensures uniformity
    by using declarative code in formats like YAML, JSON, and **HashiCorp configuration
    language** (**HCL**) to represent desired environment states. Release pipelines
    consume IaC files and apply the environment descriptions and versioned configuration
    models to set up target environments that are highly reliable and eliminate the
    runtime problems that arise from configuration inconsistencies or missing dependencies.
    Crucially, this allows the team to make edits to the source code rather than the
    target directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several popular tools that have been developed to automate these
    kinds of tasks. In the following subsections, we’ll take a detailed look at four
    of the most common ones: Terraform, Pulumi, Ansible, and Puppet.'
  prefs: []
  type: TYPE_NORMAL
- en: Terraform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Terraform** is a powerful infrastructure-as-code tool that allows you to
    define cloud and on-prem resources using easily comprehended configuration files
    written in HCL. These files can be versioned, reused, and shared, making it a
    convenient choice for managing your infrastructure. You can apply a streamlined
    workflow to accurately establish and control your infrastructure at every stage
    of its life cycle.'
  prefs: []
  type: TYPE_NORMAL
- en: Terraform has been designed to manage a wide range of components, from low-level
    ones such as computer, storage, and networking resources, to higher-level ones
    such as DNS entries, Kubernetes clusters, and SaaS features. Terraform seamlessly
    integrates with popular continuous integration and deployment systems like GitLab,
    GitHub Actions, and Jenkins. With this solution, you can optimize the entire process
    of deploying and managing your infrastructure, rapidly advancing from code to
    production.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform utilizes a plugin-based architecture to seamlessly interface with
    various cloud providers, including AWS, Google Cloud, and Azure. Every provider
    comes with a unique collection of plugins that enable Terraform to effectively
    handle its resources. Terraform processes the configuration files written in HCL
    and generates a dependency graph of the resources that require creation or modification.
    It then proceeds to execute a plan to create or modify the necessary resources
    to achieve the intended state. Terraform includes a state file that maintains
    the current state of your infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Terraform workflow is incredibly straightforward, with just three simple
    steps to effectively manage any kind of infrastructure: write, plan, apply. One
    of the simplest workflows for managing any kind of infrastructure is Terraform’s
    three-step process. It allows users to customize the workflow according to their
    specific requirements and implementation style. To illustrate how Terraform works,
    let’s examine a sample Terraform plan that can be used to create an EC2 instance
    in AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4: Example Terraform plan – configured to provision an AWS EC2 instance](img/B21803_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: Example Terraform plan – configured to provision an AWS EC2 instance'
  prefs: []
  type: TYPE_NORMAL
- en: Pulumi
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Pulumi** is a cutting-edge IaC platform. It utilizes popular programming
    languages such as TypeScript, JavaScript, Python, Go, .NET, Java, and markup languages
    like YAML, along with their respective ecosystems, to seamlessly interact with
    cloud resources. Pulumi’s comprehensive platform seamlessly integrates a downloadable
    CLI, runtime, libraries, and a hosted service to deploy virtual infrastructure.
    This flexible combination allows for efficient provisioning, updating, and management
    of cloud infrastructure.'
  prefs: []
  type: TYPE_NORMAL
- en: Pulumi programs, written in popular programming languages, outline the composition
    of your cloud infrastructure. When adding new infrastructure to your program,
    you simply assign resource objects with properties that match the desired state
    of your infrastructure. These properties can be utilized to manage dependencies
    between resources and can be exported beyond the stack, if required.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Pulumi platform is made up of various components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pulumi software development kit** (**SDK**): This offers bindings for every
    resource type that can be managed by the provider. This resource equips users
    with the essential tools and libraries to effectively define and oversee cloud
    resources across various providers and platforms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Command-line interface** (**CLI**): This allows you to deploy updates to
    cloud applications and infrastructure. It maintains a record of team updates,
    including the contributors and timestamps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment engine**: The deployment engine calculates the necessary operations
    to align your infrastructure’s current state with the desired state specified
    by your program.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Programs are stored in a project, which is a directory that holds the program’s
    source code and instructions on how to execute it. Once your program is complete,
    you can execute the `Pulumi up` command using the Pulumi CLI from your project
    directory. This command allows you to create a separate and customizable instance
    of your program, referred to as a stack. Stacks function as various deployment
    environments utilized for testing and implementing application updates. As an
    example, you can create and test separate development, staging, and production
    stacks.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s an example program that demonstrates the concepts. It creates an AWS
    EC2 security group called `web-sg` with one ingress rule and a `t2.micro-sized`
    EC2 instance that uses that security group. The EC2 resource needs the ID of the
    security group to utilize it. Pulumi facilitates this by utilizing the output
    property name on the security group resource. Pulumi has a deep understanding
    of resource dependencies, allowing it to optimize parallelism and maintain the
    correct order when a stack is created.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the IP address and DNS name of the server are exported as stack outputs
    for easy access through a CLI command or by another stack.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5: Example Pulumi code – configured to provision an AWS EC2 instance](img/B21803_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: Example Pulumi code – configured to provision an AWS EC2 instance'
  prefs: []
  type: TYPE_NORMAL
- en: Ansible
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Ansible** is an open source configuration management tool that offers a streamlined
    server automation framework using YAML definitions. Ansible has gained immense
    popularity as a configuration management tool due to its simplified infrastructure
    requirements and user-friendly syntax.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other tools in its category, such as Chef or Puppet, Ansible does not
    need any specialized software (agents) to be installed on remote nodes. A control
    machine is configured with the Ansible software, enabling it to communicate with
    the nodes through standard SSH protocols, and Python is enlisted to execute the
    remote instructions.
  prefs: []
  type: TYPE_NORMAL
- en: A task is the smallest unit of action you can automate using an Ansible playbook.
    Playbooks typically contain a series of tasks that serve a goal, such as setting
    up a web server or deploying an application to remote environments. Ansible executes
    tasks in the same order they are defined inside a playbook. Before automating
    a procedure, such as setting up a LAMP server (Linux, Apache, MySQL, PHP), you’ll
    need to assess which manual steps are necessary and the order in which they must
    be completed to get everything done. You’ll then be able to determine which tasks
    you’ll need and which modules you can use to reach your goals in fewer steps.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, Ansible offers a comprehensive range of pre-built modules that
    streamline the process of automating routine server operations. These modules
    cover a wide array of tasks, including package installation, user management,
    file manipulation, permission handling, and service management.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate how Ansible works, let’s examine a sample Ansible play that can
    be used to create an EC2 instance in AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6: Example Ansible play – configured to provision an AWS EC2 instance](img/B21803_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: Example Ansible play – configured to provision an AWS EC2 instance'
  prefs: []
  type: TYPE_NORMAL
- en: Puppet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Puppet** is a configuration management tool that utilizes its own declarative
    language for describing infrastructure state. Puppet’s language is designed to
    efficiently handle every life cycle stage of IT infrastructure. This includes
    tasks such as provisioning, patching, configuration, and management of operating
    systems and application components in both data centers and cloud infrastructures.'
  prefs: []
  type: TYPE_NORMAL
- en: Puppet is specifically designed to handle the configuration of Unix-like and
    Microsoft Windows systems. To accomplish this, a user assigns system resources
    and their state, utilizing either Puppet’s declarative language or a Ruby **domain-specific
    language** (**DSL**). In doing so, the infrastructure configurations get stored
    in configuration files referred to as Puppet manifests. When executed, the Puppet
    utility will compile the Puppet manifests into a system-specific catalog that
    includes resources and their dependencies. This catalog can then be applied to
    the target systems, and the response from Puppet’s actions is reported to the
    user.
  prefs: []
  type: TYPE_NORMAL
- en: Puppet typically adheres to a client-server architecture. In this case, the
    client is referred to as an agent, while the server is commonly referred to as
    the master. Additionally, it can function as a standalone application that can
    be executed from the command line, making it convenient for testing and basic
    configuration purposes. Puppet Server is usually installed on multiple servers,
    while Puppet Agent gets installed on all of the machines that need to be managed.
    In this way, Puppet agents communicate with the server to retrieve configuration
    instructions so that they can be deployed. The agent proceeds to implement the
    configuration on the targeted systems and promptly sends a comprehensive status
    report to the server. Notably, machines have the capability to run the Puppet
    agent as a daemon, which can be scheduled to run periodically as a Cron job or
    can be manually executed as required.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate how Puppet works, let’s examine a sample Puppet manifest that
    can be used to create an EC2 instance in AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7: Example Ansible play – configured to provision an AWS EC2 instance](img/B21803_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7: Example Ansible play – configured to provision an AWS EC2 instance'
  prefs: []
  type: TYPE_NORMAL
- en: What is the difference between infrastructure as code (IaC) and configuration
    as code (CaC)?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although there are similarities between IaC and CaC, they also have notable
    differences. As asserted previously, IaC is predominantly used for deploying virtual
    infrastructure, including server instances, storage devices, and networking components,
    as well as any additional resources and permissions needed. In contrast, configuration
    as code tools follow up on this by configuring and customizing operating systems,
    application configurations, and monitoring devices after the infrastructure has
    been generated using IaC tooling. This activity is used to automate the creation
    of computing systems precisely tailored to meet the specific requirements and
    objectives of a client or business. These two types of automation tools have unique
    strengths that make them suitable for specific use cases or when used together.
  prefs: []
  type: TYPE_NORMAL
- en: To help you understand the difference, here is an analogy. Infrastructure as
    code can be thought of as using tools to construct an office building, while configuration
    as code is a set of tools used to furnish the office building with the equipment
    and resources that a business needs to actually get work done.
  prefs: []
  type: TYPE_NORMAL
- en: Notably, when integrating cloud-based deployments, software developers have
    the ability to easily and affordably create multiple testing environments and
    iterate them. Historically, when working in on-premises environments, it was much
    more difficult to dynamically create test environments, but this is no longer
    the case. Cleverly, computer hardware manufacturers, such as HP, Dell, and SuperMircro,
    have made many improvements to their product designs that modernize the on-prem
    experience. These days, most rack-mount servers have APIs embedded into their
    firmware with native integrations for the commonly used IaC and CaC tools on the
    market. This gives on-premises hardware similar functionality to their cloud-based
    competitors, enabling them to remain relevant in a competitive landscape.
  prefs: []
  type: TYPE_NORMAL
- en: The continuous delivery pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The primary characteristic of a legitimate CD pipeline is its ability to facilitate
    software deployment at any stage of its life cycle. Put another way, well-architected
    CI/CD pipeline infrastructure should ensure that any application version can be
    easily deployed to the designated testing, staging, or production environments
    with only a few mouse clicks and with absolute idempotence. Furthermore, development
    teams should be able to receive prompt feedback from automated tests being conducted
    in any environment, and this feedback should be leveraged to facilitate product
    improvements and greater operational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'The continuous delivery pipeline has five primary phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8: The five common phases of continuous delivery](img/B21803_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.8: The five common phases of continuous delivery'
  prefs: []
  type: TYPE_NORMAL
- en: 'This figure represents the five most common phases in a continuous delivery
    strategy: commit, test, build, stage, and deploy. As you can see, the cycle is
    designed to be short, promoting the shortest possible interval from when a new
    code change is committed, to version control, to the time it takes to see it deployed
    in production. Beyond that, there are several validation steps in between to ensure
    the highest quality possible. This includes the ability to build the code, which
    can be seen as a form of testing in its own right.'
  prefs: []
  type: TYPE_NORMAL
- en: Notably, it is far easier to achieve continuous deployment workflows in a product-centric
    company rather than a services-focused company. The reason for this is that service
    companies must tailor their solutions to each individual client, whereas a product
    company is aligned with a narrow scope of value streams.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between continuous delivery and continuous deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the context of DevOps release management, the terms continuous delivery and
    continuous deployment denote two tiers of automation.
  prefs: []
  type: TYPE_NORMAL
- en: With continuous delivery, the need for the manual deployment of new code is
    reduced, saving both time and resources. First, the code is written, then automatically
    tested, then approved, and finally pushed to a repository where other engineers
    can access it. When the code is complete, the operations team can quickly fetch
    it and effortlessly deploy it to live application environments using kiosk-like,
    self-serve functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'This diagram depicts the differences between the continuous delivery and continuous
    deployment sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9: Continuous delivery versus continuous deployment](img/B21803_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.9: Continuous delivery versus continuous deployment'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, there is one defining feature that distinguishes the two: deploying
    to production. With continuous delivery, there is a manual approval step that
    is enforced before new code changes are permitted to be deployed into production
    environments. With continuous deployment, automated testing fulfills this role
    so that no manual human intervention is necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: By expanding the automation of continuous delivery to the next stage of the
    **software development life cycle** (**SDLC**), continuous deployment can help
    reduce the workload of operations teams and speed up the delivery of applications.
    Any auxiliary software release procedures will often get automated too, reducing
    or eliminating the extent of manual human interaction. For instance, a continuous
    deployment pipeline might be set up to deploy new releases after being committed
    into a Git repository and deployed into the production environment so that customers
    can take advantage of it as early as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous deployment is substantially harder to implement than continuous delivery
    since it eliminates the need for any kind of human intervention throughout the
    process of deploying authorized software products into production environments.
    This means that in order to achieve true continuous deployment, your automated
    testing regimen must be prolific, interoperable, and extensible.
  prefs: []
  type: TYPE_NORMAL
- en: How GitOps fits in with continuous delivery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some notable distinctions exist between GitOps and DevOps. Perhaps the most
    significant aspect is that GitOps places even greater emphasis on the use of automation
    and tooling in order to effectively manage and distribute code modifications.
    Conversely, DevOps places greater emphasis on fostering effective communication
    and collaboration among team members. Another distinction is that GitOps is widely
    used in tandem with containerization technologies such as Kubernetes, whereas
    DevOps can be applied to a variety of other types of application deployments.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to recognize that GitOps is a specialized domain within the
    broader field of DevOps that centers around the use of Git repositories for the
    purpose of effectively managing infrastructure state and application deployments.
    A vital distinction between GitOps and DevOps is that with GitOps, the Git repository
    serves as the single authoritative source of truth for the deployment state of
    applications and infrastructure. In this way, the Git repository acts as a ledger
    or is similar in concept to a blockchain.
  prefs: []
  type: TYPE_NORMAL
- en: Another key thing to grasp is that GitOps relies heavily on pull-based deployment
    as its primary method of implementation. With conventional DevOps approaches,
    continuous integration and continuous delivery pipelines are triggered by an external
    event, such as when new code is pushed to the application repository. With GitOps,
    instead of pushing out new code every time there’s a change in the environment,
    the pull-based strategy keeps the application current by actively comparing the
    currently deployed application state with the ideal application deployment state
    as declared in the version control repository. If any discrepancy is detected
    between the two, the GitOps operator updates the live infrastructure to match
    the configurations declared in the designated repository.
  prefs: []
  type: TYPE_NORMAL
- en: Cleverly, pull-based deployments make it easy to roll back unstable software
    deployments to the last known stable version in the event of an issue. Additionally,
    pull-based techniques are declarative, making advanced deployment strategies,
    such as blue/green and canary deployments, effortless to implement.
  prefs: []
  type: TYPE_NORMAL
- en: Blue/green deployments
  prefs: []
  type: TYPE_NORMAL
- en: Blue/green deployments produce two identical environments. One environment (blue)
    runs the existing program version and one (green) runs the new one. After testing
    passes on the green environment, live application traffic is directed there, and
    the blue environment is deprecated. By simplifying rollbacks if deployments fail,
    blue/green deployment strategies boost application availability and reduce deployment
    risk.
  prefs: []
  type: TYPE_NORMAL
- en: Since GitOps deployments are immutable, it is easy to reset any arbitrary or
    undocumented modifications to the live infrastructure. It enforces a complete
    audit trail of all changes in the Git log and helps avoid direct cluster changes
    that could result in inconsistencies in the system’s state.
  prefs: []
  type: TYPE_NORMAL
- en: Canary deployments
  prefs: []
  type: TYPE_NORMAL
- en: A canary deployment refers to a gradual and controlled release strategy for
    an application, wherein traffic is divided between an existing version and a new
    version. This approach involves initially introducing the new version to a subset
    of users before expanding its deployment to the entire user base. By following
    this approach, one can determine the reliability of the updated version of the
    application prior to its widespread distribution to consumers.
  prefs: []
  type: TYPE_NORMAL
- en: What is continuous testing?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, you should have a firm grasp on the importance of automated testing,
    at least based on the number of times the subject has been mentioned. The emphasis
    on how important automated testing is to DevOps release management cannot be overstated.
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous testing** is a practice within the broader context of CI/CD that
    contributes to software quality throughout the development life cycle. Using carefully
    curated automated testing strategies, continuous testing ensures that software
    development teams get real-time feedback, allowing them to rapidly eliminate as
    many potential risks and flaws as possible and as soon as possible, spanning the
    entire software development life cycle. Furthermore, teammates will be properly
    equipped to continuously gain new insights into their products and ways that they
    can be improved.'
  prefs: []
  type: TYPE_NORMAL
- en: However, implementing continuous testing in your organization is not a straightforward
    process because you must come up with a testing strategy that ensures a change
    will move forward without triggering any false positives. Like continuous deployment,
    it is far more difficult to implement continuous testing than it might sound,
    as they are part and parcel with one another. Traditionally, testing software
    was carried out for the very first time after the code had been written and then
    forwarded to the Quality Assurance team to be tested independently. When errors
    were discovered in the code, it got handed back to the developers so that they
    could correct it. This testing model is practical to a reasonable extent in an
    era when slower development cycles were acceptable. However, it is challenging,
    tedious, and fraught with potential for disruption and human error. Instead, contemporary
    organizations require prompt delivery of products that are of superior quality
    because this is what customers have grown to expect in today’s competitive digital
    marketplace. If the resources exist to implement it properly, there is no better
    way to test in a DevOps-centric organization than continuously.
  prefs: []
  type: TYPE_NORMAL
- en: Therein lies the value of conducting testing on an ongoing basis. Bugs can be
    found and fixed before more work is done if code is tested immediately after being
    added to the repository. It would then be unnecessary to make future code modifications
    addressing a bug fix because their existence would be avoided in the first place.
    In our modern age, developers even benefit from automated testing plugins that
    install directly into a developer’s local **integrated development environment**
    (**IDE**), such as Eclipse, Microsoft Visual Studio, and PyCharm. This gives developers
    the opportunity to detect and fix issues as they write and before code ever gets
    committed to source control in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: Quality assurance of customer-facing software requires thorough end-to-end testing
    that exercises the entire system, this will help you verify that your app is performing
    as expected. End-to-end testing necessitates that real data and environments be
    used for the most reliable results. You will be better positioned to find and
    fix problems with the code when using mock data that is representative of real-world
    production data. Leveraging this information, you can learn more about the app’s
    real-world performance by simulating it in real-world testing conditions. As a
    side note, this philosophy is core to the ethos of implementing canary deployments,
    exposing a small percentage of users to vetted pre-release versions in production.
  prefs: []
  type: TYPE_NORMAL
- en: Effective continuous testing requires both continuous integration and continuous
    delivery. Many steps in the testing process, such as code construction, deployment,
    and analysis, can be automated with the help of CI/CD tools. New features and
    bug fixes can be released more quickly while still meeting high standards of quality
    when using CI/CD and DevOps release management. Keep an eye on test results and
    user feedback to ensure your software is continually getting better. This data
    will help you spot problems in your process and make the necessary adjustments
    to improve them. Maintaining high-quality software requires maintaining high-quality
    awareness of test results over time.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we’ll examine the case study of how the financial
    institution Capital One made the most of CI/CD while conducting its own DevOps
    transformation.
  prefs: []
  type: TYPE_NORMAL
- en: The DevOps transformation of Capital One
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2010, Capital One acknowledged their customers’ preferences for online and
    mobile banking. In light of this, executive leadership decided to enhance the
    business’s technological capabilities and establish a culture that would attract
    and grow a workforce of highly skilled technologists with a knack for collaborative
    development. Prudently, Capital One prioritized the recruitment of these hearty
    souls and made sure they were working closely with relevant decision-makers who
    consummately understood the business requirements. Shortly after, the company
    embraced agile software development techniques that eventually became the basis
    for implementing DevOps release management at the company.
  prefs: []
  type: TYPE_NORMAL
- en: 'Promptly addressing customer feedback has always been the top concern at Capital
    One. Therefore, DevOps emerged as the logical option for development teams to
    attain accelerated development and deployment cycles. Between 2012 and 2020, Capital
    One experienced a series of transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: Embracing agile practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating automated test cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating deployments and tests using CI/CD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Migrating operations to public cloud providers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through these modifications, the bank transformed into an organization that
    embraced open source solutions and rapid delivery cycles. In 2020, Capital made
    history by becoming the first US bank to transfer the entirety of its legacy on-premises
    data centers to public cloud providers.
  prefs: []
  type: TYPE_NORMAL
- en: Capital One’s DevOps transformation strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite starting with a handful of employees, Capital One aimed to establish
    a company-wide DevOps approach. Over time, the corporation implemented its DevOps
    initiatives architected with a three-phased approach.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10: Capitol One’s three-phase DevOps transformation](img/B21803_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.10: Capitol One’s three-phase DevOps transformation'
  prefs: []
  type: TYPE_NORMAL
- en: Creating cross-functional teams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Capital One began implementing DevOps by assigning specialized and versatile
    SWATteams to two of its older applications within the company. These cross-functional
    teams magnanimously implemented configuration management and automation of essential
    functions and optimized the workflow of these two applications. Following that,
    each team continued to assert ownership of the delivery process for their designated
    application. This strategy was repeated for four additional applications at Capitol
    One before management encouraged the rest of the company’s development teams to
    implement these newly discovered best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Notably, Capital One’s ability to establish common objectives was greatly enhanced
    by the presence of a cross-functional team and excellent leadership during the
    earliest stages of their DevOps Journey. It was also beneficial for developers
    and operation teams to acquire essential DevOps skills needed to influence others
    and proliferate the culture across the organization.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging microservices architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like other businesses that existed during the dot com era, Capital One used
    a monolithic design while architecting its technology stack. Over time, their
    projects began to expand, making it necessary to consider future requirements.
    As a result, the bank dedicated additional resources to thoroughly examine the
    microservices architecture and its applicability to their organization.
  prefs: []
  type: TYPE_NORMAL
- en: At Capital One, the primary objective was to enhance delivery speed while maintaining
    high-quality standards. The development team chose to use automated deployments
    that align with their established quality standards. They established strict and
    clear rules for software deployment and modifications to production.
  prefs: []
  type: TYPE_NORMAL
- en: 'The team at Capital One has implemented immutable stages in their pipeline
    delivery:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing effective source control management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a safe place to store application and binary data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing robust privileged access management and authorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring that quality and safety checks are regularly performed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each application team was obligated to fulfill these requirements prior to
    releasing their code to the production environment. In the end, the benefits that
    Capital One received because of implementing microservice architectures were as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Asymmetric service deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infinitely scalable applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High availability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logical separation of duties and responsibilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improved error handling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an on-demand infrastructure on AWS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After receiving feedback from customers, product managers at Capital One focused
    their efforts on enhancing the quality of the banking and financial services to
    provide customers with an exceptional experience. Exactly for this reason, the
    organization adopted a cloud-first policy, and its architects moved the newly
    developed applications to the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'The development team at Capital One was able to obtain valuable user insights
    and respond more quickly thanks to the following Amazon Web Services tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Virtual private** **cloud** (**VPC**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simple storage** **service** (**S3**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elastic compute** **cloud** (**EC2**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relational database** **service** (**RDS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating delivery pipelines using Jenkins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Capital One employs a range of pipelines to thoroughly scan and test its code,
    ensuring high-quality standards across the company. In addition, a similar procedure
    is carried out to ensure expedited delivery. The code updates go through a thorough
    process of automated testing, which includes integration tests, unit tests, security
    scanning, and quality checks. The release is deployed automatically by the pipeline
    after the code successfully passes all the tests. By ensuring uninterrupted service,
    users can enjoy a seamless experience while teams can effortlessly deploy updates.
  prefs: []
  type: TYPE_NORMAL
- en: The development team utilized Jenkins, a widely used tool for creating continuous
    integration and delivery pipelines. In taking this approach, Capital One was able
    to avoid the need to create its own integration process from scratch. The Jenkins-based
    pipeline efficiently breaks down the entire development process into stages and
    further divides them into additional steps, such as application build, integration
    testing, and deployment. Notably, Capitol One employs boilerplate tools that are
    used to accelerate the creation of `Jenkinsfiles` for expediting the development
    of various applications.
  prefs: []
  type: TYPE_NORMAL
- en: Jenkins has allowed Capital One to streamline software delivery, enhance operational
    stability, and provide a better experience for developers overall.
  prefs: []
  type: TYPE_NORMAL
- en: Governance within Capitol One’s CI/CD pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Capital One aimed to achieve a culture of fearless releases to promote creative
    thinking. However, this also necessitated the adoption of a mindset where individuals
    take responsibility for the decisions they make and their roles in software delivery.
    Tapabrata “Topo” Pal, a well-known strategist and DevOps evangelist, and his team
    implemented the concept of **clean room** development at Capital One. They modified
    the concept for the software development life cycle to embrace a culture of courageousness
    and accountability.
  prefs: []
  type: TYPE_NORMAL
- en: Clean room
  prefs: []
  type: TYPE_NORMAL
- en: The term “clean room” refers to an engineered space that keeps the concentration
    of airborne particulates very low. It has active cleansing, good isolation, and
    good contamination control. These types of rooms are usually required for industrial
    production for all nanoscale processes, including semiconductor manufacturing,
    as well as for scientific research. Dust and other airborne organisms, such as
    vaporized particles, are to be kept away from a cleanroom in order to protect
    the materials being handled inside it.
  prefs: []
  type: TYPE_NORMAL
- en: A set of clear guidelines to guarantee code quality before release can be considered
    the company’s virtual development clean room. These policies cover procedures
    such as locating and registering each product pipeline, vetting and inspecting
    each version of the code, restricting access to production servers, and so forth.
    To put it simply, the clean room approach emphasizes preventing defects rather
    than eliminating them. In the end, Capital One utilized a clean room model to
    detect and address issues across different product pipelines, guaranteeing quality
    right from the beginning. After all, an ounce of prevention is worth a pound of
    cure.
  prefs: []
  type: TYPE_NORMAL
- en: The following illustration describes Capital One’s “Clean Room” DevOps release
    management methodology in its entirety. This process begins with the development
    phase, where application code is kept in version control management. Then, a series
    of security measures are enforced, such as restricting access to binaries and
    including static code analysis. The focus of this section is to ensure that the
    code being written is stored with integrity, confidentiality, and availability.
  prefs: []
  type: TYPE_NORMAL
- en: Further along in the clean room process is the testing phase. This step ensures
    end-to-end traceability of the quality assurance procedures, starting with tying
    functional test activity to their respective user stories. From there, the product
    owner works hand-in-hand with the development team to ensure that all critical
    testing is performed and properly documented.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final two phases in the clean room process include implementation and monitoring.
    In these steps, the production process is for peak performance, including testing
    the deployment scripts, approving changes, vetting rollback procedures, freezing
    source code, and restricting access controls to automated processes. Finally,
    a release is cut and deployed to the production environment and proper application
    monitoring is conducted. Have a look at the diagram of the entire process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11: Capital One’s clean room release methodology](img/B21803_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.11: Capital One’s clean room release methodology'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing chaos engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even with multiple access controls and safeguards, software deployment can sometimes
    become chaotic. Cloud failures can be unpredictable and unavoidable, and they
    can pose risks in certain situations, such as availability zone blackouts. One
    could argue that continuous delivery also brings about the possibility of continuous
    chaos. Capital One has a dedicated team focused on addressing that specific issue.
    No one wants to accidentally automate their own destruction.
  prefs: []
  type: TYPE_NORMAL
- en: Conventional methods struggle to anticipate every possible failure scenario
    caused by intricate request patterns, unpredictable data conditions, and more.
    In 2017, Capital One took inspiration from Netflix and introduced its own form
    of chaos engineering.
  prefs: []
  type: TYPE_NORMAL
- en: The company implemented a disruption-causing tool called “Cloud Detour” to assess
    the resiliency of the applications that they build. At this stage, the development
    team intentionally subjects mission-critical applications to various failure scenarios
    for testing purposes. This aids in developing solutions that guarantee sufficient
    resiliency and function as a powerful disaster recovery exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding security principles in DevOps workflows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At first, Capital One adhered to a labor-intensive and time-consuming security
    certification process. Nevertheless, the company quickly recognized the significance
    of fortifying container environments to enhance its encryption and overall security
    posture across all systems services within the business.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, Capital One integrated automated security checks into its DevOps
    pipeline. It facilitated the accelerated evaluation of misconfigurations and vulnerabilities
    in their containers and virtual machine images. The DevOps team quickly obtained
    API privileges for vulnerability management and policy compliance tools that could
    be implemented into the CI/CD process. This allowed them to conduct essential
    tests, acquire reports, and initiate corrective measures without requiring the
    involvement of the security team.
  prefs: []
  type: TYPE_NORMAL
- en: What can we learn from Capital One’s DevOps transformation?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you can see, there are a lot of great insights that we might gain from studying
    how Capitol One achieved its DevOps Transformation. Among the numerous improvements
    that were made, some stand out:'
  prefs: []
  type: TYPE_NORMAL
- en: A DevOps transformation can take a long time. In the case of Capitol One, they
    started in 2010 and didn’t reach a state of maturity until 2020\. That is an entire
    decade. Be prepared to commit to such long time horizons before reaping the rewards.
    They do call it a journey for a reason.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speed is crucial in meeting the ever-shifting requirements of users. That is
    exactly what you can accomplish with the help of internal team collaboration and
    process automation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embracing DevOps practices and fostering team collaborations can inspire a culture
    of innovation and continuous experimentation. Adopting a fail-fast mindset will
    lead you to a practical solution in no time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing continuous monitoring practices can help your organization accomplish
    superior outcomes together with scalability, even if your processes initially
    had a sluggish start.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centralizing delivery tooling streamlines the development and management of
    each team’s tech stack, eliminating the need for individual silos. Minimizing
    redundant work and promoting resource sharing maximizes efficiency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud infrastructure allows for the flexible utilization of resources. As a
    result, you can easily expand and adapt to changing needs without limitations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thoroughly examine all current development processes and establish a standard
    of quality to attain optimal outcomes. Then, streamline quality control processes
    to reduce human error and facilitate DevOps compliance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This concludes [*Chapter 6*](B21803_06.xhtml#_idTextAnchor095). In our discussion,
    you’ve learned the basics of CI/CD from a release manager’s perspective. You now
    grasp how continuous integration incentivizes developers to continuously push
    their code to source control repositories, unifying their work into a single release.
    From there, we’ve reviewed why continuous delivery is such a powerful companion
    to continuous integration. Then, we examined all of the appropriate stages of
    a continuous delivery pipeline, and how it differs from a continuous deployment
    pipeline. Furthermore, you have become familiar with GitOps, a contemporary DevOps
    strategy that amplifies the concept of continuous deployment by introducing pull-based
    deployment tactics. Finally, we’ve examined continuous testing, the premier quality
    assurance strategy for any DevOps-centric software organization.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will be shown how to build a docker image containing
    a simple web application that deploys to AWS EC2, using GitHub Actions.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Answer the following questions to test your knowledge of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Can CI/CD pipelines be used to automate more than just the activities required
    for releasing and deploying software?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do software development teams need a unified set of technologies to work
    with in order to attain peak productivity?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the benefit of increasing commit frequency?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a continuous integration server and what does it do?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the primary difference between continuous delivery and continuous deployment?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the five primary phases of a continuous delivery pipeline?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is GitOps and how is it different from DevOps?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the distinction between automated testing and continuous testing?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the best way for software developers to detect and fix bugs or defects
    in their code before it ever gets committed in the first place?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What do canary deployments have in common with continuous testing?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
