["```\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<flow>\n<source name=\"GenerateFlowFile\" type=\"GenerateFlowFile\">\n<property name=\"batchSize\" value=\"1\"/>\n</source>\n<processor name=\"SplitText\" type=\"SplitText\">\n<property name=\"LineSplit\" value=\"\\n\"/>\n</processor>\n<destination name=\"LogAttribute\" type=\"LogAttribute\"/>\n</flow>\n```", "```\nimport boto3\n# create an S3 client\ns3 = boto3.client('s3')\n# create a bucket and enable encryption\nbucket_name = 'my-bucket'\ns3.create_bucket(Bucket=bucket_name)\ns3.put_bucket_encryption(\nBucket=bucket_name,\nServerSideEncryptionConfiguration={\n'Rules': [\n            {\n'ApplyServerSideEncryptionByDefault': {\n'SSEAlgorithm': 'AES256',\n                },\n            },\n        ],\n    },\n)\n```", "```\nimport redis\n# create a Redis client\nclient = redis.Redis(host='my-redis-host', port=6379)\n# cache a value\nclient.set('my-key', 'my-value')\n# retrieve a cached value\nvalue = client.get('my-key')\n```", "```\n# create a Nagios service check\ndefine service{\n  use                   generic-service\n  host_name             my-host\n  service_description   my-service\n  check_command         check_bigdata\n}\n# create a Nagios check command\ndefine command{\n  command_name          check_bigdata\n  command_line          /usr/lib/nagios/plugins/check_bigdata.sh\n}\n```", "```\nimport pandas as pd\nimport numpy as np\n# create a dataframe with missing values\ndf = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8]})\n# fill missing values with mean\ndf.fillna(df.mean(), inplace=True)\n```", "```\nfrom sklearn.preprocessing import OneHotEncoder\n# create a one-hot encoder\nencoder = OneHotEncoder()\n# encode categorical variables\nencoded_data = encoder.fit_transform(data)\n```", "```\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.regularizers import l2\n# create a neural network with L2 regularization\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(input_dim,), activation='relu', kernel_regularizer=l2(0.01)))\nmodel.add(Dense(16, activation='relu', kernel_regularizer=l2(0.01)))\nmodel.add(Dense(output_dim, activation='softmax'))\n```", "```\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n# define hyperparameters\nparams = {'n_estimators': [10, 100, 1000], 'max_depth': [None, 10, 50]}\n# create a random forest classifier\nrfc = RandomForestClassifier()\n# perform grid search\ngrid_search = GridSearchCV(rfc, params, cv=3)\ngrid_search.fit(X_train, y_train)\n# print best parameters\nprint(grid_search.best_params_)\n```", "```\nimport boto3\n# create an AWS client\nclient = boto3.client('autoscaling')\n# create an auto-scaling group\nresponse = client.create_auto_scaling_group(\nAutoScalingGroupName='my-auto-scaling-group',\nLaunchConfigurationName='my-launch-config',\nMinSize=1,\nMaxSize=10,\nDesiredCapacity=2\n)\n```", "```\nfrom scipy.stats import ks_2samp\n# calculate the Kolmogorov-Smirnov statistic\nstatistic, p_value = ks_2samp(x_train, x_new)\n# check for drift\nif p_value < alpha:\nprint('Drift detected')\n```", "```\nimport boto3\nimport sagemaker\n# create a SageMaker session\nsession = sagemaker.Session()\n# create an S3 bucket for storing training data\nbucket_name = 'my-bucket'\nbucket = session.default_bucket()\ns3_input = sagemaker.s3_input(s3_data=f's3://{bucket_name}/training_data.csv', content_type='csv')\n# create a training job\nestimator = sagemaker.estimator.Estimator('my-container', role='my-role', train_instance_count=1, train_instance_type='ml.m5.large', output_path=f's3://{bucket_name}/output')\nestimator.fit({'training': s3_input})\n```", "```\nimport google.auth\nfrom google.cloud import aiplatform\n# authenticate with Google Cloud\ncreds, project = google.auth.default()\nclient_options = {\"api_endpoint\": \"us-central1-aiplatform.googleapis.com\"}\nclient = aiplatform.gapic.JobServiceClient(\n    client_options=client_options, credentials=creds\n)\n# create a training job\njob_spec = {\n    \"worker_pool_specs\": [\n        {\n            \"machine_spec\": {\n                \"machine_type\": \"n1-standard-4\",\n            },\n            \"replica_count\": 1,\n            \"container_spec\": {\n                \"image_uri\": \"my-image\",\n                \"command\": [\"python\", \"train.py\"],\n                \"args\": [\n                    \"--input-path=gs://my-bucket/training_data.csv\",\n                    \"--output-path=gs://my-bucket/output\",\n                ],\n            },\n        }\n    ],\n}\nparent = f\"projects/{project}/locations/us-central1\"\nresponse = client.create_custom_job(parent=parent, custom_job=job_spec)\n```", "```\nimport azureml.core\nfrom azureml.core import Workspace, Experiment, Datastore, Dataset, Environment, ScriptRunConfig\n# authenticate with Azure\nworkspace = Workspace.from_config()\n# create a training experiment\nexperiment = Experiment(workspace, 'my-experiment')  \ndatastore = Datastore.get(workspace, 'my-datastore')\ndataset = Dataset.File.from_files(datastore.path('training_data.csv'))\nenvironment = Environment.get(workspace, 'my-environment')\nconfig = ScriptRunConfig(\n    source_directory='.',\n    script='train.py',\n    arguments=['--input-path', dataset.as_named_input('training').as_mount(), '--output-path', datastore.path('output').as_mount()],\n    environment=environment\n)\nrun = experiment.submit(config)\n```", "```\nAWSTemplateFormatVersion: '2010-09-09'\nResources:\nNotebookInstance:\nType: AWS::SageMaker::NotebookInstance\nProperties:\nInstanceType: ml.t2.medium\nRoleArn: !Sub \"arn:aws:iam::${AWS::AccountId}:role/MySageMakerRole\"\nNotebookInstanceName: MyNotebookInstance\nDirectInternetAccess: Enabled\n```", "```\nimport tensorflow as tf\n# load the dataset\n(train_images, train_labels), (\n    test_images,\n    test_labels,\n) = tf.keras.datasets.fashion_mnist.load_data()\n# preprocess the data\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\n# define the model\nmodel = tf.keras.Sequential(\n    [\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\n        tf.keras.layers.Dense(128, activation=\"relu\"),\n        tf.keras.layers.Dense(10),\n    ]\n)\n# compile the model\nmodel.compile(\n    optimizer=\"adam\",\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"],\n)\n# train the model\nmodel.fit(train_images, train_labels, epochs=10)\n# evaluate the model\ntest_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\nprint(f\"Test accuracy: {test_acc}\")\n```", "```\nimport boto3\nfrom botocore.exceptions import ClientError\nsagemaker = boto3.client(\"sagemaker\")\n# create a model\nmodel_name = \"my-model\"\nprimary_container = {\"Image\": \"my-container-image\"}\nmodel_response = sagemaker.create_model(\n    ModelName=model_name,\n    ExecutionRoleArn=\"my-execution-role\",\n    PrimaryContainer=primary_container,\n)\n# encrypt the model\ntry:\n    sagemaker.update_model(\n        ModelName=model_name,\n        EnableNetworkIsolation=True,\n        VpcConfig={\n            \"SecurityGroupIds\": [\"sg-1234\"], \n            \"Subnets\": [\"subnet-1234\"]\n\n        },\n    )\nexcept ClientError as e:\n    print(f\"Error encrypting model: {e}\")\n```", "```\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\n# create a Spark session\nspark = SparkSession.builder.appName('preprocessing').getOrCreate()\n# load the dataset\ndf = spark.read.csv('my-dataset.csv', header=True, inferSchema=True)\n# preprocess the data\nassembler = VectorAssembler(inputCols=['feature1', 'feature2', 'feature3'], outputCol='features')\npipeline = Pipeline(stages=[assembler])\npreprocessed_data = pipeline.fit(df).transform(df)\n```"]