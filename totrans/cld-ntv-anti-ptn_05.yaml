- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Delivering Rapidly and Continuously Without Compromising Security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have looked at the organizational changes required for cloud native
    development. In this chapter, we will start to look at the cultural and development
    practices that are required for a shift to cloud native. When shifting to cloud
    native software development, we’re sold the dream of rapid delivery and secure
    systems. However, this can only be achieved in practice with corresponding organizational
    and cultural change. Let’s explore how common anti-patterns in our software delivery
    life cycle can interrupt our journey to becoming a high-performing cloud native
    organization.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Underestimating the cultural impact
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequent change to meet business needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guardrails
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shifting left
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-sufficient teams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Underestimating the cultural impact
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Delivering cloud native projects rapidly and securely is primarily a cultural
    change. The technical and business process changes required to excel at cloud
    native delivery support cultural changes. We need to align the mentality of the
    team working on the project toward shared ownership of outcomes, breaking down
    the silos that may be present in the existing delivery process. The team that
    produces a change or feature should be responsible for its delivery into the production
    environment. This shift is the most fundamental aspect of delivering rapidly.
    In this section, we will start by reviewing a typical deployment process that
    we see often in clients who are just beginning their cloud native journey.
  prefs: []
  type: TYPE_NORMAL
- en: The siloed model – a long road to production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s examine a well-intentioned yet siloed delivery process that is usually
    the artifact of taking an on-premises approach to releases and applying it to
    cloud native delivery.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – A typical siloed release process – many touchpoints with little
    end-to-end ownership](img/B22364_05_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – A typical siloed release process – many touchpoints with little
    end-to-end ownership
  prefs: []
  type: TYPE_NORMAL
- en: This model works when releases are large, cumbersome processes that take significant
    effort to deploy, and the consequences of pushing a bad change are complex to
    recover from. We might use this model when deploying to a fleet of on-premises
    customer servers of varying capacity and capability. In the cloud, these constraints
    do not exist. We can make rapid changes with quick remediation if something goes
    wrong; a single, unified, homogenous production environment simplifies deployment
    and remediation.
  prefs: []
  type: TYPE_NORMAL
- en: Under this model, we heavily scrutinize all code before it reaches our production
    environment. However, its rigidity is also its downfall. When something inevitably
    goes wrong in one of these stages, the process, more commonly, is that the developer,
    who has already begun working on the next feature, must drop what they’re doing
    to create a fix applied at the level of review reached. It’s unlikely that this
    last-minute change will go through the review process as the process cannot afford
    to start over.
  prefs: []
  type: TYPE_NORMAL
- en: DORA – measuring the road
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: “*If you can’t measure it, you cannot improve it*.” We understand that the siloed
    model is limiting when applied to cloud native software, but as we change our
    delivery process, how do we know that our changes are shifting our business in
    the right direction?
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier, in [*Chapter 1*](B22364_01.xhtml#_idTextAnchor015), we introduced
    the DORA metrics to measure how well teams perform. If you are interested in the
    science behind these metrics, we recommend reading the DORA report or *Accelerate:
    The Science of Lean Software and DevOps: Building and Scaling High Performing
    Technology Organizations*. To recap, these metrics are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Deployment frequency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lead time for changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change failure rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time to restore service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If these are the metrics that most accurately predict team performance, we can
    see that ownership of delivery is not optional.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment frequency is suboptimal in the siloed configuration as we are tied
    to such an arduous release process. This release process also dictates our lead
    time for changes, as any changes must be aligned with the estimated schedule.
    We are also deploying much larger bundles of code at a time so that the chances
    of one of the features in the deployed bundle causing a change failure is now
    much higher, as the likelihood is now the sum of the likelihoods of each of the
    sub-components. Finally, the time to restore service is also much greater due
    to either rolling back a large change or sifting through a large release to find
    the culprit and apply a hotfix (which is also unlikely to go through the release
    process).
  prefs: []
  type: TYPE_NORMAL
- en: Leaving aside the metrics for high-performing teams, we also run into another
    issue around ownership. Who owns this change and is responsible for its success?
    The developer who wrote it? The change approval board that approved it? If it’s
    a security issue, does ownership lie with the security team? Siloing the release
    process also means siloing the ownership; without end-to-end ownership of the
    process, problems are much harder to fix.
  prefs: []
  type: TYPE_NORMAL
- en: Empowered teams – increasing your speed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This brings us back to the concept of empowered teams. If your team wrote it,
    you are responsible for its entire journey into production. Or, more succinctly,
    “*You build it, you run it*.” We can’t just throw all of our teams in the deep
    end and expect them to swim; they need to be supported. This is where those siloed
    teams from before come into play. They shift from being the gatekeepers of the
    journey toward the production environment to enablers of the dev team to undertake
    that journey themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Empowering and supporting teams to own their output is the core of rapid and
    secure delivery.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, to deliver rapidly without compromising security, the cultural shift
    is one of the most essential aspects, while also being an aspect that companies
    fail to target. For a team to own its output, each must have the skills and capabilities
    required to deliver a piece of work end to end, either internally through forming
    cross-functional teams or externally through an enabling team. Later in this chapter,
    we will explore ways to approach this from a shift-left and push-down approach.
    The key here is not to hand all control over to delivery teams but to ensure they
    are empowered and supported by those traditionally siloed functions to own their
    output.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to do this is to provide both the carrot and the stick to the
    development team. Enabling teams must produce platforms and artifacts the development
    team can consume to do their jobs in line with company standards. This might be
    in the form of authentication libraries, infrastructure as code patterns, common
    UI component libraries, and so on. Then, the enabling team should seek to automate
    guardrails to enable the developers to ensure that the code they are producing
    meets the same standards that had been manually enforced. This could be through
    the use of QA testing suites, **static application security testing** (**SAST**),
    and automated ticket creation systems for site reliability alarms in the observability
    platform. By enabling developers in this way, we empower them to own their output
    and shift left the responsibility of the deployment process.
  prefs: []
  type: TYPE_NORMAL
- en: DevSecOps – the super-highway to production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now revisit our deployment model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Target state for ownership of deployable artifacts](img/B22364_05_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Target state for ownership of deployable artifacts
  prefs: []
  type: TYPE_NORMAL
- en: 'Our other teams have remained in the organization. Instead, they are maintaining
    three sets of artifacts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Platform tools**: The systems and artifacts that developers consume to produce
    software artifacts that align with the responsible team’s requirements, such as
    shared auth libraries, structured logging libraries, or cloud infrastructure modules.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated pipeline**: This pipeline codifies each team’s expectations instead
    of relying on a manual review. As mentioned earlier, this may include QA automated
    tests, SAST, or container scanning tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Observability platform**: This platform codifies the expectations around
    the application’s performance and alerts developers for situations outside of
    normal operating parameters while storing information about these events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key difference here is that instead of the development team interfacing
    with five different teams to push features out to production, the development
    team is empowered to own the code they produce and deploy artifacts that meet
    a baseline standard to production. They can also see how the artifacts they produce
    perform through the observability platform. Hence, we’ve shifted the culture away
    from gatekeeping to enablement. This is the crux of DevSecOps, enabling software
    teams to develop, secure, and operate the code they write.
  prefs: []
  type: TYPE_NORMAL
- en: The magnitude of this change is typically underestimated in organizations undergoing
    a cloud native transformation. People can take offense to being taken off the
    critical path, considering that they relinquish some power they previously had.
    The mindset to install in these teams is that they are no longer at odds with
    the development team by stopping them from deploying to production but instead
    are stewards of their specialty for the development teams, producing artifacts
    and providing insights that help guide the development teams toward their own
    secure, high-quality, observable artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, development teams become more cross-functional, and team members
    need to upskill in security, quality assurance, and site reliability engineering
    under the guidance of these enabling teams.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now see a few distinct advantages of reducing these friction points:'
  prefs: []
  type: TYPE_NORMAL
- en: Deployment frequency increases as we no longer bundle on a schedule but instead
    deploy as soon as possible. This also results in a much shorter change lead time,
    as once a change is ready and passes our pipeline checks, it can easily be deployed
    to production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are now deploying much smaller units of code that often contain only a few
    features, which decreases the likelihood of the change failing and reduces our
    change failure rate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a platform to observe our application, which means that a change that
    results in an outage can quickly be identified, and a fix can be pushed through
    the automated pipeline. This is a key difference as, typically, hotfixes needed
    to be applied out of band, and we could not afford to run the fix through the
    whole pipeline. Instead, the automated pipeline can still be used as the developers
    do not need to interface with other teams to deploy the rectification. Hence,
    we have shifted toward a process of **continuous improvement and continuous**
    **delivery** (**CI/CD**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Staying on the road
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another fallacy that companies can quickly fall into is the belief that delivering
    features using DevSecOps and CI/CD principles will result in a large increase
    in development velocity, which means tighter deadlines. While it is true that
    the underlying improvements in the process will translate to the faster delivery
    of features, focusing solely on delivery timelines will ultimately undermine efficiency
    gains by the cultural shift.
  prefs: []
  type: TYPE_NORMAL
- en: If you are migrating from a fixed release schedule with tight deadlines and
    rigid business processes, it can be tempting to translate that directly into delivery
    schedules in the new paradigm. Instead, by decoupling feature work from the release
    process, we allow our development teams to obsess over output quality and only
    release features when ready from a multifaceted perspective. This ensures that
    we retain our increase in development velocity without compromising on code quality,
    and this leads us to a sustainable, rather than temporary, increase in development
    velocity.
  prefs: []
  type: TYPE_NORMAL
- en: Conway’s law states, “*Organizations which design systems are constrained to
    produce designs which are copies of the communication structures of these organizations*.”
    When we allow our teams to be siloed, we inevitably constrain their output to
    a particular part of the development/deployment process and their responsibilities
    and produce a deployment process replicating those teams’ communication pathways.
    Therefore, the logical conclusion is that to maintain all of the cultural changes
    we have prescribed in this chapter, we must encourage our teams to become self-sufficient.
    This enables the automated, independent production of change that we desire.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing a new map
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, as we shift to fully cloud native, one of the hardest anti-patterns
    to break can be the coupling between services and the compute they run on. New
    services are cheap to create, maintain, and run. Hence, we can form bounded service
    contexts that encapsulate a business domain.
  prefs: []
  type: TYPE_NORMAL
- en: '*Domain Driven Design* is a great read in this space; it goes into this topic
    in great detail. This allows us to evolve our architecture to meet our business
    domain needs rather than apply our business needs to our architecture because
    we installed a particular number of servers. Later in this book, we will dive
    into translating your business problems into application code and introduce the
    concepts of coupling and cohesion. The key for this chapter is to break the mentality
    that your architecture must fit into a predefined configuration. Conway’s law
    also applies to your architecture, and just as we break down silos in the deployment
    process, we must also break down silos between development teams to enable us
    to build the right solution in the right place.'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the cultural shift required to create genuinely cloud native
    solutions can take some organizations by surprise, so it is crucial to consider
    its magnitude. The key shift in thinking is about empowering and enabling teams
    to be self-sufficient and own their delivery from feature inception to running
    it in production through a cultural change from a siloed ownership and delivery
    model to a lean ownership model where developers are responsible for the changes
    they make, supported through DevSecOps enablement. Empowered development will
    allow us to deliver change faster, so let’s dive into how to enable frequent atomic
    changes to meet our business goals.
  prefs: []
  type: TYPE_NORMAL
- en: Frequent changes to meet business goals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we introduced the concept of the empowered development
    team. We worked on reducing the silos in the release process to allow ownership
    of the end-to-end release process. With this process, we can release much more
    frequently. Let’s explore the development changes that are enabling us to work
    under this new paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: Pruning Git branches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most deployment strategies will be multi-stage. For example, you may have environments
    called *development*, *integration testing*, and *production*. The understanding
    is that earlier environments have changes deployed first, so we can test our changes
    before being released to the production environment. Having multi-stage deployments
    is a pattern we recommend as it allows for the testing of features, either by
    the development team or through automated tests against a live environment, before
    we deploy the changes to production. With this strategy, adopting a pattern such
    as Gitflow may be tempting, where each environment is a self-contained branch.
    Let’s look at a typical Gitflow implementation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Typical Gitflow branching model](img/B22364_05_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Typical Gitflow branching model
  prefs: []
  type: TYPE_NORMAL
- en: This anti-pattern produces a false sense of security as we assume the changes
    are thoroughly tested in the lower environments before we push them to higher
    environments. However, with more people working on the code base and teams applying
    hotfixes, each branch’s contents will tend to drift over time. In the preceding
    diagram, we can see that we applied a hotfix to production, and the first time
    we tested our deployed feature alongside the hotfix, it was actually in the production
    environment. This uncertainty is the risk we run when maintaining environment-specific
    code bases. It also leans toward a backslide in cultural attitudes, as the temptation
    to introduce manual checks between environment promotions can be detrimental.
    Instead, the principle of a single code base deployed multiple times limits our
    environmental drift. An excellent online resource, *The 12 Factor App*, ([12factor.net](http://12factor.net))
    adopts this as the first factor.
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we operate a single code base effectively? Selecting a revision-based
    branching strategy, such as trunk-based development, is the easiest way to ensure
    that we operate from a single code base. Instead of an environment running the
    latest configuration in an environment branch, we have rules for running the last
    known correct configuration in a single branch, which we will promote to higher
    environments on an as-needed basis. Let’s take a look at the typical trunk-based
    development model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Trunk-based development branching model](img/B22364_05_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Trunk-based development branching model
  prefs: []
  type: TYPE_NORMAL
- en: In this example, each environment is deployed off the main branch, with lower
    environments having the most recent changes for testing and higher environments
    trailing behind on the main branch. By continuously integrating and deploying,
    we reduce our change lead time and increase our deployment frequency.
  prefs: []
  type: TYPE_NORMAL
- en: Decoupling integration and release
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now the complexity of promoting each environment is taken care of, we run into
    a problem not apparent in the branch or code base per environment system. How
    do we test or change features in one environment but stop them from blocking other
    features from deploying in higher environments? In the previous system, we could
    cherry-pick commits to promote specific features. The answer to this is twofold:
    we want an easy way for developers to test their features before merging them
    and an easy way to manage merged features in different environments.'
  prefs: []
  type: TYPE_NORMAL
- en: The simplest solution to manage merged features is to decouple the expression
    of a feature in the end product from its inclusion in the code base. For example,
    a feature can be complete and merged but not present in the deployed system. To
    achieve this, we use a concept known as **feature flags**.
  prefs: []
  type: TYPE_NORMAL
- en: Feature flags allow us to guard specific clauses in our application. A simple
    example would be a `if` statement, and if the feature flag is off, then we wouldn’t
    show the button. When the development team works on this feature locally, they
    can turn the feature flag on for testing. The deployed environments don’t have
    the feature flag turned on, so we can merge the feature into the main code base
    without impacting the deployed application. Once the feature is complete, we can
    turn the feature on in lower environments to test the feature in an actual environment.
    By decoupling the development of a feature from its expression in the end app,
    we also decouple the release decision for a feature from being technically driven
    (i.e., the feature is present in the code base and, therefore, it will be present
    in the deployed application) to a business decision; we can add features on demand.
  prefs: []
  type: TYPE_NORMAL
- en: To truly decouple the feature release decision from the technical implementation,
    it’s crucial to store feature configuration within the application environment.
    In this case, an anti-pattern would be to have files checked into version control
    called `features.prod.yml` and `features.dev.yml`, as, once again, we are creating
    checked-in concretions in our code base.
  prefs: []
  type: TYPE_NORMAL
- en: The best methodology for feature flagging is to check in a file to version control
    that defines the feature flags available and their state. In this file, we prefer
    to use something other than Booleans for feature flags as they become hard to
    extend later. Typically, we rely on enums. An example enum might consist of values
    called `Baseline`, `Configurable`, and `Off`. In this scenario, `Baseline` ensures
    a feature is on in all environments and is turned on by default when we deploy
    new environments. These flags represent mature features that are stable enough
    to be always on and are safe to use as the basis for new features. `Configurable`
    flags are features that we want to be able to change the expression of in various
    environments. These might indicate yet-to-be-released features, features that
    are undergoing testing, or features that are incomplete but in a usable state.
    These features need a way to be either on or off in deployed environments. We
    can achieve this through application configuration if the system is small or backed
    by a database table if the feature list is extensive. Finally, we have features
    configured as `Off`; these are feature flags that should not be available on any
    environments but are for features that are a work in progress and need to be able
    to be expressed locally.
  prefs: []
  type: TYPE_NORMAL
- en: To address the problem of developers needing to test locally, this is where
    the advantages of building cloud native software shine. A common anti-pattern
    we see is attempting to replicate the cloud in a local environment, and many services
    do this. However, there is no substitute for actively running your code in the
    cloud. With cloud native software, using principles such as **infrastructure as
    code** (**IaC**) and serverless/managed services, there is no reason why developers
    cannot spin up an isolated development cloud environment. This practice allows
    your developers to truly develop in the cloud. It also ensures your IaC avoids
    its anti-patterns, such as hardcoded references, as we regularly create and destroy
    new environments. The development team can now also test infrastructure changes
    independent of deployed environments. This decoupling feeds into the concept of
    empowered teams; developers can now control the code that runs, the infrastructure
    it runs on, and the services it interacts with. They also gain familiarity with
    the deployment process and get closer to the ideal of “*You build it, you run
    it*.” By allowing our developers to test in the cloud with a blast radius limited
    to the ephemeral environment they are testing in, we enable much more destructive
    types of testing. My load test on my ephemeral environment will not impact your
    testing on your ephemeral environment. By allowing this type of comprehensive
    testing in the pipeline, we reduce our change failure rate.
  prefs: []
  type: TYPE_NORMAL
- en: Reversing the inevitable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'No system is perfect, and as we increase the rate at which we deploy, the likelihood
    of one of those changes going wrong is eventually a certainty. According to the
    DORA report, the change failure rate is one of the metrics we should track for
    organizational performance. Although we strive to keep this metric as low as possible
    if a failure occurs, another DORA metric, **mean time to restore** (**MTTR**),
    comes into play. Three key anti-patterns prevent you from optimizing your MTTR
    when the cause is a change failure:'
  prefs: []
  type: TYPE_NORMAL
- en: Mutable artifacts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Destructive changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No reverse definition for a change
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first anti-pattern is using **mutable artifacts**, and all artifacts produced
    must be immutable. When your automated build pipeline produces an artifact as
    a deployment candidate, we must preserve the artifact throughout subsequent build
    pipeline runs. This immutability can be as simple as container versioning in a
    container registry or having all previous versions of an internal library available
    for installation at any point. By having immutable artifacts, it is simple to
    roll back the system to a known correct configuration. We can simply deploy an
    earlier artifact, and then we can triage the fixes in lower environments until
    we are ready to deploy to production again.
  prefs: []
  type: TYPE_NORMAL
- en: The second anti-pattern is **destructive changes**. Deployment of a new version
    of the system should allow us to roll back to previous instances of the application.
    For example, a destructive change would be dropping a database table or dropping
    a data store. When we deploy these changes, we can never roll the system back
    to the last known correct configuration because we have lost the system’s state
    in the destructive change. If a destructive change is required, it should go through
    a deprecation schedule before the final destructive change is applied to ensure
    that removing the functionality will not impact other application areas.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final anti-pattern is **no reverse definition for a change**. This anti-pattern
    primarily applies to stores of application state, such as databases or infrastructure
    changes. It is closely related to the second point: a change fundamentally cannot
    be reversible if it is destructive. The extension this rule applies is that any
    change to the system state, architecture, or data must be recoverable. This functionality
    exists for some tooling, such as Terraform comparing deployed infrastructure to
    a state file or a SQL Server project deployed via DACPAC. In other scenarios,
    the migration must explicitly define forward and reverse operations, such as through
    database SQL migrations using a tool such as Flyway or Entity Framework migrations.
    The common anti-pattern we see here is that the draft changes contain a detailed
    upward migration, and no one builds or tests the downward migration. This strategy
    means that significant work is often required when we need to roll back a change,
    as the reverse migration may need time to be created or may be non-functional
    if testing is not performed. This results in high-pressure, high-risk situations
    where we must craft changes while production is impacted, resulting in corners
    being cut to “just get it running again.”'
  prefs: []
  type: TYPE_NORMAL
- en: To summarize this section, complex release processes allow bad practices due
    to the high barriers between development and production. We can optimize delivery
    and increase service uptime by removing those barriers and supporting good practices.
    The key is frequent, flagged, well-tested changes that are immutable, non-destructive,
    and easily reversible when required. This allows us to develop faster, but we
    still need to ensure that our developers are doing the right thing, to do so we
    typically employ guardrails.
  prefs: []
  type: TYPE_NORMAL
- en: Guardrails
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve talked about empowering developers to do more and fully own the changes
    they produce. However, developers are not the experts; we see this fundamental
    anti-pattern in adopting the shift-left mentality. We should not expect developers
    to become experts in security, **site reliability engineering** (**SRE**), DevSecOps,
    and so on. Developers will need a passing knowledge of these topics but should
    be able to deploy with certainty without being experts.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping your developers on track
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common anti-pattern that we see in this space is that because we are shifting
    responsibility left toward the development team, the development team needs more
    permissions in the cloud environment to do their job. The exact opposite is true.
    Developers should have a constrained set of permissions to diagnose, inspect,
    and support the cloud environment. Instead, the escalation of privilege should
    occur in the CI/CD pipelines, and this is how we enable our developers by providing
    tooling with elevated permissions. By doing this, we ensure that our developers
    can deploy independently but not outside the confines of the CI/CD environment.
    This process limits the chance of environmental drift through manual configuration,
    which preserves disaster recovery functions.
  prefs: []
  type: TYPE_NORMAL
- en: The primary method of enabling developers to deploy with confidence is to provide
    guardrails in the deployment process. These guardrails define an acceptable set
    of actions a developer can take to achieve their objectives. For example, an infrastructure
    guardrail might prevent a **content delivery network** (**CDN**) from being deployed
    without a **web application firewall** (**WAF**) in front of it. A code-level
    guardrail might avoid the use of insecure hash functions. In both instances, we
    prevent changes from meeting a minimum standard when deploying them to production.
  prefs: []
  type: TYPE_NORMAL
- en: We might deploy guardrails to meet regulatory compliance requirements. For example,
    a cloud-wide policy that prevents any resources from being deployed in particular
    regions to support data sovereignty requirements might be employed. This example
    would be perfect for a **service control policy** (**SCP**) from **Amazon Web
    Services** (**AWS**). These allow us to enforce boundaries of acceptable use at
    different levels of granularity, from organization-wide to individual accounts.
    For example, we lock various accounts to a specific region, and globally, we prevent
    all accounts from deploying resources in export-controlled areas.
  prefs: []
  type: TYPE_NORMAL
- en: An anti-pattern in the security space is mistaking pentesting as a guardrail
    in the critical deployment path. Pentesting is a vital security step but should
    be outside the deployment path. Instead, it should run alongside the deployment
    process. We should automate all steps in the path to deployment. If you want to
    test the application security dynamically, consider using a **dynamic application
    security testing** (**DAST**) framework on one of the pre-prod environments as
    a pre-promotion check. The essential factor for guardrails is that developers
    should be able to access guardrail evaluations on demand.
  prefs: []
  type: TYPE_NORMAL
- en: Types of guardrail
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We commonly see two main types of guardrails: preventative and detective.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preventative guardrails** are proactive guardrails that specify the outer
    bounds of what developers can do; these are punitive, preventing the pipeline
    from deploying if the guardrails are activated. This methodology is suitable for
    applying easily defined heuristics (i.e., our hashing should not be using the
    MD5 hash algorithm). The common mistake we see with preventative guardrails is
    that they typically get implemented, and then the developers are left to fend
    for themselves. If the guardrail fails, they have to go back and fix it. A better
    workflow is to have observability into guardrail activation. These metrics will
    tell you where developers have the most trouble and allow you to make developers’
    lives easier by providing training, libraries of correct implementations, or,
    even better, an enabling artifact.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have **detective guardrails**. These reactive guardrails scan your
    environment for non-compliance, then either raise the issue or take corrective
    action. For example, we could enable our developers to deploy storage with public
    access through a CDN. However, if we tag particular storage containing PII (personally
    identifiable information), this tagging process might be out of band with the
    deployment of the storage itself. In this case, we could add a detective guardrail
    that checks for storage with public access, checks whether that storage account
    has the tag indicating that it contains PII, and then activates the guardrail.
    This type of control is typically the least favorable, as it requires an insecure
    configuration to be present in the environment to detect it instead of evaluating
    it proactively.
  prefs: []
  type: TYPE_NORMAL
- en: A typical driver of guardrails is security. Several tools exist to perform SAST
    to pick up common errors and allow the security team to define custom rules they
    want to look for. This space has excellent open source tooling (such as Semgrep)
    and many proprietary solutions. There is some upfront work to codify the anti-patterns
    you want to catch, but each codified heuristic is something that the security
    team no longer needs to review manually. Many available tools are not limited
    purely to security heuristics but can also check for anti-patterns such as deeply
    nested loops or cognitive complexity.
  prefs: []
  type: TYPE_NORMAL
- en: The case for guardrail observability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building guardrails is essential, but it is also important to monitor them.
    Developers use observability platforms to understand user behavior better and
    make changes to the applications they build to support it. We should do the same
    for our development team, who are effectively our users. By monitoring our guardrails,
    we can see the common friction points for our developers and proactively fix common
    issues. Let’s imagine our preventative guardrail from before requiring developers
    to deploy a WAF in front of a CDN. We might notice that our developers are hitting
    this guardrail very often, and hence, using the metrics we collect around guardrail
    activations, we build an enabling artifact. This artifact allows developers to
    avoid activating the guardrail and produce more secure artifacts without additional
    rework.
  prefs: []
  type: TYPE_NORMAL
- en: The key to enabling artifacts is to abstract away standard configurations using
    sensible defaults. Continuing with our WAF and CDN example, as a security team,
    we may introduce a default WAF that gets added to every CDN deployment if a developer
    forgets to specify one explicitly. If we already have a detective and preventative
    guardrail for this scenario, the enabling artifact minimizes the number of activations
    we encounter. When tracking metrics for these units, we recommend monitoring everything.
    Metrics about the enabling artifact tell you how often we activate the default
    WAF. These metrics can be helpful to track as they are a great way to measure
    enabling artifacts’ impact on the development team.
  prefs: []
  type: TYPE_NORMAL
- en: If enabling artifacts are the counterpart of preventative guardrails, the equivalent
    of detective guardrails is automated remediation. For our PII tagging solution,
    we could listen for the guardrail activation event and kick off a process that
    revokes public access to the bucket. This enables our system to be secure without
    outside intervention for simple use cases.
  prefs: []
  type: TYPE_NORMAL
- en: So, for a vulnerability or misconfiguration in our application, the preference
    is to have a sensible default added through an enabling artifact, such as a library
    or automated pipeline tool, then for cases outside of this to have them caught
    by preventative guardrails, and finally, if a misconfiguration makes it to production,
    then automated remediation or a manual process is initiated to rectify it.
  prefs: []
  type: TYPE_NORMAL
- en: These tools can exist in the pipeline and environment at all times. The final
    layer of security in this system should be pentesting, but it’s important to note
    that this needs to occur asynchronously with the deployment and development of
    the application. Ideally, the findings from penetration testing will feed back
    into our system of guardrails to help us develop new enabling artifacts and preventative/detective
    controls to stop the issue from resurfacing.
  prefs: []
  type: TYPE_NORMAL
- en: Example guardrails
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In cloud environments, there are unusual ways in which systems interact, as
    not only are you able to give deployed infrastructure and services the ability
    to interact with each other but also to interact with the underlying definitions
    of those resources. Now, we will go through some common guardrails that are easily
    applicable. By no means will this be exhaustive, but it will give you a head start.
    For our example, we will use AWS.
  prefs: []
  type: TYPE_NORMAL
- en: 'The guardrail examples we will use are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Overprivileged IAM accounts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Public S3 buckets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data sovereignty requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publicly accessible remote administration tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A simple place to start would be IAM permissions policies. I won’t go into detail
    about the principle of least privilege here, we’ll save that for a later chapter,
    but it’s common to see overprivileged accounts or roles defined. Typically, this
    arises because the user can’t find the correct permissions to perform the actions
    they require, so they end up assigning a long list of permissions, or wildcard
    permissions, while trying to make it work. This is actually a great candidate
    for all three methods of control discussed earlier; we can build common permissions
    policies that contain pre-approved policies for completing common tasks, for example,
    connecting our Lambda functions to a specific DynamoDB table. We can then also
    add a preventative control, such as an SCP in our account or organization to forbid
    access to particular APIs that are not in use. Finally, we can add a detective
    control that monitors all our active roles for policies that contain wildcard
    permissions, and revoke all associated grants and trust policies when one is discovered.
  prefs: []
  type: TYPE_NORMAL
- en: Another common misconfiguration that is a good candidate for guardrails is public
    access to S3 buckets. Any scenario using public access to an S3 bucket is typically
    better served through the use of a CloudFront distribution and an origin access
    identity. We can build an enabling artifact here in the form of a Terraform module
    that sets up a default configuration for a bucket and disables public access.
    We can build a preventative guardrail that checks our infrastructure plan to prevent
    this configuration. Finally, we can build a detective guardrail that scans our
    deployed infrastructure to ensure that no public buckets exist, and if they do,
    revoke public access.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many businesses also have the constraint of data sovereignty requirements;
    data for entities in a region must be stored in that region. Through the deployment
    of resources into regions that meet our data sovereignty requirements, we can
    be compliant. However, we are not provably compliant, as this approach requires
    the constant enforcement of a process. Instead, we can use a preventative control:
    we can build SCPs that lock resources in an account from being deployed in any
    region apart from the ones we specify. This approach must be proactive, as it
    only applies to new calls to the AWS API.'
  prefs: []
  type: TYPE_NORMAL
- en: The last common misconfiguration we see is directly opening remote administration
    tools to the internet. For example, your EC2 instances might expose port `22`
    to allow SSH for your developers, but now the attack surface for those instances
    just increased by every version of SSH those instances are running. This should
    be enforced at the network level, and typically, it’s good practice to have a
    detective guardrail (alongside preventative guard rails) in this instance. The
    temptation for a developer to open the SSH port in a `22` with unrestricted access
    and automatically close it.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, through reading this section, you have come to understand the types
    of guardrails that can be configured and how to enable your developers to best
    work within the boundaries you set, without impacting their development velocity.
    The importance of monitoring your organization’s guardrails has also been discussed,
    with these metrics providing us with a clear insight into both our security posture
    and developer experience. Finally, we have also looked at some common misconfigurations
    and explored how guardrails and enabling artifacts could mitigate the risk to
    the business. So, now we have the tools to enable our developers to securely and
    safely own their output, let’s look at how we can shift left the responsibility
    of producing secure artifacts onto our development teams.
  prefs: []
  type: TYPE_NORMAL
- en: Shifting left
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have touched on the requirement to shift left the responsibility of producing
    functional, secure changes to the development team. The focus so far has been
    on the externalities of the development team. This section will be about the effect
    of shifting left on the development team and the techniques we can use to meet
    the new expectations imposed upon them. We recommend reading this chapter if you
    are a developer, as you will learn some valuable tools, or as part of an external
    function, as it will allow you to support the development team better.
  prefs: []
  type: TYPE_NORMAL
- en: Development as an iterative process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common anti-pattern involves invoking **quality assurance** (**QA**) once
    a feature is complete in the eyes of the developer rather than a process that
    takes place iteratively over the software development life cycle. We spoke earlier
    about ephemeral environments, which are helpful for a developer to develop their
    feature in isolation and provide an environment where somebody can test the feature
    in its incomplete state. Different companies have different QA functions, which
    may come from a dedicated QA role or exist as part of the product team. The key
    here is that ephemeral environments allow you to involve the QA function much
    earlier in the process.
  prefs: []
  type: TYPE_NORMAL
- en: Another anti-pattern here is using long-lived feature branches with ephemeral
    environments. We develop the feature on multiple small branches, each containing
    a portion of the work required for the entire feature to operate. The ephemeral
    environment allows us an alternative deployment with enabled incomplete features.
    We establish a fast feedback cycle between developers and the QA process by getting
    QA involved from the first commit. Shifting left the QA to be parallel or integrated
    with the development allows us to mitigate the risk that a significant feature
    may fail QA once we have completed the feature, requiring significant rework to
    fix. It also brings us closer to a no-silo model by fostering collaboration between
    QA and development functions.
  prefs: []
  type: TYPE_NORMAL
- en: Test first, code later
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the anti-patterns that teams typically seem to acknowledge but fail to
    implement correctly is **test-driven development** (**TDD**) and, by extension,
    **behavior-driven development** (**BDD**). When asking development teams whether
    they use TDD or BDD, they usually answer in the affirmative, but when asked when
    they write their tests, they respond that they write the tests after the code.
    Cognitive dissonance aside, true BDD involves defining the behavior of your system
    and writing a test that can check for compliance with that behavior before actually
    implementing the system. Another fallacy that comes into play when implementing
    BDD is a waterfall-style approach to coding tests, specifying all the edge cases
    you foresee and writing too many tests upfront. A lot of system behavior and requirements
    only emerge through the actual implementation of the system, and writing too many
    tests up front just hampers this discovery process. Instead, an excellent approach
    to use in BDD is the red, green, refactor system. You define your desired behavior
    and write simple tests to ensure the system fulfills the desired behavior. These
    tests will fail (red), and we will then implement a system that passes these tests
    (green). Through the design of this system, we then refactor the implementation
    and the test suite to exhibit the desired behavior accurately and test the emergent
    behavior of the system. We must create the initial desired behavior with the product
    owner to ensure that the desired behavior test accurately reflects the behavior
    required in the end product. This process will shift left the desired behavior’s
    description to a point before we write any code.
  prefs: []
  type: TYPE_NORMAL
- en: Once we create tests, they should enter the deployment pipeline’s critical path.
    This practice ensures that future changes to the system do not prevent it from
    exhibiting the desired behavior required. A common anti-pattern that teams can
    fall into in this stage is failing to trust their tests’ output fully. In an extreme
    case, this might look like accepting a certain percentage of failed tests as “par
    for the course.” This lack of confidence in the tests undermines the value of
    the entire test suite, as developers no longer have confidence that their changes
    do not cause regressions on existing behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second and more common pattern is the existence of intermittent test failures.
    Intermittent failure commonly occurs when tests are not properly isolated, and
    the side effects of one test may influence the result of another test. Typically,
    in this scenario, the developers rerun the pipeline until the intermittent test
    passes. This behavior is counterproductive for two reasons: firstly, we’re increasing
    the time developers are waiting for pipeline executions to finish, and secondly,
    we have a failing test that we are ignoring. In this scenario, rather than putting
    up with the inconvenience of multiple pipeline runs, we should be adequately reestablishing
    the boundaries of our tests and creating a new test that checks for regression
    of the intermittent behavior. By vigorously enforcing these test suites, we shift
    left the responsibilities of detecting and rectifying regressions to the developer
    responsible for the regression as part of their development process rather than
    waiting for the regression to become apparent in the end product.'
  prefs: []
  type: TYPE_NORMAL
- en: Shared ownership of output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider Fred Brooks’s famous quote: “*What one programmer can do in one month,
    two programmers can do in two months*.” While tongue-in-cheek, the sentiment holds
    when we program in isolation. The increased communication channels and distribution
    of knowledge can make the development process more demanding, which leads us to
    our third common anti-pattern, **isolated development**. We have seen software
    teams where they only interact with each other in the daily standup. This system
    falls back into our old patterns of slow feedback cycles. If I have a daily standup
    and need the output from one other developer on my team to progress my feature,
    it may be complete 5 minutes after the standup, and I would need to wait until
    the next day to hear about it. I’ve seen high-performing development teams on
    a virtual call all day, splitting off the primary call to pair and mob program
    as required. The key differentiator here is that the high-performing team sees
    their delivery as a shared output rather than an individual output. This differentiator
    also needs to be reflected in how we track productivity metrics, which should
    reflect a team’s productivity, not the individual’s. Management of the individual
    is informed by feedback from other team members, as the team is the value we want
    to optimize.'
  prefs: []
  type: TYPE_NORMAL
- en: Small and regular changes, merged back to the code base, are also crucial from
    a code review perspective. Show a developer a 12-line pull request, and they will
    have plenty of comments. Show them a 1,200-line pull request, and you’ll likely
    get zero comments. Maybe you will get a response of “*Looks good to me*.” The
    main enemy of this approach is long-running feature branches. If you’re not regularly
    merging code with the main branch, then the reviewer does not stand a chance of
    understanding the scope of the change. Development processes that support small,
    atomic changes are essential here, such as trunk-based development and feature
    flagging, as discussed earlier in this chapter. When working toward deadlines,
    there is typically a tendency to approve pull requests with more relaxed standards
    to integrate changes in time. This approach, however, is a false economy. By approving
    lax changes, such as code that does not conform to coding standards or code with
    high cognitive complexity, we are simply robbing ourselves of future velocity
    and building up technical debt. The other side of this coin is that when we rigorously
    enforce coding standards at the pull request stage, we slowly start to see an
    uplift in the team, and future pull requests from the same team member are less
    likely to have the same mistakes. The failure to enforce coding standards is the
    key to our false economy. Enforcement versus non-enforcement of the coding standards
    eventually brings you to an equivalent or increased development velocity.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting and getting feedback early
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have examined the feedback loops in a typical software development business
    function. However, you should also look at feedback loops that may be more specific
    to your business. An example might be that an order-matching engine from a financial
    system might require the last six months of data to be fed into the system to
    ensure it reaches the same target state as the existing engine. To shift this
    left, we might use a smaller time range with dummy data that developers can run
    locally to get instant feedback. The key to shifting left is identifying these
    feedback loops and either putting them into the hands of developers directly or
    enabling developers to interact with the business unit responsible in the early
    stages of development. This business process optimization ensures that we are
    breaking down the chance of a late part of the process coming back with significant
    changes. To facilitate this, we recommend mapping out all the parts of the deployment
    process that occur once a change leaves the hands of a developer and finding the
    areas where this process experiences the most failures (i.e., requires rework
    by the developer). These parts of the process are your best candidates for shifting
    left. It’s important to note that, once again, you need metrics on the process
    to identify these stages, so observability in your deployment process is a significant
    factor in its efficacy.
  prefs: []
  type: TYPE_NORMAL
- en: Shifting left also requires the development team to be the source of truth in
    tackling a feature. The development team must be allowed to experiment before
    committing to a solution. A great way to approach this is through timeboxed technical
    spikes, possibly multiple in parallel if different approaches need testing for
    their efficacy. The crucial factor here is allowing developers to experiment,
    with the culmination of their work validating an idea or assumption rather than
    introducing a change or a new feature. This process is another area where ephemeral
    environments shine. Having a consequence-free sandbox to test a proof-of-concept
    idea allows the development team to, in the words of Mark Zuckerberg, “*move fast
    and break things*.” Even though this approach does not aim to produce a tangible
    outcome, typically, these technical spikes, if successful, will form the basis
    of a new change or feature. So, even though the goal was not to create a change,
    the technical spike often does not result in lost productivity.
  prefs: []
  type: TYPE_NORMAL
- en: Building in security from the start
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final requirement of the shift-left methodology is ensuring that security
    is part of the cloud native solution from the inception of the changes. Developers
    must all be conscious of the effects of their decisions on the overall solution
    security posture. Developers do not need to be security experts. Instead, they
    must shift their thinking from “*Does it achieve the required objective?*” to
    consider the new attack surface their changes could provide. An excellent way
    to guide a development team that is newly adopting shift-left methodologies into
    their way of working is to undertake threat modeling of a change as part of the
    development process. By shifting into the mindset of an attacker, we can quickly
    identify threats and put mitigations into place to defend against them. This exercise
    is even more effective if the security team is involved in the first few attempts.
    While the team’s goal is to become self-sufficient (a topic we will touch on soon),
    using enabling teams is essential to set a consistent quality across teams.
  prefs: []
  type: TYPE_NORMAL
- en: By shifting left, we have enabled our developers to produce secure, complete,
    and production-ready changes. Using appropriate tooling and business processes
    has increased the development velocity and given our developers the control and
    proper safeguards to apply themselves to finding the best solution. Now we have
    teams that are expected to own their output, we will next look at how to make
    these teams truly self-sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: Self-sufficient teams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With all the changes we have made in culture, process, tooling, and delivery,
    we expect our teams to become self-sufficient change factories. But how can we
    adjust our team’s internal structures to ensure that the team can organize and
    support these new ways of working?
  prefs: []
  type: TYPE_NORMAL
- en: Trifecta leadership
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spotify popularized a model known as the *Squad Model*. While, typically, it
    also ascribes much larger structures beyond the independent squads, we will focus
    on the structure of the squad itself. There is valid criticism of the larger model.
    However, this does not take away from the validity of the atomic team structure.
    The crux of the team is that it is a unit that works on a specific product. It
    contains a trio of leaders who orient the squad’s development efforts. These leaders
    are the engineering manager, responsible for the team’s technical direction; the
    product owner, who represents the customer; and the scrum master, who organizes
    the team’s efforts. By encapsulating the responsibilities of a team within the
    team itself and allowing the team the ability to work across the entire product,
    we can now scale these squads horizontally without linearly increasing management
    overhead. We are now venturing into scalable, agile delivery, which matches well
    with the requirements for cloud native development.
  prefs: []
  type: TYPE_NORMAL
- en: The key to successfully implementing this format is understanding that while
    the leadership is a trifecta, there is minimal overlap in actual responsibility.
    A common anti-pattern in this space is all developers reporting to the engineering
    manager. Developers are the implementers of change, and there is much more nuance
    to producing change in a system than technical direction. Instead, developers
    become stewards of the changes that they are making, understanding the product
    thinking behind it and the technical direction required to implement it. A great
    way to communicate this is through a concept called *commander’s intent*. This
    refers to the abstraction of leadership direction to encompass the goal, allowing
    for flexibility in our method. In its original form, the order might require us
    to move to a particular position on the battlefield, but the intent is to take
    a specific hill in concert with other units. If we focus on the *how* (moving
    to the positions), we might miss opportunities to accomplish the *what* (taking
    the hill).
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if we dictate the steps a developer must take to implement a feature,
    emergent opportunities that are only visible to the implementer might be overlooked.
    This situation is where the trifecta leadership and collaborative model of squads
    is potent. Not only can we communicate the commander’s intent of a particular
    change but developers also have local sources of authority to present these new
    opportunities for strategic direction.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, I worked in an organization that used a particular framework to
    attempt to safely scale agile development while retaining complete control over
    the product teams. This framework implementation resulted in a misalignment of
    strategic direction. In other words, the process did not empower the teams to
    be self-sufficient and capitalize on opportunities, as the requirement was to
    surface such opportunities at multiple layers before we could take action. The
    self-sufficient team is the antithesis of this paradigm. Rather than asserting
    control, we seek to empower and provide strategic direction while enabling strategic
    opportunity.
  prefs: []
  type: TYPE_NORMAL
- en: The topology of your teams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In their seminal work, *Team Topologies*, Matthew Skelton and Manuel Pais identify
    four types of teams in a DevSecOps-focused organization, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Stream-aligned teams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling teams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complicated subsystem team
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Platform team
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The stream-aligned team is an expert in a business domain, aligning its output
    with the strategic direction of that business area. This team is your organization’s
    primary type, directly focused on changes that will solve your business’s or its
    customers’ problems. Note that the organization of these teams is by business
    domain, while Conway’s law assumes that these domains will naturally be bounded
    contexts within our architecture. We should not constrain the team to own and
    operate only a particular part of the code base.
  prefs: []
  type: TYPE_NORMAL
- en: The enabling team supports the other team types in achieving their goals by
    providing technical guidance and enabling artifacts to the development team. For
    example, a dedicated security team might assist teams with unique security problems
    in their development process. It’s important to note that the existence of these
    teams does not absolve other teams of their responsibilities. These teams are
    enhancers, not replacements for self-sufficiency.
  prefs: []
  type: TYPE_NORMAL
- en: The complicated subsystem team deals with a subsystem that takes deep technical
    or engineering capability. This type of team is generally the only time we assign
    a team to a particular part of your organization’s architecture, and typically,
    the role of this team is to abstract the complicated subsystem so that other parts
    of the business can interact with it. A typical example might be a bank that still
    has a mainframe; we manage the mainframe with a complicated subsystem team that
    provides interfaces for other teams to interact with.
  prefs: []
  type: TYPE_NORMAL
- en: 'The platform team is the development team for your developers; they build internal
    products for which your other teams are the consumers. The platform might consist
    of standardized build pipelines and guardrails, enabling artifacts and tooling
    such as Git, ticket management software, and so on. As we discussed before, your
    metrics and customer development teams should guide this team’s strategic direction.
    These teams have three main modes of operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collaboration**: This involves teams working together for some time. These
    might be teams with closely related changes in progress, a stream team working
    with the platform team to develop new tooling, or a team working with a complicated
    subsystem team to evolve the service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**X as a service**: This model typically refers to complicated subsystem teams
    abstracting away technically complex functionality behind a service that other
    teams can consume.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facilitating**: This involves teams working together to achieve a particular
    team’s goals. For example, the security enabling team might facilitate changes
    to the authorization logic required by a stream-aligned team. This mode typically
    also involves empowering the team to be self-sufficient moving forward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When identifying these modes of operation, a few anti-patterns arise. The most
    common one is assuming that too many parts of your organization are complicated
    subsystems. The critical distinction is that complicated subsystem teams focus
    on something technically complex. A complex business domain is not a complicated
    subsystem. This method of thinking returns us to the trap of aligning our teams
    with our existing architecture rather than our business domains and allowing the
    architecture to grow out of the natural bounded contexts of those emergent domains.
  prefs: []
  type: TYPE_NORMAL
- en: When enabling teams need to facilitate the goals of stream-aligned teams, a
    common mistake they make is to assume that, as the experts in that area, they
    should just make the required changes. Fundamentally, to foster team self-sufficiency,
    the enabling team needs to mentor the stream-aligned team to improve the team’s
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it can be tempting to use the X-as-a-service pattern liberally for
    things that are the entire organization’s responsibility. A key example is security.
    Security is not a system we can develop in isolation and provide to developers
    as a service. It is the responsibility of every member of every team. We can build
    our platform tooling and enabling teams to incentivize and foster good security
    practices. The purpose of the X-as-a-service mode of interaction is to remove
    technical responsibility from the service consumers, which is counterproductive
    in the case of security.
  prefs: []
  type: TYPE_NORMAL
- en: Champions and the T-shaped engineer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we shift from a traditional delivery model to a cloud native delivery model,
    we also broaden the horizon of services we can consume. Rather than solving business
    problems in the same ways over and over again, we have the opportunity to leverage
    cloud native services. However, as we broaden our horizons, inevitably, we must
    educate our teams on the best practices for these new types of services. In the
    traditional model, every developer could understand the architecture and code
    patterns required. It is unreasonable to expect all of our team to become experts
    overnight; however, each of our developers will need to acquire a broad knowledge
    of cloud native services to identify when certain patterns should be used. This
    broad knowledge forms the top bar of the T-shaped engineer, a wide but shallow
    knowledge that is typically acquired through self-learning. When they use certain
    patterns repeatedly, they develop a deep understanding of specific implementation
    idiosyncrasies of the services involved, developing a deep knowledge. This forms
    the column of our T-shaped engineer, a deep but tightly scoped expertise. The
    idea is that with a few T-shaped engineers on the team, we have a diversity of
    technical opinions available to guide the technical direction of the team.
  prefs: []
  type: TYPE_NORMAL
- en: For business-wide, job-zero initiatives, such as security, accessibility, or
    code quality, we recommend electing champions within the teams to provide self-sufficient
    teams with the internal capability to meet their goals. It is then the responsibility
    of the governing group behind this initiative, which may be an enabling team,
    to support these champions in developing their field. This may include supporting
    self-learning through certification pathways, funding them to attend conferences,
    and providing internal knowledge-sharing opportunities. The key here is that the
    company must invest in its people for the initiative to succeed and yield results.
    It is simply not enough to continue business as usual. In the cloud space, technology
    and practices evolve rapidly; as a company, to maximize your return on cloud investment,
    you must invest in people.
  prefs: []
  type: TYPE_NORMAL
- en: Building cloud native capability within teams takes time; it is important to
    recognize the need to provide teams with all the tools and opportunities to become
    self-sufficient. To achieve this, we explored using a trifecta leadership of the
    product owner, engineering manager, and scrum master. We also looked at ways for
    teams to organize their interactions with each other. Finally, we looked at how
    we can grow initiatives in the organization and provide diverse opinions by encouraging
    T-shaped engineers and champion programs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Through ownership of output and team empowerment, we have transformed our development
    teams into genuinely self-sufficient powerhouses. We have tempered their output
    with automated processes and guardrails to ensure that they are working within
    the constraints required by our business. We have also looked at mitigating the
    impact any one negative change can have on the overall system. These atomic changes
    will form the basis of our new development model going forward. Next up, we will
    be looking deeper into maintaining security and compliance in cloud native environments.
  prefs: []
  type: TYPE_NORMAL
