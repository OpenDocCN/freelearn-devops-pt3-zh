- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Running Kubernetes with Other Cloud Pals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chances are that throughout this book, and perhaps even so far, you’re going
    to get whiplash by finding out about the number of places and different ways you
    can deploy Kubernetes. The reality is, you’re going to get even more whiplash
    in the real world. Whether you’re a full-time Kubernetes engineer or a consultant,
    every company that you go to is going to feel a bit different in the ways you’re
    deploying Kubernetes and where you’re deploying it.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, you learned about the three major Kubernetes cloud
    services – AKS, EKS, and GKE. However, there are a ton of other great options
    in the wild that are on private clouds and **Platform-as-a-Service** (**PaaS**)
    solutions. Although you’ll see a lot of organizations, ranging from start-ups
    to Fortune 200 companies and up, using popular Kubernetes cloud-based services
    such as AKS, EKS, and GKE, more and more organizations are starting to use private
    clouds for secondary Kubernetes clusters or even to save money because the larger
    cloud providers are typically far more expensive. Taking it to another level,
    some organizations are completely ditching the idea of having a Kubernetes cluster
    in the cloud and going PaaS, and you’ll see why in the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter and with the help of the previous chapter, you’ll
    be able to identify what solution your organization should go with and why it
    would be useful. From an individual perspective, you’ll walk away from this chapter
    with the know-how of multiple managed Kubernetes offerings. That way, you’ll be
    far more marketable in the job space and use all the different platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Linode Kubernetes Engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring DigitalOcean Managed Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Kubernetes PaaS and how does it differ?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, you should already know a bit about cloud technologies. The
    gist is that all clouds are more or less the same. There are differences in the
    names of the services, but they’re all doing the same thing more or less.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re comfortable with the cloud and have worked in a few cloud-based services,
    you’ll be successful in navigating this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To work inside the cloud-based services, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A Linode account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A DigitalOcean account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Red Hat account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An AWS account (for the final section of this chapter)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can sign up for all of these services and get free credit. Just ensure that
    you shut down the Kubernetes environments when you’ve finished running them to
    save money.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter can be found in this book’s GitHub repository at [https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch3](https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch3).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Linode Kubernetes Engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linode, recently acquired by Akami Technologies, is a developer-friendly private
    cloud that is very well-known for its easy dashboard and feature-rich platform
    that isn’t overly complex. Linode focuses on ease of use with a cloud -for -all
    mindset. Some key callouts for Linode include transparent pricing with almost
    zero guesswork, easily scalable workloads, a full/public API, and a GUI-based
    cloud manager. Most of all, Linode is known for its *always human* customer support.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to comparing Linode and other private cloud providers, Linode
    sticks out by offering cloud GPUs and high outbound transfer speeds, along with
    its customer support.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you’re going to learn about why you’d want to use **Linode
    Kubernetes Engine** (**LKE**) and how to set up the LKE portal, create a Kubernetes
    cluster in LKE manually, take the same manual process to automate it, and deploy
    your Kubernetes workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Why LKE?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you’re choosing a cloud, the last thing you want is to have to guess how
    much your monthly bill is going to be. This is why people are nervous about the
    cloud and even more nervous about serverless technologies. The monthly cost can
    be unknown, which isn’t the best answer to give to a CFO. With Linode, costs are
    bundled together, so you know exactly what you’re going to pay for, and that’s
    very important for billing administrators and engineers alike. When it comes to
    scaling, both horizontally and vertically, the last thing that any engineer wants
    to have to sit and figure out manually is how much the environment is going to
    cost the company every month.
  prefs: []
  type: TYPE_NORMAL
- en: Another large cost saving is with the Control Plane. Much like any Kubernetes
    cloud service, the idea is to abstract the Kubernetes Control Plane/API server
    away from you. That way, you don’t have to worry about managing anything other
    than worker nodes and the application(s). Linode doesn’t charge for the Control
    Plane, whereas other clouds do. For example, EKS and GKE charge a per cluster
    management fee of $10 per hour or $73.00 per month. Although this may not seem
    like a lot, for a start-up that’s getting by with bootstrap funding and has enough
    bills, they most likely don’t want one more.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up LKE manually
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you know the theory behind why you’d want to choose Linode, along with
    some pricing metrics and other aspects that make Linode great, it’s time to get
    hands-on and learn about setting up LKE.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this section, ensure that you are signed into Linode via a web browser
    of your choosing. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the Linode dashboard, choose **Kubernetes**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.1 – The LKE portal'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 – The LKE portal
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the blue **Create** **Cluster** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.2 – The Create Cluster button'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 – The Create Cluster button
  prefs: []
  type: TYPE_NORMAL
- en: 'Choose a name for your cluster, what region/location you want the LKE cluster
    to reside in, and the Kubernetes API version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Adding a Cluster label, region, and Kubernetes version'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 – Adding a Cluster label, region, and Kubernetes version
  prefs: []
  type: TYPE_NORMAL
- en: 'When choosing node pools, you have a few options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dedicated CPU**: Good for workloads where consistent performance is crucial
    for daily workflows'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shared CPU**: Good for medium workflows, such as a secret engine (something
    that isn’t getting a lot of traffic)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High Memory**: Good for RAM-intensive applications, such as older Java applications,
    in-memory databases, and cached data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For this section, you can choose **Shared CPU** as it’s the most cost-effective:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Worker node size'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 – Worker node size
  prefs: []
  type: TYPE_NORMAL
- en: 'To continue to keep things cost-effective, choose the **Linode 2 GB** option
    and ensure you scale it down to *1* node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.5 – The Add Node Pools page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 – The Add Node Pools page
  prefs: []
  type: TYPE_NORMAL
- en: 'In any production-level environment, you always want to think about **high
    availability** (**HA**). When it comes to Kubernetes, it’s no different. LKE offers
    the ability to enable HA for the Kubernetes Control Plane. For production environments,
    you’ll 100% want to implement this. For lab/dev environments (which is what you’re
    building now for learning purposes), you don’t have to enable HA. Once done, click
    the blue **Create** **Cluster** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.6 – HA Control Plane'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.6 – HA Control Plane
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’re familiar with the manual process of creating an LKE cluster,
    it’s time to learn how to automate it and make the process repeatable for production-level
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: Automating LKE deployments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you know how to create an LKE cluster manually, it’s time to learn
    how to create it with Terraform so you can ensure repeatable processes throughout
    your environment. In many production-level cases, you’ll run the following Terraform
    code within a CI/CD pipeline to ensure repeatability. For this section, you can
    run it locally.
  prefs: []
  type: TYPE_NORMAL
- en: First, you’ll see the `main.tf` configuration and then look at `variables.tf`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, there’s the Terraform provider. The provider will utilize the newest
    version of the Linode Terraform provider. For Terraform to interact with the Linode
    API, you’ll need to pass in an API key that you can create from your Linode account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, there’s the `linode_lke_cluster` resource, which will create the LKE
    cluster. Within the dynamic block, you’ll see a `for_each` loop that specifies
    how many worker nodes will be created based on the pool amount. The pool amount
    is the number of worker nodes you want to deploy (between 3 to 4 is recommended
    for production):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The last piece of code is the output of `kubeconfig`, which contains all of
    the authentication and authorization configurations to connect to the Kubernetes
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now that you have the main Terraform configuration, you’ll need variables to
    pass in. These variables allow your code to stay repeatable so that you don’t
    have to continuously change hardcoded values or create new configurations for
    each environment. The reason why is that due to formatting, it may look out of
    the ordinary on your page while you’re reading this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: For this section, you can take a look at the variables on GitHub at [https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/blob/main/Ch3/LKE/variables.tf](https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/blob/main/Ch3/LKE/variables.tf).
  prefs: []
  type: TYPE_NORMAL
- en: Although these are all standard Terraform variables and don’t require much explanation,
    the one variable to point out is the `pools` variable. Notice how there’s a list
    type specified for the variable, which includes how many worker nodes and the
    size of the worker nodes on Linode. The reason why the variable is a list type
    is that in the `main.tf` configuration, the `dynamic “pool”` block calls for a
    list when using the `for` loop.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to keep in mind when it comes to LKE is understanding Linode. Although
    Linode is a great cloud provider, the truth is, it’s not going to have as many
    services and features for Kubernetes as, for example, EKS. Taking EKS as an example,
    there are IAM roles and RBAC-related permissions you can configure, DNS management
    with Route53, Secrets management, a container registry, and Fargate profiles for
    serverless Kubernetes. Even Azure and GCP have very similar services. With a provider
    such as Linode, however, they don’t. That’s not to discount Linode or say that
    they aren’t a good Kubernetes provider because the truth is, they very much are.
    However, a situation such as Linode not having IAM/RBAC built-in capabilities
    may be a deal breaker for many production engineering and security teams.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to create an LKE cluster both manually and automatically,
    it’s time to move on to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring DigitalOcean Managed Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DigitalOcean, much like Linode, markets toward the notion of an easy cloud to
    use compared to other large clouds with (what feels like) millions of services
    to choose from. DigitalOcean’s slogan is *Simpler cloud. Happier devs. Better
    results*. Over the years, DigitalOcean wasn’t only known for its cloud platform,
    but its blogs and how-to guides. DigitalOcean, for many engineers, became the
    standard go-to online location for learning how to do something in a hands-on
    fashion. Many writers use the DigitalOcean Technical Writing Guidelines that DigitalOcean
    created for writer/blogger best practices.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you’re going to learn about why you’d want to use **DigitalOcean
    Managed Kubernetes**, the pros of the Kubernetes service, setting up DigitalOcean
    Managed Kubernetes manually, and taking the same manual process, but doing it
    in an automated fashion with Terraform.
  prefs: []
  type: TYPE_NORMAL
- en: Why DigitalOcean Kubernetes Engine?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since DigitalOcean was founded in 2011, developers around the globe have been
    using it for its ease of use and straightforward deployments. A lot of engineers
    even use DigitalOcean for hosting their projects (personal websites, blogs, servers,
    and so on). It’s far easier in many cases than having to worry about creating
    a bunch of services in a large public cloud.
  prefs: []
  type: TYPE_NORMAL
- en: From an ease-of-use perspective, DigitalOcean Kubernetes Engine does not disappoint.
    Much like any other Kubernetes service, the purpose is to abstract away the need
    to manage the underlying Control Plane/API server. The whole idea here is to lower
    the barrier of entry when it comes to using a Kubernetes service.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to other products such as EKS/GKE/AKS, DigitalOcean Kubernetes Engine
    is more focused on the Day Two operations piece of Kubernetes. The complexities
    of a lot of the Kubernetes services out in the wild sometimes make engineers run
    away because they want something that *just works* out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: DigitalOcean Managed Kubernetes, although easy to use, appears to be a bit out
    of date from a Kubernetes API perspective compared to its counterparts. Whereas
    many Kubernetes services offer Kubernetes API version v1.23 and above, DigitalOcean
    only offers up to v1.22.8 at the time of writing. Keep this in mind and remember
    to check as you may need different API versions.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up DigitalOcean Managed Kubernetes manually
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you know the theory behind why you’d want to choose DigitalOcean, along
    with some pricing metrics and other aspects that make DigitalOcean great, it’s
    time to get hands-on and learn about setting up DigitalOcean Kubernetes Engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this section, ensure that you are signed into DigitalOcean via a web browser
    of your choosing. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the DigitalOcean dashboard, choose **Kubernetes**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.7 – DigitalOcean Managed Kubernetes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.7 – DigitalOcean Managed Kubernetes
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the blue **Create a Kubernetes** **Cluster** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.8 – The Create a Kubernetes Cluster button'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.8 – The Create a Kubernetes Cluster button
  prefs: []
  type: TYPE_NORMAL
- en: 'Choose your region, VPC name, and Kubernetes API version. The recommended API
    version, in general, is to always go with the latest unless you have a specific
    reason not to (the same rule applies to any Kubernetes environment):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Adding cluster details'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.9 – Adding cluster details
  prefs: []
  type: TYPE_NORMAL
- en: 'Choose the cluster capacity. The two very important sections here are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Machine type**: For this, you’ll have to choose what’s best for you and your
    production environment. Although DigitalOcean doesn’t have as many options as
    Linode, you can choose from a basic node, Intel-based node, or AMD-based node
    from a CPU perspective.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High availability Control Plane**: For this, you’ll always want to ensure
    that the Control Plane is highly available. The Control Plane holds the scheduler,
    etcd, and many other important Kubernetes components. Without them, Kubernetes
    wouldn’t work.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Worker node size'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.10 – Worker node size
  prefs: []
  type: TYPE_NORMAL
- en: 'Confirm your cluster by reviewing the monthly charge and clicking the green
    **Create** **Cluster** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Finalizing the cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.11 – Finalizing the cluster
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’re familiar with the manual process of creating a DigitalOcean
    Kubernetes Engine cluster, it’s time to learn how to automate it and make the
    process repeatable for production-level environments.
  prefs: []
  type: TYPE_NORMAL
- en: Automating DigitalOcean Managed Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From an automation perspective, you have a few options. Two of the most popular
    are the DigitalOcean CLI and **Infrastructure as Code** (**IaC**). In this section,
    you’ll learn how to create a DigitalOcean Managed Kubernetes cluster using Terraform.
  prefs: []
  type: TYPE_NORMAL
- en: In many production-level cases, you’ll run the following Terraform code within
    a CI/CD pipeline to ensure repeatability. For this section, you can run it locally.
  prefs: []
  type: TYPE_NORMAL
- en: Like we did for LKE, first, we’ll see the `main.tf` configuration and then you’ll
    take a look at `variables.tf`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Terraform configuration starts as all others do: with the provider. The
    DigitalOcean Terraform provider requires you to pass in a DigitalOcean API token,
    which you can generate from the DigitalOcean UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, one resource block is needed, which is used to create the entire cluster
    and the node pools. These are DigitalOcean Droplets that end up being Kubernetes
    worker nodes. It also creates horizontal auto-scaling. For some DigitalOcean accounts,
    the maximum Droplet amount is three, so you’ll most likely want to increase that
    for production environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now that you have the Terraform configuration, you’ll need variables to pass
    in. These variables allow your code to stay repeatable so that you don’t have
    to continuously change hardcoded values or create new configurations for each
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four variables, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`region`: The region that the DigitalOcean Kubernetes Engine cluster will run
    in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cluster_name`: The name of the Kubernetes cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`K8s_version`: The Kubernetes API version.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_token`: The DigitalOcean API token. For production-level environments,
    you’ll want to store this in some type of secret store and have Terraform retrieve
    it with a data block. Writing an API token into a variable and pushing it up to
    source control is a big no-no:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Wrapping up this section on DigitalOcean, one thing to keep in mind is the same
    piece that was said in the *Understanding Linode Kubernetes Engine* section –
    bigger cloud providers are going to have more services that can tie into the managed
    Kubernetes offerings. This is something you’ll have to keep in mind as you decide
    what’s going to work best for your environment.
  prefs: []
  type: TYPE_NORMAL
- en: In the next and final section of this chapter, you’ll learn about PaaS with
    OpenShift from a theoretical and hands-on perspective.
  prefs: []
  type: TYPE_NORMAL
- en: What is Kubernetes PaaS and how does it differ?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying Kubernetes clusters in different ways felt to engineers like it came
    in waves. First, there were raw Kubernetes clusters. You’d have to deploy everything
    manually, ranging from the Control Plane to the **Certificate Authority** (**CA**)
    and everything in between. After that, there were Kubernetes services in the cloud,
    such as AKS, GKE, and EKS. Now, there are serverless Kubernetes such as GKE AutoPilot
    and EKS Fargate, which you learned about in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Another option that stands out, especially in the enterprise, is PaaS-based
    Kubernetes solutions such as Red Hat’s **OpenShift**.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you’re going to learn about why you’d want to use OpenShift,
    how enterprises are utilizing PaaS-based Kubernetes such as OpenShift, and how
    to get started with a Dev environment right on your local computer with OpenShift,
    develop and deploy production-ready OpenShift clusters in major cloud providers,
    and deploy production-ready applications inside of OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenShift is an odd paradox between full-blown Kubernetes and its own orchestration
    system. Underneath the hood, OpenShift uses Kubernetes. If you write a Kubernetes
    manifest for a Deployment, Pod, and so on, you can use it on OpenShift. Fundamentally,
    nothing changes when it comes to Kubernetes and OpenShift. However, there are
    differences in how you manage OpenShift versus how you manage Kubernetes. OpenShift
    is a PaaS, whereas Kubernetes can be managed with a cloud provider, so it sort
    of feels like **Software as a Service** (**SaaS**) and can be managed from a bare-metal
    perspective. Because Kubernetes is such a versatile platform, it can’t be put
    into one category.
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing you must remember when it comes to OpenShift is that it’s enterprise-specific.
    There’s no reason that an engineer would run OpenShift for a lab environment other
    than to learn (which is what you’re doing in this chapter). With Kubernetes, you
    have far more deployment options and options regarding where you can deploy it.
    With OpenShift, you’re limited to a certain type of virtual machine and where/how
    you can deploy it. This isn’t necessarily a bad thing in the slightest. OpenShift
    wasn’t meant for engineers to do labs with like minikube and Docker Desktop. It
    was built with enterprise customers in mind. If you’re interested in diving deeper
    into this topic, I highly recommend reading this blog post from Tomasz Cholewa
    on comparing Kubernetes with OpenShift: [https://blog.cloudowski.com/articles/10-differences-between-openshift-and-kubernetes/](https://blog.cloudowski.com/articles/10-differences-between-openshift-and-kubernetes/).'
  prefs: []
  type: TYPE_NORMAL
- en: The definition of OpenShift, as per Red Hat, is that “*Red Hat OpenShift delivers
    a complete application platform for both traditional and cloud-native applications,
    allowing them to run anywhere. Built on Red Hat Enterprise Linux and compatible
    with Red Hat Ansible Automation Platform, Red Hat OpenShift enables automation
    inside and outside your* *Kubernetes clusters.*”
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, it allows you to orchestrate and manage containerized applications
    in a PaaS environment.
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift in the enterprise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, you may be wondering why anyone would want to use OpenShift over
    a standard Kubernetes deployment. Kubernetes has a ton of support, is supported
    by all major cloud providers, and is the latest and greatest. When it comes to
    the enterprise, Kubernetes is thought of a bit differently.
  prefs: []
  type: TYPE_NORMAL
- en: To leadership teams, Kubernetes is often thought of as a black box of magic
    and mystery that’s going to cost them a ton of money to maintain and support.
    The reality is that in enterprise environments, leadership teams want the ability
    to call a support number or contact an account executive when something breaks.
    They want the *enterprise software* so that if (when) something goes wrong, they
    know that the engineering teams have someone to call. Even though engineers will
    most likely spend more time waiting to hear back from support than doing it themselves,
    *enterprise licensing* gives leadership teams peace of mind. However, with peace
    of mind comes cost. OpenShift licensing is very expensive and remember, you have
    to run it somewhere, which will cost you money as well. If you run OpenShift in,
    for example, AWS, you’re paying for the cloud infrastructure running in AWS, OpenShift
    licensing, and Red Hat support. If you decide to go the OpenShift route, ensure
    that your leadership teams understand the cost.
  prefs: []
  type: TYPE_NORMAL
- en: From a technical and engineering perspective, OpenShift isn’t doing anything
    differently than what Kubernetes can do. Sure, to have Kubernetes do exactly what
    OpenShift does would require some work and engineering efforts to build it, but
    it’s all very much doable. Although OpenShift is a great platform, it’s not doing
    anything overly extraordinary compared to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with OpenShift Sandbox
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before spending money on OpenShift, you can test it out using OpenShift ReadyContainers
    in a sandbox environment. Although the sandbox environment is not production-ready,
    it’s a great way to test out and familiarize yourself with how OpenShift works.
    It’s also great for lab environments! Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If you’re on an M1 Mac, OpenShift ReadyContainers are not currently supported
    for ARM devices, so this lab won’t work for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Log into the Red Hat console: [https://console.redhat.com/](https://console.redhat.com/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on **OpenShift**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Red Hat console'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.12 – Red Hat console
  prefs: []
  type: TYPE_NORMAL
- en: 'Choose the **Clusters** option:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.13 – Clusters'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.13 – Clusters
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the **Clusters** option, you’ll see three options – **Cloud**, **Datacenter**,
    and **Local**. Choose **Local**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Local cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.14 – Local cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'Download OpenShift locally by clicking the blue **Download OpenShift** **Local**
    button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Download OpenShift Local'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.15 – Download OpenShift Local
  prefs: []
  type: TYPE_NORMAL
- en: 'Once OpenShift Local has been installed, you will need to run two commands
    (the instructions for installing CRC can be found at [https://crc.dev/crc/#minimum-system-requirements-operating-system_gsg](https://crc.dev/crc/#minimum-system-requirements-operating-system_gsg)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`crc setup`: Set up the configuration to authenticate to Red Hat'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`crc start`: Start the local OpenShift cluster:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.16 – OpenShift Local setup'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.16 – OpenShift Local setup
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you run the `crc start` command, you’ll see an output on your terminal
    similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Starting OpenShift Local'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.17 – Starting OpenShift Local
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on when you’re reading this and based on version changes, you may
    need some configurations, including passing in an OpenShift token to authenticate
    from your localhost. To keep these steps brief and since this information is already
    available from Red Hat, you can follow the installation instructions here: [https://access.redhat.com/documentation/en-us/red_hat_openshift_local/2.5/html/getting_started_guide/installation_gsg#installing_gsg](https://access.redhat.com/documentation/en-us/red_hat_openshift_local/2.5/html/getting_started_guide/installation_gsg#installing_gsg).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although we didn’t touch on it in this section, there’s a second lab environment
    option known as OpenShift Sandbox, which is different than ReadyContainers. You
    can set up OpenShift Sandbox here: [https://developers.redhat.com/developer-sandbox](https://developers.redhat.com/developer-sandbox).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to get an OpenShift sandbox up and running, let’s learn
    how to set up a production-ready OpenShift cluster on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift on AWS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CodeReady containers are amazing because they allow you to utilize your local
    computer to learn OpenShift, much like Minikube allows you to learn Kubernetes
    and Docker Desktop allows you to learn Docker for free.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know about the free version, let’s quickly dive into how to deploy
    OpenShift to the cloud. In this example, you’ll learn about AWS, but the other
    cloud providers that are supported have the same workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this section, ensure that you are logged into your AWS console via the
    AWS CLI and that you are also logged into your Red Hat account. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Log into the Red Hat console: [https://console.redhat.com/](https://console.redhat.com/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on **OpenShift**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.18 – OpenShift manager'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.18 – OpenShift manager
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the **Clusters** button and then click the blue **Create** **cluster**
    button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.19 – Creating a cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.19 – Creating a cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll see several options to choose from, including Azure and IBM Cloud. Click
    the blue **Create cluster** button under the AWS option:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.20 – AWS ROSA'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.20 – AWS ROSA
  prefs: []
  type: TYPE_NORMAL
- en: 'The first page you’ll see associates your AWS account with Red Hat if you haven’t
    done so already. To do that, click the **Select an account** button and go through
    the walk-through of configuring ROSA, which is the Red Hat OpenShift service on
    AWS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.21 – The Create a ROSA Cluster page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.21 – The Create a ROSA Cluster page
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you’ve set up the AWS account permissions and roles for ROSA, the next
    page is all about configuring the cluster, which includes the cluster name, OpenShift
    version, region, and availability options. One of the cool options that OpenShift
    gives you is the ability to encrypt etcd and create persistent volumes with customer
    keys. This added security is typically looked at closely within the enterprise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.22 – Adding cluster details'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.22 – Adding cluster details
  prefs: []
  type: TYPE_NORMAL
- en: 'Choose your machine pool, which includes the AWS EC2 instance size and autoscaling
    capabilities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.23 – Worker node size'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.23 – Worker node size
  prefs: []
  type: TYPE_NORMAL
- en: 'Next are your networking options, which include whether you want the Kubernetes
    Control Plane/API server to be public or private, and whether you want to create
    a new AWS VPC for OpenShift or install the ROSA cluster into an existing VPC.
    Once you choose the **Virtual Private Cloud (VPC)** option, you’ll have to choose
    the CIDR ranges for the internal Kubernetes Pod networking and cluster IP ranges:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.24 – Networking configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.24 – Networking configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'For cluster roles and policies, you have the option to manually set up the
    roles and policies or have OpenShift automatically do it for you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.25 – Cluster roles and policies'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B19116_03_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.25 – Cluster roles and policies
  prefs: []
  type: TYPE_NORMAL
- en: For the last step, you can choose how you want to implement updates for the
    ROSA cluster. The updates that occur are based on CVE scores from the **National
    Vulnerability** **Database** (**NVD**):![Figure 3.26 – Vulnerability scanning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B19116_03_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.26 – Vulnerability scanning
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve filled in all your options, you can officially create your ROSA
    cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that you know how to get up and running with OpenShift on-premises and in
    the cloud, let’s summarize this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regardless of what option you decide to go with when deploying Kubernetes, whether
    it’s in a big cloud, a smaller cloud, or a PaaS solution, the goal is always the
    same and never changes – build an orchestration platform that can manage your
    containerized applications.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of fancy tools out there, tons of different platforms, and many
    promises that each new and fancy platform will make your life easier from a Kubernetes
    perspective. The truth is, in one way or another, they all have the same goal.
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to use Kubernetes to orchestrate and manage containerized applications.
    Ensure that as you go through each platform and tool, you have this in mind –
    *orchestrate my containerized applications*. If you keep that in mind, it’ll make
    choosing and getting through the marketing fluff much easier.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll be learning about on-premises Kubernetes and how
    understanding the underlying components of Kubernetes clusters is important, as
    well as why.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Learning OpenShift*, by Denis Zuev, Artemii Kropachev, and Aleksey Usov: [https://www.packtpub.com/product/learn-openshift/9781788992329](https://www.packtpub.com/product/learn-openshift/9781788992329)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
