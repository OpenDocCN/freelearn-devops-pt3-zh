<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer063">&#13;
			<h1 id="_idParaDest-77" class="chapter-number"><a id="_idTextAnchor078"/>4</h1>&#13;
			<h1 id="_idParaDest-78"><a id="_idTextAnchor079"/>The On-Prem Kubernetes Reality Check</h1>&#13;
			<p>I know what you’re thinking – <em class="italic">0n-prem? Why is this guy teaching us about on-prem Kubernetes? It’s all in </em><span class="No-Break"><em class="italic">the cloud!</em></span></p>&#13;
			<p>Although it may seem like that from tech marketing and large cloud providers screaming Kubernetes at the top of their lungs, in the production world, there are a lot of on-prem Kubernetes clusters and a lot of engineers managing them. Mercedes-Benz, a popular German car manufacturer, hosts over 900 Kubernetes clusters on OpenStack, which is a private cloud solution. All those clusters are sitting in a data center, not in the public cloud. If you peel back the layers of the onion and wonder how cloud providers are running Kubernetes clusters, they’re doing something similar. They have several data centers that are running Kubernetes just like you would on-prem or on a server that you can buy on eBay. The only difference is the cloud providers are running Kubernetes at a major scale around the world. However, the <em class="italic">how</em> in how Kubernetes is running isn’t any different than how anyone else is <span class="No-Break">running Kubernetes.</span></p>&#13;
			<p>The truth is, this chapter could be an entire book – it could probably be a few books. Kubernetes, especially when it’s not abstracted away in the cloud, is an extremely large topic. There’s a reason why people say that Kubernetes is like a data center within itself. How and where you run on-prem Kubernetes alone is a deep topic. For example, what size infrastructure to use, how to scale your workloads, vertical and horizontal scaling, network bandwidth, high availability, and a lot more go into the conversation of what systems to use and where to <span class="No-Break">run them.</span></p>&#13;
			<p>By the end of this chapter, you’re going to understand just how complex running on-prem Kubernetes can be, but at the same time, how rewarding it can be to an organization that’s putting a lot of effort into Kubernetes. You’ll have the hands-on skills and theoretical knowledge to understand how to think about scaling an organization’s Kubernetes environment. One thing you’ll learn from this chapter is it’s a lot less about using the <em class="italic">cool tech</em> and more about thinking from an architecture perspective about how a platform team <span class="No-Break">should look.</span></p>&#13;
			<p>In this chapter, we’re going to cover the <span class="No-Break">following topics:</span></p>&#13;
			<ul>&#13;
				<li>Understanding operating systems <span class="No-Break">and infrastructure</span></li>&#13;
				<li>Troubleshooting on-prem <span class="No-Break">Kubernetes clusters</span></li>&#13;
				<li>Introducing <span class="No-Break">hybrid services</span></li>&#13;
				<li>Exploring networking and <span class="No-Break">system components</span></li>&#13;
				<li>Getting to know virtualized <span class="No-Break">bare metal</span></li>&#13;
			</ul>&#13;
			<p>This chapter will be a combination of hands-on and theoretical knowledge. Because we only have one chapter to cover this topic, it’s safe to say that we can’t show everything you’ll need to know. However, this should be a good starting point for your <span class="No-Break">production journey.</span></p>&#13;
			<h1 id="_idParaDest-79"><a id="_idTextAnchor080"/>Technical requirements</h1>&#13;
			<p>To complete this chapter, you should first go over <a href="B19116_02.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> and <a href="B19116_03.xhtml#_idTextAnchor060"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>. Although that might sound obvious, we want to point it out as it’s crucial to understand the different deployment methods before diving into the on-prem needs of Kubernetes. Because cloud-based Kubernetes deployments abstract a lot of what you would do with on-prem, it still shows you the overall workflow of what components need to <span class="No-Break">be deployed.</span></p>&#13;
			<p>To work on this chapter, you should have some type of infrastructure and troubleshooting background. When it comes to on-prem Kubernetes clusters, they are extremely infrastructure-heavy, so getting through this chapter without that knowledge may be difficult. At the very least, you should have <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break">Linux knowledge</span></li>&#13;
				<li><span class="No-Break">Server knowledge</span></li>&#13;
			</ul>&#13;
			<p>The code for this chapter can be found in this book’s GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch4"><span class="No-Break">https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch4</span></a><span class="No-Break">.</span></p>&#13;
			<p>For the Kubeadm section of this chapter, you can follow along if you have two virtual machines available for your use. If you don’t, it’s perfectly fine: you can view this chapter from a theoretical perspective if that’s the case. However, if you have two extra VMs available, whether they’re on-prem or in the cloud, it would help you understand the overall explanations of this chapter a <span class="No-Break">bit more..</span></p>&#13;
			<h1 id="_idParaDest-80"><a id="_idTextAnchor081"/>Understanding operating systems and infrastructure</h1>&#13;
			<p>Everything starts at a server. It doesn’t matter if you’re running workloads on the cloud, on serverless platforms, or<a id="_idIndexMarker174"/> containers – everything starts at a server. The reason why engineers don’t always think about servers, or where workloads start in today’s world, is that the underlying infrastructure is abstracted away from us. In the cloud world, there aren’t a lot of times when you’ll have to ask, <em class="italic">what hardware are you using to run these VMs? Dell? HP?</em> Instead, you’re worried about what happens after the servers are deployed, which is sometimes called Day-Two Ops (insert more buzzwords here). What we mean by that is instead of ordering servers online, racking them, and configuring some virtualized hypervisor on them (ESXi, KVM, Hyper-V, and so on), engineers are more concerned now with automation, application deployments, platforms, <span class="No-Break">and scalability.</span></p>&#13;
			<p>In many start-ups and small-to-medium-sized organizations, the typical reality is cloud computing. For larger organizations, another reality is on-prem workloads that are either virtualized or purely bare metal. If the combination of the cloud and on-prem gets brought up in discussion, this is where things such as hybrid solutions come into play, which you’ll learn about later in <span class="No-Break">this chapter.</span></p>&#13;
			<p>Let’s say you’re reading this right now and you’re working 100% in the cloud. You still need to understand VM sizes, scaling, the location of the VMs (the data center – regions, availability zones, geographies, and so on), network latency, and a large number of other pieces that fall into the systems and infrastructure category. For example, in the previous chapter, you learned about choosing worker node sizes for high CPU, high memory, and medium CPU/memory workloads within DigitalOcean <span class="No-Break">and Linode.</span></p>&#13;
			<p>In this section, you’re going to learn about the core system and infrastructure needs that you must think about when architecting an on-prem <span class="No-Break">Kubernetes platform.</span></p>&#13;
			<h2 id="_idParaDest-81"><a id="_idTextAnchor082"/>Kubeadm Deployment</h2>&#13;
			<p>Before jumping into the theory, I wanted to showcase how you can bootstrap a Kubernetes cluster with Kubeadm. The primary reason is to show you what the process of actually deploying Kubernetes looks like while the pieces aren’t abstracted away from you. Abstraction is a great thing, but it’s only a great thing once you know the manual method of deployment. Otherwise, abstraction just ends up <span class="No-Break">causing confusion.</span></p>&#13;
			<p>For the Virtual Machines, the installation is based on Ubuntu. However, if you’re using another Linux distribution, it will work, but you’ll need to change the commands a bit to reflect the specific distro. For example, Ubuntu uses the Aptitude package manager and CentOS uses the Yum <span class="No-Break">package manager.</span></p>&#13;
			<h3>Installing Control Plane and Worker Node</h3>&#13;
			<p>Let’s <span class="No-Break">get started.</span></p>&#13;
			<ol>&#13;
				<li value="1">First, ensure that you <span class="No-Break">update Ubuntu:</span><pre class="console">&#13;
<strong class="bold">sudo apt update -y</strong></pre></li>&#13;
				<li>Install <span class="No-Break">transport layer:</span><pre class="console">&#13;
<strong class="bold">sudo apt-get install -y apt-transport-https curl</strong></pre></li>&#13;
				<li>Install Kubernetes package <span class="No-Break">on Ubuntu:</span><pre class="console">&#13;
<strong class="bold">curl -s</strong><a href=""><strong class="bold"> https://packages.cloud.google.com/apt/doc/apt-key.gp</strong></a><strong class="bold">g | sudo apt-key add -</strong></pre><pre class="console">&#13;
<strong class="bold">echo "deb</strong><a href=""><strong class="bold"> https://apt.kubernetes.io</strong></a><strong class="bold">/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list</strong></pre></li>&#13;
				<li>Update Ubuntu again now that the Kubernetes <span class="No-Break">package exists:</span><pre class="console">&#13;
<strong class="bold">sudo apt update -y</strong></pre></li>&#13;
				<li>Next, change to the <span class="No-Break">root user:</span><pre class="console">&#13;
<strong class="bold">sudo su -</strong></pre></li>&#13;
				<li>Install and configure the CRI-O <span class="No-Break">container runtime:</span><pre class="console">&#13;
<strong class="bold">OS=xUbuntu_20.04</strong></pre><pre class="console">&#13;
<strong class="bold">VERSION=1.22</strong></pre><pre class="console">&#13;
<strong class="bold">echo "deb</strong><a href=""><strong class="bold"> https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS</strong></a><strong class="bold">/ /" &gt; /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list</strong></pre><pre class="console">&#13;
<strong class="bold">echo "deb</strong><a href=""><strong class="bold"> http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS</strong></a><strong class="bold">/ /" &gt; /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list</strong></pre><pre class="console">&#13;
<strong class="bold">curl -L</strong><a href=""><strong class="bold"> https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$VERSION/$OS/Release.ke</strong></a><strong class="bold">y | apt-key add -</strong></pre><pre class="console">&#13;
<strong class="bold">curl -L</strong><a href=""><strong class="bold"> https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/Release.ke</strong></a><strong class="bold">y | apt-key add -</strong></pre></li>&#13;
				<li>Exit out <span class="No-Break">of root:</span><pre class="console">&#13;
<strong class="bold">exit</strong></pre></li>&#13;
				<li>Update Ubuntu again now that <strong class="source-inline">CRI-O</strong> <span class="No-Break">is available:</span><pre class="console">&#13;
<strong class="bold">sudo apt update –y</strong></pre></li>&#13;
				<li><span class="No-Break">Install </span><span class="No-Break"><strong class="source-inline">CRI-O</strong></span><span class="No-Break">:</span><pre class="console">&#13;
<strong class="bold">sudo apt install cri-o cri-o-runc -y</strong></pre></li>&#13;
				<li>Reload the Daemon and <span class="No-Break">enable CRI-O:</span><pre class="console">&#13;
<strong class="bold">sudo systemctl daemon-reload</strong></pre><pre class="console">&#13;
<strong class="bold">sudo systemctl enable crio --now</strong></pre></li>&#13;
				<li>Check to see CRI-O is <span class="No-Break">installed properly:</span><pre class="console">&#13;
<strong class="bold">apt-cache policy cri-o</strong></pre></li>&#13;
				<li>Turn <span class="No-Break">off swap:</span><pre class="console">&#13;
<strong class="bold">swapoff -a</strong></pre></li>&#13;
				<li>Configure <strong class="source-inline">sysctl</strong> settings and <span class="No-Break"><strong class="source-inline">ip</strong></span><span class="No-Break"> tables:</span><pre class="console">&#13;
<strong class="bold">sudo modprobe overlay</strong></pre><pre class="console">&#13;
<strong class="bold">sudo modprobe br_netfilter</strong></pre><pre class="console">&#13;
<strong class="bold">&#13;
</strong></pre><pre class="console">&#13;
<strong class="bold">sudo tee /etc/sysctl.d/kubernetes.conf&lt;&lt;EOF</strong></pre><pre class="console">&#13;
<strong class="bold">net.bridge.bridge-nf-call-ip6tables = 1</strong></pre><pre class="console">&#13;
<strong class="bold">net.bridge.bridge-nf-call-iptables = 1</strong></pre><pre class="console">&#13;
<strong class="bold">net.ipv4.ip_forward = 1</strong></pre><pre class="console">&#13;
<strong class="bold">EOF</strong></pre><pre class="console">&#13;
<strong class="bold">sudo sysctl --system</strong></pre></li>&#13;
				<li><span class="No-Break">Install kubeadm:</span><pre class="console">&#13;
<strong class="bold">sudo apt-get install -y kubelet kubeadm kubectl</strong></pre></li>&#13;
			</ol>&#13;
			<p>The next step <span class="No-Break">is configuration.</span></p>&#13;
			<h3>Configuring the Control Plane</h3>&#13;
			<p>We need to define variables for the <strong class="source-inline">kubeadm init</strong> command. This will consist of IP addresses and the Pod CIDR range. Depending on where you are deploying it, you could either have just a public subnet, or a public and <span class="No-Break">private subnet.</span></p>&#13;
			<p>If you have just a public subnet, use the same value for the <strong class="source-inline">ip_address</strong> and <strong class="source-inline">publicIP</strong>, along with the <strong class="source-inline">CIDR</strong> range. If you have a private and public subnet, use the public IP for the <strong class="source-inline">publicIP</strong>, the private IP for the <strong class="source-inline">ip_address</strong>, and the private IP range for <span class="No-Break">the </span><span class="No-Break"><strong class="source-inline">CIDR</strong></span><span class="No-Break">.</span></p>&#13;
			<pre class="console">&#13;
ip_address=10.116.0.9&#13;
cidr=172.17.0.0/16&#13;
publicIP=146.190.219.123</pre>&#13;
			<p>Next, initialize <strong class="source-inline">kubeadm</strong> on the <span class="No-Break">Control Plane:</span></p>&#13;
			<pre class="console">&#13;
sudo kubeadm init --control-plane-endpoint $publicIP --apiserver-advertise-address $ip_address --pod-network-cidr=$cidr --upload-certs</pre>&#13;
			<p>If you are deploying in the cloud, you may find yourself in a situation where the <strong class="source-inline">init</strong> fails because the Kubelet connect communicate with the API server. This typically happens in public clouds due to network restrictions. If it happens to you, open up the following <span class="No-Break">ports:</span><a href=""><span class="No-Break"> </span></a><a href="https://kubernetes.io/docs/reference/ports-and-protocols/"/><span class="No-Break">.</span></p>&#13;
			<p>After the Kubeadm <strong class="source-inline">init</strong> is successful, you’ll see a few command outputs that show how to join more Control Planes and how to join Worker Nodes. Copy the Worker Node join command and run it on the Ubuntu server that you configured as the <span class="No-Break">Worker node.</span></p>&#13;
			<p>Next, install <span class="No-Break">the CNI.</span></p>&#13;
			<p>If you don’t want to use Weave, you can see the network frameworks listed <span class="No-Break">here:</span><a href=""><span class="No-Break"> </span></a><a href="https://kubernetes.io/docs/concepts/cluster-administration/addons/"/><span class="No-Break">.</span></p>&#13;
			<pre class="console">&#13;
kubectl apply -f<a href=""> https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yam</a>l</pre>&#13;
			<p>Next, we will look at <span class="No-Break">system size.</span></p>&#13;
			<h2 id="_idParaDest-82"><a id="_idTextAnchor083"/>System size</h2>&#13;
			<p>Considerations about the system type, size, and how<a id="_idIndexMarker175"/> many nodes will be incredibly crucial for how you decide to think about on-prem deployments. What it all comes down to is what you’re planning on running on a Kubernetes cluster. If you’re just starting with your first Kubernetes cluster and you want to try containerizing an application to see how it works, how the dependency works, and ultimately starting on your journey, it’s going to be different than if you’re running 50+ Kubernetes clusters that are running stock trading/quants applications. At the end of the day, the system size that you use will be solely based on what workload <span class="No-Break">you’re running.</span></p>&#13;
			<p>Before you even think about creating a Kubernetes cluster on-prem, you must think about two <span class="No-Break">important aspects:</span></p>&#13;
			<ul>&#13;
				<li>Do I have the hardware available and if not, what hardware do I have <span class="No-Break">to buy?</span></li>&#13;
				<li>What type of applications am I planning on running for the next 3 to <span class="No-Break">6 months?</span></li>&#13;
			</ul>&#13;
			<p>For example, let’s say that <a id="_idIndexMarker176"/>you buy 10 servers that you’re planning on running your application on. What size do the servers need to be? How will scaling work? Do you have a combination of memory-intensive apps and standard <span class="No-Break">everyday apps?</span></p>&#13;
			<p>Another big consideration here is <em class="italic">scaling</em>. If you’re scaling<a id="_idIndexMarker177"/> horizontally, that means more Pods will be created, so more virtualized hardware will be consumed. If you’re scaling vertically, that means your Pods’ memory and CPU are increasing without you creating more Pods (which is known as vertical autoscaling). Not only do you have to plan for what applications<a id="_idIndexMarker178"/> you’re going to be running right off the bat, but you also have to plan for how those applications will be used. If you have 500 users today and you’re planning on having 2,000 users in 3 months based on company projections, that means the Pods will have an increased velocity in usage and that you may need more Pods. More usage means autoscaling, and autoscaling means more resources <span class="No-Break">are needed.</span></p>&#13;
			<h3>Sizing considerations</h3>&#13;
			<p>The following is a list of standard sizing<a id="_idIndexMarker179"/> considerations for when you’re building out a <span class="No-Break">Kubernetes cluster:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Standard workers</strong>: These are your everyday web server Pods or middleware that still require virtualized <a id="_idIndexMarker180"/>hardware resources, but not at the same level as more intense applications. They are your more <em class="italic">generic apps</em> if you will. The worker nodes running here are mid-to-large in terms of size. If you’re just starting to get a Kubernetes cluster up and running and you’re maybe moving one or two containerized apps to Kubernetes as you get going, standard workers will be just fine to get the <span class="No-Break">ball rolling.</span></li>&#13;
				<li><strong class="bold">Memory-intensive workers</strong>: Applications that you know will require more memory/RAM than others should be accounted for with worker nodes that contain more RAM than the <a id="_idIndexMarker181"/>standard servers that are running as worker nodes. You want to ensure that if a Pod has to scale in replicas, or more Pods are added, you have enough memory. Otherwise, Pods won’t start and will stay pending until memory is allocated to them, and the scheduler can re-try scheduling the Pod for <span class="No-Break">a node.</span></li>&#13;
				<li><strong class="bold">CPU-intensive workers</strong>: Some applications will require more CPU and available threads to run the app. The <a id="_idIndexMarker182"/>same rules as those for memory-intensive apps apply here – if the scheduler <a id="_idIndexMarker183"/>can’t schedule the Pods because there aren’t enough resources available, the scheduler will wait until resources <span class="No-Break">free up.</span></li>&#13;
				<li><strong class="bold">Special-case workers</strong>: A special-<a id="_idIndexMarker184"/>case Pod would usually be something such as an application that’s running a graphically intensive workload that needs a specific type of <strong class="bold">Graphics Processing Unit</strong> (<strong class="bold">GPU</strong>), which <a id="_idIndexMarker185"/>means the worker node needs a dedicated GPU or an app that requires faster bandwidth, so<a id="_idIndexMarker186"/> it requires a certain type of <strong class="bold">Network Interface </strong><span class="No-Break"><strong class="bold">Card</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">NIC</strong></span><span class="No-Break">).</span></li>&#13;
			</ul>&#13;
			<h2 id="_idParaDest-83"><a id="_idTextAnchor084"/>System location</h2>&#13;
			<p>When you run Kubernetes, there <a id="_idIndexMarker187"/>are primarily two types <span class="No-Break">of nodes:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Control Plane</strong>: The Control Plane is<a id="_idIndexMarker188"/> where the API server lives. Without the API server, you can’t do much inside Kubernetes. It also contains the scheduler (how Pods know what node to go on), controller manager (controllers for Pods, Deployments, and <a id="_idIndexMarker189"/>so on to have the desired state), and the cluster <span class="No-Break">store/database (etcd).</span></li>&#13;
				<li><strong class="bold">Worker node</strong>: Worker nodes<a id="_idIndexMarker190"/> are where the Pods are installed after the Control Plane and on the worker node(s) run. They run the kubelet (the agent), container runtime (how containers run), kube-proxy (Kubernetes networking), and any other Pod <span class="No-Break">that’s running.</span></li>&#13;
			</ul>&#13;
			<p>Keeping these two node types in mind, you’ll have to figure out what automation techniques you want to use to run them and ultimately where/how you want to run them. It’s a consideration that you shouldn’t take lightly. The last thing that you want is to deploy Pods, then realize that the nodes they’re running on can’t handle the type of containerized app that’s running. As mentioned previously, the Control Plane is where the Kubernetes API sits. The Kubernetes API is how you do everything in Kubernetes. Without it, Kubernetes wouldn’t exist. With that being said, choosing where and how to run the Control Plane is the make <a id="_idIndexMarker191"/>or break between properly using Kubernetes and spending your days <span class="No-Break">constantly troubleshooting.</span></p>&#13;
			<p class="callout-heading">Important note</p>&#13;
			<p class="callout">Explaining the Control Plane and worker nodes could take a chapter in itself. Because this book already expects you to know Kubernetes, we’re not diving into the types of Kubernetes nodes all that much. However, if you don’t know about the control plane and worker nodes, we highly recommend you take the time to learn about them before continuing. A great place to start is the <span class="No-Break">Kubernetes docs.</span></p>&#13;
			<h3>Data centers and regional locations</h3>&#13;
			<p>Data centers go down. Regions <a id="_idIndexMarker192"/>go down. <strong class="bold">Internet Service Providers</strong> (<strong class="bold">ISPs</strong>) go down. When you’re architecting where you want Kubernetes to run, there are several things<a id="_idIndexMarker193"/> that you must take <span class="No-Break">into consideration.</span></p>&#13;
			<p>The first is where you’re running. For example, if your customers are in the UK, and you decide to run your data center in New Jersey, there’s going to be a ton of bandwidth issues and latency. Instead, it would make more sense to have a data center and a few co-locations throughout <span class="No-Break">the UK.</span></p>&#13;
			<p>Speaking of co-locations, you must make sure that you don’t have a single point of failure. The reality is that data centers do go down. They have internet issues, flooding, electric issues, and outages. If that occurs, the last thing that you want is to only have a single location. Instead, you should think about, at the very least, two data center locations. If one of the data centers fails, high availability needs to be put in place to <em class="italic">turn on</em> the other data center. For <a id="_idIndexMarker194"/>example, you could have a <em class="italic">hot/hot</em> or <em class="italic">hot/cold</em> high availability scenario. <em class="italic">Hot/hot</em> is recommended as all the data is being replicated by default to the second data center. If the first data center goes down, the second data center picks up where the first left off. Another consideration is where the data centers are. If the two data centers are only 5 miles away from each other and a storm comes, both could be impacted. Because of that, you want to put some distance between <span class="No-Break">data centers.</span></p>&#13;
			<h3>Where and how to run Kubernetes</h3>&#13;
			<p>As you saw in the <a id="_idIndexMarker195"/>previous sections, the first step to figuring out your on-prem Kubernetes infrastructure is deciding what hardware and resources you need to run the applications you’re planning on running. Next, it’s all about figuring out what type of worker nodes you need. It’ll most likely be a combination of standard worker nodes and more intensive worker nodes with extra CPU and RAM. The final step (at least for this section) is figuring out how and where to <span class="No-Break">run it.</span></p>&#13;
			<p>There are several other options, but the following are a few of the <span class="No-Break">popular ones:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">OpenStack</strong>: Although a lot of engineers in today’s world think OpenStack is dead, a lot of very large<a id="_idIndexMarker196"/> organizations are still using it. For example, almost every Telco provider uses OpenStack. Mercedes-Benz (at the time of writing) is hosting over 900 (yes, 900) Kubernetes clusters running in OpenStack. OpenStack gives you the feeling of being in the cloud, but it’s all on-prem and you’re hosting the private cloud yourself. The Open Infrastructure Foundation has put a lot of emphasis behind running Kubernetes on OpenStack with tools such as Magnum, which is the standard for running orchestration platforms (Kubernetes, Mesos, Docker Swarm, and so on), and Loki, which is the <span class="No-Break">Linux/OpenStack/Kubernetes/infrastructure stack.</span></li>&#13;
				<li><strong class="bold">Kubeadm</strong>: If you don’t want to<a id="_idIndexMarker197"/> go the OpenStack route and if you’re using something such as a hypervisor, kubeadm is arguably the best option. There are a few different automated ways to get a Kubernetes cluster up and running, but kubeadm is more or less the most sophisticated. Using kubeadm, you can create a Kubernetes cluster that conforms to best practices. Other than installing the<a id="_idIndexMarker198"/> prerequisites on the Linux server for Kubernetes to run, kubeadm is pretty much automated. kubeadm has a set of commands that you can run that goes through several checks on the Linux server to confirm that it has all of the prerequisites and then installs the Control Plane. After that, there’s an output on the terminal that gives you a command to run more Control Planes and/or run worker nodes. You copy the command from the output, paste it into another server that you’re SSH’d into via the terminal, and run it. kubeadm is cool as well because it introduces you to the fact that running Kubernetes on-prem is straightforward. You can even run it on your laptop or a Rasberry Pi. There isn’t a high threshold to meet to run it, especially in a <span class="No-Break">Dev environment.</span></li>&#13;
				<li><strong class="bold">Rancher</strong>: Rancher acts as both a Kubernetes cluster creation tool and a Kubernetes cluster <a id="_idIndexMarker199"/>manager. Within Rancher, you can create a Kubernetes cluster and host it on Rancher, create a Kubernetes cluster in the cloud, or create a raw Kubernetes cluster by provisioning Linux virtual machines. You can also manage your Kubernetes clusters from Rancher. For example, if you have a bare-metal Kubernetes cluster that’s running with kubeadm or in OpenShift, you can manage it via Rancher. You can also manage cloud <span class="No-Break">Kubernetes clusters.</span></li>&#13;
				<li><strong class="bold">Kubespray</strong>: Kubespray, although (in our opinion) isn’t the best production-level option to go for, is still an <a id="_idIndexMarker200"/>option. Kubespray uses either Ansible or Vagrant to deploy a production-ready Kubernetes cluster on virtual machines or in the cloud. Because all you need is kubeadm instead of other <em class="italic">middleware</em>, such as Vagrant or Ansible, going straight for kubeadm saves you those extra hops needed to get a cluster created. Funnily enough, Kubespray uses kubeadm underneath the hood for cluster creation (<a href="https://github.com/kubernetes-sigs/kubespray/blob/master/docs/comparisons.md">https://github.com/kubernetes-sigs/kubespray/blob/master/docs/comparisons.md</a>), so that solidifies even more that there’s something to say about not going the extra hops to use Kubespray and instead, just go straight <span class="No-Break">to kubeadm.</span></li>&#13;
			</ul>&#13;
			<h2 id="_idParaDest-84"><a id="_idTextAnchor085"/>Operating system</h2>&#13;
			<p>To run the Kubernetes platform, you<a id="_idIndexMarker201"/> need an operating system to run it on. The two options that you have are <span class="No-Break">as follows:</span></p>&#13;
			<ul>&#13;
				<li>Run a bare-metal server and have the operating system run directly on <span class="No-Break">the server</span></li>&#13;
				<li>Have a virtualized hypervisor, such as ESXi, that virtualizes the hardware and allows you to install the operating system on top <span class="No-Break">of it</span></li>&#13;
			</ul>&#13;
			<p>In today’s world, chances are you’re going to use a hypervisor. Unless there’s a specific need to run bare-metal servers and run the operating system directly on the server, engineers typically opt for a hypervisor. It’s much easier to manage, very scalable, and allows you to get a lot more out of <span class="No-Break">the hardware.</span></p>&#13;
			<p>When it comes to the operating system options, more or less, there are typically two available options. One is <a id="_idIndexMarker202"/>certainly used more than the other, but the other is gaining <span class="No-Break">increased popularity.</span></p>&#13;
			<h3>Linux</h3>&#13;
			<p>More likely than not, you’ll be running<a id="_idIndexMarker203"/> worker nodes as Linux distributions. The most popular battle-tested distributions are Red Hat, CentOS, and Ubuntu. Linux is usually the <em class="italic">out-of-the-box</em> solution when it comes to Kubernetes worker nodes, and at the time of writing this book, you can only run Kubernetes Control Planes on <span class="No-Break">Linux servers.</span></p>&#13;
			<h3>Windows</h3>&#13;
			<p>Although not seen all that<a id="_idIndexMarker204"/> much, especially with open-sourced and cross-platform versions of .NET, you can run Windows Server as a Kubernetes worker node. If you want to run Windows Server as a worker node, there are a few considerations. First, you must be running Windows Server LTSC 2019 or above. At the time of writing this book, the two options available are Windows Server 2019 and Windows <span class="No-Break">Server 2022.</span></p>&#13;
			<p>With the Windows Server option, you will have to <a id="_idIndexMarker205"/>buy licenses and keep <strong class="bold">Client Access License</strong> (<strong class="bold">CAL</strong>) considerations <span class="No-Break">in mind.</span></p>&#13;
			<p>Now that you have an understanding of the overall operating system and infrastructure components of an on-prem Kubernetes cluster, in the next section, you’ll learn how to troubleshoot the environment <span class="No-Break">you’re building.</span></p>&#13;
			<h1 id="_idParaDest-85"><a id="_idTextAnchor086"/>Troubleshooting on-prem Kubernetes clusters</h1>&#13;
			<p>If you come from a systems administration/infrastructure background, troubleshooting Kubernetes clusters is going to come to you pretty naturally. At the end of the day, a Kubernetes cluster consists<a id="_idIndexMarker206"/> of servers, networking, infrastructure, and APIs, which are essentially what infrastructure engineers are working on day <span class="No-Break">to day.</span></p>&#13;
			<p>If you’re a developer, some of these concepts may be new to you, such as troubleshooting networks. However, you’ll be very familiar with a few troubleshooting techniques as well, such as looking at and <span class="No-Break">analyzing logs.</span></p>&#13;
			<p>The whole idea of troubleshooting a Kubernetes cluster is to look at <span class="No-Break">two pieces:</span></p>&#13;
			<ul>&#13;
				<li>The <span class="No-Break">cluster itself</span></li>&#13;
				<li>The Pods running inside <span class="No-Break">the cluster</span></li>&#13;
			</ul>&#13;
			<p>The cluster itself, including networking, servers, operating systems, and scalability, is going to be thought of from more of an<a id="_idIndexMarker207"/> infrastructure perspective, where something such as the <strong class="bold">Certified Kubernetes Administrator</strong> (<strong class="bold">CKA</strong>) comes into play nicely. The Pods, Deployments, container errors, and Pods not starting properly are going to be thought of more from a<a id="_idIndexMarker208"/> developer<a id="_idIndexMarker209"/> perspective, so learning the concepts of the <strong class="bold">Certified Kubernetes Application Developer</strong> (<strong class="bold">CKAD</strong>) would be a great <span class="No-Break">stepping stone.</span></p>&#13;
			<p>In this section, you’re going to learn about the key ways to think about troubleshooting Kubernetes clusters and how to figure out problems in a <span class="No-Break">digestible way.</span></p>&#13;
			<h2 id="_idParaDest-86"><a id="_idTextAnchor087"/>Server logs and infrastructure troubleshooting</h2>&#13;
			<p>Although there’s an entire <a id="_idIndexMarker210"/>chapter in this book that goes over logging and observability (<a href="B19116_07.xhtml#_idTextAnchor161"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>) let’s talk about logging in a cluster sense. Typically, when you’re working with any type of observability metrics, such as logging, in the Kubernetes world, engineers are primarily thinking about the logs for an application. Those logs will help them troubleshoot a failing app and figure out what happened in the first place. However, from a Kubernetes on-prem perspective, server logging is a <span class="No-Break">crucial piece.</span></p>&#13;
			<p>For the most part, unless otherwise specified, all the logs from the Control Plane and worker nodes typically go into <strong class="source-inline">/var/log</strong> on the Linux server. For Control Planes, the logs are at the <span class="No-Break">following paths:</span></p>&#13;
			<ul>&#13;
				<li><strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">var/log/kube-apiserver.log</strong></span></li>&#13;
				<li><strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">var/log/kube-scheduler.log</strong></span></li>&#13;
				<li><strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">var/log/kube-controller-manager.log</strong></span></li>&#13;
			</ul>&#13;
			<p>For worker nodes, the logs are at the <span class="No-Break">following paths:</span></p>&#13;
			<ul>&#13;
				<li><strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">var/log/kubelet.log</strong></span></li>&#13;
				<li><strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">var/log/kube-proxy.log</strong></span></li>&#13;
			</ul>&#13;
			<p>Out of the box, there isn’t a specific logging mechanism that Kubernetes uses. That’s essentially up to you to<a id="_idIndexMarker211"/> decide. The two primary ways that engineers capture logs for clusters are <span class="No-Break">as follows:</span></p>&#13;
			<ul>&#13;
				<li>Use a node logging agent that runs on every node across <span class="No-Break">the cluster</span></li>&#13;
				<li>Have a log aggregator capture the logs from <strong class="source-inline">/var/log</strong> and send them to a <span class="No-Break">logging platform</span></li>&#13;
			</ul>&#13;
			<p>You can find more documentation on troubleshooting clusters <span class="No-Break">at </span><a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/"><span class="No-Break">https://kubernetes.io/docs/tasks/debug/debug-cluster/</span></a><span class="No-Break">.</span></p>&#13;
			<h2 id="_idParaDest-87"><a id="_idTextAnchor088"/>Network observability</h2>&#13;
			<p>The networking piece of a <a id="_idIndexMarker212"/>Kubernetes cluster, which you’ll learn about shortly, is an extremely complex piece of Kubernetes within itself. Networking inside of Kubernetes is just as important as the Kubernetes API and all the other pieces that make <span class="No-Break">up Kubernetes.</span></p>&#13;
			<p>The two things that you want to look <a id="_idIndexMarker213"/>out for are <strong class="bold">cluster latency</strong> and <strong class="bold">Pod latency</strong>. With cluster <a id="_idIndexMarker214"/>latency, it’s most likely going to come down to the standard systems administration troubleshooting around checking bandwidth, QoS on routers, how many packets are getting pushed through, NICs, and more. From a Pod latency perspective, it’ll most likely start at the cluster level in terms of the issues that the cluster may be having, but to troubleshoot, you’ll most likely look into something such as a service mesh. </p>&#13;
			<p>Service mesh is a huge topic in itself, which could probably cover an entire course/book, but you’ll learn how to get started with it in the <em class="italic">Exploring networking and system </em><span class="No-Break"><em class="italic">components</em></span><span class="No-Break"> section.</span></p>&#13;
			<h2 id="_idParaDest-88"><a id="_idTextAnchor089"/>Kubernetes metrics</h2>&#13;
			<p>Most resources created in Kubernetes (Deployments, Pods, Services, and so on) have a metrics endpoint that can be found <a id="_idIndexMarker215"/>via the <strong class="source-inline">/metrics</strong> path when making an API call on <span class="No-Break">Kubernetes resources.</span></p>&#13;
			<p>The metrics server collects logs from kubelets, which are agents that run on each Kubernetes node, and exposes them to the Kubernetes API server through the metrics API. However, this is primarily used for autoscaling needs. For example, the metrics will tell Kubernetes, <em class="italic">Hey, Pods are running low on memory utilization; we need a new Pod to handle application utilization</em>. Then, the vertical or horizontal autoscaler will kick off and do its job to create a new Pod, or vertically scale the <span class="No-Break">current Pod(s).</span></p>&#13;
			<p>If you want to collect metrics for, say, the monitoring platform that you use for observability, you’d want to collect metrics from the <strong class="source-inline">/metrics/resource/resource_name</strong> kubelet directly. Many observability platforms such as Prometheus will ingest these metrics and use them for troubleshooting and <span class="No-Break">performance troubleshooting.</span></p>&#13;
			<h2 id="_idParaDest-89"><a id="_idTextAnchor090"/>crictl</h2>&#13;
			<p>Inside of every Kubernetes cluster, specifically running on the worker nodes, is a container runtime. Container <a id="_idIndexMarker216"/>runtimes such as containerd and CRI-O are mostly used in Kubernetes environments. Those container runtimes help make containers and Pods run. Because of that, it’s important to ensure that the container runtime is working <span class="No-Break">as expected.</span></p>&#13;
			<p><strong class="source-inline">crictl</strong> helps you troubleshoot the container runtime. You can run a few commands directed at a Pod that’ll help you understand what’s happening inside of a container. Keep in mind that <strong class="source-inline">crictl</strong> is in beta, but it’s still a great <span class="No-Break">troubleshooting tool.</span></p>&#13;
			<p>In the following example, <strong class="source-inline">crictl</strong> is listing a set <span class="No-Break">of Pods:</span></p>&#13;
			<pre class="console">&#13;
crictl pods</pre>&#13;
			<p>Then, it can look inside each Pod to see the containers that <span class="No-Break">are running:</span></p>&#13;
			<pre class="console">&#13;
crictl pods -name pod_name</pre>&#13;
			<p>You can also list out containers that are running and <span class="No-Break">bypass Pods:</span></p>&#13;
			<pre class="console">&#13;
crictl ps -a</pre>&#13;
			<p>You can find more information about <strong class="source-inline">crictl</strong> <span class="No-Break">at </span><a href="https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md"><span class="No-Break">https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md</span></a><span class="No-Break">.</span></p>&#13;
			<h2 id="_idParaDest-90"><a id="_idTextAnchor091"/>kubectl</h2>&#13;
			<p>Wrapping up this section, let’s talk about the <strong class="source-inline">kubectl</strong> command, which is the typical way that engineers interact <a id="_idIndexMarker217"/>with Kubernetes via the CLI. You’ll see three primary commands <span class="No-Break">for troubleshooting:</span></p>&#13;
			<ul>&#13;
				<li><strong class="source-inline">kubectl describe</strong>: This command tells you the exact makeup of the Kubernetes Deployment, such as how it’s running, what containers are running inside of it, ports that are being used, and more. This is a great first step to understanding what could be going wrong inside of <span class="No-Break">a Deployment.</span></li>&#13;
			</ul>&#13;
			<p>For example, if you had a Deployment called <strong class="source-inline">nginx-deployment</strong>, you’d run the <span class="No-Break">following command:</span></p>&#13;
			<pre class="console">&#13;
<strong class="bold">kubectl describe deployment nginx-deployment</strong></pre>&#13;
			<p>The following output showcases how <strong class="source-inline">describe</strong> looks <span class="No-Break">for Deployments.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer056" class="IMG---Figure">&#13;
					<img src="Images/B19116_04_01.jpg" alt="Figure 4.1 – Kubernetes Deployment output&#13;&#10;" width="1147" height="744"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – Kubernetes Deployment output</p>&#13;
			<ul>&#13;
				<li><strong class="source-inline">kubectl cluster-info dump</strong>: This command is a literal dump of every single thing that’s happened<a id="_idIndexMarker218"/> on the cluster that was recorded. By default, all of the output is sent STDOUT, so you should ideally send the output to a file and look through it as it’s extremely verbose with a lot <span class="No-Break">of data.</span></li>&#13;
			</ul>&#13;
			<p>The following screenshot has been cut off for simplicity, but it’s an example of the information shown with the <strong class="source-inline">kubectl cluster-info </strong><span class="No-Break"><strong class="source-inline">dump</strong></span><span class="No-Break"> command:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer057" class="IMG---Figure">&#13;
					<img src="Images/B19116_04_02.jpg" alt="Figure 4.2 – Cluster dump output&#13;&#10;" width="1286" height="880"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – Cluster dump output</p>&#13;
			<ul>&#13;
				<li><strong class="source-inline">kubectl logs</strong>: This command is<a id="_idIndexMarker219"/> the bread and butter to understanding what’s happening inside of a Pod. For example, let’s say that you have a Pod called <strong class="source-inline">nginx-deployment-588c8d7b4b-wmg9z</strong>. You can run the following command to see the log output for <span class="No-Break">the Pod:</span><pre class="console">&#13;
<strong class="bold">kubectl logs nginx-deployment-588c8d7b4b-wmg9z</strong></pre></li>&#13;
			</ul>&#13;
			<p>The following screenshot shows a sample of what logs look like <span class="No-Break">for Pods.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer058" class="IMG---Figure">&#13;
					<img src="Images/B19116_04_03.jpg" alt="Figure 4.3 – Nginx Pod log output&#13;&#10;" width="1002" height="443"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – Nginx Pod log output</p>&#13;
			<p>Regardless of where a<a id="_idIndexMarker220"/> Kubernetes cluster is running, you’re always going to have to troubleshoot certain aspects of it. The tips in this section should help in an on-prem, and even sometimes a <span class="No-Break">cloud, scenario.</span></p>&#13;
			<h1 id="_idParaDest-91"><a id="_idTextAnchor092"/>Introducing hybrid services</h1>&#13;
			<p>From 2014 to 2015, what most organizations and engineers alike were reading sounded something similar to <em class="italic">data centers will go away</em>, <em class="italic">the cloud is the future</em>, and <em class="italic">everyone that isn’t in the cloud will be left behind</em>. Organizations started to feel pressured to move to the cloud and engineers started to get nervous because the skills they had honed for years <a id="_idIndexMarker221"/>were becoming obsolete. Coming back to the present, which is 2022 at the time of writing this book, mainframes still exist… so, yes, many organizations are still running on-prem workloads. Engineers that have an infrastructure and systems background are doing quite well for themselves in the new cloud-native era. The reason why is that 100% of the skills they have learned, other than racking and stacking servers, are still very relevant for the cloud <span class="No-Break">and Kubernetes.</span></p>&#13;
			<p>In the <em class="italic">Understanding operating systems and infrastructure</em> section, you may remember reading about on-prem workloads and how they’re still relevant in today’s world. Although tech marketing may be making you feel otherwise, the truth is that on-prem workloads are still very much used today. They’re used so much that organizations such as AWS, Microsoft, and Google are realizing it, and they’re building services and platforms to support the need for a true hybrid environment, which means using on-prem and cloud workloads together, often managed in the <span class="No-Break">same location.</span></p>&#13;
			<p>In this section, you’re going to learn about the major cloud provider hybrid services, along with a few other companies that are helping in <span class="No-Break">this area.</span></p>&#13;
			<h2 id="_idParaDest-92"><a id="_idTextAnchor093"/>Azure Stack HCI</h2>&#13;
			<p><strong class="bold">Azure Stack HCI</strong> is the hybrid cloud offering<a id="_idIndexMarker222"/> from Microsoft. It gives you the ability to connect your on-prem environment to Azure. Azure Stack HCI typically comes running inside of a server from a vendor, though you can install it yourself on a <a id="_idIndexMarker223"/>compatible server with compatible hardware. It installs similar to any other operating system, but there’s a lot of complexity around the requirements. A few of the requirements include <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li>At least one server with a maximum of <span class="No-Break">16 servers</span></li>&#13;
				<li>Required to deploy to two <span class="No-Break">different sites</span></li>&#13;
				<li>All servers must have the same manufacturer and use the <span class="No-Break">same model</span></li>&#13;
				<li>At least 32 GB <span class="No-Break">of RAM</span></li>&#13;
				<li>Virtualization support on the hardware (you have to turn this on in <span class="No-Break">the BIOS)</span></li>&#13;
			</ul>&#13;
			<p>You can dive into the requirements a bit more; you’ll find that it goes pretty in-depth. From a time perspective, you’re probably better off buying an Azure Stack HCI-ready server from <span class="No-Break">a vendor.</span></p>&#13;
			<p>An interesting part of Azure Stack HCI is underneath the hood – it’s pretty much just Windows Server 2022 running Windows Admin Center. Because of that, you could completely bypass Azure Stack HCI and do the <span class="No-Break">following instead:</span></p>&#13;
			<ol>&#13;
				<li value="1">Run a bunch of Windows Server 2022 <span class="No-Break">Datacenter servers.</span></li>&#13;
				<li>Cluster <span class="No-Break">them up.</span></li>&#13;
				<li>Install Windows <span class="No-Break">Admin Center.</span></li>&#13;
				<li>Connect the servers <span class="No-Break">to Azure.</span></li>&#13;
				<li>Run AKS on <span class="No-Break">the servers.</span></li>&#13;
			</ol>&#13;
			<h2 id="_idParaDest-93"><a id="_idTextAnchor094"/>Google Anthos</h2>&#13;
			<p><strong class="bold">Anthos</strong> is arguably the most mature hybrid cloud solution that’s available right now. There are a ton of ways to automate<a id="_idIndexMarker224"/> the installation of Anthos with, for<a id="_idIndexMarker225"/> example, Ansible, and the hardware requirements to get it up and running are far more lite (at the time of writing this book) compared to Azure <span class="No-Break">Stack HCI.</span></p>&#13;
			<p>The hardware requirements are <span class="No-Break">as follows:</span></p>&#13;
			<ul>&#13;
				<li>Four cores for CPU minimum, Eight <span class="No-Break">cores recommended</span></li>&#13;
				<li>16 GB of RAM minimum, 32 <span class="No-Break">GB recommended</span></li>&#13;
				<li>128 GB storage minimum, 256 <span class="No-Break">GB recommended</span></li>&#13;
			</ul>&#13;
			<p>Like Azure Stack HCI, Anthos runs on-prem in your data center and connects to GCP to be managed inside of the GCP UI or with commands/APIs for GCP. The goal here is to run GCP to manage Kubernetes clusters on-prem and in <span class="No-Break">the cloud.</span></p>&#13;
			<h2 id="_idParaDest-94"><a id="_idTextAnchor095"/>A quick note about other infrastructure managers</h2>&#13;
			<p>Although perhaps not considered hybrid cloud in itself as a platform, there are a few platforms that help you manage <a id="_idIndexMarker226"/>workloads anywhere. Two of the primary ones at the time of writing are <span class="No-Break">as follows:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Azure Arc</strong>: Azure Arc, as the name <a id="_idIndexMarker227"/>suggests, requires an Azure subscription. However, the cool thing<a id="_idIndexMarker228"/> about it is that you can manage Kubernetes clusters anywhere. If you have Kubernetes clusters in, for example, AWS, you can manage them with Azure Arc. If you have Kubernetes clusters on-prem, you can manage them with <span class="No-Break">Azure Arc.</span></li>&#13;
				<li><strong class="bold">Rancher</strong>: Rancher is a <a id="_idIndexMarker229"/>vendor-agnostic solution that does all the management goodness that Azure Arc does, with a few other key features <a id="_idIndexMarker230"/>such as logging, deployments of Kubernetes servers, and security features to help you fully manage your Kubernetes clusters that are <span class="No-Break">running anywhere.</span></li>&#13;
			</ul>&#13;
			<p>In the next section, you’ll learn about the overall network administration that’s needed inside of a <span class="No-Break">Kubernetes cluster.</span></p>&#13;
			<h1 id="_idParaDest-95"><a id="_idTextAnchor096"/>Exploring networking and system components</h1>&#13;
			<p>Networking in a Kubernetes cluster, aside from the Kubernetes API itself, is what makes Kubernetes truly <em class="italic">tick</em>. Networking <a id="_idIndexMarker231"/>comes into play in various ways, including <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break">Pod-to-Pod communication</span></li>&#13;
				<li><span class="No-Break">Service-to-Service communication</span></li>&#13;
				<li>How nodes talk to each other inside of <span class="No-Break">the cluster</span></li>&#13;
				<li>How users interact with your <span class="No-Break">containerized applications</span></li>&#13;
			</ul>&#13;
			<p>Without networking, Kubernetes wouldn’t be able to perform any actions. Even from a control plane/worker node perspective, worker nodes can’t successfully communicate with control planes unless proper networking is <span class="No-Break">set up.</span></p>&#13;
			<p>This section could be, at the very least, two chapters in itself. Because we only have one section to hammer this knowledge down, let’s talk about the <span class="No-Break">key components.</span></p>&#13;
			<h2 id="_idParaDest-96"><a id="_idTextAnchor097"/>kube-proxy</h2>&#13;
			<p>When you first start to learn about <a id="_idIndexMarker232"/>how networking works inside of Kubernetes and how all resources communicate with each other, it all starts with kube-proxy. kube-proxy is almost like your switch/router in a data center. It runs on every node and is responsible for local <span class="No-Break">cluster networking.</span></p>&#13;
			<p>It ensures <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li>That each node gets a unique <span class="No-Break">IP address</span></li>&#13;
				<li>It implements local iptables or <span class="No-Break">IPVS rules</span></li>&#13;
				<li>It handles the routing and load balancing <span class="No-Break">of traffic</span></li>&#13;
				<li>It enables communication <span class="No-Break">for Pods</span></li>&#13;
			</ul>&#13;
			<p>In short, it’s how every<a id="_idIndexMarker233"/> resource in a Kubernetes <span class="No-Break">cluster communicates.</span></p>&#13;
			<h2 id="_idParaDest-97"><a id="_idTextAnchor098"/>CNI</h2>&#13;
			<p>The first step is kube-proxy, but to get it deployed, it needs to have a backend. That <em class="italic">backend</em> is the <strong class="bold">Container Network Interface</strong> (<strong class="bold">CNI</strong>). Attempting to run kube-proxy without a CNI is like trying to run a <a id="_idIndexMarker234"/>network on Cisco without having <a id="_idIndexMarker235"/>Cisco equipment – it <span class="No-Break">doesn’t work.</span></p>&#13;
			<p>The CNI is a network plugin, sometimes called a network <a id="_idIndexMarker236"/>framework, that has the responsibility of inserting a network framework into a Kubernetes cluster to enable communication, as in, to <span class="No-Break">enable kube-proxy.</span></p>&#13;
			<p>There are a ton of popular CNIs, including <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break">Weave</span></li>&#13;
				<li><span class="No-Break">Flannel</span></li>&#13;
				<li><span class="No-Break">Calico</span></li>&#13;
			</ul>&#13;
			<h2 id="_idParaDest-98"><a id="_idTextAnchor099"/>Kubernetes resource communication</h2>&#13;
			<p>When you deploy Pods, especially microservices, which are <em class="italic">X</em> number of Pods running to make up one application, you need to ensure that Pod-to-Pod communication and Service-to-Service <span class="No-Break">communication </span><span class="No-Break"><a id="_idIndexMarker237"/></span><span class="No-Break">work.</span></p>&#13;
			<p>Pods communicate with each other via an IP address, which is given by kube-proxy. The way that services communicate with each other is by hostname and IP address, which is given by CoreDNS. Services provide a group of Pods associated with that service with a consistent DNS name and IP address. CoreDNS ensures the translation from hostnames to <span class="No-Break">IP addresses.</span></p>&#13;
			<h2 id="_idParaDest-99"><a id="_idTextAnchor100"/>DNS</h2>&#13;
			<p>Under the hood, Kubernetes <a id="_idIndexMarker238"/>runs <strong class="bold">CoreDNS</strong>, a popular open source DNS platform for converting IP addresses into names. When <a id="_idIndexMarker239"/>a Pod or a Service has a DNS name, it’s because the CoreDNS service (which is a running Pod) is running on <span class="No-Break">Kubernetes properly.</span></p>&#13;
			<h2 id="_idParaDest-100"><a id="_idTextAnchor101"/>Service mesh and Ingress</h2>&#13;
			<p>Much like a lot of the other topics in this <a id="_idIndexMarker240"/>chapter, service meshes could be an entire chapter – the two topics mentioned here could be an entire book. However, let’s try to break it down into <span class="No-Break">one section.</span></p>&#13;
			<p>An Ingress controller lets you have multiple Kubernetes Services being accessed via one controller or load balancer. For example, you could have three Kubernetes services named App1, App2, and App3, all connected to the same Ingress controller and accessible over the <strong class="source-inline">/app1</strong>, <strong class="source-inline">/app2</strong>, and <strong class="source-inline">/app3</strong> paths. This is possible via routing rules, which are created for the <span class="No-Break">Ingress controller.</span></p>&#13;
			<p>A service mesh, in short, helps you encrypt east-west traffic or service-to-service traffic, and troubleshoot network <span class="No-Break">latency issues.</span></p>&#13;
			<p>Sometimes, depending on the service mesh that you use, you may not need an Ingress controller as the service mesh may come built <span class="No-Break">with one.</span></p>&#13;
			<p>For Ingress controllers, check out Nginx Ingress, Traefik, and Istio. For service meshes, check <span class="No-Break">out Istio.</span></p>&#13;
			<p>In the next section, you’re going to learn about the ins and outs of how to think about virtualized bare metal and a few vendors that help on <span class="No-Break">this journey.</span></p>&#13;
			<h1 id="_idParaDest-101"><a id="_idTextAnchor102"/>Getting to know virtualized bare metal</h1>&#13;
			<p>If/when you’re planning to run Kubernetes <a id="_idIndexMarker241"/>on-prem, two questions may <span class="No-Break">pop up:</span></p>&#13;
			<ul>&#13;
				<li>Where are we going to <span class="No-Break">run Kubernetes?</span></li>&#13;
				<li>How are we going to <span class="No-Break">run Kubernetes?</span></li>&#13;
			</ul>&#13;
			<p>In today’s world, chances are you’re not going to run Kubernetes directly on bare metal (although you could, and some companies do). You’ll probably run Kubernetes on a hypervisor such as ESXi or in a private cloud such as OpenStack. You may also run Kubernetes on virtualized bare<a id="_idIndexMarker242"/> metal, which is different than running it on <span class="No-Break">a hypervisor.</span></p>&#13;
			<p>In this section, let’s learn what virtualized bare metal is and a few ways that you can <span class="No-Break">run it.</span></p>&#13;
			<h2 id="_idParaDest-102"><a id="_idTextAnchor103"/>Virtualizing your environment</h2>&#13;
			<p>When thinking about virtualized bare metal, a lot of engineers will most likely think about a hypervisor such as VMware’s ESXi or Microsoft’s Hyper-V. Both are great solutions and allow you to take a <a id="_idIndexMarker243"/>bare-metal server that used to only be able to run one operating system and run multiple operating systems. There are many other pieces to a virtualized environment, such as virtualized hardware, networking, and more, all of which are extensive topics and could be an entire book <span class="No-Break">in itself.</span></p>&#13;
			<p>This not only helps you use as many resources out of the server as you can, but it also allows you to save on cost because servers <span class="No-Break">are expensive.</span></p>&#13;
			<p>The other solution is to run as close to bare metal as possible, but you don’t host it. Instead, you <em class="italic">rent</em> bare-metal servers from a hosting provider. When you rent the servers, they give you SSH or RDP access and you can access them the same way that you would if the servers were running in your data center. There’s a UI that you can use to create the servers, maybe some automated ways to do so if the platform allows it, and you can create Windows and/or Linux servers like you would on any other platform, such as if you were creating a web service or a server to <span class="No-Break">host applications.</span></p>&#13;
			<h2 id="_idParaDest-103"><a id="_idTextAnchor104"/>Where to run Kubernetes</h2>&#13;
			<p>When thinking about where<a id="_idIndexMarker244"/> you’d want to run ESXi or Hyper-V, that’ll most likely come down to what servers you currently have, the partnerships with vendors your business has, and what resources (CPU, RAM, and so on) you need on <span class="No-Break">the servers.</span></p>&#13;
			<p>When it comes to the “<em class="italic">as close to bare metal as possible</em>” option, although there are many vendors, two stick out in the <span class="No-Break">Kubernetes space:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Equinix</strong>: Equinix allows you to – not only from a UI perspective but also from an automation perspective – use tools <a id="_idIndexMarker245"/>such as Terraform to create virtualized bare-metal servers for both Linux and Windows distributions. You can also manage networking pieces such as BGP and other routing mechanisms, as well as use on-demand, reserved, and <span class="No-Break">spot servers:</span></li>&#13;
			</ul>&#13;
			<div>&#13;
				<div id="_idContainer059" class="IMG---Figure">&#13;
					<img src="Images/B19116_04_04.jpg" alt="Figure 4.4 – Equinix metal server creation&#13;&#10;" width="1211" height="724"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – Equinix metal server creation</p>&#13;
			<p>In the following<a id="_idIndexMarker246"/> screenshot, you can see the general starting point in Equinix to start <span class="No-Break">deploying servers:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer060" class="IMG---Figure">&#13;
					<img src="Images/B19116_04_05.jpg" alt="Figure 4.5 – Equinix Metal deployment page&#13;&#10;" width="979" height="403"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – Equinix Metal deployment page</p>&#13;
			<ul>&#13;
				<li><strong class="bold">OpenMetal</strong>: OpenMetal is a full-blown virtualized bare-metal solution for running OpenStack. One of the cool parts <a id="_idIndexMarker247"/>about OpenMetal is that you get true SSH access to the literal servers that are running OpenStack, so you have a ton of flexibility and customization options, just like you would in any OpenStack environment. The only thing <a id="_idIndexMarker248"/>you don’t have access to is the actual hardware itself as that’s managed by OpenMetal, but you most likely don’t want access to it anyway if you’re looking for a virtualized <span class="No-Break">bare-metal solution:</span></li>&#13;
			</ul>&#13;
			<div>&#13;
				<div id="_idContainer061" class="IMG---Figure">&#13;
					<img src="Images/B19116_04_06.jpg" alt="Figure 4.6 – OpenMetal dashboard&#13;&#10;" width="1209" height="652"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6 – OpenMetal dashboard</p>&#13;
			<p>The following screenshot<a id="_idIndexMarker249"/> shows the standard UI in OpenStack, which is running on OpenMetal. This shows that nothing is different from using OpenStack on any other environment, which is great for engineers that are already used <span class="No-Break">to it:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer062" class="IMG---Figure">&#13;
					<img src="Images/B19116_04_07.jpg" alt="Figure 4.7 – OpenStack’s Overview dashboard&#13;&#10;" width="1038" height="657"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 – OpenStack’s Overview dashboard</p>&#13;
			<p>If you’re interested in running <a id="_idIndexMarker250"/>Kubernetes on-prem, but still want the feel of a <em class="italic">cloud</em>-based environment, OpenStack running on OpenMetal is a great place <span class="No-Break">to start.</span></p>&#13;
			<h1 id="_idParaDest-104"><a id="_idTextAnchor105"/>Summary</h1>&#13;
			<p>There’s a lot that wasn’t talked about in this chapter – storage, different interface types, hardware types, the ins and outs of Kubernetes clusters, and a lot more. The reason why is that this chapter could only be so long and a lot of the topics could take up an <span class="No-Break">entire chapter.</span></p>&#13;
			<p>However, the goal of this chapter was to give you a place <span class="No-Break">to start.</span></p>&#13;
			<p>As you learned throughout this chapter and may have come to realize, managing Kubernetes on-prem can almost feel like an entire data center within itself. You have networking concerns, scalability concerns, storage concerns, network concerns… the list goes on and on. However, if you want the flexibility of managing Kubernetes yourself without relying on a cloud provider, then this chapter went over what you should think about from <span class="No-Break">the beginning.</span></p>&#13;
			<p>Running Kubernetes on-prem is no easy task. You will most likely have to have a team of engineers – or at the very least two to three engineers with a very strong systems administration and network administration background. If you don’t already have those skills, or if your team doesn’t, this chapter should have given you a good starting point on where <span class="No-Break">to look.</span></p>&#13;
			<p>In the next chapter, you’ll start looking at the <em class="italic">how</em> and <em class="italic">why</em> behind deploying applications <span class="No-Break">on Kubernetes.</span></p>&#13;
			<h1 id="_idParaDest-105"><a id="_idTextAnchor106"/>Further reading</h1>&#13;
			<p>To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break">following resource:</span></p>&#13;
			<ul>&#13;
				<li><em class="italic">OpenStack Cookbook</em>, by Kevin Jackson, Cody Bunch, and Egle <span class="No-Break">Sigler: </span><a href="https://www.packtpub.com/product/openstack-cloud-computing-cookbook-fourth-edition/9781788398763"><span class="No-Break">https://www.packtpub.com/product/openstack-cloud-computing-cookbook-fourth-edition/9781788398763</span></a></li>&#13;
			</ul>&#13;
		</div>&#13;
	</div></body></html>