<html><head></head><body>
		<div id="_idContainer078">
			<h1 id="_idParaDest-90" class="chapter-number"><a id="_idTextAnchor201"/><st c="0">4</st></h1>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor202"/><st c="2"> Architecting the Platform Core – Kubernetes as a Unified Layer</st></h1>
			<p><st c="65">As a platform engineering team, you need to make a critical decision about the underlying technology stack of your core platform. </st><st c="196">This decision will have a long-term impact on your organization as it will dictate the skills and resources you will need to build a platform that will support current and future self-service </st><span class="No-Break"><st c="388">use cases.</st></span></p>
			<p><strong class="bold"><st c="398">Kubernetes</st></strong><st c="409"> – or </st><strong class="bold"><st c="415">K8s</st></strong><st c="418"> for short – is</st><a id="_idIndexMarker313"/><st c="433"> not the solution to all problems, but when building platforms, Kubernetes can build </st><span class="No-Break"><st c="518">the foundation.</st></span></p>
			<p><st c="533">In this chapter, you will gain insights into what makes Kubernetes the choice for many platform engineering teams. </st><st c="649">We will explain the concept of </st><em class="italic"><st c="680">promise theory</st></em><st c="694">, which Kubernetes is based on, and the benefits that come from the way it’s </st><span class="No-Break"><st c="771">been implemented.</st></span></p>
			<p><st c="788">You will get a better understanding of how to navigate the </st><strong class="bold"><st c="848">Cloud Native Computing Foundation</st></strong><st c="881"> (</st><strong class="bold"><st c="883">CNCF</st></strong><st c="887">) ecosystem </st><a id="_idIndexMarker314"/><st c="900">as it will be critical for you to pick the right projects to support you in your own </st><span class="No-Break"><st c="985">platform implementation.</st></span></p>
			<p><st c="1009">Once you are familiar with the benefits of Kubernetes and the ecosystem, you will learn about the considerations when defining the core layer of your platform, such as unifying infrastructure, application, and service capabilities. </st><st c="1242">You will learn how to design for interoperability with your core corporate services that sit outside of your new platform and how to design for flexibility, reliability, </st><span class="No-Break"><st c="1412">and robustness.</st></span></p>
			<p><st c="1427">As such, we will cover the following main topics in </st><span class="No-Break"><st c="1480">the chapter:</st></span></p>
			<ul>
				<li><st c="1492">Why Kubernetes plays a vital role, and why it is (not) </st><span class="No-Break"><st c="1548">for everyone</st></span></li>
				<li><st c="1560">Leveraging and managing Kubernetes </st><span class="No-Break"><st c="1596">infrastructure capabilities</st></span></li>
				<li><st c="1623">Designing for flexibility, reliability, </st><span class="No-Break"><st c="1664">and robustness</st></span></li>
			</ul>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor203"/><st c="1678">Why Kubernetes plays a vital role, and why it is (not) for everyone</st></h1>
			<p><st c="1746">For now, we will focus on Kubernetes, but there are </st><a id="_idIndexMarker315"/><st c="1799">other ways to provide a platform to run your workload. </st><st c="1854">Besides many different flavors of Kubernetes, such as OpenShift, there are alternatives, such as Nomad, CloudFoundry, Mesos, and OpenNebula. </st><st c="1995">They all have reasons for their existence, but only one has been adopted almost </st><span class="No-Break"><st c="2075">everywhere: Kubernetes!</st></span></p>
			<p><st c="2098">Besides those platforms, you can use virtual machines or services from public cloud providers for serverless, app engines, and simple container services. </st><st c="2253">In many cases, platforms utilize these services as well, when they are needed. </st><st c="2332">An exclusive all-in Kubernetes strategy might take a few years longer, as it takes organizations a while to fully commit to it. </st><st c="2460">However, there are two recent trends you </st><span class="No-Break"><st c="2501">can observe:</st></span></p>
			<ul>
				<li><st c="2513">Managing virtual machines </st><span class="No-Break"><st c="2540">from Kubernetes</st></span></li>
				<li><st c="2555">Migrating to virtual clusters and virtual machines managed by clusters to prevent cost explosions for </st><span class="No-Break"><st c="2658">hypervisor licenses</st></span></li>
			</ul>
			<p><st c="2677">Kubernetes comes with a vital ecosystem and community, a wide range of use cases implemented by other organizations, and highly motivated contributors to solve the next challenges coming up </st><span class="No-Break"><st c="2868">with Kubernete</st><a id="_idTextAnchor204"/><st c="2882">s.</st></span></p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor205"/><st c="2885">Kubernetes – a place to start, but not the endgame!</st></h2>
			<p><st c="2937">“</st><em class="italic"><st c="2939">Kubernetes is a platform to build platforms. </st><st c="2984">It’s a start but not the endgame</st></em><st c="3016">” is a quote from Kelsey Hightower, who worked at Google when, back in 2014, Kubernetes was released to the world. </st><st c="3132">However, while Kubernetes plays a vital role in building modern</st><a id="_idIndexMarker316"/><st c="3195"> cloud-native platforms, this doesn’t mean it’s the perfect fit for everyone. </st><st c="3273">Remember the product-centric approach to platform engineering? </st><st c="3336">It starts with understanding the pain points of your users. </st><st c="3396">Once we know the pain points, we can work on how we would implement the use cases and which technology choices </st><span class="No-Break"><st c="3507">to make.</st></span></p>
			<p><st c="3515">Revisit the early section in </st><a href="B31164_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic"><st c="3545">Chapter 1</st></em></span></a><st c="3554"> called </st><em class="italic"><st c="3562">Do you really need a platform?</st></em><st c="3592">, where we provided a questionnaire that helps you decide what the core of the platform will be. </st><st c="3689">The answer could be Kubernetes, but it doesn’t have to be. </st><st c="3748">Let’s start by looking into our own example use case from Financial </st><span class="No-Break"><st c="3816">One AC</st><a id="_idTextAnchor206"/><st c="3822">ME.</st></span></p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor207"/><st c="3826">Would Financial One ACME pick Kubernetes?</st></h2>
			<p><st c="3868">If we think about the use case from </st><a id="_idIndexMarker317"/><st c="3905">Financial One ACME, “</st><em class="italic"><st c="3926">Easier access to logs in production for problem triage</st></em><st c="3981">”, using the proposed solution doesn’t necessarily require Kubernetes as the </st><span class="No-Break"><st c="4059">underlying platform.</st></span></p>
			<p><st c="4079">If Kubernetes is not being used yet in our organization and the only thing we need is a new automation service that integrates into the different logging solutions, we may not want to propose Kubernetes as the underlying core platform. </st><st c="4316">This is because it brings a new level of complexity into an organization that doesn’t yet have the required experience. </st><st c="4436">We could implement the solution and operate it with all the existing tools and teams; maybe we could run it alongside other tools we already have, following the same operational processes for deployment, upgrades, monitoring, and </st><span class="No-Break"><st c="4666">so on.</st></span></p>
			<p><st c="4672">On the other hand, if there is pre-existing knowledge, or perhaps even Kubernetes is already available, then using Kubernetes as the core platform to orchestrate this new service would solve a lot of problems, such as providing </st><span class="No-Break"><st c="4901">the following:</st></span></p>
			<ul>
				<li><st c="4915">New service containers </st><span class="No-Break"><st c="4939">as Pods</st></span></li>
				<li><st c="4946">Automated health checks for </st><span class="No-Break"><st c="4975">those services</st></span></li>
				<li><st c="4989">Resiliency and scalability through concepts such as ReplicaSets </st><span class="No-Break"><st c="5054">and Auto-Scaling</st></span></li>
				<li><st c="5070">External access through ingress controllers and </st><span class="No-Break"><st c="5119">Gateway API</st></span></li>
				<li><st c="5130">Basic observability of those services through Prometheus </st><span class="No-Break"><st c="5188">or OpenTelemetry</st></span></li>
			</ul>
			<p><st c="5204">However, do we really need to run our own Kubernetes cluster when we just need to deploy a simple service? </st><st c="5312">The answer is no! </st><st c="5330">There are alternatives, such as running the implementation using the capabilities of your preferred </st><span class="No-Break"><st c="5430">cloud provider:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="5445">Serverless</st></strong><st c="5456">: The solution could be implemented as a set of serverless functions exposed via an</st><a id="_idIndexMarker318"/><st c="5540"> API gateway. </st><st c="5554">State or configuration can be stored in cloud storage services and can easily be accessed via </st><span class="No-Break"><st c="5648">an API.</st></span></li>
				<li><strong class="bold"><st c="5655">Container</st></strong><st c="5665">: If the solution is </st><a id="_idIndexMarker319"/><st c="5687">implemented in a container, that container can be lightweight and its endpoints can easily be exposed via an API gateway. </st><st c="5809">There is no need for a full-fledged Kubernetes cluster that somebody needs </st><span class="No-Break"><st c="5884">to maintain.</st></span></li>
			</ul>
			<p><st c="5896">This single use case for Financial One ACME</st><a id="_idIndexMarker320"/><st c="5940"> may not lead us to choose Kubernetes as the core platform. </st><st c="6000">However, when making this critical decision about what is to become the core of your future platform, we must also look beyond the first use case. </st><st c="6147">Platform engineering will solve many more use cases by providing many self-service capabilities to the internal engineering teams in order to improve their </st><span class="No-Break"><st c="6303">day-to-day work.</st></span></p>
			<p><st c="6319">It’s a tricky and impactful decision to make, one that needs a good balance between looking forward and over-engineering. </st><st c="6442">To make that decision easier, let’s look into the benefits of picking Kubernetes as the </st><span class="No-Break"><st c="6530">core pl</st><a id="_idTextAnchor208"/><st c="6537">atform.</st></span></p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor209"/><st c="6545">Benefits of picking Kubernetes as the core platform</st></h2>
			<p><st c="6597">To make the critical decision of </st><a id="_idIndexMarker321"/><st c="6631">picking the core of a future platform easier, let’s look at why other organizations are picking Kubernetes as the core building block. </st><st c="6766">Understanding those reasons, the benefits, and also the challenges should make it easier for architects to make this </st><span class="No-Break"><st c="6883">important d</st><a id="_idTextAnchor210"/><st c="6894">ecision.</st></span></p>
			<h3><st c="6903">Declarative desired state – promise theory</st></h3>
			<p><st c="6946">Traditional IT operations</st><a id="_idIndexMarker322"/><st c="6972"> use the </st><strong class="bold"><st c="6981">obligation model</st></strong><st c="6997">, which is </st><a id="_idIndexMarker323"/><st c="7008">when an external system instructs the target system to do certain things. </st><st c="7082">This model requires a lot of logic to be put into the external system, such as an automated pipeline. </st><st c="7184">A scripted pipeline, whether based on </st><a id="_idIndexMarker324"/><st c="7222">Jenkins, GitHub Actions, or other solutions, not only needs to apply changes to the target system. </st><st c="7321">The pipeline also needs to deal with handling unpredicted outcomes and errors from outside the system it changes. </st><st c="7435">For example, what do we do if deploying a new software version doesn’t work within a certain amount of time? </st><st c="7544">Should we roll it back? </st><st c="7568">How would the pipeline </st><span class="No-Break"><st c="7591">do that?</st></span></p>
			<p><st c="7599">In the Kubernetes Documentary Part 1 (</st><a href="https://www.youtube.com/watch?v=BE77h7dmoQU"><st c="7638">https://www.youtube.com/watch?v=BE77h7dmoQU</st></a><st c="7682">), Kelsey Hightower explained the promise theory model that Kubernetes follows with a great analogy. </st><st c="7784">It goes something </st><span class="No-Break"><st c="7802">like this:</st></span></p>
			<p class="author-quote"><st c="7812">If you write a letter, put it in an envelope, and put the destination address and the right stamps on it, then the post office promises to deliver that letter to the destination within a certain amount of time. </st><st c="8024">Whether that delivery involves trucks, trains, planes or any other form of delivery doesn’t matter to the person who wrote that letter. </st><st c="8160">The postal service will do whatever it takes to keep the promise of delivery. </st><st c="8238">If a truck breaks down, some other truck will continue until the letter gets delivered to its final destination.</st></p>
			<p><st c="8350">The same principle is true for Kubernetes! </st><st c="8394">In our analogy, the letter is a container image that we put into an envelope. </st><st c="8472">The envelope in the Kubernetes world is a custom resource of a certain </st><strong class="bold"><st c="8543">Custom Resource Definition</st></strong><st c="8569"> (</st><strong class="bold"><st c="8571">CRD</st></strong><st c="8574">). </st><st c="8578">To deliver an image, this could be a definition of a </st><a id="_idIndexMarker325"/><st c="8631">Deployment, which includes the reference to the image, the number of replicas, the namespace this image should be deployed into, and the resource requirements (CPU and memory) for the image to run correctly. </st><st c="8839">Kubernetes then does everything it can to fulfill the promise of deploying that image by finding the right Kubernetes node that meets all the requirements to run the container image with the specified amount of replicas and the required CPU </st><span class="No-Break"><st c="9080">and memory.</st></span></p>
			<p><st c="9091">Another example is an Ingress</st><a id="_idIndexMarker326"/><st c="9121"> that exposes a deployed service to the outside world. </st><st c="9176">Through annotations, it is possible to control the behavior of certain objects. </st><st c="9256">For an Ingress, this could be the automatic creation of a TLS certificate for the domain that should be used to expose the matching services to be accessible via HTTPS. </st><st c="9425">The following is an example of an Ingress object for </st><strong class="source-inline"><st c="9478">fund-transfer-service</st></strong><st c="9499"> to expose the object via a specific domain to the outside world using the Certificate Manager – a core Kubernetes ecosystem tool – to create a valid TLS certificate </st><a id="_idIndexMarker327"/><span class="No-Break"><st c="9665">from </st></span><span class="No-Break"><strong class="source-inline"><st c="9670">LetsEncrypt</st></strong></span><span class="No-Break"><st c="9681">:</st></span></p>
			<pre class="source-code"><st c="9683">
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: fund-transfer-service
  namespace: prod-useast-01
  annotations:
    cert-manager.io/issuer: "letsencrypt-prod"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - fundtransfer-prod-useast-01.finone.acme
      secretName: finone-acme-tls
  rules:
  - host: fundtransfer-prod-useast-01.finone.acme
    http:
    ...</st></pre>
			<p><st c="10029">There is a bit more to a fully working Ingress object description than what’s shown in this manifest </st><em class="italic"><st c="10131">[1]</st></em><st c="10134"> example. </st><st c="10144">However, this example does a good job of explaining how a definition will be translated by Kubernetes into the actual actions that one would expect – hence</st><a id="_idIndexMarker328"/><st c="10299"> fulfilling </st><span class="No-Break"><st c="10311">the promise.</st></span></p>
			<p><st c="10323">Now, the question is: “</st><em class="italic"><st c="10347">How does all this magic work?</st></em><st c="10377">” To answer this, we will start by exploring the concepts of controllers</st><a id="_idTextAnchor211"/> <span class="No-Break"><st c="10450">and operators.</st></span></p>
			<h3><st c="10465">Kubernetes controllers and operators</st></h3>
			<p><st c="10502">Kubernetes </st><a id="_idIndexMarker329"/><st c="10514">controllers are essentially control loops that fulfill the promise theory of Kubernetes. </st><st c="10603">In other words, controllers automate what IT admins often do manually: continuously observe a system’s current state, compare it with what we expect the system to look like, and execute remedial actions to keep the </st><span class="No-Break"><st c="10818">system running!</st></span></p>
			<p><st c="10833">A core task of Kubernetes controllers is therefore </st><em class="italic"><st c="10885">continuous reconciliation</st></em><st c="10910">. This continuous activity allows it to enforce the desired state, for example, making sure that the </st><em class="italic"><st c="11011">desired state</st></em><st c="11024"> expressed in the </st><em class="italic"><st c="11042">Ingress definition</st></em><st c="11060"> example from earlier matches the </st><em class="italic"><st c="11094">current state</st></em><st c="11107">. If either the desired state or the current state changes, it means they are </st><em class="italic"><st c="11185">out of sync</st></em><st c="11196">. The controller then tries to synchronize the two states by making changes to the managed object until the current state matches the desired </st><span class="No-Break"><st c="11338">state again!</st></span></p>
			<p><st c="11350">The following illustration shows how a controller watches the </st><em class="italic"><st c="11413">desired state</st></em><st c="11426"> (expressed through manifests and stored in etcd), compares it with the </st><em class="italic"><st c="11498">current state</st></em><st c="11511"> (the state persisted in etcd), and manages the </st><em class="italic"><st c="11559">managed objects</st></em><st c="11574"> (e.g., Ingress, Deployments, SSL certificates, and </st><span class="No-Break"><st c="11626">so on):</st></span></p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/Figure_4.01_B31164.jpg" alt="Figure 4.1: Reconciliation and self-healing by design through Kubernetes controllers"/><st c="11633"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="11706">Figure 4.1: Reconciliation and self-healing by design through Kubernetes controllers</st></p>
			<p><st c="11790">This figure already shows the core </st><a id="_idIndexMarker330"/><st c="11826">concepts and power of controllers, highlighting how the automated reconciliation loop ensures automated self-healing by design. </st><st c="11954">However, controllers fulfill other functions as well: observing cluster and node health, enforcing resource limits, running jobs on a schedule, processing life cycle events, and managing deployment rollouts </st><span class="No-Break"><st c="12161">and rollbacks.</st></span></p>
			<p><st c="12175">Now, let’s discuss Kubernetes</st><a id="_idIndexMarker331"/><st c="12205"> operators. </st><st c="12217">Operators are a subcategory of controllers and typically focus on a specific domain. </st><st c="12302">A good example is the OpenTelemetry operator, which manages OpenTelemetry Collectors and the auto-instrumentation of workloads. </st><st c="12430">This operator uses the same reconciliation loop to ensure that the desired configuration for OpenTelemetry is always applied. </st><st c="12556">If the configuration is changed or if there is a problem with the current OpenTelemetry Collector or instrumentation, the operator will do its best to keep the promise of ensuring that the desired state is the actual state. </st><st c="12780">To learn more, visit the OpenTelemetry website </st><span class="No-Break"><st c="12827">at </st></span><a href="https://opentelemetry.io/docs/kubernetes/operator/"><span class="No-Break"><st c="12830">https://opentelemetry.io/docs/kubernetes/operator/</st></span></a><span class="No-Break"><st c="12880">.</st></span></p>
			<p><st c="12881">Other use cases for operators typically relate to managing and automating core services and applications such as databases, storage, service meshes, backup and restore, CI/CD, and </st><span class="No-Break"><st c="13062">messaging systems.</st></span></p>
			<p><st c="13080">If you want to learn more about </st><a id="_idIndexMarker332"/><st c="13113">controllers</st><a id="_idIndexMarker333"/><st c="13124"> and operators, have a look at the CNCF Operator Working Group and their white paper at </st><a href="https://github.com/cncf/tag-app-delivery/tree/main/operator-wg"><st c="13212">https://github.com/cncf/tag-app-delivery/tree/main/operator-wg</st></a><st c="13274">. Another excellent overview can be found on the </st><em class="italic"><st c="13323">Kong Blog</st></em><st c="13332"> post titled </st><em class="italic"><st c="13345">What’s the Difference: Kubernetes Controllers vs Operators</st></em><st c="13403">. This blog also lists great examples of controllers and </st><span class="No-Break"><st c="13460">operators: </st></span><a href="https://konghq.com/blog/learning-center/kubernetes-controllers-vs-operators"><span class="No-Break"><st c="13471">https://konghq.com/blog/learning-center/kubernetes-controllers-vs-operators</st></span></a></p>
			<p><st c="13546">Now that we know more about controllers and operators, let’s have a look at how they also ensure built-in resiliency for all Kubernetes componen</st><a id="_idTextAnchor212"/><st c="13691">ts </st><span class="No-Break"><st c="13695">and deployments!</st></span></p>
			<h3><st c="13711">Built-in resilience driven by probes</st></h3>
			<p><st c="13748">Controllers continuously </st><a id="_idIndexMarker334"/><st c="13774">validate that our system is in its desired state by observing the health of the Kubernetes cluster, along with its nodes and all deployed Pods. </st><st c="13918">If one of the observed components is not healthy, the system tries to bring it back into a healthy state through certain automated actions. </st><st c="14058">Take Pods, for example. </st><st c="14082">If Pods are no longer healthy, they eventually get restarted to ensure the overall system’s resiliency. </st><st c="14186">Restarting components is also often the default action an IT admin would execute following the “</st><em class="italic"><st c="14282">Let’s try to turn it off and on again and see what </st></em><span class="No-Break"><em class="italic"><st c="14334">happens!</st></em></span><span class="No-Break"><st c="14342">” approach.</st></span></p>
			<p><st c="14354">Just like IT admins who probably won’t just turn things on and off at random, Kubernetes follows a more sophisticated approach to ensuring the resiliency of our Kubernetes clusters, nodes, </st><span class="No-Break"><st c="14544">and workloads.</st></span></p>
			<p><st c="14558">Kubelet – a </st><a id="_idIndexMarker335"/><st c="14571">core component of Kubernetes – continuously observes the life cycle and the health state of Pods using several types of probes: startup, readiness, and liveness. </st><st c="14733">The following illustration shows the different health states a pod can be in depending on the results of startup, readiness, and liveness </st><span class="No-Break"><st c="14871">probe checks:</st></span></p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/Figure_4.02_B31164.jpg" alt="Figure 4.2: Kubelet determining the health status of Pods using probes"/><st c="14884"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="15031">Figure 4.2: Kubelet determining the health status of Pods using probes</st></p>
			<p><st c="15101">Once Pods are no longer healthy, Kubernetes will try to restart Pods and bring the Pods back to a healthy state. </st><st c="15215">There are a lot of different settings for both the evaluation of the probe results and the restart policies, which you must familiarize yourself with to fully take advantage of the built-in resiliency of Kubernetes. </st><st c="15431">All those settings are declared on your Deployment and </st><span class="No-Break"><st c="15486">Pod definitions.</st></span></p>
			<p><st c="15502">If you want to learn </st><a id="_idIndexMarker336"/><st c="15524">more about how </st><a id="_idIndexMarker337"/><st c="15539">Kubelet manages the different probes and see some best practices, we can recommend checking out blog posts such as the one from Roman Belshevits on liveness </st><span class="No-Break"><st c="15696">probes: </st></span><a href="https://dev.to/otomato_io/liveness-probes-feel-the-pulse-of-the-app-133e "><span class="No-Break"><st c="15704">https://dev.to/otomato_io/liveness-probes-feel-the-pulse-of-the-app-133e</st></span></a></p>
			<p><st c="15776">Another great resource is the official Kubernetes documentation on configuring liveness, readiness, and startup </st><span class="No-Break"><st c="15889">probes: </st></span><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/"><span class="No-Break"><st c="15897">https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/</st></span></a></p>
			<p class="callout-heading"><st c="15998">Health probes are only valid within Kubernetes</st></p>
			<p class="callout"><st c="16045">It’s important to understand that all these health checks are only done within the Kubernetes cluster and don’t tell us whether a service that is exposed via an Ingress to our end users is also considered healthy from the end users’ perspective. </st><st c="16292">A best practice is to additionally check health and availability from an “outside-in” perspective as an external control. </st><st c="16414">For example, you can use synthetic tests to validate that all exposed endpoints are reachable and return </st><span class="No-Break"><st c="16519">successful responses.</st></span></p>
			<p><st c="16540">Now that we have learned about built-in resiliency for Pods, how about more complex constructs, such as applications that are typically made up of several different Pods and other objects, such a</st><a id="_idTextAnchor213"/><st c="16736">s Ingress </st><span class="No-Break"><st c="16747">and storage?</st></span></p>
			<h3><st c="16759">Workload and application life cycle orchestration</st></h3>
			<p><st c="16809">As we have learned, Kubernetes provides</st><a id="_idIndexMarker338"/><st c="16849"> built-in orchestration of the life cycle of Pods, as explained in the</st><a id="_idIndexMarker339"/><st c="16919"> previous section. </st><st c="16938">However, business applications that we deploy on Kubernetes typically have multiple dependent Pods and workloads that make up the application. </st><st c="17081">Take our Financial One ACME as an example: the financial services applications deployed to support its customers contain multiple components, such as a frontend, a backend, caches, databases, and Ingress. </st><st c="17286">Unfortunately, Kubernetes doesn’t have the concept of applications. </st><st c="17354">While there are several initiatives and working groups to define an application, we currently have to rely on other approaches for managing applications, which are composites of </st><span class="No-Break"><st c="17532">multiple components.</st></span></p>
			<p><st c="17552">In the </st><em class="italic"><st c="17560">Batching changes to combat dependencies</st></em><st c="17599"> section in </st><a href="B31164_05.xhtml#_idTextAnchor255"><span class="No-Break"><em class="italic"><st c="17611">Chapter 5</st></em></span></a><st c="17620">, we will learn about tools such as Crossplane. </st><st c="17668">Crossplane allows you to define so-called composites, which make it easy for application owners to define individual components of an application and then deploy individual instances, as shown in the </st><span class="No-Break"><st c="17868">following example:</st></span></p>
			<pre class="source-code"><st c="17886">
apiVersion: composites.financialone.acme/v1alpha1
kind: FinancialBackend
metadata:
  name: tenantABC-eu-west
spec:
  service-versions:
    fund-transfer: 2.34.3
    account-info: 1.17.0
  redis-cache:
    version: 7.4.2
    name: transfer-cache
    size: medium
  database:
    size: large
    name: accounts-db
  region: "eu-west"
  ingress:
    url: „https://tenantABC-eu-west.financialone.acme"</st></pre>
			<p><st c="18240">Crossplane</st><a id="_idIndexMarker340"/><st c="18251"> provides </st><a id="_idIndexMarker341"/><st c="18261">application </st><a id="_idIndexMarker342"/><st c="18273">and infrastructure orchestration and uses the operator pattern to continuously ensure that every application instance – as defined in the composite – is running </st><span class="No-Break"><st c="18434">as expected.</st></span></p>
			<p><st c="18446">Another tool we will learn more about</st><a id="_idIndexMarker343"/><st c="18484"> in </st><a href="B31164_05.xhtml#_idTextAnchor255"><span class="No-Break"><em class="italic"><st c="18488">Chapter 5</st></em></span></a><st c="18497"> is the </st><strong class="bold"><st c="18505">Keptn</st></strong><st c="18510"> CNCF project. </st><st c="18525">Keptn provides automated application-aware life cycle orchestration and observability. </st><st c="18612">It gives you the option to declaratively define pre- and post-deployment checks (validate dependencies, run tests, evaluate health, enforce SLO-based quality gates, and so on) without having to write your own Kubernetes operator to implement those actions. </st><st c="18869">Keptn also provides automated deployment observability to better understand how many deployments happen, how many are successful, and where and why they fail by emitting OpenTelemetry traces and metrics for easier troubleshooting </st><span class="No-Break"><st c="19099">and reporting.</st></span></p>
			<p><st c="19113">Kubernetes provides a lot of the building blocks for building resilient systems. </st><st c="19195">While you can write your own operators to expand this to your own problem domain, you can also use existing CNCF tools such</st><a id="_idIndexMarker344"/><st c="19318"> as Crossplane</st><a id="_idIndexMarker345"/><st c="19332"> or Keptn as they provide an easier declarative way to apply the concept of promise theory to </st><a id="_idIndexMarker346"/><st c="19426">more complex </st><a id="_idIndexMarker347"/><st c="19439">applications and </st><span class="No-Break"><st c="19456">infrastructure compositions.</st></span></p>
			<p><st c="19484">Restarting components is one way of ensuring resiliency, but there are more. </st><st c="19562">Let’s have a look at auto-scaling, which solves another critical </st><a id="_idTextAnchor214"/><st c="19627">problem in </st><span class="No-Break"><st c="19638">dynamic environments!</st></span></p>
			<h3><st c="19659">Auto-scaling clusters and workloads</st></h3>
			<p><st c="19695">In most industries, the load </st><a id="_idIndexMarker348"/><st c="19725">expected on a system is not equally distributed across every day of the year. </st><st c="19803">There is always some type of seasonality: retail gets spikes on Black Friday and Cyber Monday, tax services get spikes on tax day, and finance often spikes when paychecks are coming. </st><st c="19986">The same is true for our own Financial One ACME customers. </st><st c="20045">As a financial services organization, there is always some basic traffic from end users, but there will be spikes at the beginning and end of </st><span class="No-Break"><st c="20187">the month.</st></span></p>
			<p><st c="20197">Kubernetes </st><a id="_idIndexMarker349"/><st c="20209">provides several ways to scale application</st><a id="_idIndexMarker350"/><st c="20251"> workloads: manually (e.g., setting ReplicaSets) or automatically through tools such as </st><strong class="bold"><st c="20339">Horizontal Pod Autoscaler</st></strong><st c="20364"> (</st><strong class="bold"><st c="20366">HPA</st></strong><st c="20369">), </st><strong class="bold"><st c="20373">Vertical Pod Autoscaler</st></strong><st c="20396"> (</st><strong class="bold"><st c="20398">VPA</st></strong><st c="20401">), or </st><strong class="bold"><st c="20408">Kubernetes Event Driven Autoscaler</st></strong><st c="20442"> (</st><strong class="bold"><st c="20444">KEDA</st></strong><st c="20448">). </st><st c="20452">Those</st><a id="_idIndexMarker351"/><st c="20457"> scaling options allow you to scale when your workloads run low on CPU or memory, when applications see a spike in incoming traffic, or when response time is starting </st><span class="No-Break"><st c="20624">to increase!</st></span></p>
			<p><st c="20636">Besides workloads, you can and most likely have to also scale the size of your clusters and nodes through tools </st><a id="_idIndexMarker352"/><st c="20749">such</st><a id="_idIndexMarker353"/><st c="20753"> as </st><strong class="bold"><st c="20757">Cluster Autoscaler</st></strong><st c="20775"> (</st><strong class="bold"><st c="20777">CA</st></strong><st c="20779">) or </st><strong class="bold"><st c="20785">Karpenter</st></strong> <em class="italic"><st c="20794">[2]</st></em><st c="20798">, or through options available via your managed Kubernetes </st><span class="No-Break"><st c="20857">cloud vendor.</st></span></p>
			<p><st c="20870">As a platform engineering team, you need to make yourself familiar with all the different options but also be aware of all </st><span class="No-Break"><st c="20994">the considerations:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="21013">Setting limits</st></strong><st c="21028">: Don’t allow applications to scale endlessly. </st><st c="21076">You have options to enforce maximum limits per application, workload, namespaces, </st><span class="No-Break"><st c="21158">and more.</st></span></li>
				<li><strong class="bold"><st c="21167">Cost control</st></strong><st c="21180">: Auto-scaling is great but has a price tag. </st><st c="21226">Make sure to report costs to the </st><span class="No-Break"><st c="21259">application owners.</st></span></li>
				<li><strong class="bold"><st c="21278">Scale down</st></strong><st c="21289">: Scaling up is easy! </st><st c="21312">Make sure to also define indicators for when to scale down. </st><st c="21372">This </st><a id="_idIndexMarker354"/><st c="21377">will keep costs </st><span class="No-Break"><st c="21393">under control.</st></span></li>
			</ul>
			<p><st c="21407">To learn more about them, please review the </st><span class="No-Break"><st c="21452">documentation: </st></span><a href="https://kubernetes.io/docs/concepts/workloads/autoscaling/ "><span class="No-Break"><st c="21467">https://kubernetes.io/docs/concepts/workloads/autoscaling/</st></span></a></p>
			<p><st c="21525">Now that we have learned about the options for scaling within a Kubernetes environment, how about sc</st><a id="_idTextAnchor215"/><st c="21626">aling out to other </st><span class="No-Break"><st c="21646">Kubernetes clusters?</st></span></p>
			<h3><st c="21666">Declare once – run anywhere (in theory)</st></h3>
			<p><st c="21706">The promise of Kubernetes as an open</st><a id="_idIndexMarker355"/><st c="21743"> standard is that any declared state (Ingress, workloads, secrets, storage, network, etc.) will behave the same whether you run it on a single cluster or on multiple clusters to meet certain requirements, such as the separation of stages (dev, staging, and production) or the separation of regions (US, Europe, </st><span class="No-Break"><st c="22054">and Asia).</st></span></p>
			<p><st c="22064">The same promise holds true in theory whether you operate your own Kubernetes cluster, use OpenShift, or use a managed Kubernetes service from one of the cloud vendors. </st><st c="22234">What does </st><em class="italic"><st c="22244">in theory</st></em><st c="22253"> mean here? </st><st c="22265">There are some specific technical differences between the different offerings you need to take into consideration. </st><st c="22380">Depending on the offering, networking or storage may act slightly differently because the underlying implementation depends on the cloud vendor. </st><st c="22525">Certain offerings will also come with specific versions of core Kubernetes services, services meshes, and operators that come with a managed installation. </st><st c="22680">Some offerings require you to use vendor-specific annotations to configure the behavior of certain services. </st><st c="22789">That’s why applying the same declarative state definition across different vendors will, </st><em class="italic"><st c="22878">in theory</st></em><st c="22887">, work – in practice, you have to consider certain small differences that require some </st><span class="No-Break"><st c="22974">vendor-specific configuration!</st></span></p>
			<p><st c="23004">As the technical details and differences are constantly changing, it wouldn’t make sense to provide a current side-by-side comparison as part of this book. </st><st c="23161">What you must understand is that while, in theory, you can take any Kubernetes object and deploy it on any flavor of Kubernetes, the outcome and behavior might be slightly different depending on where you deploy it. </st><st c="23377">That’s why we suggest doing some technical research on the chosen target Kubernetes offering and how it differs from other offerings in case you want to go for multi-cloud/multi-Kubernetes, because the same Kubernetes objects might behave </st><span class="No-Break"><st c="23616">slightly differently!</st></span></p>
			<p><st c="23637">The good news is </st><a id="_idIndexMarker356"/><st c="23655">that the global community is working to solve this problem by providing better guidance and tools to make the </st><em class="italic"><st c="23765">declare once – run anywhere</st></em><st c="23792"> promise </st><span class="No-Break"><st c="23801">a reality.</st></span></p>
			<p><st c="23811">We’ve looked at a lot of the benefits of picking Kubernetes as the core platform. </st><st c="23894">However, there is another good reason why Kubernetes has seen such great adoption over the past 10 years since its first re</st><a id="_idTextAnchor216"/><st c="24017">lease: the global community and </st><span class="No-Break"><st c="24050">the CNCF!</st></span></p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor217"/><st c="24059">Global community and CNCF</st></h2>
			<p><st c="24085">Kubernetes</st><a id="_idIndexMarker357"/><st c="24096"> was announced by Google in June 2014, and version 1.0 was released on July 21, 2015. </st><st c="24182">Google then worked with the Linux Foundation and formed the CNCF with Kubernetes as its initial project! </st><st c="24287">Since then, the community and the projects have taken the world </st><span class="No-Break"><st c="24351">by storm!</st></span></p>
			<p><st c="24360">10 years later (at the time of writing this book), the CNCF has 188 projects, 244,000 contributors, 16.6 million contributions, and members in 193 countries worldwide. </st><st c="24529">Many presentations that introduce Kubernetes and the </st><a id="_idIndexMarker358"/><st c="24582">CNCF often start by showing the CNCF </st><span class="No-Break"><st c="24619">landscape: </st></span><a href="https://landscape.cncf.io/ "><span class="No-Break"><st c="24630">https://landscape.cncf.io/</st></span></a></p>
			<p><st c="24656">While the landscape is impressive, it has also been the source of many memes about how hard and complex it is to navigate the landscape of all projects this global community is working on. </st><st c="24846">However, don’t be scared. </st><st c="24872">The global CNCF community is part of the Linux Foundation and has the mission to provide support, oversight, and direction for fast-growing cloud-native projects, including Kubernetes, Envoy, </st><span class="No-Break"><st c="25064">and Prometheus.</st></span></p>
			<p><st c="25079">Here are a few things you should be aware of because they will help you navigate the ever-growing list of CNCF projects in the </st><span class="No-Break"><st c="25207">project landscape:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="25225">Project status</st></strong><st c="25240">: CNCF </st><a id="_idIndexMarker359"/><st c="25248">actively tracks the status and activity of every project. </st><st c="25306">The number of contributors and adopters, as well as how active development is for a project, are good indicators of whether you should look closer at a project. </st><st c="25467">Projects that are stale, only have a single maintainer, or hardly have any adopters might not be of any use if you are deciding on tools that will help you for the long term in </st><span class="No-Break"><st c="25644">your platform.</st></span></li>
				<li><strong class="bold"><st c="25658">Maturity level</st></strong><st c="25673">: The CNCF also specifies a maturity level</st><a id="_idIndexMarker360"/><st c="25716"> of sandbox, incubating, or graduated, which corresponds to the Innovators, Early Adopters, and Early Majority tiers of </st><a id="_idIndexMarker361"/><st c="25836">the </st><em class="italic"><st c="25840">Crossing the Chasm</st></em><st c="25858"> diagram (</st><a href="https://en.wikipedia.org/wiki/Crossing_the_Chasm"><st c="25868">https://en.wikipedia.org/wiki/Crossing_the_Chasm</st></a><st c="25917">). </st><st c="25921">Graduated projects have been adopted widely across various industries and are a safe choice for the use cases they support. </st><st c="26045">Incubating projects have crossed over from a technical playground to seeing good adoption with growing numbers of a diverse set of maintainers. </st><st c="26189">To learn more about the criteria for CNCF maturity and to see who is at which level, check out the official site </st><span class="No-Break"><st c="26302">at </st></span><a href="https://www.cncf.io/project-metrics/"><span class="No-Break"><st c="26305">https://www.cncf.io/project-metrics/</st></span></a><span class="No-Break"><st c="26341">.</st></span></li>
				<li><strong class="bold"><st c="26342">Adopters</st></strong><st c="26351">: Every </st><a id="_idIndexMarker362"/><st c="26360">CNCF project tries to increase and track adoption. </st><st c="26411">One way of doing this is by making organizations that actively adopt a project add themselves to the </st><strong class="source-inline"><st c="26512">ADOPTERS.md</st></strong><st c="26523"> file, which every CNCF project typically has in its GitHub repository. </st><st c="26595">If you decide to adopt one of those projects, we encourage you to also add your name to the list of adopters by opening up a pull request. </st><st c="26734">This helps the project and will help other organizations decide whether this is a project </st><span class="No-Break"><st c="26824">worth pursuing!</st></span></li>
			</ul>
			<p class="callout-heading"><st c="26839">Kubernetes is vital because of its community</st></p>
			<p class="callout"><st c="26884">While Kubernetes has a strong technology base, it is really the community and the ecosystem that was built over the past 10+ years that makes Kubernetes a viable option for platform engineers to use as their </st><span class="No-Break"><st c="27093">core platform.</st></span></p>
			<p><st c="27107">We have now learned more about what Kelsey Hightower meant when he said: “</st><em class="italic"><st c="27182">Kubernetes is a platform to build platforms. </st><st c="27228">It’s a start but not the endgame</st></em><st c="27260">.” There are many benefits of picking </st><a id="_idIndexMarker363"/><st c="27298">Kubernetes as the core platform, especially as it is built on the concept </st><a id="_idIndexMarker364"/><st c="27372">of </st><em class="italic"><st c="27375">promise theory</st></em><st c="27389">. Kubernetes provides automated resiliency, scaling, and life cycle management of components. </st><st c="27483">The ever-growing community provides solutions to many common problems through hundreds of open source CNCF projects that every organization </st><span class="No-Break"><st c="27623">can use.</st></span></p>
			<p><st c="27631">While we often focus on the benefit of Kubernetes for deploying and orchestrating applications, let’s have a look at how we can use Kubernetes to lift our infrastructure capabilities into our </st><span class="No-Break"><st c="27824">future platform!</st></span></p>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor218"/><st c="27840">Leveraging and managing Kubernetes Infrastructure Capabilities</st></h1>
			<p><st c="27903">Back in </st><a href="B31164_02.xhtml#_idTextAnchor055"><span class="No-Break"><em class="italic"><st c="27912">Chapter 2</st></em></span></a><st c="27921">, you were introduced to the Platform Reference Components model and the capability plane. </st><st c="28012">When we are writing about lifting infrastructure capabilities to Kubernetes, the end user becomes aware of those capabilities when using the platform. </st><st c="28163">We must differentiate between resources that need to be integrated with Kubernetes and those configured by specifications deployed to Kubernetes and manipulated or created new resources outside of the cluster. </st><st c="28373">In the following figure, you can find examples in the resource integration and network section that require a solid integration; otherwise, they actively prevent a useful and </st><span class="No-Break"><st c="28548">functioning platform.</st></span></p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/Figure_4.03_B31164.jpg" alt="Figure 4.3: Capability plane with example tools"/><st c="28569"/>
				</div>
			</div>
			<p class="IMG---Figure"><a id="_idTextAnchor219"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="28825">Figure 4.3: Capability plane with example tools</st></p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor220"/><st c="28872">Integrating infrastructure resources</st></h2>
			<p><st c="28909">We will discuss the basic components and design decisions you must make for the platform’s underlying technologies. </st><st c="29026">Firstly, due to Kubernetes’ power, you are more flexible in your tooling and can extend it as needed. </st><st c="29128">This is especially helpful when you’re adjusting the </st><a id="_idTextAnchor221"/><st c="29181">platform’s capabilities for different </st><span class="No-Break"><st c="29219">use cases.</st></span></p>
			<h3><st c="29229">Container storage interface</st></h3>
			<p><st c="29257">The </st><strong class="bold"><st c="29262">Container Storage Interface</st></strong><st c="29289"> (</st><strong class="bold"><st c="29291">CSI</st></strong><st c="29294">) provides </st><a id="_idIndexMarker365"/><st c="29306">access to the storage technology that is attached to the cluster or the nodes running the cluster. </st><st c="29405">In the CSI developer documentation </st><em class="italic"><st c="29440">[3]</st></em><st c="29443">, you can find a driver for almost every storage provider. </st><st c="29502">The list contains </st><a id="_idIndexMarker366"/><st c="29520">cloud provider drivers such as AWS </st><strong class="bold"><st c="29555">Elastic Block Storage</st></strong><st c="29576"> (</st><strong class="bold"><st c="29578">EBS</st></strong><st c="29581">), software-defined storage such as Ceph, or a connector for commercial solutions such as NetApp. </st><st c="29680">In addition, the CSI driver also supports tool-specific drivers such as cert-manager and HashiCorp Vault. </st><st c="29786">In short, the CSI is vital to any data that should live longer than the container it belongs to and is not stored in a database, or is needed for </st><span class="No-Break"><st c="29932">database storage.</st></span></p>
			<p><st c="29949">The installation of the driver depends on the infrastructure and storage technology. </st><st c="30035">For a cloud provider, for example, you usually require </st><span class="No-Break"><st c="30090">the following:</st></span></p>
			<ul>
				<li><st c="30104">A service account or </st><span class="No-Break"><st c="30126">permission policies</st></span></li>
				<li><st c="30145">Configuration for startup taints </st><span class="No-Break"><st c="30179">and tolerations</st></span></li>
				<li><st c="30194">Pre-installed </st><span class="No-Break"><st c="30209">external snapshotter</st></span></li>
				<li><st c="30229">The driver </st><span class="No-Break"><st c="30241">installation itself</st></span></li>
			</ul>
			<p><st c="30260">Due to their complexity, these components are deployed with Helm or other package management solutions. </st><st c="30365">Sometimes, they require more privileges on the node, which can be a security concern when designing the platform. </st><st c="30479">You will also need to consider how storage will </st><span class="No-Break"><st c="30527">be accessed:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="30539">ReadWriteOnce</st></strong><st c="30553"> (</st><strong class="bold"><st c="30555">RWO</st></strong><st c="30558">): One </st><a id="_idIndexMarker367"/><st c="30566">Pod claims a portion of the available storage, which it can read from and write to, while other Pods cannot access it unless the original Pod releases </st><span class="No-Break"><st c="30717">the storage.</st></span></li>
				<li><strong class="bold"><st c="30729">ReadWriteMany</st></strong><st c="30743"> (</st><strong class="bold"><st c="30745">RWM</st></strong><st c="30748">): Multiple </st><a id="_idIndexMarker368"/><st c="30761">Pods can claim one portion of the available storage. </st><st c="30814">They can read and write to it, and share that storage </st><span class="No-Break"><st c="30868">with others.</st></span></li>
				<li><strong class="bold"><st c="30880">ReadOnlyMany</st></strong><st c="30893"> (</st><strong class="bold"><st c="30895">ROM</st></strong><st c="30898">): Multiple</st><a id="_idIndexMarker369"/><st c="30910"> Pods can claim one portion of the storage, but only to read </st><span class="No-Break"><st c="30971">from it.</st></span></li>
				<li><strong class="bold"><st c="30979">ReadWriteOncePod</st></strong><st c="30996"> (</st><strong class="bold"><st c="30998">RWOP</st></strong><st c="31002">): Can be</st><a id="_idIndexMarker370"/><st c="31012"> claimed by only one Pod; no other Pod can take it, and it allows read and </st><span class="No-Break"><st c="31087">write operations.</st></span></li>
			</ul>
			<p><st c="31104">The overview of the </st><a id="_idIndexMarker371"/><st c="31125">CSI driver provides further information on which access modes are supported. </st><st c="31202">As there is no one-size-fits-all solution, you have to make your options transparent to the user and explain how to </st><span class="No-Break"><st c="31318">use them.</st></span></p>
			<p><st c="31327">The core elements to know and to understand for your users are </st><strong class="source-inline"><st c="31391">StorageClass</st></strong><st c="31403">, </st><strong class="source-inline"><st c="31405">PersistentVolume</st></strong><st c="31421">, and </st><strong class="source-inline"><st c="31427">PersistentVolumeClaim</st></strong><st c="31448">. When a Pod/Deployment requires a volume, </st><strong class="source-inline"><st c="31491">StorageClass</st></strong><st c="31503"> will trigger the creation of a new volume. </st><st c="31547">In case a Pod/Deployment has already claimed a volume once, and it didn’t get destroyed, the Kubernetes control plane will re-assign the volume to </st><span class="No-Break"><st c="31694">the Pod/Deployment.</st></span></p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/Figure_4.04_B31164.jpg" alt="Figure 4.4: Dynamically provisioning new persistent volumes"/><st c="31713"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="31893">Figure 4.4: Dynamically provisioning new persistent volumes</st></p>
			<p><st c="31952">You define </st><strong class="source-inline"><st c="31964">StorageClass</st></strong><st c="31976"> as the platform</st><a id="_idIndexMarker372"/><st c="31992"> team, ideally collaborating with your storage experts. </st><st c="32048">The following example highlights the common definition of </st><strong class="source-inline"><st c="32106">StorageClass</st></strong><st c="32118">. There is plenty of room to make a mistake in the configuration; for example, setting no </st><strong class="source-inline"><st c="32208">reclaimPolicy</st></strong><st c="32221"> will, by default, delete the later created and attached volume. </st><st c="32286">However, </st><strong class="source-inline"><st c="32295">StorageClass</st></strong><st c="32307"> supports you in the dynamic creation of volumes for user-requested </st><strong class="source-inline"><st c="32375">PersistantVolumeClaim</st></strong><st c="32396"> and is therefore a strong enabler </st><span class="No-Break"><st c="32431">of self-service:</st></span></p>
			<pre class="source-code"><st c="32447">
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ultra-fast
  annotations:
    storageclass.kubernetes.io/is-default-class: "false"
provisioner: csi-driver.example-vendor.example
reclaimPolicy: Retain # default value is Delete
allowVolumeExpansion: true
mountOptions:
  - discard
volumeBindingMode: WaitForFirstConsumer
parameters:
  guaranteedReadWriteLatency: "true" # provider-specific</st></pre>
			<p><st c="32842">The downside of this is that you have to consider human error, which can take down your platf</st><a id="_idTextAnchor222"/><st c="32936">orm by filling up the storage until the </st><span class="No-Break"><st c="32977">system freezes.</st></span></p>
			<h3><st c="32992">CSI challenges</st></h3>
			<p><st c="33007">The definition of a CSI is </st><a id="_idIndexMarker373"/><st c="33035">a fairly new approach, but the underlying technologies of storage management and software-defined storage are way older than Kubernetes. </st><st c="33172">We can also discover this in many CSI drivers, which are nothing but a shim wrapping legacy code. </st><st c="33270">For less flexible and scalable clusters, this might not be a problem, but in environments where you have a lot of action going on, you don’t want to have a CSI in your system that becomes the bottleneck. </st><st c="33474">Some CSIs even have restrictions and limitations to prevent their failure at scale. </st><st c="33558">To be fair, we usually see this with on-premises installations and some old storage technologies. </st><st c="33656">With those, we can add the </st><strong class="bold"><st c="33683">Logical Unit Number</st></strong><st c="33702"> (</st><strong class="bold"><st c="33704">LUN</st></strong><st c="33707">) presentation and </st><a id="_idIndexMarker374"/><st c="33727">connection limits on top of the things to consider. </st><st c="33779">The LUN is for the Pods to make requests from storage space and retrieve data. </st><st c="33858">There are limits on how many connections a physical server can have to the storage. </st><st c="33942">Again, this is important when you manage your own storage </st><span class="No-Break"><st c="34000">and SANs.</st></span></p>
			<p><st c="34009">Why are some CSIs so poor? </st><st c="34037">The CSI does more than provide storage capacity. </st><st c="34086">It communicates with the storage provider, promises the availability of the demanded capacity, and waits until the RAID controller, backup, and snapshot mechanisms are ready. </st><st c="34261">In large-scale storage systems, we get even more: we can find storage capacity allocation and </st><span class="No-Break"><st c="34355">optimization procedures.</st></span></p>
			<p><st c="34379">To overcome such issues, you need to evaluate the storage, especially the storage drivers, of their cloud-native storage capabilities. </st><st c="34515">This will require large-scale performance tests and a ridiculous number of created volumes. </st><st c="34607">The CSI driver shouldn’t do any cut-downs or performance decrease and the created volume should be in a </st><span class="No-Break"><st c="34711">millisecond area.</st></span></p>
			<p><st c="34728">Furthermore, ensure that the </st><a id="_idIndexMarker375"/><st c="34758">driver allows cross-storage and cross-cloud/infrastructure migration; provides synchronous and asynchronous replication between those different infrastructures; provides feature parity across sites; and supports local storage types if needed, such as for </st><span class="No-Break"><st c="35013">edge scenarios.</st></span></p>
			<p><st c="35028">A good CSI will enable your platform to </st><a id="_idTextAnchor223"/><st c="35069">operate anywhere and to support a wide range of </st><span class="No-Break"><st c="35117">use cases.</st></span></p>
			<h3><st c="35127">Container network interface</st></h3>
			<p><st c="35155">The </st><strong class="bold"><st c="35160">Container Network Interface</st></strong><st c="35187"> (</st><strong class="bold"><st c="35189">CNI</st></strong><st c="35192">) can </st><a id="_idIndexMarker376"/><st c="35199">become a platform’s most relevant component, but it is also its most underrated one. </st><st c="35284">For many projects we have seen, some platform teams don’t care what they use as CNI, nor do they heavily utilize network policies, encryption, or fine-grained network configurations. </st><st c="35467">Thanks to its simple abstraction of the network layer, it’s not overwhelming when getting started. </st><st c="35566">Yet, on the other hand, there are many use cases where the most crucial component is the CNI. </st><st c="35660">I have even seen projects fail because of the dynamic nature of a CNI that didn’t play along with the very traditional and legacy approach of </st><span class="No-Break"><st c="35802">implementing networks.</st></span></p>
			<p><st c="35824">A CNI always requires a dedicated implementation because it is a set of specifications and libraries for writing plugins to configure network interfaces. </st><st c="35979">The CNI concerns itself only with the network connectivity of Linux containers and removing allocated resources when the container is deleted. </st><st c="36122">Due to this focus, CNIs have a wide range of support, and the specifications are simple </st><span class="No-Break"><st c="36210">to implement.</st></span></p>
			<p><st c="36223">Therefore, we architects should never treat the CNI as “</st><em class="italic"><st c="36280">just another object in Kubernetes</st></em><st c="36314">.” We have to evaluate and introduce it. </st><st c="36355">Some CNIs are made for easy maintenance and a solid but simple set of features. </st><st c="36435">For example, if the primary focus is on layer 3, consider Flannel. </st><st c="36502">Other CNIs, such as Calico, are a rock-solid choice with a rich feature set; Cilium introduced the usage of eBPF to provide even faster and more secure networking. </st><st c="36666">If it’s still difficult to choose between those options because you may have additional requirements such as providing different levels of networks, then the community still has an answer for you: Multus. </st><st c="36871">Take your time and discover your options. </st><st c="36913">There are dozens </st><span class="No-Break"><st c="36930">of CNIs.</st></span></p>
			<p><st c="36938">The CNI can have serious effects on </st><span class="No-Break"><st c="36975">your platforms:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="36990">Security</st></strong><st c="36999">: CNIs can provide different capabilities for network policies to achieve fine-grained control over your network. </st><st c="37114">They can have additional encryption features, integrations into identity and access management systems, and </st><span class="No-Break"><st c="37222">detailed observability.</st></span></li>
				<li><strong class="bold"><st c="37245">Scalability</st></strong><st c="37257">: The larger the cluster gets, the more communication happens throughout the network. </st><st c="37344">The CNI must support your growth target and stay efficient even with complex routing and chatting across </st><span class="No-Break"><st c="37449">the wires.</st></span></li>
				<li><strong class="bold"><st c="37459">Performance</st></strong><st c="37471">: How fast and direct can Pod-to-Pod communication be? </st><st c="37527">How much complexity does the CNI introduce? </st><st c="37571">How efficiently can it handle communication? </st><st c="37616">Can it deal with many complex </st><span class="No-Break"><st c="37646">network policies?</st></span></li>
				<li><strong class="bold"><st c="37663">Operability</st></strong><st c="37675">: High-level CNIs are not very inversive and simple to maintain. </st><st c="37741">Powerful CNIs can, in theory, be replaced as long as they adhere to the CNI specification, but each comes with its own set of features, which are often </st><span class="No-Break"><st c="37893">not replaceable.</st></span></li>
			</ul>
			<p><st c="37909">Be aware that not every </st><a id="_idIndexMarker377"/><st c="37934">CNI supports all of those features. </st><st c="37970">Some, for example, do not even provide network policy support. </st><st c="38033">Other CNIs are cloud-provider-specific integrating with just one cloud provider </st><a id="_idTextAnchor224"/><st c="38113">and they do enable some cloud </st><span class="No-Break"><st c="38143">provider-centric capabilities.</st></span></p>
			<h3><st c="38173">Architectural challenges – CNI chaining and multiple CNIs</st></h3>
			<p><st c="38231">Platforms are predestined for CNI chaining. </st><st c="38276">CNI chaining</st><a id="_idIndexMarker378"/><st c="38288"> introduces the sequential usage of</st><a id="_idIndexMarker379"/><st c="38323"> multiple CNIs. </st><st c="38339">The order in which CNIs are taken and for what purpose is defined in the </st><strong class="source-inline"><st c="38412">/etc/cni/net.d</st></strong><st c="38426"> directory and handled by the kubelet. </st><st c="38465">This allows the platform team to handle one part of the network and provide a guarded approach, while platform users can freely configure their network at a higher level. </st><st c="38636">For example, a platform user can access Antrea as a CNI to configure their networking to some extent. </st><st c="38738">They can also apply network policies and egress configurations to prevent their application from chatting with everyone. </st><st c="38859">On the other side of the platform, the platform engineering team will manage, via Cilium, the global cross-cluster communication, as well as the network encryption, to enforce security best practices. </st><st c="39060">In addition, the networking data made visible by Cilium is made available to the operations and security teams. </st><st c="39172">Where those use cases are most suitable is in the interaction with the cloud providers’ own CNIs. </st><st c="39270">They often enable better integration between the platform and the cloud but lack many advanced features on the </st><span class="No-Break"><st c="39381">other side.</st></span></p>
			<p><st c="39392">Another approach to be evaluated would be to assign a Pod multiple network interfaces via Multus or CNI-Genie. </st><st c="39504">Normally, a Pod has just one interface, but with Multus, for example, this could be multiple network interfaces. </st><st c="39617">When does this become relevant? </st><st c="39649">The following instances are examples when it </st><span class="No-Break"><st c="39694">becomes relevant:</st></span></p>
			<ul>
				<li><st c="39711">Separating control and operational data from </st><span class="No-Break"><st c="39757">application data</st></span></li>
				<li><st c="39773">Providing flexible network options for an extremely </st><span class="No-Break"><st c="39826">heterogeneous workload</st></span></li>
				<li><st c="39848">Taking multi-tenancy to another level by assigning completely different networks for </st><span class="No-Break"><st c="39934">each tenant</st></span></li>
				<li><st c="39945">Supporting unusual network protocols and connections, such as in edge scenarios or </st><span class="No-Break"><st c="40029">telco environments</st></span></li>
				<li><strong class="bold"><st c="40047">Network Function Virtualization</st></strong><st c="40079"> (</st><strong class="bold"><st c="40081">NFV</st></strong><st c="40084">), which</st><a id="_idIndexMarker380"/><st c="40093"> requires multiple networks due to </st><span class="No-Break"><st c="40128">its complexity</st></span></li>
			</ul>
			<p><st c="40142">The Multus CNI</st><a id="_idIndexMarker381"/><st c="40157"> is a kind of meta-plugin on the node and sits between the actual CNIs and the Pod network interfaces, as shown in the following illustration. </st><st c="40300">It attaches the different network interfaces to the Pod on one side and handles the connection to the anticipated CNIs for the right network interface on the </st><span class="No-Break"><st c="40458">other side.</st></span></p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/Figure_4.05_B31164.jpg" alt="Figure 4.5: Multus meta-CNI plugin"/><st c="40469"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="40544">Figure 4.5: Multus meta-CNI plugin</st></p>
			<p><st c="40578">Both approaches must be evaluated well. </st><st c="40619">They have a significan</st><a id="_idTextAnchor225"/><st c="40641">t impact on your network complexity, performance, </st><span class="No-Break"><st c="40692">and security.</st></span></p>
			<h3><st c="40705">Providing different CPU architectures</st></h3>
			<p><st c="40743">Kubernetes supports </st><a id="_idIndexMarker382"/><st c="40764">multiple CPU architectures: AMD64, ARM64, 386, ARM, ppc64le, and even mainframes with s390x. </st><st c="40857">Many clusters today run on AMD64, but at the time of writing, a strong interest in ARM64 is causing a shift. </st><st c="40966">This discussion is primarily about saving costs and gaining a little bit of extra performance while reducing the total power consumption. </st><st c="41104">At least on paper, it is a win-win-win situation. </st><st c="41154">Not only is the ARM64 a possible change in the infrastructure, the open source architecture project RISC-V is gaining speed and is the first cloud provider to create RISC-V </st><span class="No-Break"><st c="41327">offerings </st></span><span class="No-Break"><em class="italic"><st c="41337">[4]</st></em></span><span class="No-Break"><st c="41340">.</st></span></p>
			<p><st c="41341">As platform engineers, we can enable those migrations and changes. </st><st c="41409">A Kubernetes cluster can run multiple architectures simultaneously—not on the same node but with different groups of nodes. </st><st c="41533">Remember that this change also requires an adjustment in the container build. </st><st c="41611">With some software components, you can do a multi-architecture build; with others, it might require adjustments before the container for a different architecture can </st><span class="No-Break"><st c="41777">be created.</st></span></p>
			<p><st c="41788">To select the architecture on which a Deployment should be delivered, you just need to add a </st><strong class="source-inline"><st c="41882">nodeSelector</st></strong><st c="41894"> like this to the </st><strong class="source-inline"><st c="41912">Spec</st></strong><st c="41916"> section of the Deployment </st><span class="No-Break"><st c="41943">YAML file:</st></span></p>
			<pre class="source-code"><st c="41953">
nodeSelector:
        kubernetes.io/arch: arm64</st></pre>
			<p><st c="41993">An upcoming alternative to provide different container image</st><a id="_idTextAnchor226"/><st c="42054">s is to compile the software as a </st><strong class="bold"><st c="42089">WebAssembly</st></strong><st c="42100"> (</st><span class="No-Break"><strong class="bold"><st c="42102">Wasm</st></strong></span><span class="No-Break"><st c="42106">) container.</st></span></p>
			<h3><st c="42119">Wasm runtime</st></h3>
			<p><st c="42132">The usage of Wasm as an </st><a id="_idIndexMarker383"/><st c="42157">alternative container format has increased drastically in the last year. </st><st c="42230">Wasm is a binary instruction format for a stack-based virtual machine. </st><st c="42301">Think of it as an intermediate layer between various programming languages and many different execution environments. </st><st c="42419">You can take code written in over 30 different languages, compile it into a </st><strong class="source-inline"><st c="42495">*.wasm</st></strong><st c="42501"> file, and then execute that file on any </st><span class="No-Break"><st c="42542">Wasm runtime.</st></span></p>
			<p><st c="42555">The name </st><em class="italic"><st c="42565">WebAssembly</st></em><st c="42576">, however, is misleading. </st><st c="42602">Initially designed to make code run quickly on the web, today, it can run anywhere. </st><st c="42686">Why should we use it? </st><st c="42708">Wasm is secure and sandboxed by default. </st><st c="42749">In a Wasm container, there is no operating system or anything else except the binary compiled code. </st><st c="42849">This means there is nothing to break into or claim the context or service account from. </st><st c="42937">Wasm also has an incredibly fast startup time where the limits are set by the runtime rather than the module. </st><st c="43047">Some runtimes claim to be as fast as around 50 ms. </st><st c="43098">In comparison, your brain requires &gt;110 ms to recognize whether something passes in front of your eyes. </st><st c="43202">Furthermore, a Wasm container size is around 0.5 MB – 1.5 MB, whereas a very slim container can be around 5 MB. </st><st c="43314">However, what we can see on the market is usually image sizes in the range of 300 MB – 600 MB, 1 GB – 3 GB, or even sometimes above 10 GB. </st><st c="43453">With this reduced image size, a Wasm container also has a drastically reduced storage footprint. </st><st c="43550">Wasm is also hardware-independent. </st><st c="43585">The exact same image can run anywhere, as long as you have a Wasm </st><span class="No-Break"><st c="43651">runtime available.</st></span></p>
			<p><st c="43669">In the context of</st><a id="_idIndexMarker384"/><st c="43687"> Kubernetes, the OCI and CRI runtimes support Wasm. </st><st c="43739">This means you can run a Wasm container alongside a regular container. </st><st c="43810">As you can see in the following figure, no further changes are required. </st><st c="43883">The Wasm app image is stored at the node level and executed by the </st><span class="No-Break"><st c="43950">layers above.</st></span></p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/Figure_4.06_B31164.jpg" alt="Figure 4.6: Wasm on a Kubernetes node"/><st c="43963"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="44087">Figure 4.6: Wasm on a Kubernetes node</st></p>
			<p><st c="44124">To make </st><a id="_idIndexMarker385"/><st c="44133">Wasm executable and available in your platform, you have to specify a runtime class and define its usage at the Deployment/Pod level. </st><st c="44267">In the following example, you can see the specification for </st><strong class="source-inline"><st c="44327">crun</st></strong><st c="44331"> as </st><strong class="source-inline"><st c="44335">RuntimeClass</st></strong><st c="44347"> on the left side, and a </st><strong class="source-inline"><st c="44372">Pod</st></strong><st c="44375"> definition where, for </st><strong class="source-inline"><st c="44398">spec.runtimeClassName</st></strong><st c="44419">, we assign </st><strong class="source-inline"><st c="44431">crun</st></strong><st c="44435"> on the right side. </st><st c="44455">For </st><strong class="source-inline"><st c="44459">crun</st></strong><st c="44463">, we also have to add an annotation to inform </st><strong class="source-inline"><st c="44509">crun</st></strong><st c="44513"> that this </st><strong class="source-inline"><st c="44524">Pod</st></strong><st c="44527"> has a </st><span class="No-Break"><st c="44534">Wasm image:</st></span></p>
			<table id="table001-3" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline"><st c="44545">apiVersion: node.k8s.io/v1</st></strong></span></p>
							<p><span class="No-Break"><strong class="source-inline"><st c="44572">kind: RuntimeClass</st></strong></span></p>
							<p><span class="No-Break"><strong class="source-inline"><st c="44591">metadata:</st></strong></span></p>
							<p><strong class="source-inline">  </strong><span class="No-Break"><strong class="source-inline"><st c="44601">name: crun</st></strong></span></p>
							<p><span class="No-Break"><strong class="source-inline"><st c="44612">scheduling:</st></strong></span></p>
							<p><strong class="source-inline">  </strong><span class="No-Break"><strong class="source-inline"><st c="44624">nodeSelector:</st></strong></span></p>
							<p><strong class="source-inline">    </strong><span class="No-Break"><strong class="source-inline"><st c="44638">runtime: crun</st></strong></span></p>
							<p><span class="No-Break"><strong class="source-inline"><st c="44652">handler: crun</st></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline"><st c="44666">apiVersion: v1</st></strong></span></p>
							<p><span class="No-Break"><strong class="source-inline"><st c="44681">kind: Pod</st></strong></span></p>
							<p><span class="No-Break"><strong class="source-inline"><st c="44691">metadata:</st></strong></span></p>
							<p><strong class="source-inline"/><span class="No-Break"><strong class="source-inline"><st c="44701">name: wasm-demo-app</st></strong></span></p>
							<p><strong class="source-inline"/><span class="No-Break"><strong class="source-inline"><st c="44721">annotations:</st></strong></span></p>
							<p><strong class="source-inline"/><span class="No-Break"><strong class="source-inline"><st c="44734">module.wasm.image/variant: compat</st></strong></span></p>
							<p><span class="No-Break"><strong class="source-inline"><st c="44768">spec:</st></strong></span></p>
							<p><strong class="source-inline"/><span class="No-Break"><strong class="source-inline"><st c="44774">runtimeClassName: crun</st></strong></span></p>
							<p><strong class="source-inline"/><span class="No-Break"><strong class="source-inline"><st c="44797">containers:</st></strong></span></p>
							<p><strong class="source-inline"/><span class="No-Break"><strong class="source-inline"><st c="44809">name: wasm-demo-app</st></strong></span></p>
							<p><strong class="source-inline"/><span class="No-Break"><strong class="source-inline"><st c="44829">image:</st></strong></span></p>
							<p><strong class="source-inline"/><span class="No-Break"><strong class="source-inline"><st c="44836">docker.io/cr7258/wasm-demo-app:v1</st></strong></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="44870">Table 4.1: Definition of a RuntimeClass and a Pod that will be executed in the Wasm runtime</st></p>
			<p><st c="44962">As an alternative to that, new developments such as SpinKube</st><a id="_idIndexMarker386"/><st c="45023"> come with a whole set of tools to utilize Kubernetes resources in the best manner </st><em class="italic"><st c="45106">[5]</st></em><st c="45109">. That approach allows the integration of the development experience into the deployment and the execution of the Wasm containerized app. </st><st c="45247">The following image shows the workflow and how the different components work together. </st><st c="45334">It makes the development process straightforward and brings a fast but robust new runtime environment to </st><span class="No-Break"><st c="45439">the platform.</st></span></p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/Figure_4.07_B31164.jpg" alt="Figure 4.7: SpinKube overview [6]"/><st c="45452"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="46231">Figure 4.7: SpinKube overview [6]</st></p>
			<p><st c="46264">However, is Wasm</st><a id="_idIndexMarker387"/><st c="46281"> really a technology that the industry has adopted? </st><st c="46333">Here are just some examples and use cases showing </st><span class="No-Break"><st c="46383">its adoption:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="46396">Interoperability</st></strong><st c="46413">: Figma runs as Wasm on </st><span class="No-Break"><st c="46438">any computer</st></span></li>
				<li><strong class="bold"><st c="46450">Plugin system</st></strong><st c="46464">: You can write extensions in Wasm </st><span class="No-Break"><st c="46500">for Envoy</st></span></li>
				<li><strong class="bold"><st c="46509">Sandboxing</st></strong><st c="46520">: A Firefox or Chrome browser can run Wasm in a sandboxed environment to protect </st><span class="No-Break"><st c="46602">your system</st></span></li>
				<li><strong class="bold"><st c="46613">Blockchain</st></strong><st c="46624">: CosmWasm or the ICP uses versions of Wasm to </st><span class="No-Break"><st c="46672">run applications</st></span></li>
				<li><strong class="bold"><st c="46688">Container</st></strong><st c="46698">: WasmCloud has a different concept to execute containers, or SpinKube, a Wasm runtime with a CLI for a simple </st><span class="No-Break"><st c="46810">development process</st></span></li>
				<li><strong class="bold"><st c="46829">Serverless platforms</st></strong><st c="46850">: Cloudflare Workers or Fermyon Cloud run your </st><span class="No-Break"><st c="46898">Wasmized app</st></span></li>
			</ul>
			<p><st c="46910">Wasm is not a container replacement yet. </st><st c="46952">It is an evolutionary step that suits a few cases very well, but it lacks adoption and has issues in the debugging process. </st><st c="47076">However, it is a matter of time before these obstacles </st><span class="No-Break"><st c="47131">are solved.</st></span><a id="_idTextAnchor227"/></p>
			<h3><st c="47142">Enable platforms for GPU utilization</st></h3>
			<p><st c="47179">Similar to the support for different CPU architectures or the extension of the container runtime to include Wasm, GPU enablement</st><a id="_idIndexMarker388"/><st c="47308"> for users has become extremely relevant lately. </st><st c="47357">A device plugin must be installed that is specific to the type of GPU, such as AMD, Intel, or Nvidia. </st><st c="47459">This exposes custom schedulable resources such as </st><strong class="source-inline"><st c="47509">nvidia.com/gpu</st></strong><st c="47523"> to Kubernetes and its users. </st><st c="47553">Also, we are not experts in GPUs and their capabilities; from a platform engineering perspective, the different providers have developed diverse feature sets and extensions for </st><span class="No-Break"><st c="47730">their plugins.</st></span></p>
			<p><st c="47744">I highly recommend developing or finding experts for this field if your user requires GPUs. </st><st c="47837">The field of AI and LLMs is undergoing rapid expansion. </st><st c="47893">Hardware and software providers come up with new tools, systems, and approaches monthly to take advantage of those technologies. </st><st c="48022">Any scenario has its own very specific demands. </st><st c="48070">Training a model requires a tremendous amount of data, GPUs, storage, and memory. </st><st c="48152">Fine-tuning a model comes primarily down to how large a model you want to train. </st><st c="48233">A seven-billion-parameter model can fit into a 14 GB VRAM, but increasing the precision can increase its size easily to 24 GB or more. </st><st c="48368">Lastly, providing an inference engine to serve an LLM for users requires a lot of </st><span class="No-Break"><st c="48450">network communication.</st></span></p>
			<p class="callout-heading"><st c="48472">Important note</st></p>
			<p class="callout"><st c="48487">Inferencing means sending a prompt to an LLM. </st><st c="48534">Most people believe that the LLM then creates the story like a human would. </st><st c="48610">However, what is really happening is that after every word, the LLM has to send the whole prompt, including the new words, to the LLM again, so it can decide on the next word added to </st><span class="No-Break"><st c="48794">the sentence.</st></span></p>
			<p><st c="48807">As a rule of thumb, for the GPU VRAMs (Video RAM is the GPU’s memory) needed, you double the model’s size. </st><st c="48915">Here are some </st><span class="No-Break"><st c="48929">more examples:</st></span></p>
			<ul>
				<li><st c="48943">Llama-2-70b requires 2 * 70 GB = 140 </st><span class="No-Break"><st c="48981">GB VRAM</st></span></li>
				<li><st c="48988">Falcon-40b requires 2 * 40 GB = 80 </st><span class="No-Break"><st c="49024">GB VRAM</st></span></li>
				<li><st c="49031">MPT-30b requires 2 * 30 GB = 60 </st><span class="No-Break"><st c="49064">GB VRAM</st></span></li>
				<li><st c="49071">bigcode/starcoder requires 2 * 15.5 = 31 </st><span class="No-Break"><st c="49113">GB VRAM</st></span></li>
			</ul>
			<p><st c="49120">Therefore, collaboration between the platform and the machine learning team is required. </st><st c="49210">Depending on the models they want to use, your chosen hardware may no longer fit </st><span class="No-Break"><st c="49291">the requirements</st><a id="_idTextAnchor228"/><st c="49307">.</st></span></p>
			<h3><st c="49308">Architectural challenges</st></h3>
			<p><st c="49333">While the integration of GPUs is</st><a id="_idIndexMarker389"/><st c="49366"> very straightforward, they have a few elements that need to be discussed; you should be aware of </st><span class="No-Break"><st c="49464">the following:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="49478">GPU costs</st></strong><st c="49488">: Cheap GPUs, with low VRAM and computational power, might not fit your use cases. </st><st c="49572">Also, you might not use a GPU 24/7, but you should think about more dynamic and flexible possibilities. </st><st c="49676">However, owning GPUs can be cheaper in the long run as compared to their regular CPU counterparts if your use case requires GPU </st><span class="No-Break"><st c="49804">computational power.</st></span></li>
				<li><strong class="bold"><st c="49824">Privileged rights</st></strong><st c="49842">: Many machine learning tools require further customization and tweaking, especially for the rights </st><span class="No-Break"><st c="49943">they demand.</st></span></li>
				<li><strong class="bold"><st c="49955">End user requirements</st></strong><st c="49977">: Besides the model sizes, what the data scientists and machine learning engineers want to do with the model depends very much on the actual use case they want to implement. </st><st c="50152">Any minor change in the approach can make the platform unusable. </st><st c="50217">This must be considered in the architecture for such a platform and to provide the most resources and greatest scaling </st><span class="No-Break"><st c="50336">flexibility possible.</st></span></li>
				<li><strong class="bold"><st c="50357">External models pulled to the cluster</st></strong><st c="50395">: As with containers, it is a common practice to pull models from pages such as HuggingFace. </st><st c="50489">You might consider this in the network of a platform supporting machine </st><span class="No-Break"><st c="50561">learning activities.</st></span></li>
			</ul>
			<p><st c="50581">Creating platforms that suit machine learning and LLM operations requires a new level of optimization. </st><st c="50685">Non-running GPUs are a waste of money. </st><st c="50724">Poorly used GPUs are a waste of money. </st><st c="50763">However, there is more we have to ensure from an infrastructure perspective: data protection, platform security, and specialized observability for the models. </st><st c="50922">In my opinion, machine learning and LLMs are an exciting use case and offer a playground for </st><span class="No-Break"><st c="51015">platform engineer</st><a id="_idTextAnchor229"/><st c="51032">s.</st></span></p>
			<h3><st c="51035">Solution space</st></h3>
			<p><st c="51050">To optimize </st><a id="_idIndexMarker390"/><st c="51063">GPU usage, there are some </st><span class="No-Break"><st c="51089">approaches available:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="51110">Multi-Process </st></strong><span class="No-Break"><strong class="bold"><st c="51125">Server</st></strong></span><span class="No-Break"><st c="51131"> (</st></span><span class="No-Break"><strong class="bold"><st c="51133">MPS</st></strong></span><span class="No-Break"><st c="51136">)</st></span></li>
				<li><strong class="bold"><st c="51138">Multi-Instance </st></strong><span class="No-Break"><strong class="bold"><st c="51153">GPU</st></strong></span><span class="No-Break"><st c="51156"> (</st></span><span class="No-Break"><strong class="bold"><st c="51158">MIG</st></strong></span><span class="No-Break"><st c="51161">)</st></span></li>
				<li><span class="No-Break"><strong class="bold"><st c="51163">Time-slicing/sharing</st></strong></span></li>
			</ul>
			<p><st c="51183">Depending on the </st><a id="_idIndexMarker391"/><st c="51201">GPU driver, you will find a full range of possibilities, as you do with Nvidia. </st><st c="51281">Other GPU drivers might not be mature enough or feature-rich yet, but of </st><a id="_idIndexMarker392"/><st c="51354">course, you should evaluate </st><span class="No-Break"><st c="51382">this frequently.</st></span></p>
			<p><st c="51398">Time-slicing is the </st><a id="_idIndexMarker393"/><st c="51419">worst option to take. </st><st c="51441">Although it is better than nothing, MPS would be at least twice as efficient as the time-slice approach. </st><st c="51546">However, MPS has one major drawback: processes are not strictly isolated, which leads to correlated failures between the slices. </st><st c="51675">This is where MIG comes into the picture. </st><st c="51717">It provides good process isolation and a static partitioning of the GPU. </st><st c="51790">Static on a super dynamic, scalable, and anytime adjustable Kubernetes cluster? </st><st c="51870">Yes, because machine learning training will not run just for a few seconds or </st><span class="No-Break"><st c="51948">minutes </st></span><span class="No-Break"><em class="italic"><st c="51956">[7]</st></em></span><span class="No-Break"><st c="51959">.</st></span></p>
			<p><st c="51960">The following figure shows the GPU’s memory partitions at a high level, to which different workloads can </st><span class="No-Break"><st c="52066">be assigned.</st></span></p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/Figure_4.08_B31164.jpg" alt="Figure 4.8: Example of splitting a GPU into three GPU instances"/><st c="52078"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="52479">Figure 4.8: Example of splitting a GPU into three GPU instances</st></p>
			<p><st c="52542">Those GPU</st><a id="_idIndexMarker394"/><st c="52552"> instances can be used either by a single pod, a pod with one container running multiple processes (not ideal), or by using something such as CUDA from Nvidia. </st><st c="52712">CUDA</st><a id="_idIndexMarker395"/><st c="52716"> is an MPS, so you can combine the different approaches, as shown in the </st><span class="No-Break"><st c="52789">following diagram:</st></span></p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/Figure_4.09_B31164.jpg" alt="Figure 4.9: Example of using three GPU instances in parallel"/><st c="52807"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="53128">Figure 4.9: Example of using three GPU instances in parallel</st></p>
			<p><st c="53188">From here, we can take a look at the research and experiment corner where you can combine </st><a id="_idIndexMarker396"/><st c="53279">Kubernetes </st><strong class="bold"><st c="53290">Dynamic Resource Allocation</st></strong><st c="53317"> (</st><strong class="bold"><st c="53319">DRA</st></strong><st c="53322">) with MIG. </st><st c="53335">It provides an approach to being flexible in the assignment while ensuring the resources for the deployments. </st><st c="53445">The DRA was introduced with Kubernetes </st><strong class="source-inline"><st c="53484">v1.26</st></strong><st c="53489"> and is still in Alpha, therefore breaking API changes are likely with every release. </st><st c="53575">There are some interesting articles and talks about it. </st><st c="53631">Depending on when you are reading this, it might be out of </st><span class="No-Break"><st c="53690">dat</st><a id="_idTextAnchor230"/><st c="53693">e </st></span><span class="No-Break"><em class="italic"><st c="53696">[8]</st></em></span><span class="No-Break"><st c="53699">.</st></span></p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor231"/><st c="53700">Enable cluster scalability</st></h2>
			<p><st c="53727">As mentioned earlier in this</st><a id="_idIndexMarker397"/><st c="53756"> chapter, the ability of a Kubernetes cluster to adjust its scale is beneficial for different demands, from resiliency to growing with the workload to fallback re-initiation, to providing the highest availability across data centers and availability zones. </st><st c="54013">At its core, we differentiate between the horizontal and vertical autoscaler, which targets the Pods, and the horizontal CA, which adjusts the number </st><span class="No-Break"><st c="54163">of nodes.</st></span></p>
			<p><st c="54172">As with many capabilities, Kubernetes provides the specification but expects someone else to implement it. </st><st c="54280">This at least applies to the VPA and the CA, which require at least a metrics server running at the </st><span class="No-Break"><st c="54380">cluster level.</st></span></p>
			<p><st c="54394">However, the HPA</st><a id="_idIndexMarker398"/><st c="54411"> is feature-rich and allows metric-based scaling. </st><st c="54461">Look at the following example of an HPA. </st><st c="54502">With </st><strong class="source-inline"><st c="54507">stabilizationWindowsSeconds</st></strong><st c="54534">, we can also tell Kubernetes to wait on previous actions to prevent flapping. </st><st c="54613">Flapping is</st><a id="_idIndexMarker399"/><st c="54624"> defined as follows according to the </st><span class="No-Break"><st c="54661">Kubernetes documentation:</st></span></p>
			<p class="author-quote"><st c="54686">When managing the scale of a group of replicas using the HorizontalPodAutoscaler, it is possible that the number of replicas keeps fluctuating frequently due to the dynamic nature of the metrics evaluated. </st><st c="54893">This is sometimes referred to as thrashing, or flapping. </st><st c="54950">It’s similar to the concept of hysteresis in cybernetics.</st></p>
			<p><st c="55007">We can take the following configuration for an HPA. </st><st c="55060">It looks simple; you can see that based on the policies, the behavior can change drastically. </st><st c="55154">For example, when reducing 10% of the Pods while having a large deployment with hundreds of replicas, we want to be very careful. </st><st c="55284">The shown scaling-down configuration will prevent deleting more than two Pods at the </st><span class="No-Break"><st c="55369">same time:</st></span></p>
			<pre class="source-code"><st c="55379">
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
spec:
  maxReplicas: 7        #maximum replicas to scale up to
  minReplicas: 3        #expected minimum replicas
scaleTargetRef: ...
  </st><st c="55553">targetCPUUtilizationPercentage: 75     #metric to react on
  behavior:
    scaleDown:
     stabilizationWindowSeconds: 300     #wait 300 sec
     policies:
     - type: Percent
       value: 10                #reducing 10% of Pods
       periodSeconds: 60        #per minute
     - type: Pods
       value: 5                 #reducing not more
       periodSeconds: 60        #than 2 Pods per min
     selectPolicy: Min
   scaleUp:
     stabilizationWindowSeconds: 0
     policies:
     - type: Percent
       value: 75                #increase the Pod by 75%
       periodSeconds: 15        #every 15 seconds
     selectP</st><a id="_idTextAnchor232"/><st c="56004">olicy: Max</st></pre>
			<h3><st c="56015">Issues with VPA, HPA, and CA</st></h3>
			<p><st c="56044">There are some constraints that you have to consider for the </st><span class="No-Break"><st c="56106">autoscaler family:</st></span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold"><st c="56124">HPA</st></strong></span><span class="No-Break"><st c="56128">:</st></span><ul><li><st c="56130">You </st><a id="_idIndexMarker400"/><st c="56134">have to set CPU and memory limits and requests on Pods correctly to prevent resource waste or frequently </st><span class="No-Break"><st c="56239">terminated Pods.</st></span></li><li><st c="56255">When HPA hits the limit of the available nodes, it can’t schedule more Pods. </st><st c="56333">However, it might utilize all available resources, which could lead </st><span class="No-Break"><st c="56401">to issues.</st></span></li></ul></li>
				<li><span class="No-Break"><strong class="bold"><st c="56411">VPA</st></strong></span><span class="No-Break"><st c="56415">:</st></span><ul><li><st c="56417">VPA and </st><a id="_idIndexMarker401"/><st c="56425">HPA shouldn’t be used for scaling based on the same metric. </st><st c="56485">For example, while both can use CPU utilization to trigger a scale-up, HPA deploys more Pods, while VPA increases the CPU limits on existing Pods, which can lead to excessive scaling based on the same metric. </st><st c="56694">Therefore, if using both VPA and HPA, one should use CPU for one and, for instance, memory for </st><span class="No-Break"><st c="56789">the other.</st></span></li><li><st c="56799">VPA might recommend using more resources than available within the cluster or the node. </st><st c="56888">This causes the Pod to </st><span class="No-Break"><st c="56911">become unschedulable.</st></span></li></ul></li>
				<li><span class="No-Break"><strong class="bold"><st c="56932">CA</st></strong></span><span class="No-Break"><st c="56935">:</st></span><ul><li><st c="56937">The </st><a id="_idIndexMarker402"/><st c="56941">CA scales are based on the requests and limits of the Pods. </st><st c="57001">This can cause a lot of unused resources, poor utilization, and </st><span class="No-Break"><st c="57065">high costs.</st></span></li><li><st c="57076">When a CA triggers a scaling command to the cloud provider, this might take minutes to provide new nodes for the cluster. </st><st c="57199">During this time, the application performance is degraded. </st><st c="57258">In the worst case, it </st><span class="No-Break"><st c="57280">become</st><a id="_idTextAnchor233"/><st c="57286">s unservable.</st></span></li></ul></li>
			</ul>
			<h3><st c="57300">Solution space</st></h3>
			<p><st c="57315">As platform engineers, we want to ensure that the user can define scaling behavior without putting the platform at risk. </st><st c="57437">Utilizing HPA, VPA, and CA requires perfect configuration and control, guardrails provided by a policy engine, and close monitoring. </st><st c="57570">It becomes mission-critical to control cluster scaling and descaling while enabling in-namespace autoscaling for </st><span class="No-Break"><st c="57683">your users.</st></span></p>
			<p><st c="57694">Managing scaling your Kubernetes cluster on cloud providers requires you to look into the CA and the different available cloud integrations for it. </st><st c="57843">Besides, if you use the </st><strong class="bold"><st c="57867">Cluster API</st></strong><st c="57878"> (</st><strong class="bold"><st c="57880">CAPI</st></strong><st c="57884">), you </st><a id="_idIndexMarker403"/><st c="57892">can also build on its capability for </st><span class="No-Break"><st c="57929">cluster</st><a id="_idTextAnchor234"/><st c="57936"> autoscaling.</st></span></p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor235"/><st c="57949">Network capabilities and extensions</st></h2>
			<p><st c="57985">Now, let’s look at the final resource integration of Kubernetes and the underlying infrastructure. </st><st c="58085">To do this, we will start within the cluster networking mechanisms and work down to the DNS and load balancing. </st><st c="58197">DNS and load balancing can happen within the cluster and in coordination with the infrastructure that Kuberne</st><a id="_idTextAnchor236"/><st c="58306">tes </st><span class="No-Break"><st c="58311">runs on.</st></span></p>
			<h3><st c="58319">Ingress – the old way</st></h3>
			<p><st c="58341">Ingress</st><a id="_idIndexMarker404"/><st c="58349"> is the old definition of how an end user request from outside the cluster is routed into the system and toward the application that is exposed. </st><st c="58494">For almost a decade, it was the way to go to define incoming network traffic. </st><st c="58572">The ingress is usually represented by an ingress controller such as NGINX, HAProxy, or Envoy, to name a few. </st><st c="58681">Those inherently take the routing rules defined as a standard resource from Kubernetes and manage the rest of it. </st><st c="58795">As you can see in the following figure, from there on, the traffic is redirected to the right service, which forwards it to the Pod. </st><st c="58928">Physically, the traffic will go from the network interface to the ingress controller to the Pod, but as good an orchestrator as Kubernetes is, there are some logical steps </st><span class="No-Break"><st c="59100">in between.</st></span></p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/Figure_4.10_B31164.jpg" alt="Figure 4.10: Ingress controller (source: Kubernetes docs)"/><st c="59111"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="59192">Figure 4.10: Ingress controller (source: Kubernetes docs)</st></p>
			<p><st c="59249">While it scales and is robust for some </st><a id="_idIndexMarker405"/><st c="59289">of the largest deployments out there, it has </st><span class="No-Break"><st c="59334">some downsides:</st></span></p>
			<ul>
				<li><st c="59349">The Ingress API only supports TLS termination and simple content-based request routing of </st><span class="No-Break"><st c="59440">HTTP traffic</st></span></li>
				<li><st c="59452">It is limited in its available syntax and kept </st><span class="No-Break"><st c="59500">very simple</st></span></li>
				<li><st c="59511">It requires annotations </st><span class="No-Break"><st c="59536">for extensibility</st></span></li>
				<li><st c="59553">It reduces portability because every implementation has its own approach to </st><span class="No-Break"><st c="59630">do so</st></span></li>
			</ul>
			<p><st c="59635">Doing a multi-tenant cluster is more challenging because Ingress is usually at the cluster level and has a poor permission model. </st><st c="59766">Also, it supports namespaced configurations, but it is not suitable for multi-teams and shared </st><span class="No-Break"><st c="59861">load-balancing infrastructure.</st></span></p>
			<p><st c="59891">Some of the beauty of the Ingress approach is its wide support and integration with other tools, such as the cert-manager for certificate management and handling of the external DNS, which we will see soon. </st><st c="60099">Also, the Kubernetes maintainer claims that there is no plan to deprecate Ingress as it perfectly supports simple web traffic in an </st><span class="No-Break"><st c="60231">uncomp</st><a id="_idTextAnchor237"/><st c="60237">licated manner.</st></span></p>
			<h3><st c="60253">Gateway API – the new way</st></h3>
			<p><st c="60279">In the autumn of 2023, the Gateway API</st><a id="_idIndexMarker406"/><st c="60318"> became generally available. </st><st c="60347">Instead of a single resource, the gateway API consists of multiple resource types following a pattern that was already used in other </st><span class="No-Break"><st c="60480">critical integrations:</st></span></p>
			<ul>
				<li><strong class="source-inline"><st c="60502">Gateway</st></strong><st c="60510">: Cluster entry point for </st><span class="No-Break"><st c="60537">incoming traffic</st></span></li>
				<li><strong class="source-inline"><st c="60553">GatewayClass</st></strong><st c="60566">: Defines the gateway control type that will handle </st><span class="No-Break"><st c="60619">the gateway</st></span></li>
				<li><strong class="source-inline"><st c="60630">*Route</st></strong><st c="60637">: Implements the traffic routing from the gateway to </st><span class="No-Break"><st c="60691">the service:</st></span><ul><li><span class="No-Break"><strong class="source-inline"><st c="60703">HTTPRoute</st></strong></span></li><li><span class="No-Break"><strong class="source-inline"><st c="60713">GRPCRoute</st></strong></span></li><li><span class="No-Break"><strong class="source-inline"><st c="60723">TLSRoute</st></strong></span></li><li><span class="No-Break"><strong class="source-inline"><st c="60732">TCPRoute</st></strong></span></li><li><span class="No-Break"><strong class="source-inline"><st c="60741">UDPRoute</st></strong></span></li></ul></li>
			</ul>
			<p><st c="60750">Comparing the following diagram </st><a id="_idIndexMarker407"/><st c="60783">with the Ingress approach, we can see the two steps for incoming traffic going through the gateway and being redirected </st><span class="No-Break"><st c="60903">by </st></span><span class="No-Break"><strong class="source-inline"><st c="60906">*Route</st></strong></span><span class="No-Break"><st c="60912">.</st></span></p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/Figure_4.11_B31164.jpg" alt="Figure 4.11: Gateway API"/><st c="60913"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="60972">Figure 4.11: Gateway API</st></p>
			<p><st c="60996">How do </st><span class="No-Break"><st c="61004">they compare?</st></span></p>
			<table id="table002-2" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><st c="61017">Ingress</st></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><st c="61025">Gateway API</st></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="61037">Protocols</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="61047">HTTP/HTTPS only</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="61063">L4 and </st><span class="No-Break"><st c="61071">L7 protocols</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="61083">Multi-Tenancy</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="61097">Difficult/Custom Extensions</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="61125">Multi-Tenant </st><span class="No-Break"><st c="61139">by design</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="61148">Specifications</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="61163">Annotation-based, controller-specific</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="61201">Controller independent, own </st><span class="No-Break"><st c="61230">resource, standardized</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="61252">Definition/Resource</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="61272">Ingress resource</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="61289">Gateway, GatewayClass, *</st><span class="No-Break"><st c="61314">Route resources</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="61330">Routing</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="61338">host/path-based</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="61354">Supports header</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="61370">Traffic Management</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="61389">Limited to </st><span class="No-Break"><st c="61401">the vendor</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="61411">Build-in/defined</st></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="61428">Table 4.2: Comparing Ingress and the Gateway API</st></p>
			<p><st c="61477">This rich set of features is a game-changer for those who have to build platforms. </st><st c="61561">Previously, most of these capabilities had to be bought commercially or brutally forced into the cluster by some hacky workaround and self-developed tools. </st><st c="61717">However, with the Gateway API, a wide range of protocols is supported, and it comes with two</st><a id="_idIndexMarker408"/><st c="61809"> interesting additional </st><span class="No-Break"><st c="61833">features loaded.</st></span></p>
			<p><st c="61849">You can create cross-namespace routes with the Gateway API, either with </st><strong class="source-inline"><st c="61922">ReferenceGrant</st></strong><st c="61936"> or with </st><strong class="source-inline"><st c="61945">AllowedRoutes</st></strong><st c="61958">. The allowed routes are implemented via reference bindings, which need to be defined by the gateway owner as from which namespaces traffic is expected. </st><st c="62111">In practice, the Gateway configuration will be extended </st><span class="No-Break"><st c="62167">as follows:</st></span></p>
			<pre class="source-code"><st c="62178">
namespaces:
  from: Selector
  selector:
    matchExpressions:
    - key: kubernetes.io/metadata.name
      operator: In
      values:
      - alpha
      - omega</st></pre>
			<p><st c="62305">This will allow traffic from the </st><strong class="source-inline"><st c="62339">alpha</st></strong><st c="62344"> and </st><span class="No-Break"><strong class="source-inline"><st c="62349">omega</st></strong></span><span class="No-Break"><st c="62354"> namespaces.</st></span></p>
			<p><st c="62366">Alternatively, we can use a </st><strong class="source-inline"><st c="62395">ReferenceGrant</st></strong><st c="62409">, which is described </st><span class="No-Break"><st c="62430">as follows:</st></span></p>
			<p class="author-quote"><st c="62441">ReferenceGrant can be used to enable cross namespace references within Gateway API. </st><st c="62526">In particular, Routes may forward traffic to backends in other namespaces, or Gateways may refer to Secrets in another namespace.</st></p>
			<p><st c="62655">They sound similar, but they aren’t. </st><st c="62693">Let’s </st><span class="No-Break"><st c="62699">compare them:</st></span></p>
			<ul>
				<li><strong class="source-inline"><st c="62712">ReferenceGrant</st></strong><st c="62727">: A Gateway and Route in namespace A grant a service in namespace B to </st><span class="No-Break"><st c="62799">forward traffic</st></span></li>
				<li><strong class="source-inline"><st c="62814">AllowedRoutes</st></strong><st c="62828">: A Route to namespace B is configured in a Gateway in </st><span class="No-Break"><st c="62884">namespace C</st></span></li>
			</ul>
			<p><st c="62895">Besides those cross-namespace additional security layers, the Gateway API also comes with its own extension capabilities. </st><st c="63018">With them, you can define your own PolicyAttachment, such as BackendTLSPolicy, to validate the proper usage </st><span class="No-Break"><st c="63126">of TLS.</st></span></p>
			<p><st c="63133">One last thing before we move on. </st><st c="63168">The Gateway API comes with personas. </st><st c="63205">Personas are pre-defined roles that allow fine-grained usage of gateway capabilities. </st><st c="63291">Ingress just has one persona, a </st><a id="_idIndexMarker409"/><st c="63323">user, independent of whether it is an admin or a developer. </st><st c="63383">The following table shows the write permissions of those personas in a </st><span class="No-Break"><st c="63454">four-tier model:</st></span></p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/Figure_4.12_B31164.jpg" alt="Figure 4.12: Write permissions for an advanced four-tier model"/><st c="63470"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="63693">Figure 4.12: Write permissions for an advanced four-tier model</st></p>
			<p><st c="63755">At this point, the </st><a id="_idIndexMarker410"/><st c="63775">Gateway API is the true savior of countless nights of getting inbound traffic right and building a multi-tenant platform with a clear and </st><span class="No-Break"><st c="63913">transparent approach</st><a id="_idTextAnchor238"/><st c="63933">.</st></span></p>
			<h3><st c="63934">ExternalDNS</st></h3>
			<p><st c="63946">The </st><strong class="bold"><st c="63951">ExternalDNS</st></strong><st c="63962"> project, developed</st><a id="_idIndexMarker411"/><st c="63981"> by the Kubernetes contributors, is a tool that is often used within cloud-provided Kubernetes clusters, but still not often highlighted as a relevant implementation. </st><st c="64148">Yet it bridges the gap between some random IPs of Pods in the cluster, takes it toward a proper DNS that is publicly or privately reachable, and routes traffic to the application within the platform. </st><st c="64348">ExternalDNS provides support for almost every cloud and cloud-like environment, as well as traffic- and content-focused services such </st><span class="No-Break"><st c="64482">as CloudFlare.</st></span></p>
			<p><st c="64496">However, ExternalDNS is not a DNS. </st><st c="64532">Surprise! </st><st c="64542">It reads the resources from the Kubernetes API and configures external DNS to point to the cluster’s public endpoints. </st><st c="64661">You could also say that ExternalDNS allows you to control DNS records dynamically via Kubernetes resources in a DNS </st><span class="No-Break"><st c="64777">provider-agnostic way.</st></span></p>
			<p><st c="64799">Let’s have a look at how ExternalDNS works together with the CoreDNS of Kubernetes and the Cloud DNS Service. </st><st c="64910">In the next diagram, you can see the managed Kubernetes on the left. </st><st c="64979">In this case, AWS EKS and the CoreDNS are running on Kubernetes to resolve internal DNS calls. </st><st c="65074">When ExternalDNS is deployed, it observes the gateway, ingress, and service resources. </st><st c="65161">When changes apply or new services </st><a id="_idIndexMarker412"/><st c="65196">come up, ExternalDNS updates the DNS records on the cloud provider or DNS </st><span class="No-Break"><st c="65270">service provider.</st></span></p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/Figure_4.13_B31164.jpg" alt="Figure 4.13: ExternalDNS"/><st c="65287"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="65554">Figure 4.13: ExternalDNS</st></p>
			<p><st c="65578">However, when looking for an alternative approach, you usually have only two fallback options: create the DNS entries by yourself (manually) or have it automated in some way with a custom controller or function, or during the infrastructure </st><span class="No-Break"><st c="65820">creation process.</st></span></p>
			<p><st c="65837">Therefore, it is double painful to see the limitations</st><a id="_idIndexMarker413"/> <span class="No-Break"><st c="65892">of ExternalDNS:</st></span></p>
			<ul>
				<li><st c="65908">Missing fine-grained control; for example, ExternalDNS will create DNS records for all services and ingresses from </st><span class="No-Break"><st c="66024">any namespace</st></span></li>
				<li><st c="66037">ExternalDNS gives you only A records; if you need a TXT or CNAME record, you have to do </st><span class="No-Break"><st c="66126">it manually</st></span></li>
				<li><st c="66137">You will get default DNS configurations for the DNS name; otherwise, you have to manually</st><a id="_idIndexMarker414"/> <span class="No-Break"><st c="66227">define them</st></span></li>
			</ul>
			<p><st c="66239">Besides that, you can also find increased costs (due to its outrageous DNS record creation behavior) and added complexity or latency. </st><st c="66374">I can’t fully agree with those types of issues as it is a question of how you </st><span class="No-Break"><st c="66452">handle them.</st></span></p>
			<p><st c="66464">Consider using ExternalDNS only when you have mastered other parts of Kubernetes networking and are able to do fine-grained management of network policies and the gateway API so that you have strict control over which services are reachable, and how. </st><st c="66716">In addition, consider enabling the DNSSEC feature and also establishing </st><span class="No-Break"><st c="66788">DNS monitori</st><a id="_idTextAnchor239"/><st c="66800">ng.</st></span></p>
			<h3><st c="66804">Load balancing, EndpointSlices, and Topology-Aware Routing</st></h3>
			<p><st c="66863">Lastly, to </st><a id="_idIndexMarker415"/><st c="66875">round off</st><a id="_idIndexMarker416"/><st c="66884"> the network segment, we will briefly </st><a id="_idIndexMarker417"/><st c="66922">discuss </st><strong class="bold"><st c="66930">load balancing</st></strong><st c="66944">, </st><strong class="bold"><st c="66946">EndpointSlices</st></strong><st c="66960">, and </st><span class="No-Break"><strong class="bold"><st c="66966">Topology-Aware Routing</st></strong></span><span class="No-Break"><st c="66988">.</st></span></p>
			<p><st c="66989">Load balancing is</st><a id="_idIndexMarker418"/><st c="67007"> handled by the ingress controller or gateway API within the cluster. </st><st c="67077">This can be outsourced to an external load balancer in combination with a cloud provider. </st><st c="67167">All major cloud providers have their own approach, usually via their own controller, to manage the managed service load balancer. </st><st c="67297">What is the difference between these options? </st><st c="67343">Running your own load balancer within Kubernetes means that, first, all traffic gets routed to that entry point. </st><st c="67456">With the correct setup, this can be multiple nodes with a very simple load distribution. </st><st c="67545">The downside is that if one node is overloaded for some reason, it still gets the traffic to handle and distribute internally. </st><st c="67672">A cloud load balancer will distribute the load across multiple nodes, and, depending on how the integration is done, it is aware of whether a node can handle more load or whether it should be redirected to another one. </st><st c="67891">A </st><a id="_idIndexMarker419"/><st c="67893">downside of the public cloud load balancer is that you must also pay </st><span class="No-Break"><st c="67962">for it.</st></span></p>
			<p><st c="67969">EndpointSlices</st><a id="_idIndexMarker420"/><st c="67984"> become relevant for large-scale clusters. </st><st c="68027">Endpoints are API objects that define a list of IP addresses and ports. </st><st c="68099">These addresses belong to the Pods that are dynamically assigned to a service. </st><st c="68178">When a service is created, Kubernetes automatically creates an associated Endpoint object. </st><st c="68269">The Endpoint object maintains the Pods’ IP addresses and port numbers that match the service’s selector criteria. </st><st c="68383">EndpointSlices were introduced in Kubernetes 1.16. </st><st c="68434">They provide a way to distribute the network endpoints across multiple resources, reducing the load on the Kubernetes API server and improving the performance of large clusters. </st><st c="68612">While in the past, a single large Endpoint object for a service became slow, multiple small EndpointSlice objects are now created, each representing a portion of </st><span class="No-Break"><st c="68774">the endpoints.</st></span></p>
			<p><st c="68788">The control plane creates and manages EndpointSlices to have no more than 100 endpoints per slice. </st><st c="68888">This can be changed, up to a maximum of 1,000. </st><st c="68935">For the kube-proxy, an EndpointSlice is the source of truth for routing </st><span class="No-Break"><st c="69007">internal traffic.</st></span></p>
			<p><st c="69024">EndpointSlices are also required for </st><a id="_idIndexMarker421"/><st c="69062">Topology-Aware Routing. </st><st c="69086">With Topology-Aware Routing, you can keep the traffic within the cloud provider’s </st><strong class="bold"><st c="69168">Availability Zone</st></strong><st c="69185"> (</st><strong class="bold"><st c="69187">AZ</st></strong><st c="69189">). </st><st c="69193">This has two main advantages: it reduces the </st><a id="_idIndexMarker422"/><st c="69238">network costs and improves the performance. </st><st c="69282">Instead of a Pod in AZ 1 communicating with another Pod in AZ 2 and sending a lot of data, the Pod in AZ 1 will now talk to a replica (if available) that is also in AZ 1. </st><st c="69453">To make this work best, the incoming traffic should be distributed evenly via an external load balancer and you should have at least three endpoints per zone. </st><st c="69612">Otherwise, the controller will fail to assign that endpoint with a chance of</st><a id="_idTextAnchor240"/> <span class="No-Break"><st c="69688">around 50%.</st></span></p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor241"/><st c="69700">Kubernetes as part of the platform control plane</st></h2>
			<p><st c="69749">Looking back to the second chapter, we can see in the reference architecture that a platform can be highly distributed, based on many services that, from a higher viewpoint, might not even belong together. </st><st c="69956">We discussed that Kubernetes might often become a central part of your platform. </st><st c="70037">However, how central can it become? </st><st c="70073">As said, Kubernetes is not just there to run workload; it is a platform (based on promise theory and a standardized model and API) for building platforms. </st><st c="70228">This shifts Kubernetes with one foot into the platform control plane and does this in two ways: as a resource controller and as a </st><span class="No-Break"><st c="70358">platform or</st><a id="_idTextAnchor242"/><st c="70369">chestrator.</st></span></p>
			<h3><st c="70381">Steering resources from within Kubernetes</st></h3>
			<p><st c="70423">The open source </st><a id="_idIndexMarker423"/><st c="70440">Crossplane project is the only provider-independent solution for managing cloud resources from within Kubernetes. </st><st c="70554">Initially created to manage other Kubernetes clusters from within Kubernetes, it quickly became the universal solution for handling cloud resources. </st><st c="70703">Cloud resources are available as CRDs and can be defined as Kubernetes-native resources through a specification file. </st><st c="70821">This gives users the option to define what they need and leave the resource creation on the promise theory of Kubernetes. </st><st c="70943">For the different clouds, so-called providers are available, which define the available resources. </st><st c="71042">A user can create single resources or whole compositions, which are multiple resources that </st><span class="No-Break"><st c="71134">bel</st><a id="_idTextAnchor243"/><st c="71137">ong together.</st></span></p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor244"/><st c="71151">The problem of external versus internally defined resources</st></h2>
			<p><st c="71211">What is the right approach to managing resources? </st><st c="71262">Should they be defined by an infrastructure team or through input from demand forms? </st><st c="71347">Can they be defined by the user via the platform? </st><st c="71397">Anything is possible, but no simple </st><span class="No-Break"><st c="71433">answer exists.</st></span></p>
			<p><st c="71447">Let’s have a look at the two approaches. </st><st c="71489">First, let us take a look at a scenario at Financial One ACME, coming from a more traditional, conservative background: infrastructure teams have been fighting over the last few years for automation and a declarative approach. </st><st c="71716">While they manage their on-premises environments through Ansible, they decided to use something simpler for the cloud providers: Terraform (or OpenTofu). </st><st c="71870">We will not go through the whole stack, but until we hit the Kubernetes platform, everything is orchestrated through classic CI/CD push principles and IaC </st><span class="No-Break"><st c="72025">via Terraform.</st></span></p>
			<p><st c="72039">Financial One ACME is starting a new project to develop custom software for their internal usage. </st><st c="72138">The team will utilize the ACME platform and work on the system’s base architecture. </st><st c="72222">They have defined that they will require certain file storage, a cache, a relational database, a notification service, and a message streaming service. </st><st c="72374">As the platform provides some self-service, the team can copy the Terraform modules into their repository. </st><st c="72481">From here, a predefined CI/CD pipeline will take over the configuration and deploy the resources in the defined environments. </st><st c="72607">These self-defined but still externally managed resources are, in some ways, isolated from the rest of the system. </st><st c="72722">They may live in the same repository or hierarchy and are managed by the team, but they are not </st><span class="No-Break"><st c="72818">strongly integrated.</st></span></p>
			<p><st c="72838">On the other hand, they provide a certain level of stability. </st><st c="72901">From within the platform user space, those resources are invisible, except that a discovery service exists. </st><st c="73009">When an organization matures, the pipelines might be customized without notifying the owner or user. </st><st c="73110">However, infrastructure and application are clearly separated, which is an advantage because of the totally different life cycles of </st><span class="No-Break"><st c="73243">the elements.</st></span></p>
			<p><st c="73256">Fast forward and Financial One ACME has undergone a cloud-native transformation, leveraging platform engineering and IDPs to the maximum. </st><st c="73395">Again, they plan to do a new internal project, which, of course, is completely different from the one before but somehow has exactly the same requirements. </st><st c="73551">Some organizations’ behavior will never change. </st><st c="73599">This time, the project team created a new project in their developer portal. </st><st c="73676">Automatically, all base requirements will be pushed into a new Git repository. </st><st c="73755">The chosen resources are selected and pushed to the platform, where a controller decides where it deploys those resources. </st><st c="73878">Some end up as managed services on the cloud provider, others in a shared service account from a specialized team, and a few in the project’s namespace. </st><st c="74031">After some time, the team understood that they had chosen the wrong configuration and, due to the adjustments, a migration to the new service took place. </st><st c="74185">This scenario can be as true as the previous one but has </st><span class="No-Break"><st c="74242">different impacts.</st></span></p>
			<p><st c="74260">The team needs to know in more detail which requirements they have; on the other hand, they have to trust in the predefined deployments and configurations. </st><st c="74417">The platform engineering team has, in collaboration with the operational team, defined best practices with guardrails, ensuring operability but also matching almost all the requirements of the users. </st><st c="74617">Within the cluster user space, the team can find all deployed resources and address them as a service within the platform, even though they are not running in it. </st><st c="74780">However, sudden changes on the user side might cause resources to suddenly spike or get deleted in other places. </st><st c="74893">Managed service teams have to handle such changes without warning. </st><st c="74960">In total, all depending resources are acting extremely volatile and dynamic, hard to predict, and difficult to manage. </st><st c="75079">On the other hand, the project can focus fully on the flow and progress as external resources are handled from within the cluster, rather than learning how to manage those with outer-cluster resource management solutions such </st><span class="No-Break"><st c="75305">as IaC.</st></span></p>
			<p><st c="75312">Both approaches are fine. </st><st c="75339">Both have pros and cons, and the one that is best for your organization often depends more on the human factor than any technological factor. </st><st c="75481">However, what is clear is that internally defined cluster resources are more dynamic and shifted to the left, into the user’s responsibility, than in externally </st><span class="No-Break"><st c="75642">defined resources.</st></span></p>
			<p><st c="75660">In the end, it becomes a philosophical discussion. </st><st c="75712">Externally defined resources are more traditional, whereas the internally defined approach is progressive and future-oriented. </st><st c="75839">However, we don’t have too many options to run cluster internal provisioning processes. </st><st c="75927">Besides Crossplane, we have seen many meta-implementation controllers that read custom resources from the cluster and trigger CI/CD pipelines, for example. </st><st c="76083">That’s a poor workaround, if </st><span class="No-Break"><st c="76112">someone asks.</st></span></p>
			<p><st c="76125">In this section, we looked at the fundamental capabilities that are required for Kubernetes, the challenges around them, and how we can solve them. </st><st c="76274">Also, they don’t feel like any of those crazy implementations you see at conferences. </st><st c="76360">Getting these basics right will make the difference and decide whether everything else on top will be a pleasure or a pain. </st><st c="76484">Up next, we will close this chapter by looking into the approach of finding the right node sizes and the ramifications for flexibility </st><span class="No-Break"><st c="76619">and</st><a id="_idTextAnchor245"/><st c="76622"> reliability.</st></span></p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor246"/><st c="76635">Designing for flexibility, reliability, and robustness</st></h1>
			<p><st c="76690">In the previous part, we discussed cluster scalability and how the VPA, HPA, and CA play together. </st><st c="76790">This helps to create a flexible, reliable, and robust system. </st><st c="76852">A key part of this is also allowing customization as long as it doesn’t harm your system. </st><st c="76942">Components of the cluster must play together seamlessly but must also be exchangeable where needed. </st><st c="77042">This is sensitive: you have, on the one hand, a breathing cluster that grows and shrinks its demand over time; then, you have all the extensions on and around the cluster that allow you to serve the best possible feature set for your use case. </st><st c="77286">You also have the continuously evolving open source community that frequently delivers updates and new developments, which should be integrated and made available for your users. </st><st c="77465">As we told you earlier, this is why you must have a product mindset – to build the best possible platform for your users. </st><st c="77587">Throw away things you don’t need or that are outdated, but keep the whole system as </st><span class="No-Break"><st c="77671">your core.</st></span></p>
			<p><st c="77681">Now, for some reason, we still see discussions about whether you should put all your workload on Kubernetes, and whether it is reliable. </st><st c="77819">We have to look at this discussion from two perspectives: bottom-up from Kubernetes’s infrastructure and core responsibilities, and top-down from what the user can see </st><span class="No-Break"><st c="77987">an</st><a id="_idTextAnchor247"/><st c="77989">d experience.</st></span></p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor248"/><st c="78003">Optimize consumption versus leaving enough head space</st></h2>
			<p><st c="78057">As we learned in </st><a href="B31164_02.xhtml#_idTextAnchor055"><span class="No-Break"><em class="italic"><st c="78075">Chapter 2</st></em></span></a><st c="78084">, the Kubernetes cluster and the workload it manages have an interesting relationship and influence on each other. </st><st c="78199">Some applications require more CPU power, others scale up instead, and the rest just run on demand. </st><st c="78299">Finding the right match isn’t easy, but an ideal target is high resource consumption as it optimizes the usage of the available resources and therefor</st><a id="_idTextAnchor249"/><st c="78449">e </st><span class="No-Break"><st c="78452">the costs.</st></span></p>
			<h3><st c="78462">How to make the clusters the right size</st></h3>
			<p><st c="78502">Evaluating the right size</st><a id="_idIndexMarker424"/><st c="78528"> for the cluster is always a challenge. </st><st c="78568">Finding the right solution is a clear case of </st><em class="italic"><st c="78614">it depends</st></em><st c="78624">. Let’s take Financial One ACME, which has to provide a new cluster. </st><st c="78693">We don’t know a lot about the expected workload, just that it requires some memory and has a few moving parts that are not that resource-demanding. </st><st c="78841">So, we can take one of the </st><span class="No-Break"><st c="78868">following options:</st></span></p>
			<ul>
				<li><st c="78886">1x 16 vCPU, 64 </st><span class="No-Break"><st c="78902">GB memory</st></span></li>
				<li><st c="78911">2x 8 vCPU, 32 </st><span class="No-Break"><st c="78926">GB memory</st></span></li>
				<li><st c="78935">4x 4vCPU,  16 </st><span class="No-Break"><st c="78949">GB memory</st></span></li>
				<li><st c="78958">8x 2vCPU, 8 </st><span class="No-Break"><st c="78971">GB memory</st></span></li>
			</ul>
			<p><st c="78980">For availability reasons, the first option would be a bad idea. </st><st c="79045">If you run an update in the cluster and the whole system goes down or you have to provision one new node, you need to shift all the apps over and shut down the old one. </st><st c="79214">Option 4 comes with many nodes. </st><st c="79246">Due to its small size, this can lead to a resource shortage as some base components that are required for the cluster will consume a part of the CPU and memory. </st><st c="79407">In addition, depending on the cloud provider, it might be that you have other limitations, such as available IP addresses, bandwidth, and storage capacity. </st><st c="79563">Also, if you have an app that needs 1 vCPU, and might scale to 1.5 – 2.0 vCPU, it would kill an </st><span class="No-Break"><st c="79659">entire node.</st></span></p>
			<p><st c="79671">However, how many resources are used per node by Kubernetes? </st><st c="79733">By default, for the CPU, we can use the </st><span class="No-Break"><st c="79773">following rules:</st></span></p>
			<ul>
				<li><st c="79789">6% of the </st><span class="No-Break"><st c="79800">first core</st></span></li>
				<li><st c="79810">1% of the </st><span class="No-Break"><st c="79821">second core</st></span></li>
				<li><st c="79832">0.5% of the next </st><span class="No-Break"><st c="79850">two cores</st></span></li>
				<li><st c="79859">0.25% from the 5th </st><span class="No-Break"><st c="79879">core onward</st></span></li>
			</ul>
			<p><st c="79890">We also have some rough rules for </st><span class="No-Break"><st c="79925">the memory:</st></span></p>
			<ul>
				<li><st c="79936">25% of the first </st><span class="No-Break"><st c="79954">4 GB</st></span></li>
				<li><st c="79958">20% of the following </st><span class="No-Break"><st c="79980">4 GB</st></span></li>
				<li><st c="79984">10% of the next </st><span class="No-Break"><st c="80001">8 GB</st></span></li>
				<li><st c="80005">6% of the next 112 GB (up to </st><span class="No-Break"><st c="80035">128 GB)</st></span></li>
				<li><st c="80042">2% of anything above </st><span class="No-Break"><st c="80064">128 GB</st></span></li>
				<li><st c="80070">In case the node has less than 1GB of memory, it is </st><span class="No-Break"><st c="80123">255 MiB</st></span></li>
			</ul>
			<p><st c="80130">In addition, every </st><a id="_idIndexMarker425"/><st c="80150">node has an eviction threshold of 100 MB. </st><st c="80192">If the resources are completely utilized and the threshold is crossed, Kubernetes starts cleaning up some Pods to prevent completely running out of memory. </st><st c="80348">At learnk8s</st><a id="_idIndexMarker426"/><st c="80359">, you can find a very detailed blog about </st><span class="No-Break"><st c="80401">it (</st></span><a href="https://learnk8s.io/kubernetes-node-size"><span class="No-Break"><st c="80405">https://learnk8s.io/kubernetes-node-size</st></span></a><span class="No-Break"><st c="80446">).</st></span></p>
			<p><st c="80449">Let’s visualize </st><span class="No-Break"><st c="80466">these numbers:</st></span></p>
			<table id="table003-2" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><strong class="bold"><st c="80480">2 vCPU 8 </st></strong><span class="No-Break"><strong class="bold"><st c="80490">GB RAM</st></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold"><st c="80496">4 vCPU 16 </st></strong><span class="No-Break"><strong class="bold"><st c="80507">GB RAM</st></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold"><st c="80513">8 vCPU 32 </st></strong><span class="No-Break"><strong class="bold"><st c="80524">GB RAM</st></strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><st c="80530">Kubelet + </st></strong><span class="No-Break"><strong class="bold"><st c="80541">OS vCPU</st></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="80548">70 m or </st><span class="No-Break"><st c="80557">0.07 vCPU</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="80566">80 m or </st><span class="No-Break"><st c="80575">0.08 vCPU</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="80584">90 m or </st><span class="No-Break"><st c="80593">0.09 vCPU</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><st c="80602">Kubelet + </st></strong><span class="No-Break"><strong class="bold"><st c="80613">OS memory</st></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="80622">1.8 GB</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="80629">2.6 GB</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="80636">3.56 GB</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><st c="80644">Eviction threshold</st></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="80663">100 MB</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="80670">100 MB</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="80677">100 MB</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><st c="80684">Available vCPU</st></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="80699">1930 m or </st><span class="No-Break"><st c="80710">1.9 vCPU</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="80718">3920 m or </st><span class="No-Break"><st c="80729">3.9 vCPU</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="80737">7910n  or </st><span class="No-Break"><st c="80747">7.9 vCPU</st></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold"><st c="80755">Available memory</st></strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="80772">6.1 GB</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="80779">13.3 GB</st></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><st c="80787">28.34 GB</st></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="80796">Table 4.3: Resource consumption and available resources for different node sizes</st></p>
			<p><st c="80877">From those available </st><a id="_idIndexMarker427"/><st c="80899">resources, you also have to take away anything that is required to cluster, such as </st><strong class="source-inline"><st c="80983">DaemonSet</st></strong><st c="80992"> for logging and monitoring, </st><strong class="source-inline"><st c="81021">kube-proxy</st></strong><st c="81031">, storage driver, and </st><span class="No-Break"><st c="81053">so on.</st></span></p>
			<p><st c="81059">Doing the math, if you have many small nodes, the basic layer of used resources is larger than that of the one with big nodes. </st><st c="81187">Let us look at an example. </st><st c="81214">Two 2vCPU 8 GB RAM nodes require 140 m of CPU and 3.6 GB of RAM in total, while one 4vCPU 16 GB RAM only requires 80 m of CPU and 2.6 GB of RAM. </st><st c="81359">Those are the costs for higher availability, but in a side-by-side comparison of just the available resources, a single node requires fewer resources to be assigned to Kubernetes. </st><st c="81539">However, ideally, a node is utilized at its maximum capacity; we don’t run a virtual machine where 80% head space of non-utilized resources is standard! </st><st c="81692">At least 80% utilization of a node would be the target to get the best performance/price ratio. </st><st c="81788">This is also because of the energy proportionality of a server. </st><st c="81852">The energy needed by a CPU doesn’t linearly scale with </st><span class="No-Break"><st c="81907">the workload.</st></span></p>
			<p><st c="81920">Imagine that a server can consume up to 200 watts. </st><st c="81972">Then, most servers would need between 150 and 180 watts for around 50% of CPU utilization. </st><st c="82063">This means the more a server is utilized, the better the energy/CPU utilization ratio, which is also more cost-efficient </st><span class="No-Break"><st c="82184">and sustainable.</st></span></p>
			<p><st c="82200">We have to consider other factors while choosing the node sizes for </st><span class="No-Break"><st c="82269">the cluster:</st></span></p>
			<ul>
				<li><st c="82281">If you provide just a few large nodes, but the user defines an anti-affinity so that on each node only one Pod of a replica is running and you don’t have enough nodes because the expected replicas are too high, then some Pods </st><span class="No-Break"><st c="82508">become unschedulable.</st></span></li>
				<li><st c="82529">Large nodes tend to be underutilized, so you spend more money on something you </st><span class="No-Break"><st c="82609">don’t use.</st></span></li>
				<li><st c="82619">If you have too many small nodes and you continuously run into pending Pods for which each cluster has to scale, that might make the users unhappy as it always takes time to scale up new nodes. </st><st c="82814">Also, it might affect the servability of an app if it runs continuously under </st><span class="No-Break"><st c="82892">resource limitations.</st></span></li>
				<li><st c="82913">Many nodes cause a higher amount of network communication, container image pulls, and duplicate image storage on each node. </st><st c="83038">Otherwise, in the worst case, you always pull an image from a registry again. </st><st c="83116">With a small cluster, that isn’t a problem, but with a large number of nodes, this becomes </st><span class="No-Break"><st c="83207">quite chatty.</st></span></li>
				<li><st c="83220">The more </st><a id="_idIndexMarker428"/><st c="83230">nodes there are, the more communication happens between them and the control plane. </st><st c="83314">At some level of requests, this means increasing the node sizes of the </st><span class="No-Break"><st c="83385">control plane.</st></span></li>
			</ul>
			<p><st c="83399">Finding the right node and cluster size is a science in itself. </st><st c="83464">Neither end of the extreme is good. </st><st c="83500">Start looking at the kind of workload you expect. </st><st c="83550">If you’re not sure, start with something medium-sized and adjust the node sizes if needed. </st><st c="83641">Also, consider always having a little bit more memory available. </st><st c="83706">The core components of Kubernetes per node don’t require a lot of CPU, but do require at least something between 2 and 3 GB of memory plus all the other </st><span class="No-Break"><st c="83859">default components</st><a id="_idTextAnchor250"/><st c="83877">.</st></span></p>
			<h3><st c="83878">Solution space</st></h3>
			<p><st c="83893">Most public cloud providers come with the ability to run multiple instance types for Kubernetes nodes. </st><st c="83997">This is helpful for different use cases, from isolating workloads to optimizing the utilization or migrating from one CPU architecture to another. </st><st c="84144">We also talked about GPU utilization, which you can combine in such scenarios to run non-GPU workloads on CPU nodes while doing model training on the </st><span class="No-Break"><st c="84294">GPU instances.</st></span></p>
			<p><st c="84308">To do this, you have to manage and label the nodes correctly, and provide users with a transparent approach and support for defining their </st><span class="No-Break"><st c="84448">affinity settings.</st></span></p>
			<p><st c="84466">To identify the right node sizes, learnk8s also provides you with an instance calculator (</st><a href="https://learnk8s.io/kubernetes-instance-calculator"><st c="84557">https://learnk8s.io/kubernetes-instance-calculator</st></a><st c="84608">), which you might consider before you start building your own Excel sheet for doing </st><span class="No-Break"><st c="84694">the math.</st></span></p>
			<p><st c="84703">Also, when defining the cluster size and scale doesn’t look that relevant, with the right node sizes, you can have a direct impact on costs, utilization, application availability, user experience, and how many additional implementations you have to do to compensate for </st><span class="No-Break"><st c="84974">potential drawbacks</st><a id="_idTextAnchor251"/><st c="84993">.</st></span></p>
			<h1 id="_idParaDest-105"><a id="_idTextAnchor252"/><st c="84994">Summary</st></h1>
			<p><st c="85002">In this chapter, we got closer to some of the relevant components of Kubernetes as the cornerstone for your platform. </st><st c="85121">We first examined whether Kubernetes is the right choice, and also looked at why it often is the right way to go. </st><st c="85235">With the promise theory at its heart and many robust features for running and extending a platform, Kubernetes is an almost perfect foundation for a platform. </st><st c="85394">From here, we looked into some of the very basic elements of Kubernetes: storage, networking, CPU architectures, and GPU support. </st><st c="85524">In this context, we learned about some design considerations and problems we might face while implementing Kubernetes. </st><st c="85643">While Kubernetes as a foundation might feel different in every environment, it is possible to create a unified experience. </st><st c="85766">This will come with major drawbacks, such as losing the features of certain cloud providers, flexibility, </st><span class="No-Break"><st c="85872">and customizability.</st></span></p>
			<p><st c="85892">Next, we discussed finding the balance between a very stiff and highly flexible system. </st><st c="85981">Both can be seen as robust and reliable, but they come with very different challenges and problems. </st><st c="86081">Therefore, we did a short thought experiment to find the right cluster sizes and node types before we closed this section by discussing approaches for implementing guardrails for the user space. </st><st c="86276">This helps us provide flexibility within the user space but protects the platform from misbehavior and wrongly configured services by users. </st><st c="86417">We learned about this in </st><span class="No-Break"><st c="86442">this chapter.</st></span></p>
			<p><st c="86455">In the next chapter, we will focus on the automation of platforms. </st><st c="86523">Besides the infrastructure, automation is a critical component of a platform and, as you will see later on, can be a bottleneck and cost driver in the long run. </st><st c="86684">You will learn how to design a proper release process, how to implement it in CI/CD and GitOps, and how to use this combination for the life cycle of the platform artifacts. </st><st c="86858">We will also show you how to effectively observe </st><span class="No-Break"><st c="86907">this process</st><a id="_idTextAnchor253"/><st c="86919">.</st></span></p>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor254"/><st c="86920">Further Reading</st></h1>
			<ul>
				<li><st c="86936">[1] Objects in </st><span class="No-Break"><st c="86952">Kubernetes: </st></span><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/ "><span class="No-Break"><st c="86964">https://kubernetes.io/docs/concepts/overview/working-with-objects/</st></span></a></li>
				<li><st c="87030">[2] </st><span class="No-Break"><st c="87035">Karpenter: </st></span><a href="https://karpenter.sh/ "><span class="No-Break"><st c="87046">https://karpenter.sh/</st></span></a></li>
				<li><st c="87067">[3] CSI drivers </st><span class="No-Break"><st c="87084">index: </st></span><a href="https://kubernetes-csi.github.io/docs/drivers.html "><span class="No-Break"><st c="87091">https://kubernetes-csi.github.io/docs/drivers.html</st></span></a></li>
				<li><st c="87141">[4] RISC-V Kubernetes nodes on </st><span class="No-Break"><st c="87173">Scaleway: </st></span><a href="https://www.scaleway.com/en/docs/bare-metal/elastic-metal/how-to/kubernetes-on-riscv/ "><span class="No-Break"><st c="87183">https://www.scaleway.com/en/docs/bare-metal/elastic-metal/how-to/kubernetes-on-riscv/</st></span></a></li>
				<li><st c="87268">[5] </st><span class="No-Break"><st c="87273">SpinKube: </st></span><a href="https://www.spinkube.dev/ "><span class="No-Break"><st c="87283">https://www.spinkube.dev/</st></span></a></li>
				<li><st c="87308">[6] SpinKube </st><span class="No-Break"><st c="87322">Overview: </st></span><a href="https://www.spinkube.dev/docs/overview/ "><span class="No-Break"><st c="87332">https://www.spinkube.dev/docs/overview/</st></span></a></li>
				<li><st c="87371">[7] Nvidia – Improving GPU Utilization in </st><span class="No-Break"><st c="87414">Kubernetes: </st></span><a href="https://developer.nvidia.com/blog/improving-gpu-utilization-in-kubernetes/"><span class="No-Break"><st c="87426">https://developer.nvidia.com/blog/improving-gpu-utilization-in-kubernetes/</st></span></a></li>
				<li><st c="87500">[8] DRA </st><span class="No-Break"><st c="87509">with GPU:</st></span><ul><li><a href="https://docs.google.com/document/d/1BNWqgx_SmZDi-va_V31v3DnuVwYnF2EmN7D-O_fB6Oo/edit#heading=h.bxuci8gx6hna"><span class="No-Break"><st c="87518">https://docs.google.com/document/d/1BNWqgx_SmZDi-va_V31v3DnuVwYnF2EmN7D-O_fB6Oo/edit#heading=h.bxuci8gx6hna</st></span></a></li><li><a href="https://static.sched.com/hosted_files/colocatedeventseu2024/83/Best%20Practices%20for%20LLM%20Serving%20with%20DRA.pdf"><span class="No-Break"><st c="87626">https://static.sched.com/hosted_files/colocatedeventseu2024/83/Best%20Practices%20for%20LLM%20Serving%20with%20DRA.pdf</st></span></a></li><li><a href="https://github.com/NVIDIA/k8s-dra-driver?tab=re"><span class="No-Break"><st c="87745">https://github.com/NVIDIA/k8s-dra-driver?tab=re</st></span></a></li></ul></li>
			</ul>
		</div>
	<div id="charCountTotal" value="87793"/></body></html>