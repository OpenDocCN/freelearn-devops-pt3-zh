- en: <st c="0">4</st>
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="0">4</st>
- en: <st c="2">Architecting the Platform Core – Kubernetes as a Unified Layer</st>
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="2">平台核心架构 – Kubernetes作为统一层</st>
- en: <st c="65">As a platform engineering team, you need to make a critical decision
    about the underlying technology stack of your core platform.</st> <st c="196">This
    decision will have a long-term impact on your organization as it will dictate
    the skills and resources you will need to build a platform that will support current
    and future self-service</st> <st c="388">use cases.</st>
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="65">作为一个平台工程团队，你需要做出一个关键决定，关于你核心平台的底层技术栈。</st> <st c="196">这个决定将对你的组织产生长期影响，因为它将决定你需要哪些技能和资源来构建一个支持当前和未来自服务</st>
    <st c="388">用例的平台。</st>
- en: '**<st c="398">Kubernetes</st>** <st c="409">– or</st> **<st c="415">K8s</st>**
    <st c="418">for short – is</st> <st c="433">not the solution to all problems,
    but when building platforms, Kubernetes can build</st> <st c="518">the foundation.</st>'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="398">Kubernetes</st>** <st c="409">— 或简称</st> **<st c="415">K8s</st>**
    <st c="418">— 不是解决所有问题的方案，但在构建平台时，Kubernetes可以构建</st> <st c="518">基础。</st>'
- en: <st c="533">In this chapter, you will gain insights into what makes Kubernetes
    the choice for many platform engineering teams.</st> <st c="649">We will explain
    the concept of</st> *<st c="680">promise theory</st>*<st c="694">, which Kubernetes
    is based on, and the benefits that come from the way it’s</st> <st c="771">been
    implemented.</st>
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="533">在本章中，你将深入了解是什么使得Kubernetes成为许多平台工程团队的首选。</st> <st c="649">我们将解释Kubernetes所基于的*<st
    c="680">承诺理论</st>*<st c="694">概念，以及它的实现方式带来的好处。</st>
- en: <st c="788">You will get a better understanding of how to navigate the</st>
    **<st c="848">Cloud Native Computing Foundation</st>** <st c="881">(</st>**<st
    c="883">CNCF</st>**<st c="887">) ecosystem</st> <st c="900">as it will be critical
    for you to pick the right projects to support you in your own</st> <st c="985">platform
    implementation.</st>
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="788">你将更好地理解如何导航</st> **<st c="848">云原生计算基金会</st>** <st c="881">(</st>**<st
    c="883">CNCF</st>**<st c="887">) 生态系统</st> <st c="900">，因为在实施你自己的</st> <st c="985">平台时，选择合适的项目对你至关重要。</st>
- en: <st c="1009">Once you are familiar with the benefits of Kubernetes and the ecosystem,
    you will learn about the considerations when defining the core layer of your platform,
    such as unifying infrastructure, application, and service capabilities.</st> <st
    c="1242">You will learn how to design for interoperability with your core corporate
    services that sit outside of your new platform and how to design for flexibility,
    reliability,</st> <st c="1412">and robustness.</st>
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="1009">一旦你熟悉了Kubernetes及其生态系统的优势，你将学习在定义平台核心层时需要考虑的事项，例如统一基础设施、应用和服务能力。</st>
    <st c="1242">你将学习如何为与新平台之外的核心企业服务实现互操作性而设计，并如何设计平台的灵活性、可靠性</st> <st c="1412">和健壮性。</st>
- en: <st c="1427">As such, we will cover the following main topics in</st> <st c="1480">the
    chapter:</st>
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="1427">因此，我们将在</st> <st c="1480">本章中涵盖以下主要内容：</st>
- en: <st c="1492">Why Kubernetes plays a vital role, and why it is (not)</st> <st
    c="1548">for everyone</st>
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="1492">为什么Kubernetes扮演着至关重要的角色，以及它为什么（不）适合所有人</st>
- en: <st c="1560">Leveraging and managing Kubernetes</st> <st c="1596">infrastructure
    capabilities</st>
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="1560">利用和管理Kubernetes</st> <st c="1596">基础设施能力</st>
- en: <st c="1623">Designing for flexibility, reliability,</st> <st c="1664">and robustness</st>
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="1623">为灵活性、可靠性</st> <st c="1664">和健壮性设计</st>
- en: <st c="1678">Why Kubernetes plays a vital role, and why it is (not) for everyone</st>
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="1678">为什么Kubernetes扮演着至关重要的角色，以及它为什么（不）适合所有人</st>
- en: '<st c="1746">For now, we will focus on Kubernetes, but there are</st> <st c="1799">other
    ways to provide a platform to run your workload.</st> <st c="1854">Besides many
    different flavors of Kubernetes, such as OpenShift, there are alternatives, such
    as Nomad, CloudFoundry, Mesos, and OpenNebula.</st> <st c="1995">They all have
    reasons for their existence, but only one has been adopted almost</st> <st c="2075">everywhere:
    Kubernetes!</st>'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="1746">目前，我们将专注于Kubernetes，但也有</st> <st c="1799">其他方式可以提供运行工作负载的平台。</st>
    <st c="1854">除了Kubernetes的许多不同版本（如OpenShift）外，还有一些替代方案，如Nomad、CloudFoundry、Mesos和OpenNebula。</st>
    <st c="1995">它们都有存在的理由，但只有一个几乎被**无处不在**地采纳：Kubernetes！</st>
- en: <st c="2098">Besides those platforms, you can use virtual machines or services
    from public cloud providers for serverless, app engines, and simple container
    services.</st> <st c="2253">In many cases, platforms utilize these services as
    well, when they are needed.</st> <st c="2332">An exclusive all-in Kubernetes strategy
    might take a few years longer, as it takes organizations a while to fully commit
    to it.</st> <st c="2460">However, there are two recent trends you</st> <st c="2501">can
    observe:</st>
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2098">除了这些平台，你还可以使用公共云提供商的虚拟机或服务来实现无服务器、应用引擎和简单的容器服务。</st> <st c="2253">在许多情况下，平台在需要时也会利用这些服务。</st>
    <st c="2332">采用纯 Kubernetes 策略可能需要更长的时间，因为组织需要一段时间才能完全投入其中。</st> <st c="2460">不过，你可以观察到两个最近的趋势：</st>
- en: <st c="2513">Managing virtual machines</st> <st c="2540">from Kubernetes</st>
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="2513">从 Kubernetes 管理虚拟机</st>
- en: <st c="2555">Migrating to virtual clusters and virtual machines managed by clusters
    to prevent cost explosions for</st> <st c="2658">hypervisor licenses</st>
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="2555">迁移到虚拟集群和由集群管理的虚拟机，以防止由于</st> <st c="2658">虚拟化许可证</st> 带来的成本激增。
- en: <st c="2677">Kubernetes comes with a vital ecosystem and community, a wide range
    of use cases implemented by other organizations, and highly motivated contributors
    to solve the next challenges coming up</st> <st c="2868">with Kubernete</st><st
    c="2882">s.</st>
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2677">Kubernetes 拥有一个重要的生态系统和社区，许多其他组织实现的广泛用例，以及高度积极的贡献者来解决随 Kubernetes
    出现的下一个挑战。</st> <st c="2868">与 Kubernetes</st><st c="2882">相关。</st>
- en: <st c="2885">Kubernetes – a place to start, but not the endgame!</st>
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="2885">Kubernetes – 一个起点，但不是终极目标！</st>
- en: <st c="2937">“</st>*<st c="2939">Kubernetes is a platform to build platforms.</st>
    <st c="2984">It’s a start but not the endgame</st>*<st c="3016">” is a quote from
    Kelsey Hightower, who worked at Google when, back in 2014, Kubernetes was released
    to the world.</st> <st c="3132">However, while Kubernetes plays a vital role in
    building modern</st> <st c="3195">cloud-native platforms, this doesn’t mean it’s
    the perfect fit for everyone.</st> <st c="3273">Remember the product-centric approach
    to platform engineering?</st> <st c="3336">It starts with understanding the pain
    points of your users.</st> <st c="3396">Once we know the pain points, we can work
    on how we would implement the use cases and which technology choices</st> <st
    c="3507">to make.</st>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2937">“</st>*<st c="2939">Kubernetes 是一个构建平台的工具。</st> <st c="2984">这是一个开始，但不是最终目标</st>*<st
    c="3016">” 这句话来自于 Kelsey Hightower，他曾在 2014 年 Kubernetes 推出时为 Google 工作。</st>
    <st c="3132">然而，尽管 Kubernetes 在构建现代</st> <st c="3195">云原生平台中发挥着至关重要的作用，但这并不意味着它适合每一个人。</st>
    <st c="3273">还记得平台工程的产品导向方法吗？</st> <st c="3336">它从理解用户的痛点开始。</st> <st c="3396">一旦我们了解了痛点，就可以着手实现用例，并决定采用哪些技术方案。</st>
    <st c="3507">来做出选择。</st>
- en: <st c="3515">Revisit the early section in</st> [*<st c="3545">Chapter 1</st>*](B31164_01.xhtml#_idTextAnchor014)
    <st c="3554">called</st> *<st c="3562">Do you really need a platform?</st>*<st
    c="3592">, where we provided a questionnaire that helps you decide what the core
    of the platform will be.</st> <st c="3689">The answer could be Kubernetes, but
    it doesn’t have to be.</st> <st c="3748">Let’s start by looking into our own example
    use case from Financial</st> <st c="3816">One AC</st><st c="3822">ME.</st>
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3515">请回顾</st> [*<st c="3545">第 1 章</st>*](B31164_01.xhtml#_idTextAnchor014)
    <st c="3554">中名为</st> *<st c="3562">你真的需要一个平台吗？</st>*<st c="3592">的部分，我们提供了一份问卷帮助你决定平台的核心内容。</st>
    <st c="3689">答案可能是 Kubernetes，但也不一定非得是。</st> <st c="3748">让我们从查看 Financial One
    ACME 的示例用例开始。</st>
- en: <st c="3826">Would Financial One ACME pick Kubernetes?</st>
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="3826">Financial One ACME 会选择 Kubernetes 吗？</st>
- en: <st c="3868">If we think about the use case from</st> <st c="3905">Financial
    One ACME, “</st>*<st c="3926">Easier access to logs in production for problem
    triage</st>*<st c="3981">”, using the proposed solution doesn’t necessarily require
    Kubernetes as the</st> <st c="4059">underlying platform.</st>
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3868">如果我们考虑 Financial One ACME 的用例，“</st>*<st c="3926">在生产环境中更容易访问日志以进行问题诊断</st>*<st
    c="3981">”，使用提议的解决方案并不一定需要 Kubernetes 作为</st> <st c="4059">底层平台。</st>
- en: <st c="4079">If Kubernetes is not being used yet in our organization and the
    only thing we need is a new automation service that integrates into the different
    logging solutions, we may not want to propose Kubernetes as the underlying core
    platform.</st> <st c="4316">This is because it brings a new level of complexity
    into an organization that doesn’t yet have the required experience.</st> <st c="4436">We
    could implement the solution and operate it with all the existing tools and teams;
    maybe we could run it alongside other tools we already have, following the same
    operational processes for deployment, upgrades, monitoring, and</st> <st c="4666">so
    on.</st>
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="4079">如果我们的组织尚未使用 Kubernetes，且我们仅需要一个集成到不同日志解决方案中的新自动化服务，那么我们可能不想将 Kubernetes
    提出作为底层核心平台。</st> <st c="4316">这是因为它会为尚未具备所需经验的组织带来新的复杂性。</st> <st c="4436">我们可以使用所有现有的工具和团队来实现并操作该解决方案；也许我们可以将其与其他已经拥有的工具一起运行，遵循相同的部署、升级、监控等操作流程。</st>
- en: <st c="4672">On the other hand, if there is pre-existing knowledge, or perhaps
    even Kubernetes is already available, then using Kubernetes as the core platform
    to orchestrate this new service would solve a lot of problems, such as providing</st>
    <st c="4901">the following:</st>
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="4672">另一方面，如果已经存在相关知识，或者 Kubernetes 已经可用，那么使用 Kubernetes 作为核心平台来编排这个新服务将解决很多问题，例如提供</st>
    <st c="4901">以下内容：</st>
- en: <st c="4915">New service containers</st> <st c="4939">as Pods</st>
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="4915">新服务容器</st> <st c="4939">作为 Pods</st>
- en: <st c="4946">Automated health checks for</st> <st c="4975">those services</st>
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="4946">针对</st> <st c="4975">这些服务的自动化健康检查</st>
- en: <st c="4989">Resiliency and scalability through concepts such as ReplicaSets</st>
    <st c="5054">and Auto-Scaling</st>
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="4989">通过 ReplicaSets 等概念提供的弹性和可扩展性</st> <st c="5054">以及自动扩展</st>
- en: <st c="5070">External access through ingress controllers and</st> <st c="5119">Gateway
    API</st>
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="5070">通过 Ingress 控制器和</st> <st c="5119">网关 API 实现外部访问</st>
- en: <st c="5130">Basic observability of those services through Prometheus</st> <st
    c="5188">or OpenTelemetry</st>
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="5130">通过 Prometheus 或</st> <st c="5188">OpenTelemetry 对这些服务的基本可观测性</st>
- en: <st c="5204">However, do we really need to run our own Kubernetes cluster when
    we just need to deploy a simple service?</st> <st c="5312">The answer is no!</st>
    <st c="5330">There are alternatives, such as running the implementation using
    the capabilities of your preferred</st> <st c="5430">cloud provider:</st>
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="5204">然而，当我们仅仅需要部署一个简单服务时，是否真的需要运行我们自己的 Kubernetes 集群呢？</st> <st c="5312">答案是否定的！</st>
    <st c="5330">有一些替代方案，例如使用你首选的</st> <st c="5430">云服务商的能力来运行实现：</st>
- en: '**<st c="5445">Serverless</st>**<st c="5456">: The solution could be implemented
    as a set of serverless functions exposed via an</st> <st c="5540">API gateway.</st>
    <st c="5554">State or configuration can be stored in cloud storage services and
    can easily be accessed via</st> <st c="5648">an API.</st>'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="5445">无服务器</st>**<st c="5456">：该解决方案可以作为一组通过 API 网关暴露的无服务器函数来实现。</st>
    <st c="5540">状态或配置可以存储在云存储服务中，并可以通过</st> <st c="5648">API 轻松访问。</st>'
- en: '**<st c="5655">Container</st>**<st c="5665">: If the solution is</st> <st c="5687">implemented
    in a container, that container can be lightweight and its endpoints can easily
    be exposed via an API gateway.</st> <st c="5809">There is no need for a full-fledged
    Kubernetes cluster that somebody needs</st> <st c="5884">to maintain.</st>'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="5655">容器</st>**<st c="5665">：如果该解决方案在容器中实现，该容器可以是轻量级的，并且其端点可以通过 API
    网关轻松暴露。</st> <st c="5809">无需一个完整的 Kubernetes 集群，且无需某人进行</st> <st c="5884">维护。</st>'
- en: <st c="5896">This single use case for Financial One ACME</st> <st c="5940">may
    not lead us to choose Kubernetes as the core platform.</st> <st c="6000">However,
    when making this critical decision about what is to become the core of your future
    platform, we must also look beyond the first use case.</st> <st c="6147">Platform
    engineering will solve many more use cases by providing many self-service capabilities
    to the internal engineering teams in order to improve their</st> <st c="6303">day-to-day
    work.</st>
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="5896">这个仅仅是 Financial One ACME 的一个使用案例</st> <st c="5940">可能不会促使我们选择 Kubernetes
    作为核心平台。</st> <st c="6000">然而，在做出这个关于未来平台核心的关键决策时，我们还必须超越第一个使用案例来看待问题。</st> <st
    c="6147">平台工程将通过为内部工程团队提供许多自服务功能来解决更多的使用案例，从而提高他们的</st> <st c="6303">日常工作效率。</st>
- en: <st c="6319">It’s a tricky and impactful decision to make, one that needs a
    good balance between looking forward and over-engineering.</st> <st c="6442">To
    make that decision easier, let’s look into the benefits of picking Kubernetes
    as the</st> <st c="6530">core pl</st><st c="6537">atform.</st>
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="6319">这是一个棘手且影响深远的决策，需要在前瞻性和过度工程之间找到良好的平衡。</st> <st c="6442">为了让这个决策更容易做出，让我们来看看选择
    Kubernetes 作为</st> <st c="6530">核心平台</st><st c="6537">的好处。</st>
- en: <st c="6545">Benefits of picking Kubernetes as the core platform</st>
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="6545">选择 Kubernetes 作为核心平台的好处</st>
- en: <st c="6597">To make the critical decision of</st> <st c="6631">picking the
    core of a future platform easier, let’s look at why other organizations are picking
    Kubernetes as the core building block.</st> <st c="6766">Understanding those reasons,
    the benefits, and also the challenges should make it easier for architects to
    make this</st> <st c="6883">important d</st><st c="6894">ecision.</st>
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="6597">为了让做出选择未来平台核心的关键决策变得更容易，让我们来看一下为什么其他组织选择 Kubernetes 作为核心构建块。</st>
    <st c="6766">理解这些原因、好处以及挑战，应该能让架构师更容易做出这个</st> <st c="6883">重要决策。</st>
- en: <st c="6903">Declarative desired state – promise theory</st>
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="6903">声明性期望状态——承诺理论</st>
- en: <st c="6946">Traditional IT operations</st> <st c="6972">use the</st> **<st
    c="6981">obligation model</st>**<st c="6997">, which is</st> <st c="7008">when
    an external system instructs the target system to do certain things.</st> <st
    c="7082">This model requires a lot of logic to be put into the external system,
    such as an automated pipeline.</st> <st c="7184">A scripted pipeline, whether
    based on</st> <st c="7222">Jenkins, GitHub Actions, or other solutions, not only
    needs to apply changes to the target system.</st> <st c="7321">The pipeline also
    needs to deal with handling unpredicted outcomes and errors from outside the system
    it changes.</st> <st c="7435">For example, what do we do if deploying a new software
    version doesn’t work within a certain amount of time?</st> <st c="7544">Should
    we roll it back?</st> <st c="7568">How would the pipeline</st> <st c="7591">do
    that?</st>
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="6946">传统的 IT 运维</st> <st c="6972">使用</st> **<st c="6981">义务模型</st>**<st
    c="6997">，这是</st> <st c="7008">当一个外部系统指示目标系统执行某些操作时的情况。</st> <st c="7082">这种模型需要将大量的逻辑嵌入到外部系统中，例如自动化管道。</st>
    <st c="7184">一个脚本化的管道，无论是基于</st> <st c="7222">Jenkins、GitHub Actions 还是其他解决方案，不仅需要将更改应用于目标系统。</st>
    <st c="7321">该管道还需要处理外部系统未预测的结果和错误。</st> <st c="7435">例如，如果部署新软件版本在一定时间内未成功，我们该怎么办？</st>
    <st c="7544">我们应该回滚吗？</st> <st c="7568">管道该如何</st> <st c="7591">处理这种情况？</st>
- en: <st c="7599">In the Kubernetes Documentary Part 1 (</st>[<st c="7638">https://www.youtube.com/watch?v=BE77h7dmoQU</st>](https://www.youtube.com/watch?v=BE77h7dmoQU)<st
    c="7682">), Kelsey Hightower explained the promise theory model that Kubernetes
    follows with a great analogy.</st> <st c="7784">It goes something</st> <st c="7802">like
    this:</st>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="7599">在《Kubernetes 纪录片第一部分》（</st>[<st c="7638">https://www.youtube.com/watch?v=BE77h7dmoQU</st>](https://www.youtube.com/watch?v=BE77h7dmoQU)<st
    c="7682">）中，Kelsey Hightower 用一个很好的类比解释了 Kubernetes 遵循的承诺理论模型。</st> <st c="7784">它大致是这样的：</st>
    <st c="7802">它是这样的：</st>
- en: <st c="7812">If you write a letter, put it in an envelope, and put the destination
    address and the right stamps on it, then the post office promises to deliver that
    letter to the destination within a certain amount of time.</st> <st c="8024">Whether
    that delivery involves trucks, trains, planes or any other form of delivery doesn’t
    matter to the person who wrote that letter.</st> <st c="8160">The postal service
    will do whatever it takes to keep the promise of delivery.</st> <st c="8238">If
    a truck breaks down, some other truck will continue until the letter gets delivered
    to its final destination.</st>
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="7812">如果你写了一封信，把它放进信封里，并贴上目的地地址和正确的邮票，那么邮局承诺会在一定时间内将这封信送达目的地。</st> <st
    c="8024">无论这个投递过程涉及卡车、火车、飞机或其他任何形式的运输，对写信的人来说都无关紧要。</st> <st c="8160">邮政服务会尽一切努力履行投递承诺。</st>
    <st c="8238">如果一辆卡车发生故障，另一辆卡车将继续前行，直到信件送达最终目的地。</st>
- en: <st c="8350">The same principle is true for Kubernetes!</st> <st c="8394">In
    our analogy, the letter is a container image that we put into an envelope.</st>
    <st c="8472">The envelope in the Kubernetes world is a custom resource of a certain</st>
    **<st c="8543">Custom Resource Definition</st>** <st c="8569">(</st>**<st c="8571">CRD</st>**<st
    c="8574">).</st> <st c="8578">To deliver an image, this could be a definition
    of a</st> <st c="8631">Deployment, which includes the reference to the image,
    the number of replicas, the namespace this image should be deployed into, and
    the resource requirements (CPU and memory) for the image to run correctly.</st>
    <st c="8839">Kubernetes then does everything it can to fulfill the promise of
    deploying that image by finding the right Kubernetes node that meets all the requirements
    to run the container image with the specified amount of replicas and the required
    CPU</st> <st c="9080">and memory.</st>
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="8350">同样的原则适用于Kubernetes！</st> <st c="8394">在我们的类比中，信件是我们放入信封中的容器镜像。</st>
    <st c="8472">在Kubernetes世界中，信封是某个</st> **<st c="8543">自定义资源定义</st>** <st c="8569">(</st>**<st
    c="8571">CRD</st>**<st c="8574">)。</st> <st c="8578">为了传递镜像，这可以是一个</st> <st c="8631">Deployment的定义，其中包括镜像的引用、副本数、该镜像应部署到的命名空间，以及该镜像运行所需的资源要求（CPU和内存）。</st>
    <st c="8839">Kubernetes然后会尽其所能，履行部署该镜像的承诺，通过找到合适的Kubernetes节点，确保其满足所有要求，以运行指定副本数的容器镜像，并具备所需的CPU</st>
    <st c="9080">和内存。</st>
- en: <st c="9091">Another example is an Ingress</st> <st c="9121">that exposes a
    deployed service to the outside world.</st> <st c="9176">Through annotations,
    it is possible to control the behavior of certain objects.</st> <st c="9256">For
    an Ingress, this could be the automatic creation of a TLS certificate for the
    domain that should be used to expose the matching services to be accessible via
    HTTPS.</st> <st c="9425">The following is an example of an Ingress object for</st>
    `<st c="9478">fund-transfer-service</st>` <st c="9499">to expose the object via
    a specific domain to the outside world using the Certificate Manager – a core
    Kubernetes ecosystem tool – to create a valid TLS certificate</st> <st c="9665">from</st>
    `<st c="9670">LetsEncrypt</st>`<st c="9681">:</st>
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是Ingress</st> <st c="9091">，它将已部署的服务暴露到外部世界。</st> <st c="9176">通过注解，可以控制某些对象的行为。</st>
    <st c="9256">对于Ingress，这可能是为应暴露的域自动创建TLS证书，使得匹配的服务可以通过HTTPS访问。</st> <st c="9425">以下是一个Ingress对象示例，用于</st>
    `<st c="9478">fund-transfer-service</st>` <st c="9499">通过特定域将对象暴露到外部世界，使用证书管理器——一个Kubernetes核心生态系统工具——来创建有效的TLS证书</st>
    <st c="9665">来自</st> `<st c="9670">LetsEncrypt</st>`<st c="9681">：</st>
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: <st c="10029">There is a bit more to a fully working Ingress object description
    than what’s shown in this manifest</st> *<st c="10131">[1]</st>* <st c="10134">example.</st>
    <st c="10144">However, this example does a good job of explaining how a definition
    will be translated by Kubernetes into the actual actions that one would expect
    – hence</st> <st c="10299">fulfilling</st> <st c="10311">the promise.</st>
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 完全工作的Ingress对象描述比这个清单中展示的内容稍微复杂一些</st> *<st c="10131">[1]</st>* <st c="10134">示例。</st>
    <st c="10144">然而，这个示例很好地解释了Kubernetes如何将定义转换为预期的实际操作——从而</st> <st c="10299">实现</st>
    <st c="10311">承诺。</st>
- en: '<st c="10323">Now, the question is: “</st>*<st c="10347">How does all this
    magic work?</st>*<st c="10377">” To answer this, we will start by exploring the
    concepts of controllers</st> <st c="10450">and operators.</st>'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，问题是：“</st>*<st c="10347">这一切魔法是如何运作的？</st>*<st c="10377">” 为了回答这个问题，我们将从探讨控制器</st>
    <st c="10450">和操作符的概念开始。</st>
- en: <st c="10465">Kubernetes controllers and operators</st>
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="10465">Kubernetes控制器和操作符</st>
- en: '<st c="10502">Kubernetes</st> <st c="10514">controllers are essentially control
    loops that fulfill the promise theory of Kubernetes.</st> <st c="10603">In other
    words, controllers automate what IT admins often do manually: continuously observe
    a system’s current state, compare it with what we expect the system to look like,
    and execute remedial actions to keep the</st> <st c="10818">system running!</st>'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes控制器本质上是实现Kubernetes承诺理论的控制循环。</st> <st c="10502">换句话说，控制器自动化了IT管理员通常手动执行的任务：持续观察系统的当前状态，将其与我们期望的系统状态进行比较，并执行纠正措施，以确保</st>
    <st c="10818">系统运行！</st>
- en: <st c="10833">A core task of Kubernetes controllers is therefore</st> *<st c="10885">continuous
    reconciliation</st>*<st c="10910">. This continuous activity allows it to enforce
    the desired state, for example, making sure that the</st> *<st c="11011">desired
    state</st>* <st c="11024">expressed in the</st> *<st c="11042">Ingress definition</st>*
    <st c="11060">example from earlier matches the</st> *<st c="11094">current state</st>*<st
    c="11107">. If either the desired state or the current state changes, it means
    they are</st> *<st c="11185">out of sync</st>*<st c="11196">. The controller then
    tries to synchronize the two states by making changes to the managed object until
    the current state matches the desired</st> <st c="11338">state again!</st>
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10833">因此，Kubernetes 控制器的一个核心任务是</st> *<st c="10885">持续对账</st>*<st c="10910">。
    这种持续活动使其能够强制执行期望的状态，例如确保</st> *<st c="11011">在先前的 Ingress 定义示例中</st>* <st c="11024">匹配的</st>
    *<st c="11042">期望状态</st>* <st c="11060">与</st> *<st c="11094">当前状态</st>*<st c="11107">。
    如果期望状态或当前状态发生变化，表示它们</st> *<st c="11185">不同步</st>*<st c="11196">。 然后，控制器尝试通过对管理对象进行更改来同步两个状态，直到当前状态再次与期望</st>
    <st c="11338">状态一致！</st>
- en: <st c="11350">The following illustration shows how a controller watches the</st>
    *<st c="11413">desired state</st>* <st c="11426">(expressed through manifests
    and stored in etcd), compares it with the</st> *<st c="11498">current state</st>*
    <st c="11511">(the state persisted in etcd), and manages the</st> *<st c="11559">managed
    objects</st>* <st c="11574">(e.g., Ingress, Deployments, SSL certificates, and</st>
    <st c="11626">so on):</st>
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="11350">以下插图显示了控制器如何监视</st> *<st c="11413">期望状态</st>* <st c="11426">（通过清单表达并存储在
    etcd 中），将其与</st> *<st c="11498">当前状态</st>* <st c="11511">（在 etcd 中持久化的状态）进行比较，并管理</st>
    *<st c="11559">管理对象</st>* <st c="11574">（例如 Ingress、部署、SSL 证书等）：</st>
- en: '![Figure 4.1: Reconciliation and self-healing by design through Kubernetes
    controllers](img/Figure_4.01_B31164.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1：通过 Kubernetes 控制器实现的对账和自愈设计](img/Figure_4.01_B31164.jpg)'
- en: '<st c="11706">Figure 4.1: Reconciliation and self-healing by design through
    Kubernetes controllers</st>'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="11706">图 4.1：通过 Kubernetes 控制器实现的对账和自愈设计</st>
- en: '<st c="11790">This figure already shows the core</st> <st c="11826">concepts
    and power of controllers, highlighting how the automated reconciliation loop ensures
    automated self-healing by design.</st> <st c="11954">However, controllers fulfill
    other functions as well: observing cluster and node health, enforcing resource
    limits, running jobs on a schedule, processing life cycle events, and managing
    deployment rollouts</st> <st c="12161">and rollbacks.</st>'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="11790">这个图表已经显示了控制器的核心</st> <st c="11826">概念和能力，强调了自动对账循环如何通过设计自动实现自愈。</st>
    <st c="11954">然而，控制器还有其他功能：观察集群和节点健康，强制资源限制，按计划运行作业，处理生命周期事件，并管理部署的推进</st> <st
    c="12161">和回滚。</st>
- en: <st c="12175">Now, let’s discuss Kubernetes</st> <st c="12205">operators.</st>
    <st c="12217">Operators are a subcategory of controllers and typically focus on
    a specific domain.</st> <st c="12302">A good example is the OpenTelemetry operator,
    which manages OpenTelemetry Collectors and the auto-instrumentation of workloads.</st>
    <st c="12430">This operator uses the same reconciliation loop to ensure that the
    desired configuration for OpenTelemetry is always applied.</st> <st c="12556">If
    the configuration is changed or if there is a problem with the current OpenTelemetry
    Collector or instrumentation, the operator will do its best to keep the promise
    of ensuring that the desired state is the actual state.</st> <st c="12780">To
    learn more, visit the OpenTelemetry website</st> <st c="12827">at</st> [<st c="12830">https://opentelemetry.io/docs/kubernetes/operator/</st>](https://opentelemetry.io/docs/kubernetes/operator/)<st
    c="12880">.</st>
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="12175">现在，让我们讨论 Kubernetes</st> <st c="12205">操作员。</st> <st c="12217">操作员是控制器的一个子类，通常专注于特定领域。</st>
    <st c="12302">一个很好的例子是 OpenTelemetry 操作员，它管理 OpenTelemetry 收集器和工作负载的自动插装。</st>
    <st c="12430">该操作员使用相同的对账循环来确保始终应用 OpenTelemetry 的期望配置。</st> <st c="12556">如果配置更改或当前的
    OpenTelemetry 收集器或插装存在问题，操作员将尽最大努力保持承诺，确保期望状态是实际状态。</st> <st c="12780">要了解更多信息，请访问
    OpenTelemetry 网站</st> <st c="12827">在</st> [<st c="12830">https://opentelemetry.io/docs/kubernetes/operator/</st>](https://opentelemetry.io/docs/kubernetes/operator/)<st
    c="12880">。</st>
- en: <st c="12881">Other use cases for operators typically relate to managing and
    automating core services and applications such as databases, storage, service
    meshes, backup and restore, CI/CD, and</st> <st c="13062">messaging systems.</st>
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 操作器的其他使用案例通常涉及管理和自动化核心服务和应用程序，如数据库、存储、服务网格、备份与恢复、CI/CD 以及消息传递系统。
- en: '<st c="13080">If you want to learn more about</st> <st c="13113">controllers</st>
    <st c="13124">and operators, have a look at the CNCF Operator Working Group and
    their white paper at</st> [<st c="13212">https://github.com/cncf/tag-app-delivery/tree/main/operator-wg</st>](https://github.com/cncf/tag-app-delivery/tree/main/operator-wg)<st
    c="13274">. Another excellent overview can be found on the</st> *<st c="13323">Kong
    Blog</st>* <st c="13332">post titled</st> *<st c="13345">What’s the Difference:
    Kubernetes Controllers vs Operators</st>*<st c="13403">. This blog also lists
    great examples of controllers and</st> <st c="13460">operators:</st> [<st c="13471">https://konghq.com/blog/learning-center/kubernetes-controllers-vs-operators</st>](https://konghq.com/blog/learning-center/kubernetes-controllers-vs-operators)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '如果你想了解更多关于**控制器**和**操作器**的信息，可以查看 CNCF 操作器工作组及其白皮书，网址为 [https://github.com/cncf/tag-app-delivery/tree/main/operator-wg](https://github.com/cncf/tag-app-delivery/tree/main/operator-wg)。另一个精彩的概述可以在
    *Kong Blog* 上找到，标题为 *What’s the Difference: Kubernetes Controllers vs Operators*。这篇博客还列出了控制器和操作器的优秀示例：[https://konghq.com/blog/learning-center/kubernetes-controllers-vs-operators](https://konghq.com/blog/learning-center/kubernetes-controllers-vs-operators)'
- en: <st c="13546">Now that we know more about controllers and operators, let’s have
    a look at how they also ensure built-in resiliency for all Kubernetes componen</st><st
    c="13691">ts</st> <st c="13695">and deployments!</st>
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对控制器和操作器有了更多了解，接下来让我们看看它们是如何确保所有 Kubernetes 组件和部署具备内建弹性的！
- en: <st c="13711">Built-in resilience driven by probes</st>
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 由探针驱动的内建弹性
- en: <st c="13748">Controllers continuously</st> <st c="13774">validate that our
    system is in its desired state by observing the health of the Kubernetes cluster,
    along with its nodes and all deployed Pods.</st> <st c="13918">If one of the observed
    components is not healthy, the system tries to bring it back into a healthy state
    through certain automated actions.</st> <st c="14058">Take Pods, for example.</st>
    <st c="14082">If Pods are no longer healthy, they eventually get restarted to
    ensure the overall system’s resiliency.</st> <st c="14186">Restarting components
    is also often the default action an IT admin would execute following the “</st>*<st
    c="14282">Let’s try to turn it off and on again and see what</st>* *<st c="14334">happens!</st>*<st
    c="14342">” approach.</st>
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器持续验证我们的系统是否处于期望的状态，通过观察 Kubernetes 集群、节点和所有已部署的 Pod 的健康状况。如果其中某个组件不健康，系统会通过某些自动化操作将其恢复到健康状态。以
    Pod 为例。如果 Pod 不再健康，它们最终会被重启，以确保整个系统的弹性。重启组件通常是 IT 管理员在“*我们试试关掉再开看看会发生什么*”方法下执行的默认操作。
- en: <st c="14354">Just like IT admins who probably won’t just turn things on and
    off at random, Kubernetes follows a more sophisticated approach to ensuring the
    resiliency of our Kubernetes clusters, nodes,</st> <st c="14544">and workloads.</st>
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 IT 管理员通常不会随意开关设备，Kubernetes 采取了更为复杂的方法来确保我们的 Kubernetes 集群、节点和工作负载的弹性。
- en: '<st c="14558">Kubelet – a</st> <st c="14571">core component of Kubernetes –
    continuously observes the life cycle and the health state of Pods using several
    types of probes: startup, readiness, and liveness.</st> <st c="14733">The following
    illustration shows the different health states a pod can be in depending on the
    results of startup, readiness, and liveness</st> <st c="14871">probe checks:</st>'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Kubelet——Kubernetes 的核心组件之一——通过使用多种类型的探针（启动探针、就绪探针和存活探针）持续监控 Pod 的生命周期和健康状态。以下图示展示了根据启动、就绪和存活探针检查结果，Pod
    可以处于的不同健康状态：
- en: '![Figure 4.2: Kubelet determining the health status of Pods using probes](img/Figure_4.02_B31164.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2：Kubelet 使用探针判断 Pod 健康状态](img/Figure_4.02_B31164.jpg)'
- en: '<st c="15031">Figure 4.2: Kubelet determining the health status of Pods using
    probes</st>'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：Kubelet 使用探针判断 Pod 健康状态
- en: <st c="15101">Once Pods are no longer healthy, Kubernetes will try to restart
    Pods and bring the Pods back to a healthy state.</st> <st c="15215">There are
    a lot of different settings for both the evaluation of the probe results and the
    restart policies, which you must familiarize yourself with to fully take advantage
    of the built-in resiliency of Kubernetes.</st> <st c="15431">All those settings
    are declared on your Deployment and</st> <st c="15486">Pod definitions.</st>
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15101">一旦 Pods 不再健康，Kubernetes 会尝试重新启动 Pods，并将 Pods 恢复到健康状态。</st> <st
    c="15215">对于探针结果的评估和重启策略，有很多不同的设置，你必须熟悉这些设置，才能充分利用 Kubernetes 的内建弹性。</st> <st
    c="15431">所有这些设置都在你的 Deployment 和</st> <st c="15486">Pod 定义中声明。</st>
- en: <st c="15502">If you want to learn</st> <st c="15524">more about how</st> <st
    c="15539">Kubelet manages the different probes and see some best practices, we
    can recommend checking out blog posts such as the one from Roman Belshevits on
    liveness</st> <st c="15696">probes:</st> [<st c="15704">https://dev.to/otomato_io/liveness-probes-feel-the-pulse-of-the-app-133e</st>](https://dev.to/otomato_io/liveness-probes-feel-the-pulse-of-the-app-133e
    )
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15502">如果你想了解</st> <st c="15524">更多关于</st> <st c="15539">Kubelet 如何管理不同的探针并查看一些最佳实践，我们建议查看一些博文，例如
    Roman Belshevits 关于存活性</st> <st c="15696">探针的文章：</st> [<st c="15704">https://dev.to/otomato_io/liveness-probes-feel-the-pulse-of-the-app-133e</st>](https://dev.to/otomato_io/liveness-probes-feel-the-pulse-of-the-app-133e)
- en: <st c="15776">Another great resource is the official Kubernetes documentation
    on configuring liveness, readiness, and startup</st> <st c="15889">probes:</st>
    [<st c="15897">https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/</st>](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15776">另一个很好的资源是官方 Kubernetes 文档，介绍如何配置存活性、就绪性和启动</st> <st c="15889">探针：</st>
    [<st c="15897">https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/</st>](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)
- en: <st c="15998">Health probes are only valid within Kubernetes</st>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15998">健康探针仅在 Kubernetes 内部有效</st>
- en: <st c="16045">It’s important to understand that all these health checks are
    only done within the Kubernetes cluster and don’t tell us whether a service that
    is exposed via an Ingress to our end users is also considered healthy from the
    end users’ perspective.</st> <st c="16292">A best practice is to additionally
    check health and availability from an “outside-in” perspective as an external
    control.</st> <st c="16414">For example, you can use synthetic tests to validate
    that all exposed endpoints are reachable and return</st> <st c="16519">successful
    responses.</st>
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="16045">重要的是要理解，所有这些健康检查仅在 Kubernetes 集群内部进行，并不能告诉我们通过 Ingress 对外暴露的服务，从最终用户的角度是否也是健康的。</st>
    <st c="16292">最佳实践是额外从“外到内”的角度进行健康检查和可用性检查，作为外部控制。</st> <st c="16414">例如，你可以使用合成测试来验证所有暴露的端点是否可达并返回</st>
    <st c="16519">成功响应。</st>
- en: <st c="16540">Now that we have learned about built-in resiliency for Pods, how
    about more complex constructs, such as applications that are typically made up
    of several different Pods and other objects, such a</st><st c="16736">s Ingress</st>
    <st c="16747">and storage?</st>
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="16540">现在我们已经了解了 Pod 的内建弹性，接下来讨论一些更复杂的构造，例如通常由多个不同的 Pods 和其他对象组成的应用程序，</st><st
    c="16736">如 Ingress</st> <st c="16747">和存储。</st>
- en: <st c="16759">Workload and application life cycle orchestration</st>
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="16759">工作负载和应用生命周期编排</st>
- en: '<st c="16809">As we have learned, Kubernetes provides</st> <st c="16849">built-in
    orchestration of the life cycle of Pods, as explained in the</st> <st c="16919">previous
    section.</st> <st c="16938">However, business applications that we deploy on Kubernetes
    typically have multiple dependent Pods and workloads that make up the application.</st>
    <st c="17081">Take our Financial One ACME as an example: the financial services
    applications deployed to support its customers contain multiple components, such
    as a frontend, a backend, caches, databases, and Ingress.</st> <st c="17286">Unfortunately,
    Kubernetes doesn’t have the concept of applications.</st> <st c="17354">While
    there are several initiatives and working groups to define an application, we
    currently have to rely on other approaches for managing applications, which are
    composites of</st> <st c="17532">multiple components.</st>'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="16809">正如我们所学，Kubernetes 提供了</st> <st c="16849">内建的 Pods 生命周期编排，如</st>
    <st c="16919">前一节所述。</st> <st c="16938">然而，我们在 Kubernetes 上部署的业务应用通常有多个依赖的 Pods
    和工作负载，这些共同构成了应用。</st> <st c="17081">以我们的 Financial One ACME 为例：支持客户的金融服务应用包含多个组件，如前端、后端、缓存、数据库和
    Ingress。</st> <st c="17286">不幸的是，Kubernetes 没有应用的概念。</st> <st c="17354">尽管有多个倡议和工作组在定义应用，但我们目前必须依赖其他方法来管理应用，而这些应用是由</st>
    <st c="17532">多个组件组成的。</st>
- en: <st c="17552">In the</st> *<st c="17560">Batching changes to combat dependencies</st>*
    <st c="17599">section in</st> [*<st c="17611">Chapter 5</st>*](B31164_05.xhtml#_idTextAnchor255)<st
    c="17620">, we will learn about tools such as Crossplane.</st> <st c="17668">Crossplane
    allows you to define so-called composites, which make it easy for application
    owners to define individual components of an application and then deploy individual
    instances, as shown in the</st> <st c="17868">following example:</st>
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17552">在</st> *<st c="17560">批处理变更以应对依赖性</st>* <st c="17599">一节中，</st>
    [*<st c="17611">第五章</st>*](B31164_05.xhtml#_idTextAnchor255)<st c="17620">，我们将学习诸如
    Crossplane 之类的工具。</st> <st c="17668">Crossplane 允许你定义所谓的复合应用，这使得应用所有者能够轻松定义应用的单个组件，然后部署单独的实例，如</st>
    <st c="17868">以下示例所示：</st>
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: <st c="18240">Crossplane</st> <st c="18251">provides</st> <st c="18261">application</st>
    <st c="18273">and infrastructure orchestration and uses the operator pattern to
    continuously ensure that every application instance – as defined in the composite
    – is running</st> <st c="18434">as expected.</st>
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="18240">Crossplane</st> <st c="18251">提供</st> <st c="18261">应用程序</st>
    <st c="18273">和基础设施编排，使用操作符模式持续确保每个应用程序实例——如在复合应用中定义——按预期运行</st> <st c="18434">。
- en: <st c="18446">Another tool we will learn more about</st> <st c="18484">in</st>
    [*<st c="18488">Chapter 5</st>*](B31164_05.xhtml#_idTextAnchor255) <st c="18497">is
    the</st> **<st c="18505">Keptn</st>** <st c="18510">CNCF project.</st> <st c="18525">Keptn
    provides automated application-aware life cycle orchestration and observability.</st>
    <st c="18612">It gives you the option to declaratively define pre- and post-deployment
    checks (validate dependencies, run tests, evaluate health, enforce SLO-based quality
    gates, and so on) without having to write your own Kubernetes operator to implement
    those actions.</st> <st c="18869">Keptn also provides automated deployment observability
    to better understand how many deployments happen, how many are successful, and
    where and why they fail by emitting OpenTelemetry traces and metrics for easier
    troubleshooting</st> <st c="19099">and reporting.</st>
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="18446">另一个我们将在</st> [*<st c="18488">第五章</st>*](B31164_05.xhtml#_idTextAnchor255)
    <st c="18497">中深入了解的工具是</st> **<st c="18505">Keptn</st>** <st c="18510">CNCF 项目。</st>
    <st c="18525">Keptn 提供自动化的应用感知生命周期编排和可观察性。</st> <st c="18612">它让你可以声明性地定义部署前后检查（验证依赖关系、运行测试、评估健康状况、强制基于
    SLO 的质量门控等），而无需编写自己的 Kubernetes 操作符来实现这些操作。</st> <st c="18869">Keptn 还提供自动化的部署可观察性，以更好地理解发生了多少次部署，成功了多少次，以及它们在哪些地方和为何失败，通过发出
    OpenTelemetry 跟踪和指标，方便故障排除</st> <st c="19099">和报告。</st>
- en: <st c="19113">Kubernetes provides a lot of the building blocks for building
    resilient systems.</st> <st c="19195">While you can write your own operators to
    expand this to your own problem domain, you can also use existing CNCF tools such</st>
    <st c="19318">as Crossplane</st> <st c="19332">or Keptn as they provide an easier
    declarative way to apply the concept of promise theory to</st> <st c="19426">more
    complex</st> <st c="19439">applications and</st> <st c="19456">infrastructure
    compositions.</st>
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="19113">Kubernetes 提供了构建韧性系统的许多基础构件。</st> <st c="19195">虽然你可以编写自己的操作员来扩展到你自己的问题领域，但你也可以使用现有的
    CNCF 工具，例如</st> <st c="19318">Crossplane</st> <st c="19332">或 Keptn，因为它们提供了一种更简单的声明性方式，应用承诺理论的概念，来处理</st>
    <st c="19426">更复杂的</st> <st c="19439">应用程序和</st> <st c="19456">基础设施组成。</st>
- en: <st c="19484">Restarting components is one way of ensuring resiliency, but there
    are more.</st> <st c="19562">Let’s have a look at auto-scaling, which solves another
    critical</st> <st c="19627">problem in</st> <st c="19638">dynamic environments!</st>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="19484">重启组件是确保系统韧性的一种方式，但还有更多的方法。</st> <st c="19562">让我们来看看自动扩展，它解决了动态环境中另一个关键的</st>
    <st c="19627">问题！</st>
- en: <st c="19659">Auto-scaling clusters and workloads</st>
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="19659">自动扩展集群和工作负载</st>
- en: '<st c="19695">In most industries, the load</st> <st c="19725">expected on a
    system is not equally distributed across every day of the year.</st> <st c="19803">There
    is always some type of seasonality: retail gets spikes on Black Friday and Cyber
    Monday, tax services get spikes on tax day, and finance often spikes when paychecks
    are coming.</st> <st c="19986">The same is true for our own Financial One ACME
    customers.</st> <st c="20045">As a financial services organization, there is always
    some basic traffic from end users, but there will be spikes at the beginning and
    end of</st> <st c="20187">the month.</st>'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="19695">在大多数行业中，系统预期的负载在一年中的每一天分布不均。</st> <st c="19725">总会有某种季节性波动：零售在黑色星期五和网络星期一有激增，税务服务在报税日有激增，金融服务通常在发放工资时会有激增。</st>
    <st c="19986">我们的 Financial One ACME 客户也面临同样的情况。</st> <st c="20045">作为一个金融服务组织，始终会有来自终端用户的基本流量，但在月初和月末</st>
    <st c="20187">会有流量激增。</st>
- en: '<st c="20197">Kubernetes</st> <st c="20209">provides several ways to scale
    application</st> <st c="20251">workloads: manually (e.g., setting ReplicaSets)
    or automatically through tools such as</st> **<st c="20339">Horizontal Pod Autoscaler</st>**
    <st c="20364">(</st>**<st c="20366">HPA</st>**<st c="20369">),</st> **<st c="20373">Vertical
    Pod Autoscaler</st>** <st c="20396">(</st>**<st c="20398">VPA</st>**<st c="20401">),
    or</st> **<st c="20408">Kubernetes Event Driven Autoscaler</st>** <st c="20442">(</st>**<st
    c="20444">KEDA</st>**<st c="20448">).</st> <st c="20452">Those</st> <st c="20457">scaling
    options allow you to scale when your workloads run low on CPU or memory, when
    applications see a spike in incoming traffic, or when response time is starting</st>
    <st c="20624">to increase!</st>'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20197">Kubernetes</st> <st c="20209">提供了几种扩展应用</st> <st c="20251">工作负载的方式：手动（例如，设置
    ReplicaSets）或通过工具自动扩展，如</st> **<st c="20339">Horizontal Pod Autoscaler</st>**
    <st c="20364">(</st>**<st c="20366">HPA</st>**<st c="20369">)、</st> **<st c="20373">Vertical
    Pod Autoscaler</st>** <st c="20396">(</st>**<st c="20398">VPA</st>**<st c="20401">)，或者</st>
    **<st c="20408">Kubernetes Event Driven Autoscaler</st>** <st c="20442">(</st>**<st
    c="20444">KEDA</st>**<st c="20448">)。</st> <st c="20452">这些</st> <st c="20457">扩展选项允许你在工作负载的
    CPU 或内存不足时、应用程序流量激增时，或者响应时间开始</st> <st c="20624">增加时进行扩展！</st>
- en: <st c="20636">Besides workloads, you can and most likely have to also scale
    the size of your clusters and nodes through tools</st> <st c="20749">such</st>
    <st c="20753">as</st> **<st c="20757">Cluster Autoscaler</st>** <st c="20775">(</st>**<st
    c="20777">CA</st>**<st c="20779">) or</st> **<st c="20785">Karpenter</st>** *<st
    c="20794">[2]</st>*<st c="20798">, or through options available via your managed
    Kubernetes</st> <st c="20857">cloud vendor.</st>
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20636">除了工作负载，你还可以并且很可能需要通过工具来扩展你的集群和节点的规模，</st> <st c="20749">例如</st>
    **<st c="20757">Cluster Autoscaler</st>** <st c="20775">(</st>**<st c="20777">CA</st>**<st
    c="20779">) 或</st> **<st c="20785">Karpenter</st>** *<st c="20794">[2]</st>*<st
    c="20798">，或者通过你所使用的托管 Kubernetes 云服务提供的选项。</st>
- en: <st c="20870">As a platform engineering team, you need to make yourself familiar
    with all the different options but also be aware of all</st> <st c="20994">the
    considerations:</st>
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20870">作为一个平台工程团队，你需要熟悉所有不同的选项，同时也要注意所有的</st> <st c="20994">考虑因素：</st>
- en: '**<st c="21013">Setting limits</st>**<st c="21028">: Don’t allow applications
    to scale endlessly.</st> <st c="21076">You have options to enforce maximum limits
    per application, workload, namespaces,</st> <st c="21158">and more.</st>'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="21013">设置限制</st>**<st c="21028">：不要允许应用程序无限制地扩展。</st> <st c="21076">您可以选择为每个应用程序、工作负载、命名空间等强制执行最大限制。</st>'
- en: '**<st c="21167">Cost control</st>**<st c="21180">: Auto-scaling is great but
    has a price tag.</st> <st c="21226">Make sure to report costs to the</st> <st
    c="21259">application owners.</st>'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="21167">成本控制</st>**<st c="21180">：自动扩展是很棒的，但也有其成本。</st> <st c="21226">确保将成本报告给</st>
    <st c="21259">应用程序所有者。</st>'
- en: '**<st c="21278">Scale down</st>**<st c="21289">: Scaling up is easy!</st> <st
    c="21312">Make sure to also define indicators for when to scale down.</st> <st
    c="21372">This</st> <st c="21377">will keep costs</st> <st c="21393">under control.</st>'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="21278">缩小规模</st>**<st c="21289">：扩展很容易！</st> <st c="21312">确保同时定义何时缩小规模的指标。</st>
    <st c="21372">这样</st> <st c="21377">可以保持成本</st> <st c="21393">在可控范围内。</st>'
- en: <st c="21407">To learn more about them, please review the</st> <st c="21452">documentation:</st>
    [<st c="21467">https://kubernetes.io/docs/concepts/workloads/autoscaling/</st>](https://kubernetes.io/docs/concepts/workloads/autoscaling/
    )
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21407">要了解更多信息，请查看以下</st> <st c="21452">文档：</st> [<st c="21467">https://kubernetes.io/docs/concepts/workloads/autoscaling/</st>](https://kubernetes.io/docs/concepts/workloads/autoscaling/)
- en: <st c="21525">Now that we have learned about the options for scaling within
    a Kubernetes environment, how about sc</st><st c="21626">aling out to other</st>
    <st c="21646">Kubernetes clusters?</st>
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21525">现在我们已经了解了在Kubernetes环境中扩展的选项，那么如何扩展到其他</st><st c="21626">Kubernetes集群呢？</st>
    <st c="21646">我们来看看这个问题。</st>
- en: <st c="21666">Declare once – run anywhere (in theory)</st>
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="21666">一次声明——到处运行（理论上）</st>
- en: <st c="21706">The promise of Kubernetes as an open</st> <st c="21743">standard
    is that any declared state (Ingress, workloads, secrets, storage, network, etc.)
    will behave the same whether you run it on a single cluster or on multiple clusters
    to meet certain requirements, such as the separation of stages (dev, staging,
    and production) or the separation of regions (US, Europe,</st> <st c="22054">and
    Asia).</st>
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21706">Kubernetes作为开放标准的承诺是，任何声明的状态（Ingress、工作负载、机密、存储、网络等）无论是在单一集群中运行，还是在多个集群中运行以满足特定需求（如阶段分离：开发、预发布和生产，或区域分离：美国、欧洲，</st>
    <st c="22054">亚洲等）都会表现相同。</st>
- en: <st c="22064">The same promise holds true in theory whether you operate your
    own Kubernetes cluster, use OpenShift, or use a managed Kubernetes service from
    one of the cloud vendors.</st> <st c="22234">What does</st> *<st c="22244">in
    theory</st>* <st c="22253">mean here?</st> <st c="22265">There are some specific
    technical differences between the different offerings you need to take into consideration.</st>
    <st c="22380">Depending on the offering, networking or storage may act slightly
    differently because the underlying implementation depends on the cloud vendor.</st>
    <st c="22525">Certain offerings will also come with specific versions of core
    Kubernetes services, services meshes, and operators that come with a managed installation.</st>
    <st c="22680">Some offerings require you to use vendor-specific annotations to
    configure the behavior of certain services.</st> <st c="22789">That’s why applying
    the same declarative state definition across different vendors will,</st> *<st
    c="22878">in theory</st>*<st c="22887">, work – in practice, you have to consider
    certain small differences that require some</st> <st c="22974">vendor-specific
    configuration!</st>
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22064">无论是操作您自己的Kubernetes集群，使用OpenShift，还是使用云供应商提供的托管Kubernetes服务，这一承诺在理论上都是成立的。</st>
    <st c="22234">那么，*<st c="22244">在理论上</st>*<st c="22253">是什么意思呢？</st> <st c="22265">不同的服务之间确实有一些具体的技术差异，您需要加以考虑。</st>
    <st c="22380">根据不同的服务，网络或存储可能会有所不同，因为底层实现依赖于云供应商。</st> <st c="22525">某些服务还会提供核心Kubernetes服务、服务网格和运维工具的特定版本，这些都会随着托管安装一起提供。</st>
    <st c="22680">一些服务要求您使用供应商特定的注释来配置某些服务的行为。</st> <st c="22789">这就是为什么在不同供应商之间应用相同的声明式状态定义理论上是可行的——但实际上，您需要考虑一些小的差异，并进行一些</st>
    *<st c="22878">在理论上</st>*<st c="22887">的调整，这些差异需要特定的供应商配置！</st>
- en: <st c="23004">As the technical details and differences are constantly changing,
    it wouldn’t make sense to provide a current side-by-side comparison as part of
    this book.</st> <st c="23161">What you must understand is that while, in theory,
    you can take any Kubernetes object and deploy it on any flavor of Kubernetes,
    the outcome and behavior might be slightly different depending on where you deploy
    it.</st> <st c="23377">That’s why we suggest doing some technical research on
    the chosen target Kubernetes offering and how it differs from other offerings
    in case you want to go for multi-cloud/multi-Kubernetes, because the same Kubernetes
    objects might behave</st> <st c="23616">slightly differently!</st>
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23004">由于技术细节和差异不断变化，因此在本书中提供当前的对比并不合适。</st> <st c="23161">您必须理解的是，尽管理论上您可以将任何Kubernetes对象部署到任何版本的Kubernetes上，但结果和行为可能会有所不同，具体取决于您部署的环境。</st>
    <st c="23377">这就是为什么我们建议对所选的Kubernetes目标平台进行一些技术研究，并了解它与其他平台的差异，特别是当您想要进行多云/多Kubernetes部署时，因为相同的Kubernetes对象可能会有</st>
    <st c="23616">细微的差异！</st>
- en: <st c="23637">The good news is</st> <st c="23655">that the global community
    is working to solve this problem by providing better guidance and tools to make
    the</st> *<st c="23765">declare once – run anywhere</st>* <st c="23792">promise</st>
    <st c="23801">a reality.</st>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23637">好消息是</st> <st c="23655">全球社区正在通过提供更好的指导和工具来解决这个问题，使得</st> *<st
    c="23765">声明一次——随处运行</st>* <st c="23792">的承诺</st> <st c="23801">成为现实。</st>
- en: '<st c="23811">We’ve looked at a lot of the benefits of picking Kubernetes as
    the core platform.</st> <st c="23894">However, there is another good reason why
    Kubernetes has seen such great adoption over the past 10 years since its first
    re</st><st c="24017">lease: the global community and</st> <st c="24050">the CNCF!</st>'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23811">我们已经看到了选择Kubernetes作为核心平台的许多好处。</st> <st c="23894">然而，还有另一个重要的原因，解释了为什么Kubernetes在过去10年里自首次发布以来获得了如此广泛的采用：那就是全球社区和</st>
    <st c="24017">CNCF！</st>
- en: <st c="24059">Global community and CNCF</st>
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="24059">全球社区和CNCF</st>
- en: <st c="24085">Kubernetes</st> <st c="24096">was announced by Google in June
    2014, and version 1.0 was released on July 21, 2015\.</st> <st c="24182">Google
    then worked with the Linux Foundation and formed the CNCF with Kubernetes as its
    initial project!</st> <st c="24287">Since then, the community and the projects
    have taken the world</st> <st c="24351">by storm!</st>
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="24085">Kubernetes</st> <st c="24096">于2014年6月由Google宣布，版本1.0于2015年7月21日发布。</st>
    <st c="24182">随后，Google与Linux基金会合作，并成立了CNCF，将Kubernetes作为其初始项目！</st> <st c="24287">自那时以来，社区和项目迅速席卷了全球！</st>
- en: <st c="24360">10 years later (at the time of writing this book), the CNCF has
    188 projects, 244,000 contributors, 16.6 million contributions, and members in
    193 countries worldwide.</st> <st c="24529">Many presentations that introduce
    Kubernetes and the</st> <st c="24582">CNCF often start by showing the CNCF</st>
    <st c="24619">landscape:</st> [<st c="24630">https://landscape.cncf.io/</st>](https://landscape.cncf.io/
    )
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="24360">十年后（在撰写本书时），CNCF拥有188个项目，244,000名贡献者，1660万次贡献，成员遍布全球193个国家。</st>
    <st c="24529">许多介绍Kubernetes和CNCF的演示文稿通常从展示CNCF</st> <st c="24582">的生态图开始：</st>
    [<st c="24630">https://landscape.cncf.io/</st>](https://landscape.cncf.io/)
- en: <st c="24656">While the landscape is impressive, it has also been the source
    of many memes about how hard and complex it is to navigate the landscape of all
    projects this global community is working on.</st> <st c="24846">However, don’t
    be scared.</st> <st c="24872">The global CNCF community is part of the Linux Foundation
    and has the mission to provide support, oversight, and direction for fast-growing
    cloud-native projects, including Kubernetes, Envoy,</st> <st c="25064">and Prometheus.</st>
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="24656">虽然生态图令人印象深刻，但它也成为了许多关于如何在这个全球社区的所有项目中导航的艰难和复杂性的梗的来源。</st> <st
    c="24846">然而，不要害怕。</st> <st c="24872">全球CNCF社区是Linux基金会的一部分，其使命是为快速发展的云原生项目提供支持、监督和指导，包括Kubernetes、Envoy、</st>
    <st c="25064">和Prometheus。</st>
- en: <st c="25079">Here are a few things you should be aware of because they will
    help you navigate the ever-growing list of CNCF projects in the</st> <st c="25207">project
    landscape:</st>
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25079">以下是您需要注意的一些事项，它们将帮助您在不断增长的CNCF项目列表中导航：</st>
- en: '**<st c="25225">Project status</st>**<st c="25240">: CNCF</st> <st c="25248">actively
    tracks the status and activity of every project.</st> <st c="25306">The number
    of contributors and adopters, as well as how active development is for a project,
    are good indicators of whether you should look closer at a project.</st> <st c="25467">Projects
    that are stale, only have a single maintainer, or hardly have any adopters might
    not be of any use if you are deciding on tools that will help you for the long
    term in</st> <st c="25644">your platform.</st>'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="25225">项目状态</st>**<st c="25240">：CNCF</st> <st c="25248">会积极跟踪每个项目的状态和活动。</st>
    <st c="25306">贡献者和采纳者的数量，以及项目的开发活跃度，是你是否应该进一步关注某个项目的良好指标。</st> <st c="25467">如果一个项目处于停滞状态，只有一个维护者，或者几乎没有采纳者，那么对于你选择长期支持的平台工具而言，可能并没有什么用处。</st>
    <st c="25644">你的平台。</st>'
- en: '**<st c="25658">Maturity level</st>**<st c="25673">: The CNCF also specifies
    a maturity level</st> <st c="25716">of sandbox, incubating, or graduated, which
    corresponds to the Innovators, Early Adopters, and Early Majority tiers of</st>
    <st c="25836">the</st> *<st c="25840">Crossing the Chasm</st>* <st c="25858">diagram
    (</st>[<st c="25868">https://en.wikipedia.org/wiki/Crossing_the_Chasm</st>](https://en.wikipedia.org/wiki/Crossing_the_Chasm)<st
    c="25917">).</st> <st c="25921">Graduated projects have been adopted widely across
    various industries and are a safe choice for the use cases they support.</st>
    <st c="26045">Incubating projects have crossed over from a technical playground
    to seeing good adoption with growing numbers of a diverse set of maintainers.</st>
    <st c="26189">To learn more about the criteria for CNCF maturity and to see who
    is at which level, check out the official site</st> <st c="26302">at</st> [<st
    c="26305">https://www.cncf.io/project-metrics/</st>](https://www.cncf.io/project-metrics/)<st
    c="26341">.</st>'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="25658">成熟度级别</st>**<st c="25673">：CNCF 还指定了成熟度级别</st> <st c="25716">，包括沙箱、孵化、或已毕业，分别对应于《创新者的窄道》中的创新者、早期采纳者和早期多数这几个层级</st>
    <st c="25836">的</st> *<st c="25840">跨越鸿沟</st>* <st c="25858">图表（</st>[<st c="25868">https://en.wikipedia.org/wiki/Crossing_the_Chasm</st>](https://en.wikipedia.org/wiki/Crossing_the_Chasm)<st
    c="25917">）。</st> <st c="25921">已毕业的项目已经在多个行业广泛采纳，是其支持的用例的安全选择。</st> <st c="26045">孵化中的项目已经从技术试验场跨越到获得良好采纳，并且维护者的数量也在不断增长，维护者的背景也日益多样化。</st>
    <st c="26189">想了解更多关于 CNCF 成熟度的标准，并查看各项目处于哪个级别，可以访问官方站点</st> <st c="26302">：</st>
    [<st c="26305">https://www.cncf.io/project-metrics/</st>](https://www.cncf.io/project-metrics/)<st
    c="26341">。</st>'
- en: '`<st c="26512">ADOPTERS.md</st>` <st c="26523">file, which every CNCF project
    typically has in its GitHub repository.</st> <st c="26595">If you decide to adopt
    one of those projects, we encourage you to also add your name to the list of adopters
    by opening up a pull request.</st> <st c="26734">This helps the project and will
    help other organizations decide whether this is a project</st> <st c="26824">worth
    pursuing!</st>'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="26512">ADOPTERS.md</st>` <st c="26523">文件，几乎每个 CNCF 项目在其 GitHub 仓库中都有。</st>
    <st c="26595">如果你决定采纳这些项目中的一个，我们鼓励你通过提交拉取请求将你的名字添加到采纳者名单中。</st> <st c="26734">这不仅有助于项目，也有助于其他组织决定该项目是否</st>
    <st c="26824">值得追求！</st>'
- en: <st c="26839">Kubernetes is vital because of its community</st>
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26839">Kubernetes 至关重要，因为它有一个强大的社区</st>
- en: <st c="26884">While Kubernetes has a strong technology base, it is really the
    community and the ecosystem that was built over the past 10+ years that makes
    Kubernetes a viable option for platform engineers to use as their</st> <st c="27093">core
    platform.</st>
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26884">尽管 Kubernetes 拥有强大的技术基础，但正是过去 10 多年中建立起来的社区和生态系统使 Kubernetes 成为平台工程师将其作为</st>
    <st c="27093">核心平台的可行选择。</st>
- en: '<st c="27107">We have now learned more about what Kelsey Hightower meant when
    he said: “</st>*<st c="27182">Kubernetes is a platform to build platforms.</st>
    <st c="27228">It’s a start but not the endgame</st>*<st c="27260">.” There are
    many benefits of picking</st> <st c="27298">Kubernetes as the core platform, especially
    as it is built on the concept</st> <st c="27372">of</st> *<st c="27375">promise
    theory</st>*<st c="27389">. Kubernetes provides automated resiliency, scaling,
    and life cycle management of components.</st> <st c="27483">The ever-growing community
    provides solutions to many common problems through hundreds of open source CNCF
    projects that every organization</st> <st c="27623">can use.</st>'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27107">我们现在已经更清楚地理解了 Kelsey Hightower 所说的：“</st>*<st c="27182">Kubernetes
    是一个用来构建平台的平台。</st> <st c="27228">这是一个开始，但不是最终目标</st>*<st c="27260">。”选择 Kubernetes
    作为核心平台有很多好处，特别是因为它是基于</st> *<st c="27375">承诺理论</st>*<st c="27389">的概念构建的。Kubernetes
    提供了组件的自动化弹性、扩展性和生命周期管理。</st> <st c="27483">不断壮大的社区通过数百个开源 CNCF 项目为许多常见问题提供了解决方案，任何组织</st>
    <st c="27623">都可以使用。</st>
- en: <st c="27631">While we often focus on the benefit of Kubernetes for deploying
    and orchestrating applications, let’s have a look at how we can use Kubernetes
    to lift our infrastructure capabilities into our</st> <st c="27824">future platform!</st>
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27631">虽然我们通常关注 Kubernetes 在部署和编排应用程序方面的好处，但让我们看看如何利用 Kubernetes 将我们的基础设施能力提升到未来平台！</st>
- en: <st c="27840">Leveraging and managing Kubernetes Infrastructure Capabilities</st>
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="27840">利用和管理 Kubernetes 基础设施能力</st>
- en: <st c="27903">Back in</st> [*<st c="27912">Chapter 2</st>*](B31164_02.xhtml#_idTextAnchor055)<st
    c="27921">, you were introduced to the Platform Reference Components model and
    the capability plane.</st> <st c="28012">When we are writing about lifting infrastructure
    capabilities to Kubernetes, the end user becomes aware of those capabilities when
    using the platform.</st> <st c="28163">We must differentiate between resources
    that need to be integrated with Kubernetes and those configured by specifications
    deployed to Kubernetes and manipulated or created new resources outside of the
    cluster.</st> <st c="28373">In the following figure, you can find examples in
    the resource integration and network section that require a solid integration;
    otherwise, they actively prevent a useful and</st> <st c="28548">functioning platform.</st>
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27903">回顾一下</st> [*<st c="27912">第 2 章</st>*](B31164_02.xhtml#_idTextAnchor055)<st
    c="27921">，你已经了解了平台参考组件模型和能力平面。</st> <st c="28012">当我们谈论将基础设施能力提升到 Kubernetes
    时，最终用户在使用该平台时会意识到这些能力。</st> <st c="28163">我们必须区分需要与 Kubernetes 集成的资源和那些通过规范配置并部署到
    Kubernetes 的资源，以及在集群外部操作或创建的新资源。</st> <st c="28373">在下图中，您可以找到需要坚实集成的资源整合和网络部分的示例；否则，它们会主动阻碍平台的有用性和</st>
    <st c="28548">功能。</st>
- en: '![Figure 4.3: Capability plane with example tools](img/Figure_4.03_B31164.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3：带有示例工具的能力平面](img/Figure_4.03_B31164.jpg)'
- en: '<st c="28825">Figure 4.3: Capability plane with example tools</st>'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28825">图 4.3：带有示例工具的能力平面</st>
- en: <st c="28872">Integrating infrastructure resources</st>
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="28872">集成基础设施资源</st>
- en: <st c="28909">We will discuss the basic components and design decisions you
    must make for the platform’s underlying technologies.</st> <st c="29026">Firstly,
    due to Kubernetes’ power, you are more flexible in your tooling and can extend
    it as needed.</st> <st c="29128">This is especially helpful when you’re adjusting
    the</st> <st c="29181">platform’s capabilities for different</st> <st c="29219">use
    cases.</st>
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28909">我们将讨论平台底层技术的基本组件和你必须做出的设计决策。</st> <st c="29026">首先，由于 Kubernetes
    的强大，你在工具选择上更加灵活，并可以根据需要进行扩展。</st> <st c="29128">当你调整平台能力以适应不同</st> <st c="29181">的使用案例时，这尤其有帮助。</st>
- en: <st c="29229">Container storage interface</st>
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="29229">容器存储接口</st>
- en: <st c="29257">The</st> **<st c="29262">Container Storage Interface</st>** <st
    c="29289">(</st>**<st c="29291">CSI</st>**<st c="29294">) provides</st> <st c="29306">access
    to the storage technology that is attached to the cluster or the nodes running
    the cluster.</st> <st c="29405">In the CSI developer documentation</st> *<st c="29440">[3]</st>*<st
    c="29443">, you can find a driver for almost every storage provider.</st> <st
    c="29502">The list contains</st> <st c="29520">cloud provider drivers such as
    AWS</st> **<st c="29555">Elastic Block Storage</st>** <st c="29576">(</st>**<st
    c="29578">EBS</st>**<st c="29581">), software-defined storage such as Ceph, or
    a connector for commercial solutions such as NetApp.</st> <st c="29680">In addition,
    the CSI driver also supports tool-specific drivers such as cert-manager and HashiCorp
    Vault.</st> <st c="29786">In short, the CSI is vital to any data that should live
    longer than the container it belongs to and is not stored in a database, or is
    needed for</st> <st c="29932">database storage.</st>
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="29257">该</st> **<st c="29262">容器存储接口</st>** <st c="29289">(</st>**<st
    c="29291">CSI</st>**<st c="29294">) 提供</st> <st c="29306">对附加到集群或运行集群的节点的存储技术的访问。</st>
    <st c="29405">在CSI开发者文档中</st> *<st c="29440">[3]</st>*<st c="29443">，你可以找到几乎所有存储提供商的驱动程序。</st>
    <st c="29502">该列表包含</st> <st c="29520">云提供商的驱动程序，例如 AWS</st> **<st c="29555">弹性块存储</st>**
    <st c="29576">(</st>**<st c="29578">EBS</st>**<st c="29581">)，软件定义存储如 Ceph，或者商用解决方案如
    NetApp 的连接器。</st> <st c="29680">此外，CSI 驱动程序还支持特定工具的驱动程序，如 cert-manager 和 HashiCorp
    Vault。</st> <st c="29786">简而言之，CSI 对于任何需要存活时间超过其所属容器且未存储在数据库中的数据至关重要，或者对于</st>
    <st c="29932">数据库存储是必需的。</st>
- en: <st c="29949">The installation of the driver depends on the infrastructure and
    storage technology.</st> <st c="30035">For a cloud provider, for example, you
    usually require</st> <st c="30090">the following:</st>
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="29949">驱动程序的安装依赖于基础设施和存储技术。</st> <st c="30035">例如，对于云提供商，你通常需要</st> <st
    c="30090">以下内容：</st>
- en: <st c="30104">A service account or</st> <st c="30126">permission policies</st>
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="30104">服务帐户或</st> <st c="30126">权限策略</st>
- en: <st c="30145">Configuration for startup taints</st> <st c="30179">and tolerations</st>
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="30145">启动污点</st> <st c="30179">和容忍配置</st>
- en: <st c="30194">Pre-installed</st> <st c="30209">external snapshotter</st>
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="30194">预安装的</st> <st c="30209">外部快照工具</st>
- en: <st c="30229">The driver</st> <st c="30241">installation itself</st>
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="30229">驱动程序</st> <st c="30241">安装本身</st>
- en: <st c="30260">Due to their complexity, these components are deployed with Helm
    or other package management solutions.</st> <st c="30365">Sometimes, they require
    more privileges on the node, which can be a security concern when designing the
    platform.</st> <st c="30479">You will also need to consider how storage will</st>
    <st c="30527">be accessed:</st>
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30260">由于其复杂性，这些组件通常通过 Helm 或其他软件包管理工具进行部署。</st> <st c="30365">有时，它们需要更多的节点权限，这在平台设计时可能带来安全隐患。</st>
    <st c="30479">你还需要考虑如何访问存储：</st>
- en: '**<st c="30539">ReadWriteOnce</st>** <st c="30553">(</st>**<st c="30555">RWO</st>**<st
    c="30558">): One</st> <st c="30566">Pod claims a portion of the available storage,
    which it can read from and write to, while other Pods cannot access it unless
    the original Pod releases</st> <st c="30717">the storage.</st>'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="30539">ReadWriteOnce</st>** <st c="30553">(</st>**<st c="30555">RWO</st>**<st
    c="30558">)：一个</st> <st c="30566">Pod 声明了一部分可用存储，它可以进行读写操作，而其他 Pods 不能访问该存储，除非原
    Pod 释放</st> <st c="30717">该存储。</st>'
- en: '**<st c="30729">ReadWriteMany</st>** <st c="30743">(</st>**<st c="30745">RWM</st>**<st
    c="30748">): Multiple</st> <st c="30761">Pods can claim one portion of the available
    storage.</st> <st c="30814">They can read and write to it, and share that storage</st>
    <st c="30868">with others.</st>'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="30729">ReadWriteMany</st>** <st c="30743">(</st>**<st c="30745">RWM</st>**<st
    c="30748">)：多个</st> <st c="30761">Pods 可以声明一部分可用存储。</st> <st c="30814">它们可以进行读写操作，并与其他
    Pods 共享该存储。</st>'
- en: '**<st c="30880">ReadOnlyMany</st>** <st c="30893">(</st>**<st c="30895">ROM</st>**<st
    c="30898">): Multiple</st> <st c="30910">Pods can claim one portion of the storage,
    but only to read</st> <st c="30971">from it.</st>'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="30880">ReadOnlyMany</st>** <st c="30893">(</st>**<st c="30895">ROM</st>**<st
    c="30898">)：多个</st> <st c="30910">Pods 可以声明一部分存储，但只能从中读取数据。</st> <st c="30971">从中读取。</st>'
- en: '**<st c="30979">ReadWriteOncePod</st>** <st c="30996">(</st>**<st c="30998">RWOP</st>**<st
    c="31002">): Can be</st> <st c="31012">claimed by only one Pod; no other Pod can
    take it, and it allows read and</st> <st c="31087">write operations.</st>'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="30979">ReadWriteOncePod</st>** <st c="30996">(</st>**<st c="30998">RWOP</st>**<st
    c="31002">)：仅能被一个 Pod 声明；没有其他 Pod 可以占用它，并允许读写操作。</st>'
- en: <st c="31104">The overview of the</st> <st c="31125">CSI driver provides further
    information on which access modes are supported.</st> <st c="31202">As there is
    no one-size-fits-all solution, you have to make your options transparent to the
    user and explain how to</st> <st c="31318">use them.</st>
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="31104">CSI 驱动程序的概述提供了更多关于哪些访问模式被支持的信息。</st> <st c="31202">由于没有一刀切的解决方案，你需要将你的选项透明地展示给用户，并解释如何</st>
    <st c="31318">使用它们。</st>
- en: <st c="31327">The core elements to know and to understand for your users are</st>
    `<st c="31391">StorageClass</st>`<st c="31403">,</st> `<st c="31405">PersistentVolume</st>`<st
    c="31421">, and</st> `<st c="31427">PersistentVolumeClaim</st>`<st c="31448">.
    When a Pod/Deployment requires a volume,</st> `<st c="31491">StorageClass</st>`
    <st c="31503">will trigger the creation of a new volume.</st> <st c="31547">In
    case a Pod/Deployment has already claimed a volume once, and it didn’t get destroyed,
    the Kubernetes control plane will re-assign the volume to</st> <st c="31694">the
    Pod/Deployment.</st>
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="31327">用户需要了解和理解的核心元素是</st> `<st c="31391">StorageClass</st>`<st c="31403">，</st>
    `<st c="31405">PersistentVolume</st>`<st c="31421">，以及</st> `<st c="31427">PersistentVolumeClaim</st>`<st
    c="31448">。当一个 Pod/Deployment 需要一个卷时，</st> `<st c="31491">StorageClass</st>` <st
    c="31503">将触发新卷的创建。</st> <st c="31547">如果一个 Pod/Deployment 已经声明了一个卷，而且它没有被销毁，Kubernetes
    控制平面将会重新分配该卷给</st> <st c="31694">Pod/Deployment。</st>
- en: '![Figure 4.4: Dynamically provisioning new persistent volumes](img/Figure_4.04_B31164.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4：动态配置新的持久卷](img/Figure_4.04_B31164.jpg)'
- en: '<st c="31893">Figure 4.4: Dynamically provisioning new persistent volumes</st>'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="31893">图 4.4：动态配置新的持久卷</st>
- en: <st c="31952">You define</st> `<st c="31964">StorageClass</st>` <st c="31976">as
    the platform</st> <st c="31992">team, ideally collaborating with your storage
    experts.</st> <st c="32048">The following example highlights the common definition
    of</st> `<st c="32106">StorageClass</st>`<st c="32118">. There is plenty of room
    to make a mistake in the configuration; for example, setting no</st> `<st c="32208">reclaimPolicy</st>`
    <st c="32221">will, by default, delete the later created and attached volume.</st>
    <st c="32286">However,</st> `<st c="32295">StorageClass</st>` <st c="32307">supports
    you in the dynamic creation of volumes for user-requested</st> `<st c="32375">PersistantVolumeClaim</st>`
    <st c="32396">and is therefore a strong enabler</st> <st c="32431">of self-service:</st>
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="31952">你作为平台团队定义</st> `<st c="31964">StorageClass</st>` <st c="31976">，理想情况下，与存储专家合作。</st>
    <st c="32048">以下示例突出显示了</st> `<st c="32106">StorageClass</st>`<st c="32118">的常见定义。配置中有很多容易出错的地方；例如，未设置</st>
    `<st c="32208">reclaimPolicy</st>` <st c="32221">将默认删除后创建并附加的卷。</st> <st c="32286">然而，</st>
    `<st c="32295">StorageClass</st>` <st c="32307">支持你为用户请求的</st> `<st c="32375">PersistantVolumeClaim</st>`
    <st c="32396">动态创建卷，因此是自助服务的强大推动力：</st>
- en: '[PRE2]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: <st c="32842">The downside of this is that you have to consider human error,
    which can take down your platf</st><st c="32936">orm by filling up the storage
    until the</st> <st c="32977">system freezes.</st>
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="32842">这带来的缺点是你需要考虑人为错误，错误可能会导致存储被填满，从而使</st><st c="32936">平台崩溃，直到</st>
    <st c="32977">系统冻结。</st>
- en: <st c="32992">CSI challenges</st>
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="32992">CSI 挑战</st>
- en: <st c="33007">The definition of a CSI is</st> <st c="33035">a fairly new approach,
    but the underlying technologies of storage management and software-defined storage
    are way older than Kubernetes.</st> <st c="33172">We can also discover this in
    many CSI drivers, which are nothing but a shim wrapping legacy code.</st> <st
    c="33270">For less flexible and scalable clusters, this might not be a problem,
    but in environments where you have a lot of action going on, you don’t want to
    have a CSI in your system that becomes the bottleneck.</st> <st c="33474">Some
    CSIs even have restrictions and limitations to prevent their failure at scale.</st>
    <st c="33558">To be fair, we usually see this with on-premises installations and
    some old storage technologies.</st> <st c="33656">With those, we can add the</st>
    **<st c="33683">Logical Unit Number</st>** <st c="33702">(</st>**<st c="33704">LUN</st>**<st
    c="33707">) presentation and</st> <st c="33727">connection limits on top of the
    things to consider.</st> <st c="33779">The LUN is for the Pods to make requests
    from storage space and retrieve data.</st> <st c="33858">There are limits on how
    many connections a physical server can have to the storage.</st> <st c="33942">Again,
    this is important when you manage your own storage</st> <st c="34000">and SANs.</st>
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="33007">CSI的定义是</st> <st c="33035">一种相对较新的方法，但存储管理和软件定义存储的底层技术比Kubernetes要早得多。</st>
    <st c="33172">我们还可以在许多CSI驱动程序中发现这一点，它们不过是对传统代码的封装。</st> <st c="33270">对于那些不太灵活且可扩展性较差的集群来说，这可能不是问题，但在你有大量活动的环境中，你可不想让CSI成为系统中的瓶颈。</st>
    <st c="33474">一些CSI甚至有限制和约束，以防止它们在规模扩展时失败。</st> <st c="33558">公平地说，我们通常会看到这种情况出现在本地安装和一些老旧的存储技术中。</st>
    <st c="33656">对于这些，我们还可以考虑</st> **<st c="33683">逻辑单元号</st>** <st c="33702">(</st>**<st
    c="33704">LUN</st>**<st c="33707">)</st> <st c="33727">的呈现和连接限制。</st> <st c="33779">LUN是Pod从存储空间请求并获取数据的方式。</st>
    <st c="33858">物理服务器与存储之间的连接数量是有限制的。</st> <st c="33942">同样，当您管理自己的存储</st> <st c="34000">和SAN时，这一点尤为重要。</st>
- en: '<st c="34009">Why are some CSIs so poor?</st> <st c="34037">The CSI does more
    than provide storage capacity.</st> <st c="34086">It communicates with the storage
    provider, promises the availability of the demanded capacity, and waits until
    the RAID controller, backup, and snapshot mechanisms are ready.</st> <st c="34261">In
    large-scale storage systems, we get even more: we can find storage capacity allocation
    and</st> <st c="34355">optimization procedures.</st>'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="34009">为什么有些CSI这么差劲？</st> <st c="34037">CSI不仅仅是提供存储容量。</st> <st c="34086">它与存储提供者进行通信，承诺提供所需的容量，并等待RAID控制器、备份和快照机制准备就绪。</st>
    <st c="34261">在大规模存储系统中，我们还会看到更多：我们可以找到存储容量分配和</st> <st c="34355">优化过程。</st>
- en: <st c="34379">To overcome such issues, you need to evaluate the storage, especially
    the storage drivers, of their cloud-native storage capabilities.</st> <st c="34515">This
    will require large-scale performance tests and a ridiculous number of created
    volumes.</st> <st c="34607">The CSI driver shouldn’t do any cut-downs or performance
    decrease and the created volume should be in a</st> <st c="34711">millisecond
    area.</st>
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="34379">为了克服这些问题，您需要评估存储，特别是其云原生存储能力的存储驱动程序。</st> <st c="34515">这将需要进行大规模的性能测试，并创建大量的卷。</st>
    <st c="34607">CSI驱动程序不应做任何削减或性能下降，创建的卷应处于</st> <st c="34711">毫秒级别。</st>
- en: <st c="34728">Furthermore, ensure that the</st> <st c="34758">driver allows
    cross-storage and cross-cloud/infrastructure migration; provides synchronous and
    asynchronous replication between those different infrastructures; provides feature
    parity across sites; and supports local storage types if needed, such as for</st>
    <st c="35013">edge scenarios.</st>
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="34728">此外，确保驱动程序支持跨存储和跨云/基础设施迁移；提供不同基础设施之间的同步和异步复制；提供各站点之间的功能对等；并在需要时支持本地存储类型，例如</st>
    <st c="35013">边缘场景。</st>
- en: <st c="35028">A good CSI will enable your platform to</st> <st c="35069">operate
    anywhere and to support a wide range of</st> <st c="35117">use cases.</st>
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="35028">一个好的CSI将使您的平台能够</st> <st c="35069">在任何地方运行，并支持广泛的</st> <st c="35117">使用场景。</st>
- en: <st c="35127">Container network interface</st>
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="35127">容器网络接口</st>
- en: <st c="35155">The</st> **<st c="35160">Container Network Interface</st>** <st
    c="35187">(</st>**<st c="35189">CNI</st>**<st c="35192">) can</st> <st c="35199">become
    a platform’s most relevant component, but it is also its most underrated one.</st>
    <st c="35284">For many projects we have seen, some platform teams don’t care what
    they use as CNI, nor do they heavily utilize network policies, encryption, or
    fine-grained network configurations.</st> <st c="35467">Thanks to its simple abstraction
    of the network layer, it’s not overwhelming when getting started.</st> <st c="35566">Yet,
    on the other hand, there are many use cases where the most crucial component is
    the CNI.</st> <st c="35660">I have even seen projects fail because of the dynamic
    nature of a CNI that didn’t play along with the very traditional and legacy approach
    of</st> <st c="35802">implementing networks.</st>
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="35155">容器网络接口</st>** <st c="35160">（**CNI**）</st> <st c="35187">可以成为平台最相关的组件，但它也是最被低估的组件。</st>
    <st c="35284">在我们看到的许多项目中，一些平台团队并不关心他们使用什么作为CNI，也不太利用网络策略、加密或细粒度的网络配置。</st> <st
    c="35467">由于它对网络层的简单抽象，刚开始使用时并不会让人感到压倒性。</st> <st c="35566">然而，另一方面，有许多用例中，最关键的组件就是CNI。</st>
    <st c="35660">我甚至见过因为CNI的动态特性与传统的网络实现方法不兼容而导致项目失败。</st>'
- en: <st c="35824">A CNI always requires a dedicated implementation because it is
    a set of specifications and libraries for writing plugins to configure network
    interfaces.</st> <st c="35979">The CNI concerns itself only with the network connectivity
    of Linux containers and removing allocated resources when the container is deleted.</st>
    <st c="36122">Due to this focus, CNIs have a wide range of support, and the specifications
    are simple</st> <st c="36210">to implement.</st>
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="35824">CNI总是需要专门的实现，因为它是一套规范和库，用于编写插件来配置网络接口。</st> <st c="35979">CNI只关注Linux容器的网络连接，并在容器被删除时清除分配的资源。</st>
    <st c="36122">由于这一点，CNI有广泛的支持，并且其规范实现简单。</st> <st c="36210">易于实现。</st>
- en: '<st c="36223">Therefore, we architects should never treat the CNI as “</st>*<st
    c="36280">just another object in Kubernetes</st>*<st c="36314">.” We have to evaluate
    and introduce it.</st> <st c="36355">Some CNIs are made for easy maintenance and
    a solid but simple set of features.</st> <st c="36435">For example, if the primary
    focus is on layer 3, consider Flannel.</st> <st c="36502">Other CNIs, such as
    Calico, are a rock-solid choice with a rich feature set; Cilium introduced the
    usage of eBPF to provide even faster and more secure networking.</st> <st c="36666">If
    it’s still difficult to choose between those options because you may have additional
    requirements such as providing different levels of networks, then the community
    still has an answer for you: Multus.</st> <st c="36871">Take your time and discover
    your options.</st> <st c="36913">There are dozens</st> <st c="36930">of CNIs.</st>'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="36223">因此，我们作为架构师绝不应将CNI视为“</st>*<st c="36280">Kubernetes中的另一个对象</st>*<st
    c="36314">。” 我们必须评估并引入它。</st> <st c="36355">一些CNI旨在简化维护，提供一个坚固但简单的功能集。</st> <st
    c="36435">例如，如果主要关注的是第3层，考虑使用Flannel。</st> <st c="36502">其他CNI，如Calico，是一个坚实的选择，功能丰富；Cilium则引入了eBPF的使用，提供更快速、更安全的网络连接。</st>
    <st c="36666">如果你因为可能有其他需求，如提供不同级别的网络，而仍然难以选择，可以选择社区为你提供的解决方案：Multus。</st> <st
    c="36871">慢慢思考并探索你的选项。</st> <st c="36913">有成百上千</st> <st c="36930">种CNI。</st>
- en: <st c="36938">The CNI can have serious effects on</st> <st c="36975">your platforms:</st>
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="36938">CNI对</st> <st c="36975">你的平台可能有重要影响：</st>
- en: '**<st c="36990">Security</st>**<st c="36999">: CNIs can provide different capabilities
    for network policies to achieve fine-grained control over your network.</st> <st
    c="37114">They can have additional encryption features, integrations into identity
    and access management systems, and</st> <st c="37222">detailed observability.</st>'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="36990">安全性</st>**<st c="36999">：CNI可以提供不同的网络策略功能，以实现对网络的细粒度控制。</st>
    <st c="37114">它们可以具备额外的加密功能，集成身份和访问管理系统，以及</st> <st c="37222">详细的可观测性。</st>'
- en: '**<st c="37245">Scalability</st>**<st c="37257">: The larger the cluster gets,
    the more communication happens throughout the network.</st> <st c="37344">The
    CNI must support your growth target and stay efficient even with complex routing
    and chatting across</st> <st c="37449">the wires.</st>'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="37245">可扩展性</st>**<st c="37257">：集群规模越大，网络中的通信就越频繁。</st> <st c="37344">CNI必须支持你的增长目标，即使在复杂的路由和数据传输下仍然保持高效。</st>'
- en: '**<st c="37459">Performance</st>**<st c="37471">: How fast and direct can Pod-to-Pod
    communication be?</st> <st c="37527">How much complexity does the CNI introduce?</st>
    <st c="37571">How efficiently can it handle communication?</st> <st c="37616">Can
    it deal with many complex</st> <st c="37646">network policies?</st>'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="37459">性能</st>**<st c="37471">：Pod之间的通信有多快和直接？</st> <st c="37527">CNI引入了多少复杂性？</st>
    <st c="37571">它能多高效地处理通信？</st> <st c="37616">它能处理很多复杂的</st> <st c="37646">网络策略吗？</st>'
- en: '**<st c="37663">Operability</st>**<st c="37675">: High-level CNIs are not very
    inversive and simple to maintain.</st> <st c="37741">Powerful CNIs can, in theory,
    be replaced as long as they adhere to the CNI specification, but each comes with
    its own set of features, which are often</st> <st c="37893">not replaceable.</st>'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="37663">可操作性</st>**<st c="37675">：高级CNI不是很侵入式，且易于维护。</st> <st c="37741">强大的CNI理论上可以被替换，只要它们遵循CNI规范，但每个CNI都有自己的一套功能，这些功能往往是</st>
    <st c="37893">无法替代的。</st>'
- en: <st c="37909">Be aware that not every</st> <st c="37934">CNI supports all of
    those features.</st> <st c="37970">Some, for example, do not even provide network
    policy support.</st> <st c="38033">Other CNIs are cloud-provider-specific integrating
    with just one cloud provider</st> <st c="38113">and they do enable some cloud</st>
    <st c="38143">provider-centric capabilities.</st>
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="37909">请注意，并非所有的</st> <st c="37934">CNI都支持这些功能。</st> <st c="37970">例如，有些甚至不提供网络策略支持。</st>
    <st c="38033">其他CNI是云提供商特定的，只与一个云提供商集成，</st> <st c="38113">并且它们确实启用一些以云提供商为中心的功能。</st>
- en: <st c="38173">Architectural challenges – CNI chaining and multiple CNIs</st>
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="38173">架构挑战 – CNI链式调用与多个CNI</st>
- en: <st c="38231">Platforms are predestined for CNI chaining.</st> <st c="38276">CNI
    chaining</st> <st c="38288">introduces the sequential usage of</st> <st c="38323">multiple
    CNIs.</st> <st c="38339">The order in which CNIs are taken and for what purpose
    is defined in the</st> `<st c="38412">/etc/cni/net.d</st>` <st c="38426">directory
    and handled by the kubelet.</st> <st c="38465">This allows the platform team to
    handle one part of the network and provide a guarded approach, while platform
    users can freely configure their network at a higher level.</st> <st c="38636">For
    example, a platform user can access Antrea as a CNI to configure their networking
    to some extent.</st> <st c="38738">They can also apply network policies and egress
    configurations to prevent their application from chatting with everyone.</st>
    <st c="38859">On the other side of the platform, the platform engineering team
    will manage, via Cilium, the global cross-cluster communication, as well as the
    network encryption, to enforce security best practices.</st> <st c="39060">In
    addition, the networking data made visible by Cilium is made available to the
    operations and security teams.</st> <st c="39172">Where those use cases are most
    suitable is in the interaction with the cloud providers’ own CNIs.</st> <st c="39270">They
    often enable better integration between the platform and the cloud but lack many
    advanced features on the</st> <st c="39381">other side.</st>
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="38231">平台非常适合CNI链式调用。</st> <st c="38276">CNI链式调用</st> <st c="38288">引入了多个CNI的顺序使用。</st>
    <st c="38323">CNIs的使用顺序及其目的在</st> `<st c="38412">/etc/cni/net.d</st>` <st c="38426">目录中定义，并由kubelet处理。</st>
    <st c="38465">这允许平台团队处理网络的一部分，并提供一种受控的方法，而平台用户则可以在更高的层次上自由配置他们的网络。</st> <st c="38636">例如，平台用户可以访问Antrea作为CNI，以某种程度上配置他们的网络。</st>
    <st c="38738">他们还可以应用网络策略和出口配置，以防止他们的应用与所有人进行通信。</st> <st c="38859">在平台的另一侧，平台工程团队将通过Cilium管理全局跨集群通信以及网络加密，以执行安全最佳实践。</st>
    <st c="39060">此外，Cilium提供的可见网络数据将提供给运营和安全团队。</st> <st c="39172">这些用例最适合与云提供商自己的CNI进行交互。</st>
    <st c="39270">它们通常能够更好地将平台与云集成，但在另一面缺乏许多高级功能。</st> <st c="39381">其他方面的功能则较少。</st>
- en: <st c="39392">Another approach to be evaluated would be to assign a Pod multiple
    network interfaces via Multus or CNI-Genie.</st> <st c="39504">Normally, a Pod
    has just one interface, but with Multus, for example, this could be multiple network
    interfaces.</st> <st c="39617">When does this become relevant?</st> <st c="39649">The
    following instances are examples when it</st> <st c="39694">becomes relevant:</st>
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="39392">另一个待评估的方法是通过Multus或CNI-Genie为Pod分配多个网络接口。</st> <st c="39504">通常，Pod只有一个接口，但例如使用Multus时，这可以是多个网络接口。</st>
    <st c="39617">什么时候这变得相关？</st> <st c="39649">以下实例是它变得相关的示例：</st> <st c="39694">变得相关：</st>
- en: <st c="39711">Separating control and operational data from</st> <st c="39757">application
    data</st>
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="39711">将控制数据和操作数据与</st> <st c="39757">应用数据分离</st>
- en: <st c="39773">Providing flexible network options for an extremely</st> <st c="39826">heterogeneous
    workload</st>
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="39773">为极为</st> <st c="39826">异构的工作负载提供灵活的网络选项</st>
- en: <st c="39848">Taking multi-tenancy to another level by assigning completely
    different networks for</st> <st c="39934">each tenant</st>
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="39848">通过为每个租户分配完全不同的网络，将多租户架构提升到一个新水平</st>
- en: <st c="39945">Supporting unusual network protocols and connections, such as
    in edge scenarios or</st> <st c="40029">telco environments</st>
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="39945">支持不寻常的网络协议和连接，例如在边缘场景或</st> <st c="40029">电信环境中</st>
- en: '**<st c="40047">Network Function Virtualization</st>** <st c="40079">(</st>**<st
    c="40081">NFV</st>**<st c="40084">), which</st> <st c="40093">requires multiple
    networks due to</st> <st c="40128">its complexity</st>'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="40047">网络功能虚拟化</st>** <st c="40079">(</st>**<st c="40081">NFV</st>**<st
    c="40084">)，由于</st> <st c="40093">其复杂性</st> <st c="40128">，需要多个网络</st>'
- en: <st c="40142">The Multus CNI</st> <st c="40157">is a kind of meta-plugin on
    the node and sits between the actual CNIs and the Pod network interfaces, as shown
    in the following illustration.</st> <st c="40300">It attaches the different network
    interfaces to the Pod on one side and handles the connection to the anticipated
    CNIs for the right network interface on the</st> <st c="40458">other side.</st>
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="40142">Multus CNI</st> <st c="40157">是一种位于节点上的元插件，处于实际 CNI 和 Pod 网络接口之间，如下图所示。</st>
    <st c="40300">它一方面将不同的网络接口连接到 Pod，另一方面处理与预期 CNI 的连接，以确保正确的网络接口。</st> <st c="40458">的连接。</st>
- en: '![Figure 4.5: Multus meta-CNI plugin](img/Figure_4.05_B31164.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.5：Multus 元 CNI 插件](img/Figure_4.05_B31164.jpg)'
- en: '<st c="40544">Figure 4.5: Multus meta-CNI plugin</st>'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="40544">图 4.5：Multus 元 CNI 插件</st>
- en: <st c="40578">Both approaches must be evaluated well.</st> <st c="40619">They
    have a significan</st><st c="40641">t impact on your network complexity, performance,</st>
    <st c="40692">and security.</st>
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="40578">这两种方法都必须进行充分评估。</st> <st c="40619">它们对网络的复杂性、性能</st> <st c="40641">和安全性有重大影响。</st>
- en: <st c="40705">Providing different CPU architectures</st>
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="40705">提供不同的 CPU 架构</st>
- en: '<st c="40743">Kubernetes supports</st> <st c="40764">multiple CPU architectures:
    AMD64, ARM64, 386, ARM, ppc64le, and even mainframes with s390x.</st> <st c="40857">Many
    clusters today run on AMD64, but at the time of writing, a strong interest in
    ARM64 is causing a shift.</st> <st c="40966">This discussion is primarily about
    saving costs and gaining a little bit of extra performance while reducing the
    total power consumption.</st> <st c="41104">At least on paper, it is a win-win-win
    situation.</st> <st c="41154">Not only is the ARM64 a possible change in the infrastructure,
    the open source architecture project RISC-V is gaining speed and is the first
    cloud provider to create RISC-V</st> <st c="41327">offerings</st> *<st c="41337">[4]</st>*<st
    c="41340">.</st>'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="40743">Kubernetes 支持</st> <st c="40764">多种 CPU 架构：AMD64、ARM64、386、ARM、ppc64le，甚至包括使用
    s390x 的大型机。</st> <st c="40857">目前许多集群运行在 AMD64 架构上，但在本文写作时，ARM64 正受到强烈关注，导致这一趋势的变化。</st>
    <st c="40966">本讨论主要关于节省成本并获得一些额外性能，同时减少总体电力消耗。</st> <st c="41104">至少在理论上，这是一个三赢的局面。</st>
    <st c="41154">不仅 ARM64 是基础设施可能发生的变化，开源架构项目 RISC-V 也在加速发展，并且是第一个创建 RISC-V</st>
    <st c="41327">服务</st> *<st c="41337">[4]</st>*<st c="41340">的云服务提供商。</st>
- en: <st c="41341">As platform engineers, we can enable those migrations and changes.</st>
    <st c="41409">A Kubernetes cluster can run multiple architectures simultaneously—not
    on the same node but with different groups of nodes.</st> <st c="41533">Remember
    that this change also requires an adjustment in the container build.</st> <st
    c="41611">With some software components, you can do a multi-architecture build;
    with others, it might require adjustments before the container for a different
    architecture can</st> <st c="41777">be created.</st>
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="41341">作为平台工程师，我们可以启用这些迁移和变化。</st> <st c="41409">Kubernetes 集群可以同时运行多种架构——不是在同一个节点上，而是在不同的节点组中。</st>
    <st c="41533">请记住，这一变化也需要容器构建方面的调整。</st> <st c="41611">对于某些软件组件，你可以进行多架构构建；而对于其他组件，可能需要在为不同架构创建容器之前进行调整。</st>
    <st c="41777">容器才可以创建。</st>
- en: <st c="41788">To select the architecture on which a Deployment should be delivered,
    you just need to add a</st> `<st c="41882">nodeSelector</st>` <st c="41894">like
    this to the</st> `<st c="41912">Spec</st>` <st c="41916">section of the Deployment</st>
    <st c="41943">YAML file:</st>
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="41788">要选择一个部署应该交付到的架构，你只需要在</st> `<st c="41882">nodeSelector</st>` <st
    c="41894">中像这样添加</st> `<st c="41912">Spec</st>` <st c="41916">部分到部署</st> <st c="41943">YAML
    文件：</st>
- en: '[PRE3]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: <st c="41993">An upcoming alternative to provide different container image</st><st
    c="42054">s is to compile the software as a</st> **<st c="42089">WebAssembly</st>**
    <st c="42100">(</st>**<st c="42102">Wasm</st>**<st c="42106">) container.</st>
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="41993">提供不同容器镜像的一个即将到来的替代方案是将软件编译为</st> **<st c="42089">WebAssembly</st>**
    <st c="42100">(</st>**<st c="42102">Wasm</st>**<st c="42106">) 容器。</st>
- en: <st c="42119">Wasm runtime</st>
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="42119">Wasm 运行时</st>
- en: <st c="42132">The usage of Wasm as an</st> <st c="42157">alternative container
    format has increased drastically in the last year.</st> <st c="42230">Wasm is
    a binary instruction format for a stack-based virtual machine.</st> <st c="42301">Think
    of it as an intermediate layer between various programming languages and many
    different execution environments.</st> <st c="42419">You can take code written
    in over 30 different languages, compile it into a</st> `<st c="42495">*.wasm</st>`
    <st c="42501">file, and then execute that file on any</st> <st c="42542">Wasm
    runtime.</st>
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42132">作为一种替代容器格式，Wasm 的使用在过去一年中大幅增加。</st> <st c="42230">Wasm 是一种面向栈式虚拟机的二进制指令格式。</st>
    <st c="42301">可以把它看作是各种编程语言和多种不同执行环境之间的中间层。</st> <st c="42419">你可以将用超过 30 种不同语言编写的代码编译成一个</st>
    `<st c="42495">*.wasm</st>` <st c="42501">文件，然后在任何</st> <st c="42542">Wasm 运行时上执行该文件。</st>
- en: <st c="42555">The name</st> *<st c="42565">WebAssembly</st>*<st c="42576">,
    however, is misleading.</st> <st c="42602">Initially designed to make code run
    quickly on the web, today, it can run anywhere.</st> <st c="42686">Why should
    we use it?</st> <st c="42708">Wasm is secure and sandboxed by default.</st> <st
    c="42749">In a Wasm container, there is no operating system or anything else except
    the binary compiled code.</st> <st c="42849">This means there is nothing to break
    into or claim the context or service account from.</st> <st c="42937">Wasm also
    has an incredibly fast startup time where the limits are set by the runtime rather
    than the module.</st> <st c="43047">Some runtimes claim to be as fast as around
    50 ms.</st> <st c="43098">In comparison, your brain requires >110 ms to recognize
    whether something passes in front of your eyes.</st> <st c="43202">Furthermore,
    a Wasm container size is around 0.5 MB – 1.5 MB, whereas a very slim container
    can be around 5 MB.</st> <st c="43314">However, what we can see on the market
    is usually image sizes in the range of 300 MB – 600 MB, 1 GB – 3 GB, or even sometimes
    above 10 GB.</st> <st c="43453">With this reduced image size, a Wasm container
    also has a drastically reduced storage footprint.</st> <st c="43550">Wasm is also
    hardware-independent.</st> <st c="43585">The exact same image can run anywhere,
    as long as you have a Wasm</st> <st c="43651">runtime available.</st>
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42555">然而，名称</st> *<st c="42565">WebAssembly</st>*<st c="42576">是具有误导性的。</st>
    <st c="42602">最初设计是为了让代码在网页上快速运行，但今天它可以在任何地方运行。</st> <st c="42686">我们为什么要使用它？</st>
    <st c="42708">Wasm 默认是安全且沙盒化的。</st> <st c="42749">在一个 Wasm 容器中，除了经过编译的二进制代码外，没有操作系统或其他任何东西。</st>
    <st c="42849">这意味着没有可以破坏或窃取上下文或服务帐户的东西。</st> <st c="42937">Wasm 还具有极快的启动时间，其限制由运行时设置，而不是模块。</st>
    <st c="43047">一些运行时声称启动速度可达到大约 50 毫秒。</st> <st c="43098">相比之下，你的大脑需要超过 110 毫秒才能识别某物是否在你眼前经过。</st>
    <st c="43202">此外，Wasm 容器的大小大约是 0.5 MB – 1.5 MB，而一个非常精简的容器可能约为 5 MB。</st> <st c="43314">然而，我们在市场上看到的通常是
    300 MB – 600 MB，1 GB – 3 GB，甚至有时超过 10 GB 的镜像大小。</st> <st c="43453">通过减小镜像大小，Wasm
    容器的存储占用也大大减少。</st> <st c="43550">Wasm 还具有硬件独立性。</st> <st c="43585">相同的镜像可以在任何地方运行，只要你有一个可用的
    Wasm</st> <st c="43651">运行时。</st>
- en: <st c="43669">In the context of</st> <st c="43687">Kubernetes, the OCI and CRI
    runtimes support Wasm.</st> <st c="43739">This means you can run a Wasm container
    alongside a regular container.</st> <st c="43810">As you can see in the following
    figure, no further changes are required.</st> <st c="43883">The Wasm app image
    is stored at the node level and executed by the</st> <st c="43950">layers above.</st>
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="43669">在 Kubernetes 环境中，OCI 和 CRI 运行时支持 Wasm。</st> <st c="43739">这意味着你可以同时运行一个
    Wasm 容器和一个常规容器。</st> <st c="43810">如以下图所示，不需要做进一步的修改。</st> <st c="43883">Wasm
    应用镜像存储在节点级别，并由上层执行。</st>
- en: '![Figure 4.6: Wasm on a Kubernetes node](img/Figure_4.06_B31164.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6：Wasm 在 Kubernetes 节点上](img/Figure_4.06_B31164.jpg)'
- en: '<st c="44087">Figure 4.6: Wasm on a Kubernetes node</st>'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="44087">图 4.6：Wasm 在 Kubernetes 节点上</st>
- en: <st c="44124">To make</st> <st c="44133">Wasm executable and available in your
    platform, you have to specify a runtime class and define its usage at the Deployment/Pod
    level.</st> <st c="44267">In the following example, you can see the specification
    for</st> `<st c="44327">crun</st>` <st c="44331">as</st> `<st c="44335">RuntimeClass</st>`
    <st c="44347">on the left side, and a</st> `<st c="44372">Pod</st>` <st c="44375">definition
    where, for</st> `<st c="44398">spec.runtimeClassName</st>`<st c="44419">, we assign</st>
    `<st c="44431">crun</st>` <st c="44435">on the right side.</st> <st c="44455">For</st>
    `<st c="44459">crun</st>`<st c="44463">, we also have to add an annotation to
    inform</st> `<st c="44509">crun</st>` <st c="44513">that this</st> `<st c="44524">Pod</st>`
    <st c="44527">has a</st> <st c="44534">Wasm image:</st>
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="44124">为了使</st> <st c="44133">Wasm 可执行并在你的平台上可用，你需要指定一个运行时类并在部署/Pod 级别定义其使用。</st>
    <st c="44267">在以下示例中，你可以看到左侧指定了</st> `<st c="44327">crun</st>` <st c="44331">作为</st>
    `<st c="44335">RuntimeClass</st>`，右侧则是一个</st> `<st c="44372">Pod</st>` <st c="44375">定义，其中</st>
    `<st c="44398">spec.runtimeClassName</st>`<st c="44419">的值为</st> `<st c="44431">crun</st>`
    <st c="44435">。</st> <st c="44455">对于</st> `<st c="44459">crun</st>`<st c="44463">，我们还需要添加一个注释来告知</st>
    `<st c="44509">crun</st>` <st c="44513">该</st> `<st c="44524">Pod</st>` <st c="44527">具有一个</st>
    <st c="44534">Wasm 镜像：</st>
- en: '| `<st c="44545">apiVersion: node.k8s.io/v1</st>``<st c="44572">kind: RuntimeClass</st>``<st
    c="44591">metadata:</st>``<st c="44601">name: crun</st>``<st c="44612">scheduling:</st>``<st
    c="44624">nodeSelector:</st>``<st c="44638">runtime: crun</st>``<st c="44652">handler:
    crun</st>` | `<st c="44666">apiVersion: v1</st>``<st c="44681">kind: Pod</st>``<st
    c="44691">metadata:</st>``**<st c="44701">name: wasm-demo-app</st>**` **`**<st
    c="44721">annotations:</st>**`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '| `<st c="44545">apiVersion: node.k8s.io/v1</st>``<st c="44572">kind: RuntimeClass</st>``<st
    c="44591">metadata:</st>``<st c="44601">name: crun</st>``<st c="44612">scheduling:</st>``<st
    c="44624">nodeSelector:</st>``<st c="44638">runtime: crun</st>``<st c="44652">handler:
    crun</st>` | `<st c="44666">apiVersion: v1</st>``<st c="44681">kind: Pod</st>``<st
    c="44691">metadata:</st>``**<st c="44701">name: wasm-demo-app</st>**` **`**<st
    c="44721">annotations:</st>**`'
- en: '**`**<st c="44734">module.wasm.image/variant: compat</st>**`'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**`**<st c="44734">module.wasm.image/variant: compat</st>**`'
- en: '**`<st c="44768">spec:</st>`'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**`<st c="44768">spec:</st>`'
- en: '`**<st c="44774">runtimeClassName: crun</st>**`'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`**<st c="44774">runtimeClassName: crun</st>**`'
- en: '**`**<st c="44797">containers:</st>**`'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**`**<st c="44797">容器：</st>**`'
- en: '**`**<st c="44809">name: wasm-demo-app</st>**`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**`**<st c="44809">name: wasm-demo-app</st>**`'
- en: '**`**<st c="44829">image:</st>**`'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**`**<st c="44829">image:</st>**`'
- en: '**`**<st c="44836">docker.io/cr7258/wasm-demo-app:v1</st>**`**************  |'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**`**<st c="44836">docker.io/cr7258/wasm-demo-app:v1</st>**`**************  |'
- en: '<st c="44870">Table 4.1: Definition of a RuntimeClass and a Pod that will be
    executed in the Wasm runtime</st>'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="44870">表 4.1：定义一个将在 Wasm 运行时执行的 RuntimeClass 和 Pod</st>
- en: <st c="44962">As an alternative to that, new developments such as SpinKube</st>
    <st c="45023">come with a whole set of tools to utilize Kubernetes resources in
    the best manner</st> *<st c="45106">[5]</st>*<st c="45109">. That approach allows
    the integration of the development experience into the deployment and the execution
    of the Wasm containerized app.</st> <st c="45247">The following image shows the
    workflow and how the different components work together.</st> <st c="45334">It
    makes the development process straightforward and brings a fast but robust new
    runtime environment to</st> <st c="45439">the platform.</st>
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="44962">作为替代方案，像 SpinKube 这样的新开发</st> <st c="45023">提供了一整套工具来以最佳方式利用 Kubernetes
    资源</st> *<st c="45106">[5]</st>*<st c="45109">。这种方法使得开发体验能够与 Wasm 容器化应用的部署和执行紧密集成。</st>
    <st c="45247">下图展示了工作流以及不同组件如何协同工作。</st> <st c="45334">它使开发过程变得简单直观，并为</st> <st
    c="45439">平台带来了一个快速但稳定的新运行时环境。</st>
- en: '![Figure 4.7: SpinKube overview [6]](img/Figure_4.07_B31164.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.7：SpinKube 概览 [6]](img/Figure_4.07_B31164.jpg)'
- en: '<st c="46231">Figure 4.7: SpinKube overview [6]</st>'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="46231">图 4.7：SpinKube 概览 [6]</st>
- en: <st c="46264">However, is Wasm</st> <st c="46281">really a technology that the
    industry has adopted?</st> <st c="46333">Here are just some examples and use cases
    showing</st> <st c="46383">its adoption:</st>
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="46264">然而，Wasm</st> <st c="46281">真的是业界已经采纳的技术吗？</st> <st c="46333">以下是一些展示</st>
    <st c="46383">其采用情况的示例和用例：</st>
- en: '**<st c="46396">Interoperability</st>**<st c="46413">: Figma runs as Wasm on</st>
    <st c="46438">any computer</st>'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`<st c="46396">互操作性</st>**<st c="46413">：Figma 作为 Wasm 在</st> <st c="46438">任何计算机上运行</st>'
- en: '**<st c="46450">Plugin system</st>**<st c="46464">: You can write extensions
    in Wasm</st> <st c="46500">for Envoy</st>'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="46450">插件系统</st>**<st c="46464">：您可以为 Envoy 编写 Wasm 扩展</st>'
- en: '**<st c="46509">Sandboxing</st>**<st c="46520">: A Firefox or Chrome browser
    can run Wasm in a sandboxed environment to protect</st> <st c="46602">your system</st>'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="46509">沙箱</st>**<st c="46520">：Firefox 或 Chrome 浏览器可以在沙箱环境中运行 Wasm，以保护</st>
    <st c="46602">您的系统</st>'
- en: '**<st c="46613">Blockchain</st>**<st c="46624">: CosmWasm or the ICP uses versions
    of Wasm to</st> <st c="46672">run applications</st>'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="46613">区块链</st>**<st c="46624">：CosmWasm 或 ICP 使用 Wasm 的版本来</st> <st
    c="46672">运行应用程序</st>'
- en: '**<st c="46688">Container</st>**<st c="46698">: WasmCloud has a different concept
    to execute containers, or SpinKube, a Wasm runtime with a CLI for a simple</st>
    <st c="46810">development process</st>'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="46688">容器</st>**<st c="46698">：WasmCloud 执行容器的概念不同，或者是 SpinKube，一个具有
    CLI 的 Wasm 运行时，提供简单的</st> <st c="46810">开发流程</st>'
- en: '**<st c="46829">Serverless platforms</st>**<st c="46850">: Cloudflare Workers
    or Fermyon Cloud run your</st> <st c="46898">Wasmized app</st>'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="46829">无服务器平台</st>**<st c="46850">：Cloudflare Workers 或 Fermyon Cloud
    运行您的</st> <st c="46898">Wasm 应用</st>'
- en: <st c="46910">Wasm is not a container replacement yet.</st> <st c="46952">It
    is an evolutionary step that suits a few cases very well, but it lacks adoption
    and has issues in the debugging process.</st> <st c="47076">However, it is a matter
    of time before these obstacles</st> <st c="47131">are solved.</st>
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="46910">Wasm 目前还不能替代容器。</st> <st c="46952">它是一个进化的步骤，非常适合一些特定的应用场景，但它尚未得到广泛采用，并且在调试过程中存在问题。</st>
    <st c="47076">然而，这些障碍的解决只是时间问题。</st> <st c="47131">它们会得到解决。</st>
- en: <st c="47142">Enable platforms for GPU utilization</st>
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="47142">为 GPU 利用启用平台</st>
- en: <st c="47179">Similar to the support for different CPU architectures or the
    extension of the container runtime to include Wasm, GPU enablement</st> <st c="47308">for
    users has become extremely relevant lately.</st> <st c="47357">A device plugin
    must be installed that is specific to the type of GPU, such as AMD, Intel, or
    Nvidia.</st> <st c="47459">This exposes custom schedulable resources such as</st>
    `<st c="47509">nvidia.com/gpu</st>` <st c="47523">to Kubernetes and its users.</st>
    <st c="47553">Also, we are not experts in GPUs and their capabilities; from a
    platform engineering perspective, the different providers have developed diverse
    feature sets and extensions for</st> <st c="47730">their plugins.</st>
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="47179">类似于对不同 CPU 架构的支持，或者将容器运行时扩展到包括 Wasm，GPU 的支持</st> <st c="47308">最近对用户变得极为相关。</st>
    <st c="47357">必须安装特定于 GPU 类型的设备插件，如 AMD、Intel 或 Nvidia。</st> <st c="47459">这将向
    Kubernetes 及其用户暴露自定义可调度资源，如</st> `<st c="47509">nvidia.com/gpu</st>` <st c="47523">。</st>
    <st c="47553">此外，我们不是 GPU 及其功能的专家；从平台工程的角度来看，不同的供应商为其插件开发了不同的功能集和扩展。</st>
- en: <st c="47744">I highly recommend developing or finding experts for this field
    if your user requires GPUs.</st> <st c="47837">The field of AI and LLMs is undergoing
    rapid expansion.</st> <st c="47893">Hardware and software providers come up with
    new tools, systems, and approaches monthly to take advantage of those technologies.</st>
    <st c="48022">Any scenario has its own very specific demands.</st> <st c="48070">Training
    a model requires a tremendous amount of data, GPUs, storage, and memory.</st>
    <st c="48152">Fine-tuning a model comes primarily down to how large a model you
    want to train.</st> <st c="48233">A seven-billion-parameter model can fit into
    a 14 GB VRAM, but increasing the precision can increase its size easily to 24
    GB or more.</st> <st c="48368">Lastly, providing an inference engine to serve
    an LLM for users requires a lot of</st> <st c="48450">network communication.</st>
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="47744">如果您的用户需要 GPU，我强烈建议开发或者寻找该领域的专家。</st> <st c="47837">AI 和 LLM 的领域正在迅速扩展。</st>
    <st c="47893">硬件和软件提供商每月都会推出新工具、新系统和新方法，以便充分利用这些技术。</st> <st c="48022">任何场景都有其非常具体的需求。</st>
    <st c="48070">训练一个模型需要大量的数据、GPU、存储和内存。</st> <st c="48152">微调一个模型主要取决于您想训练的模型有多大。</st>
    <st c="48233">一个七十亿参数的模型可以适应 14 GB 的 VRAM，但提高精度可能会轻松将其大小增加到 24 GB 或更多。</st> <st
    c="48368">最后，为用户提供推理引擎以服务 LLM 需要大量的</st> <st c="48450">网络通信。</st>
- en: <st c="48472">Important note</st>
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="48472">重要提示</st>
- en: <st c="48487">Inferencing means sending a prompt to an LLM.</st> <st c="48534">Most
    people believe that the LLM then creates the story like a human would.</st> <st
    c="48610">However, what is really happening is that after every word, the LLM
    has to send the whole prompt, including the new words, to the LLM again, so it
    can decide on the next word added to</st> <st c="48794">the sentence.</st>
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="48487">推理意味着向LLM发送一个提示。</st> <st c="48534">大多数人认为LLM接着会像人类一样创建故事。</st>
    <st c="48610">然而，实际上发生的是，在每个单词之后，LLM必须将整个提示（包括新词）再次发送给LLM，以便它能决定添加到句子中的下一个词。</st>
- en: <st c="48807">As a rule of thumb, for the GPU VRAMs (Video RAM is the GPU’s
    memory) needed, you double the model’s size.</st> <st c="48915">Here are some</st>
    <st c="48929">more examples:</st>
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="48807">作为经验法则，对于所需的GPU VRAM（视频内存是GPU的内存），你需要将模型的大小翻倍。</st> <st c="48915">以下是一些</st>
    <st c="48929">更多的例子：</st>
- en: <st c="48943">Llama-2-70b requires 2 * 70 GB = 140</st> <st c="48981">GB VRAM</st>
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="48943">Llama-2-70b需要2 * 70 GB = 140</st> <st c="48981">GB VRAM</st>
- en: <st c="48988">Falcon-40b requires 2 * 40 GB = 80</st> <st c="49024">GB VRAM</st>
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="48988">Falcon-40b需要2 * 40 GB = 80</st> <st c="49024">GB VRAM</st>
- en: <st c="49031">MPT-30b requires 2 * 30 GB = 60</st> <st c="49064">GB VRAM</st>
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="49031">MPT-30b需要2 * 30 GB = 60</st> <st c="49064">GB VRAM</st>
- en: <st c="49071">bigcode/starcoder requires 2 * 15.5 = 31</st> <st c="49113">GB
    VRAM</st>
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="49071">bigcode/starcoder需要2 * 15.5 = 31</st> <st c="49113">GB VRAM</st>
- en: <st c="49120">Therefore, collaboration between the platform and the machine
    learning team is required.</st> <st c="49210">Depending on the models they want
    to use, your chosen hardware may no longer fit</st> <st c="49291">the requirements</st><st
    c="49307">.</st>
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="49120">因此，需要平台与机器学习团队之间的合作。</st> <st c="49210">根据他们想使用的模型，你选择的硬件可能不再符合</st>
    <st c="49291">要求</st><st c="49307">。</st>
- en: <st c="49308">Architectural challenges</st>
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="49308">架构挑战</st>
- en: <st c="49333">While the integration of GPUs is</st> <st c="49366">very straightforward,
    they have a few elements that need to be discussed; you should be aware of</st>
    <st c="49464">the following:</st>
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="49333">虽然GPU的集成非常简单，但它们有一些需要讨论的要素；你应该了解以下内容：</st>
- en: '**<st c="49478">GPU costs</st>**<st c="49488">: Cheap GPUs, with low VRAM and
    computational power, might not fit your use cases.</st> <st c="49572">Also, you
    might not use a GPU 24/7, but you should think about more dynamic and flexible
    possibilities.</st> <st c="49676">However, owning GPUs can be cheaper in the long
    run as compared to their regular CPU counterparts if your use case requires GPU</st>
    <st c="49804">computational power.</st>'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="49478">GPU成本</st>**<st c="49488">：便宜的GPU，具有较低的VRAM和计算能力，可能不适合你的使用案例。</st>
    <st c="49572">另外，你可能不会24/7使用GPU，但你应该考虑更具动态性和灵活性的可能性。</st> <st c="49676">然而，如果你的使用案例需要GPU计算能力，拥有GPU从长远来看可能比常规的CPU更便宜。</st>'
- en: '**<st c="49824">Privileged rights</st>**<st c="49842">: Many machine learning
    tools require further customization and tweaking, especially for the rights</st>
    <st c="49943">they demand.</st>'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="49824">特权权限</st>**<st c="49842">：许多机器学习工具需要进一步的定制和调整，特别是它们所要求的权限。</st>'
- en: '**<st c="49955">End user requirements</st>**<st c="49977">: Besides the model
    sizes, what the data scientists and machine learning engineers want to do with
    the model depends very much on the actual use case they want to implement.</st>
    <st c="50152">Any minor change in the approach can make the platform unusable.</st>
    <st c="50217">This must be considered in the architecture for such a platform
    and to provide the most resources and greatest scaling</st> <st c="50336">flexibility
    possible.</st>'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="49955">最终用户需求</st>**<st c="49977">：除了模型大小外，数据科学家和机器学习工程师希望如何使用模型，很大程度上取决于他们想要实施的实际使用案例。</st>
    <st c="50152">方法的任何小变化都可能使平台无法使用。</st> <st c="50217">这必须在此类平台的架构中加以考虑，以提供最多的资源和最大的扩展</st>
    <st c="50336">灵活性。</st>'
- en: '**<st c="50357">External models pulled to the cluster</st>**<st c="50395">:
    As with containers, it is a common practice to pull models from pages such as
    HuggingFace.</st> <st c="50489">You might consider this in the network of a platform
    supporting machine</st> <st c="50561">learning activities.</st>'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="50357">外部模型拉取到集群</st>**<st c="50395">：与容器类似，从像HuggingFace这样的页面拉取模型是一种常见做法。</st>
    <st c="50489">你可能会考虑在支持机器学习活动的平台网络中进行此操作。</st>'
- en: '<st c="50581">Creating platforms that suit machine learning and LLM operations
    requires a new level of optimization.</st> <st c="50685">Non-running GPUs are
    a waste of money.</st> <st c="50724">Poorly used GPUs are a waste of money.</st>
    <st c="50763">However, there is more we have to ensure from an infrastructure
    perspective: data protection, platform security, and specialized observability
    for the models.</st> <st c="50922">In my opinion, machine learning and LLMs are
    an exciting use case and offer a playground for</st> <st c="51015">platform engineer</st><st
    c="51032">s.</st>'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="50581">为机器学习和大语言模型（LLM）操作创建适合的平台需要一种全新的优化层次。</st> <st c="50685">未运行的
    GPU 是浪费金钱。</st> <st c="50724">使用不当的 GPU 是浪费金钱。</st> <st c="50763">然而，从基础设施的角度来看，我们还必须确保更多：数据保护、平台安全性以及模型的专门可观察性。</st>
    <st c="50922">在我看来，机器学习和大语言模型是一个令人兴奋的应用场景，提供了一个平台工程师的游乐场。</st>
- en: <st c="51035">Solution space</st>
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="51035">解决方案空间</st>
- en: <st c="51050">To optimize</st> <st c="51063">GPU usage, there are some</st>
    <st c="51089">approaches available:</st>
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="51050">为了优化</st> <st c="51063">GPU 的使用，有一些</st> <st c="51089">可用的方法：</st>
- en: '**<st c="51110">Multi-Process</st>** **<st c="51125">Server</st>** <st c="51131">(</st>**<st
    c="51133">MPS</st>**<st c="51136">)</st>'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="51110">多进程</st>** **<st c="51125">服务器</st>** <st c="51131">(</st>**<st
    c="51133">MPS</st>**<st c="51136">)</st>'
- en: '**<st c="51138">Multi-Instance</st>** **<st c="51153">GPU</st>** <st c="51156">(</st>**<st
    c="51158">MIG</st>**<st c="51161">)</st>'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="51138">多实例</st>** **<st c="51153">GPU</st>** <st c="51156">(</st>**<st
    c="51158">MIG</st>**<st c="51161">)</st>'
- en: '**<st c="51163">Time-slicing/sharing</st>**'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="51163">时间分片/共享</st>**'
- en: <st c="51183">Depending on the</st> <st c="51201">GPU driver, you will find
    a full range of possibilities, as you do with Nvidia.</st> <st c="51281">Other
    GPU drivers might not be mature enough or feature-rich yet, but of</st> <st c="51354">course,
    you should evaluate</st> <st c="51382">this frequently.</st>
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="51183">根据</st> <st c="51201">GPU 驱动程序，您将会发现多种可能性，就像使用 Nvidia 一样。</st>
    <st c="51281">其他 GPU 驱动程序可能还不够成熟或功能丰富，但</st> <st c="51354">当然，您应该经常评估</st> <st
    c="51382">这个问题。</st>
- en: '<st c="51398">Time-slicing is the</st> <st c="51419">worst option to take.</st>
    <st c="51441">Although it is better than nothing, MPS would be at least twice
    as efficient as the time-slice approach.</st> <st c="51546">However, MPS has one
    major drawback: processes are not strictly isolated, which leads to correlated
    failures between the slices.</st> <st c="51675">This is where MIG comes into the
    picture.</st> <st c="51717">It provides good process isolation and a static partitioning
    of the GPU.</st> <st c="51790">Static on a super dynamic, scalable, and anytime
    adjustable Kubernetes cluster?</st> <st c="51870">Yes, because machine learning
    training will not run just for a few seconds or</st> <st c="51948">minutes</st>
    *<st c="51956">[7]</st>*<st c="51959">.</st>'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="51398">时间分片是最差的选择。</st> <st c="51419">虽然它比什么都没有好，但 MPS 至少会比时间分片方法高效两倍。</st>
    <st c="51441">然而，MPS 有一个主要缺点：进程之间没有严格的隔离，这会导致切片之间出现相关故障。</st> <st c="51546">这就是
    MIG 的优势所在。</st> <st c="51675">它提供了良好的进程隔离和 GPU 的静态分区。</st> <st c="51717">在一个超级动态、可扩展且随时可调整的
    Kubernetes 集群中，静态分区如何实现？</st> <st c="51790">是的，因为机器学习训练不会仅仅运行几秒钟或</st> <st c="51948">几分钟</st>
    *<st c="51956">[7]</st>*<st c="51959">。</st>
- en: <st c="51960">The following figure shows the GPU’s memory partitions at a high
    level, to which different workloads can</st> <st c="52066">be assigned.</st>
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="51960">下图展示了 GPU 的内存分区，您可以将不同的工作负载分配到这些分区中。</st>
- en: '![Figure 4.8: Example of splitting a GPU into three GPU instances](img/Figure_4.08_B31164.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.8：将 GPU 拆分成三个 GPU 实例的示例](img/Figure_4.08_B31164.jpg)'
- en: '<st c="52479">Figure 4.8: Example of splitting a GPU into three GPU instances</st>'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="52479">图 4.8：将 GPU 拆分成三个 GPU 实例的示例</st>
- en: <st c="52542">Those GPU</st> <st c="52552">instances can be used either by a
    single pod, a pod with one container running multiple processes (not ideal), or
    by using something such as CUDA from Nvidia.</st> <st c="52712">CUDA</st> <st
    c="52716">is an MPS, so you can combine the different approaches, as shown in
    the</st> <st c="52789">following diagram:</st>
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="52542">这些 GPU</st> <st c="52552">实例可以由单个 Pod 使用，也可以由一个包含多个进程的 Pod（不理想）使用，或者通过使用
    Nvidia 的 CUDA 等技术来使用。</st> <st c="52712">CUDA</st> <st c="52716">是一个 MPS，因此您可以结合不同的方法，如下图所示：</st>
- en: '![Figure 4.9: Example of using three GPU instances in parallel](img/Figure_4.09_B31164.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.9：使用三个 GPU 实例并行的示例](img/Figure_4.09_B31164.jpg)'
- en: '<st c="53128">Figure 4.9: Example of using three GPU instances in parallel</st>'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="53128">图 4.9：使用三个 GPU 实例并行的示例</st>
- en: <st c="53188">From here, we can take a look at the research and experiment corner
    where you can combine</st> <st c="53279">Kubernetes</st> `<st c="53484">v1.26</st>`
    <st c="53489">and is still in Alpha, therefore breaking API changes are likely
    with every release.</st> <st c="53575">There are some interesting articles and
    talks about it.</st> <st c="53631">Depending on when you are reading this, it
    might be out of</st> <st c="53690">dat</st><st c="53693">e</st> *<st c="53696">[8]</st>*<st
    c="53699">.</st>
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="53188">从这里，我们可以看一下研究和实验部分，你可以结合</st> <st c="53279">Kubernetes</st> `<st
    c="53484">v1.26</st>` <st c="53489">，并且它仍处于 Alpha 阶段，因此每个版本可能会有 API 的重大变动。</st>
    <st c="53575">有一些有趣的文章和演讲讨论了它。</st> <st c="53631">根据你阅读的时间，它可能已经过时了。</st> <st
    c="53690">dat</st><st c="53693">e</st> *<st c="53696">[8]</st>*<st c="53699">.</st>
- en: <st c="53700">Enable cluster scalability</st>
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="53700">启用集群可扩展性</st>
- en: <st c="53727">As mentioned earlier in this</st> <st c="53756">chapter, the ability
    of a Kubernetes cluster to adjust its scale is beneficial for different demands,
    from resiliency to growing with the workload to fallback re-initiation, to providing
    the highest availability across data centers and availability zones.</st> <st
    c="54013">At its core, we differentiate between the horizontal and vertical autoscaler,
    which targets the Pods, and the horizontal CA, which adjusts the number</st> <st
    c="54163">of nodes.</st>
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="53727">如前所述，在本</st> <st c="53756">章节中，Kubernetes 集群的伸缩能力对于不同需求是有益的，包括从弹性、随工作负载增长到回退重新初始化，再到在数据中心和可用区提供最高可用性。</st>
    <st c="54013">从本质上讲，我们区分水平和垂直自动扩展器，后者针对 Pods，水平 CA 则调整节点数量。</st>
- en: <st c="54172">As with many capabilities, Kubernetes provides the specification
    but expects someone else to implement it.</st> <st c="54280">This at least applies
    to the VPA and the CA, which require at least a metrics server running at the</st>
    <st c="54380">cluster level.</st>
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="54172">与许多功能一样，Kubernetes 提供了规范，但期望其他人来实现它。</st> <st c="54280">这至少适用于
    VPA 和 CA，它们至少需要在</st> <st c="54380">集群级别运行一个指标服务器。</st>
- en: <st c="54394">However, the HPA</st> <st c="54411">is feature-rich and allows
    metric-based scaling.</st> <st c="54461">Look at the following example of an HPA.</st>
    <st c="54502">With</st> `<st c="54507">stabilizationWindowsSeconds</st>`<st c="54534">,
    we can also tell Kubernetes to wait on previous actions to prevent flapping.</st>
    <st c="54613">Flapping is</st> <st c="54624">defined as follows according to the</st>
    <st c="54661">Kubernetes documentation:</st>
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="54394">然而，HPA</st> <st c="54411">功能丰富，支持基于指标的伸缩。</st> <st c="54461">请看以下
    HPA 的例子。</st> <st c="54502">使用</st> `<st c="54507">stabilizationWindowsSeconds</st>`<st
    c="54534">，我们还可以告诉 Kubernetes 等待先前的操作，以防止抖动。</st> <st c="54613">抖动的定义如下，根据</st>
    <st c="54624">Kubernetes 文档：</st> <st c="54661">Kubernetes 文档：</st>
- en: <st c="54686">When managing the scale of a group of replicas using the HorizontalPodAutoscaler,
    it is possible that the number of replicas keeps fluctuating frequently due to
    the dynamic nature of the metrics evaluated.</st> <st c="54893">This is sometimes
    referred to as thrashing, or flapping.</st> <st c="54950">It’s similar to the
    concept of hysteresis in cybernetics.</st>
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="54686">在使用 HorizontalPodAutoscaler 管理一组副本的规模时，由于所评估的指标具有动态特性，副本数量可能会频繁波动。</st>
    <st c="54893">这有时被称为抖动，或者说是振荡。</st> <st c="54950">它类似于控制论中的滞后效应概念。</st>
- en: <st c="55007">We can take the following configuration for an HPA.</st> <st c="55060">It
    looks simple; you can see that based on the policies, the behavior can change
    drastically.</st> <st c="55154">For example, when reducing 10% of the Pods while
    having a large deployment with hundreds of replicas, we want to be very careful.</st>
    <st c="55284">The shown scaling-down configuration will prevent deleting more
    than two Pods at the</st> <st c="55369">same time:</st>
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="56106">我们可以参考以下 HPA 的配置。</st> <st c="55060">看起来很简单；你可以看到，根据策略，行为可以发生巨大变化。</st>
    <st c="55154">例如，当在拥有数百个副本的大型部署中减少 10% 的 Pods 时，我们需要非常小心。</st> <st c="55284">所示的缩小配置将防止同时删除超过两个
    Pods：</st>
- en: '[PRE4]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: <st c="56015">Issues with VPA, HPA, and CA</st>
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="56015">VPA、HPA 和 CA 的问题</st>
- en: <st c="56044">There are some constraints that you have to consider for the</st>
    <st c="56106">autoscaler family:</st>
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="56044">你需要考虑自动扩展器家族的一些约束：</st>
- en: '**<st c="56124">HPA</st>**<st c="56128">:</st>'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="56124">HPA</st>**<st c="56128">:</st>'
- en: <st c="56130">You</st> <st c="56134">have to set CPU and memory limits and requests
    on Pods correctly to prevent resource waste or frequently</st> <st c="56239">terminated
    Pods.</st>
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="56130">你</st> <st c="56134">必须正确设置 Pods 的 CPU 和内存限制及请求，以防止资源浪费或频繁</st>
    <st c="56239">终止的 Pods。</st>
- en: <st c="56255">When HPA hits the limit of the available nodes, it can’t schedule
    more Pods.</st> <st c="56333">However, it might utilize all available resources,
    which could lead</st> <st c="56401">to issues.</st>
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="56255">当 HPA 达到可用节点的限制时，它无法调度更多的 Pods。</st> <st c="56333">然而，它可能会利用所有可用资源，这可能会导致</st>
    <st c="56401">问题。</st>
- en: '**<st c="56411">VPA</st>**<st c="56415">:</st>'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="56411">VPA</st>**<st c="56415">：</st>'
- en: <st c="56417">VPA and</st> <st c="56425">HPA shouldn’t be used for scaling based
    on the same metric.</st> <st c="56485">For example, while both can use CPU utilization
    to trigger a scale-up, HPA deploys more Pods, while VPA increases the CPU limits
    on existing Pods, which can lead to excessive scaling based on the same metric.</st>
    <st c="56694">Therefore, if using both VPA and HPA, one should use CPU for one
    and, for instance, memory for</st> <st c="56789">the other.</st>
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="56417">VPA 和</st> <st c="56425">HPA 不应基于相同的指标进行缩放。</st> <st c="56485">例如，尽管两者都可以使用
    CPU 利用率触发扩容，HPA 部署更多的 Pods，而 VPA 增加现有 Pods 上的 CPU 限制，这可能导致基于相同指标的过度扩展。</st> <st
    c="56694">因此，如果同时使用 VPA 和 HPA，应将 CPU 用于其中一个，而将内存用于</st> <st c="56789">另一个。</st>
- en: <st c="56799">VPA might recommend using more resources than available within
    the cluster or the node.</st> <st c="56888">This causes the Pod to</st> <st c="56911">become
    unschedulable.</st>
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="56799">VPA 可能建议使用比集群或节点中可用资源更多的资源。</st> <st c="56888">这会导致 Pod</st> <st
    c="56911">变得不可调度。</st>
- en: '**<st c="56932">CA</st>**<st c="56935">:</st>'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="56932">CA</st>**<st c="56935">：</st>'
- en: <st c="56937">The</st> <st c="56941">CA scales are based on the requests and
    limits of the Pods.</st> <st c="57001">This can cause a lot of unused resources,
    poor utilization, and</st> <st c="57065">high costs.</st>
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="56937">CA</st> <st c="56941">缩放基于 Pods 的请求和限制。</st> <st c="57001">这可能导致许多未使用的资源、利用率低下以及</st>
    <st c="57065">高成本。</st>
- en: <st c="57076">When a CA triggers a scaling command to the cloud provider, this
    might take minutes to provide new nodes for the cluster.</st> <st c="57199">During
    this time, the application performance is degraded.</st> <st c="57258">In the
    worst case, it</st> <st c="57280">become</st><st c="57286">s unservable.</st>
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="57076">当 CA 触发云提供商的缩放命令时，可能需要几分钟时间为集群提供新的节点。</st> <st c="57199">在此期间，应用程序性能会下降。</st>
    <st c="57258">最坏的情况是，它</st> <st c="57280">变得</st><st c="57286">无法服务。</st>
- en: <st c="57300">Solution space</st>
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="57300">解决方案空间</st>
- en: <st c="57315">As platform engineers, we want to ensure that the user can define
    scaling behavior without putting the platform at risk.</st> <st c="57437">Utilizing
    HPA, VPA, and CA requires perfect configuration and control, guardrails provided
    by a policy engine, and close monitoring.</st> <st c="57570">It becomes mission-critical
    to control cluster scaling and descaling while enabling in-namespace autoscaling
    for</st> <st c="57683">your users.</st>
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="57315">作为平台工程师，我们希望确保用户能够定义缩放行为，同时不会让平台面临风险。</st> <st c="57437">利用 HPA、VPA
    和 CA 需要完美的配置与控制，由策略引擎提供的保护措施以及密切的监控。</st> <st c="57570">控制集群的扩展和缩减变得至关重要，同时要为</st>
    <st c="57683">用户启用命名空间内的自动缩放。</st>
- en: <st c="57694">Managing scaling your Kubernetes cluster on cloud providers requires
    you to look into the CA and the different available cloud integrations for it.</st>
    <st c="57843">Besides, if you use the</st> **<st c="57867">Cluster API</st>**
    <st c="57878">(</st>**<st c="57880">CAPI</st>**<st c="57884">), you</st> <st c="57892">can
    also build on its capability for</st> <st c="57929">cluster</st> <st c="57936">autoscaling.</st>
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="57694">在云提供商上管理 Kubernetes 集群的缩放需要你查看 CA 及其可用的各种云集成。</st> <st c="57843">此外，如果你使用</st>
    **<st c="57867">Cluster API</st>** <st c="57878">(</st>**<st c="57880">CAPI</st>**<st
    c="57884">)，你</st> <st c="57892">还可以利用其能力进行</st> <st c="57929">集群</st> <st c="57936">自动缩放。</st>
- en: <st c="57949">Network capabilities and extensions</st>
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="57949">网络能力和扩展</st>
- en: <st c="57985">Now, let’s look at the final resource integration of Kubernetes
    and the underlying infrastructure.</st> <st c="58085">To do this, we will start
    within the cluster networking mechanisms and work down to the DNS and load balancing.</st>
    <st c="58197">DNS and load balancing can happen within the cluster and in coordination
    with the infrastructure that Kuberne</st><st c="58306">tes</st> <st c="58311">runs
    on.</st>
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="57985">现在，让我们来看一下 Kubernetes 与底层基础设施的最终资源集成。</st> <st c="58085">为此，我们将从集群的网络机制入手，逐步探讨
    DNS 和负载均衡。</st> <st c="58197">DNS 和负载均衡可以在集群内发生，并与 Kubernetes 运行的基础设施协同工作。</st>
- en: <st c="58319">Ingress – the old way</st>
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="58319">Ingress – 旧方式</st>
- en: <st c="58341">Ingress</st> <st c="58349">is the old definition of how an end
    user request from outside the cluster is routed into the system and toward the
    application that is exposed.</st> <st c="58494">For almost a decade, it was the
    way to go to define incoming network traffic.</st> <st c="58572">The ingress is
    usually represented by an ingress controller such as NGINX, HAProxy, or Envoy,
    to name a few.</st> <st c="58681">Those inherently take the routing rules defined
    as a standard resource from Kubernetes and manage the rest of it.</st> <st c="58795">As
    you can see in the following figure, from there on, the traffic is redirected
    to the right service, which forwards it to the Pod.</st> <st c="58928">Physically,
    the traffic will go from the network interface to the ingress controller to the
    Pod, but as good an orchestrator as Kubernetes is, there are some logical steps</st>
    <st c="59100">in between.</st>
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="58341">Ingress</st> <st c="58349">是旧版定义，描述了如何将外部集群的最终用户请求路由到系统中并传递给暴露的应用程序。</st>
    <st c="58494">近十年来，它一直是定义传入网络流量的方式。</st> <st c="58572">Ingress 通常由如 NGINX、HAProxy
    或 Envoy 等 Ingress 控制器表示，举几个例子。</st> <st c="58681">这些控制器本质上会采用 Kubernetes 定义的标准资源中的路由规则，并管理其余部分。</st>
    <st c="58795">如下面的图所示，从那里开始，流量会被重定向到正确的服务，服务再将其转发到 Pod。</st> <st c="58928">从物理层面上讲，流量将从网络接口流向
    Ingress 控制器，再到 Pod，但正如 Kubernetes 是一位优秀的调度器一样，仍然有一些逻辑步骤</st> <st c="59100">介于其中。</st>
- en: '![Figure 4.10: Ingress controller (source: Kubernetes docs)](img/Figure_4.10_B31164.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10：Ingress 控制器（来源：Kubernetes 文档）](img/Figure_4.10_B31164.jpg)'
- en: '<st c="59192">Figure 4.10: Ingress controller (source: Kubernetes docs)</st>'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="59192">图 4.10：Ingress 控制器（来源：Kubernetes 文档）</st>
- en: <st c="59249">While it scales and is robust for some</st> <st c="59289">of the
    largest deployments out there, it has</st> <st c="59334">some downsides:</st>
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="59249">虽然它具有可扩展性，适用于一些</st> <st c="59289">最大规模的部署，但它也有</st> <st c="59334">一些缺点：</st>
- en: <st c="59349">The Ingress API only supports TLS termination and simple content-based
    request routing of</st> <st c="59440">HTTP traffic</st>
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="59349">Ingress API 仅支持 TLS 终止和简单的基于内容的 HTTP 流量请求路由</st>
- en: <st c="59452">It is limited in its available syntax and kept</st> <st c="59500">very
    simple</st>
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="59452">它的语法有限，保持</st> <st c="59500">非常简单</st>
- en: <st c="59511">It requires annotations</st> <st c="59536">for extensibility</st>
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="59511">它需要注解</st> <st c="59536">以支持扩展性</st>
- en: <st c="59553">It reduces portability because every implementation has its own
    approach to</st> <st c="59630">do so</st>
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="59553">它降低了可移植性，因为每个实现都有自己的方式来</st> <st c="59630">执行此操作</st>
- en: <st c="59635">Doing a multi-tenant cluster is more challenging because Ingress
    is usually at the cluster level and has a poor permission model.</st> <st c="59766">Also,
    it supports namespaced configurations, but it is not suitable for multi-teams
    and shared</st> <st c="59861">load-balancing infrastructure.</st>
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="59635">构建多租户集群更加具有挑战性，因为 Ingress 通常是在集群级别，并且权限模型较弱。</st> <st c="59766">此外，Ingress
    支持命名空间配置，但不适合多团队和共享的</st> <st c="59861">负载均衡基础设施。</st>
- en: <st c="59891">Some of the beauty of the Ingress approach is its wide support
    and integration with other tools, such as the cert-manager for certificate management
    and handling of the external DNS, which we will see soon.</st> <st c="60099">Also,
    the Kubernetes maintainer claims that there is no plan to deprecate Ingress as
    it perfectly supports simple web traffic in an</st> <st c="60231">uncomp</st><st
    c="60237">licated manner.</st>
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="59891">Ingress 方法的优美之处在于它广泛的支持与其他工具的集成，例如用于证书管理的 cert-manager 和即将看到的外部
    DNS 处理。</st> <st c="60099">此外，Kubernetes 的维护者声称没有计划废弃 Ingress，因为它以一种简单的方式完美地支持简单的
    web 流量。</st> <st c="60231">方式。</st>
- en: <st c="60253">Gateway API – the new way</st>
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="60253">Gateway API – 新方式</st>
- en: <st c="60279">In the autumn of 2023, the Gateway API</st> <st c="60318">became
    generally available.</st> <st c="60347">Instead of a single resource, the gateway
    API consists of multiple resource types following a pattern that was already used
    in other</st> <st c="60480">critical integrations:</st>
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="60279">在 2023 年秋季，网关 API</st> <st c="60318">正式发布。</st> <st c="60347">网关
    API 由多个资源类型组成，而不是单一资源，它遵循的模式在其他</st> <st c="60480">关键集成中已被使用：</st>
- en: '`<st c="60502">Gateway</st>`<st c="60510">: Cluster entry point for</st> <st
    c="60537">incoming traffic</st>'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="60502">网关</st>`<st c="60510">: 进入集群的入口点</st> <st c="60537">流入的流量</st>'
- en: '`<st c="60553">GatewayClass</st>`<st c="60566">: Defines the gateway control
    type that will handle</st> <st c="60619">the gateway</st>'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="60553">GatewayClass</st>`<st c="60566">: 定义将处理</st> <st c="60619">网关的控制类型</st>'
- en: '`<st c="60630">*Route</st>`<st c="60637">: Implements the traffic routing from
    the gateway to</st> <st c="60691">the service:</st>'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="60630">*Route</st>`<st c="60637">: 实现从网关到</st> <st c="60691">服务的流量路由：</st>'
- en: '`<st c="60703">HTTPRoute</st>`'
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="60703">HTTPRoute</st>`'
- en: '`<st c="60713">GRPCRoute</st>`'
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="60713">GRPCRoute</st>`'
- en: '`<st c="60723">TLSRoute</st>`'
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="60723">TLSRoute</st>`'
- en: '`<st c="60732">TCPRoute</st>`'
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="60732">TCPRoute</st>`'
- en: '`<st c="60741">UDPRoute</st>`'
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="60741">UDPRoute</st>`'
- en: <st c="60750">Comparing the following diagram</st> <st c="60783">with the Ingress
    approach, we can see the two steps for incoming traffic going through the gateway
    and being redirected</st> <st c="60903">by</st> `<st c="60906">*Route</st>`<st
    c="60912">.</st>
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="60750">将下图与 Ingress 方法进行比较时，我们可以看到流入的流量通过网关并被重定向的两个步骤</st> <st c="60783">通过</st>
    `<st c="60906">*Route</st>`<st c="60912">。</st>
- en: '![Figure 4.11: Gateway API](img/Figure_4.11_B31164.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.11: 网关 API](img/Figure_4.11_B31164.jpg)'
- en: '<st c="60972">Figure 4.11: Gateway API</st>'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '<st c="60972">图 4.11: 网关 API</st>'
- en: <st c="60996">How do</st> <st c="61004">they compare?</st>
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="60996">它们如何比较？</st> <st c="61004">如何比较？</st>
- en: '|  | **<st c="61017">Ingress</st>** | **<st c="61025">Gateway API</st>** |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  | **<st c="61017">Ingress</st>** | **<st c="61025">网关 API</st>** |'
- en: '| <st c="61037">Protocols</st> | <st c="61047">HTTP/HTTPS only</st> | <st c="61063">L4
    and</st> <st c="61071">L7 protocols</st> |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| <st c="61037">协议</st> | <st c="61047">仅限 HTTP/HTTPS</st> | <st c="61063">L4
    和</st> <st c="61071">L7 协议</st> |'
- en: '| <st c="61083">Multi-Tenancy</st> | <st c="61097">Difficult/Custom Extensions</st>
    | <st c="61125">Multi-Tenant</st> <st c="61139">by design</st> |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| <st c="61083">多租户</st> | <st c="61097">困难/自定义扩展</st> | <st c="61125">按设计支持多租户</st>
    <st c="61139">设计</st> |'
- en: '| <st c="61148">Specifications</st> | <st c="61163">Annotation-based, controller-specific</st>
    | <st c="61201">Controller independent, own</st> <st c="61230">resource, standardized</st>
    |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| <st c="61148">规范</st> | <st c="61163">基于注释，控制器特定</st> | <st c="61201">控制器独立，拥有</st>
    <st c="61230">资源，标准化</st> |'
- en: '| <st c="61252">Definition/Resource</st> | <st c="61272">Ingress resource</st>
    | <st c="61289">Gateway, GatewayClass, *</st><st c="61314">Route resources</st>
    |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| <st c="61252">定义/资源</st> | <st c="61272">Ingress 资源</st> | <st c="61289">网关，GatewayClass，*</st><st
    c="61314">Route 资源</st> |'
- en: '| <st c="61330">Routing</st> | <st c="61338">host/path-based</st> | <st c="61354">Supports
    header</st> |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| <st c="61330">路由</st> | <st c="61338">基于主机/路径</st> | <st c="61354">支持头部</st>
    |'
- en: '| <st c="61370">Traffic Management</st> | <st c="61389">Limited to</st> <st
    c="61401">the vendor</st> | <st c="61411">Build-in/defined</st> |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| <st c="61370">流量管理</st> | <st c="61389">仅限于</st> <st c="61401">供应商</st> |
    <st c="61411">内建/定义</st> |'
- en: '<st c="61428">Table 4.2: Comparing Ingress and the Gateway API</st>'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '<st c="61428">表 4.2: 比较 Ingress 和网关 API</st>'
- en: <st c="61477">This rich set of features is a game-changer for those who have
    to build platforms.</st> <st c="61561">Previously, most of these capabilities
    had to be bought commercially or brutally forced into the cluster by some hacky
    workaround and self-developed tools.</st> <st c="61717">However, with the Gateway
    API, a wide range of protocols is supported, and it comes with two</st> <st c="61809">interesting
    additional</st> <st c="61833">features loaded.</st>
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="61477">这一丰富的功能集对需要构建平台的人来说是一个游戏规则改变者。</st> <st c="61561">此前，这些大多数功能必须通过商业购买，或通过一些变通办法和自开发工具强行集成到集群中。</st>
    <st c="61717">然而，借助网关 API，支持广泛的协议，并且还附带了两个</st> <st c="61809">有趣的额外</st> <st c="61833">功能。</st>
- en: <st c="61849">You can create cross-namespace routes with the Gateway API, either
    with</st> `<st c="61922">ReferenceGrant</st>` <st c="61936">or with</st> `<st
    c="61945">AllowedRoutes</st>`<st c="61958">. The allowed routes are implemented
    via reference bindings, which need to be defined by the gateway owner as from
    which namespaces traffic is expected.</st> <st c="62111">In practice, the Gateway
    configuration will be extended</st> <st c="62167">as follows:</st>
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="61849">您可以使用网关 API 创建跨命名空间路由，可以使用</st> `<st c="61922">ReferenceGrant</st>`
    <st c="61936">或者</st> `<st c="61945">AllowedRoutes</st>`<st c="61958">。允许的路由通过引用绑定实现，需要网关所有者定义来自哪些命名空间的流量。</st>
    <st c="62111">在实践中，网关配置将会扩展</st> <st c="62167">如下：</st>
- en: '[PRE5]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: <st c="62305">This will allow traffic from the</st> `<st c="62339">alpha</st>`
    <st c="62344">and</st> `<st c="62349">omega</st>` <st c="62354">namespaces.</st>
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="62305">这将允许来自</st> `<st c="62339">alpha</st>` <st c="62344">和</st> `<st
    c="62349">omega</st>` <st c="62354">命名空间的流量。</st>
- en: <st c="62366">Alternatively, we can use a</st> `<st c="62395">ReferenceGrant</st>`<st
    c="62409">, which is described</st> <st c="62430">as follows:</st>
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="62366">或者，我们可以使用</st> `<st c="62395">ReferenceGrant</st>`<st c="62409">，如下所述：</st>
- en: <st c="62441">ReferenceGrant can be used to enable cross namespace references
    within Gateway API.</st> <st c="62526">In particular, Routes may forward traffic
    to backends in other namespaces, or Gateways may refer to Secrets in another namespace.</st>
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="62441">ReferenceGrant 可用于启用网关 API 内的跨命名空间引用。</st> <st c="62526">具体来说，路由可以将流量转发到其他命名空间中的后端，或者网关可以引用另一个命名空间中的密钥。</st>
- en: <st c="62655">They sound similar, but they aren’t.</st> <st c="62693">Let’s</st>
    <st c="62699">compare them:</st>
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="62655">它们听起来相似，但实际上不是。</st> <st c="62693">让我们</st> <st c="62699">来比较一下：</st>
- en: '`<st c="62712">ReferenceGrant</st>`<st c="62727">: A Gateway and Route in namespace
    A grant a service in namespace B to</st> <st c="62799">forward traffic</st>'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="62712">ReferenceGrant</st>`<st c="62727">：在命名空间 A 中的网关和路由授予命名空间 B 中的服务</st>
    <st c="62799">转发流量</st>'
- en: '`<st c="62814">AllowedRoutes</st>`<st c="62828">: A Route to namespace B is
    configured in a Gateway in</st> <st c="62884">namespace C</st>'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="62814">AllowedRoutes</st>`<st c="62828">：在命名空间 C 中配置了网关对命名空间 B 的路由。</st>'
- en: <st c="62895">Besides those cross-namespace additional security layers, the
    Gateway API also comes with its own extension capabilities.</st> <st c="63018">With
    them, you can define your own PolicyAttachment, such as BackendTLSPolicy, to validate
    the proper usage</st> <st c="63126">of TLS.</st>
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="62895">除了这些跨命名空间的额外安全层外，网关 API 还具备自己的扩展能力。</st> <st c="63018">通过它们，您可以定义自己的策略附加，如后端
    TLS 策略，以验证正确的 TLS 使用方式</st> <st c="63126">。</st>
- en: <st c="63133">One last thing before we move on.</st> <st c="63168">The Gateway
    API comes with personas.</st> <st c="63205">Personas are pre-defined roles that
    allow fine-grained usage of gateway capabilities.</st> <st c="63291">Ingress just
    has one persona, a</st> <st c="63323">user, independent of whether it is an admin
    or a developer.</st> <st c="63383">The following table shows the write permissions
    of those personas in a</st> <st c="63454">four-tier model:</st>
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="63133">在我们继续之前的最后一件事。</st> <st c="63168">网关 API 自带角色。</st> <st c="63205">角色是预定义的角色，允许细粒度使用网关功能。</st>
    <st c="63291">入口只有一个角色，即</st> <st c="63323">用户，无论是管理员还是开发者。</st> <st c="63383">以下表格显示了这些角色在</st>
    <st c="63454">四层模型中的写权限：</st>
- en: '![Figure 4.12: Write permissions for an advanced four-tier model](img/Figure_4.12_B31164.jpg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 4.12: Write permissions for an advanced four-tier model](img/Figure_4.12_B31164.jpg)'
- en: '<st c="63693">Figure 4.12: Write permissions for an advanced four-tier model</st>'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="63693">图 4.12：高级四层模型的写权限</st>
- en: <st c="63755">At this point, the</st> <st c="63775">Gateway API is the true
    savior of countless nights of getting inbound traffic right and building a multi-tenant
    platform with a clear and</st> <st c="63913">transparent approach</st><st c="63933">.</st>
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="63755">在这一点上，</st> <st c="63775">网关 API 是解决无数夜晚正确处理入站流量并构建具有清晰和</st>
    <st c="63913">透明方法的多租户平台的真正救星。</st><st c="63933">。</st>
- en: <st c="63934">ExternalDNS</st>
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="63934">ExternalDNS</st>
- en: <st c="63946">The</st> **<st c="63951">ExternalDNS</st>** <st c="63962">project,
    developed</st> <st c="63981">by the Kubernetes contributors, is a tool that is
    often used within cloud-provided Kubernetes clusters, but still not often highlighted
    as a relevant implementation.</st> <st c="64148">Yet it bridges the gap between
    some random IPs of Pods in the cluster, takes it toward a proper DNS that is publicly
    or privately reachable, and routes traffic to the application within the platform.</st>
    <st c="64348">ExternalDNS provides support for almost every cloud and cloud-like
    environment, as well as traffic- and content-focused services such</st> <st c="64482">as
    CloudFlare.</st>
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="63946">该</st> **<st c="63951">ExternalDNS</st>** <st c="63962">项目由 Kubernetes
    贡献者开发，是一个常在云提供的 Kubernetes 集群中使用的工具，但仍然没有作为一个重要实现被广泛强调。</st> <st c="64148">然而，它弥合了集群中某些随机
    IP 和一个公开或私有可访问的 DNS 之间的鸿沟，并将流量路由到平台内的应用程序。</st> <st c="64348">ExternalDNS 支持几乎所有云环境以及类似云的环境，还支持如
    CloudFlare 这样的流量和内容聚焦型服务。</st>
- en: <st c="64496">However, ExternalDNS is not a DNS.</st> <st c="64532">Surprise!</st>
    <st c="64542">It reads the resources from the Kubernetes API and configures external
    DNS to point to the cluster’s public endpoints.</st> <st c="64661">You could also
    say that ExternalDNS allows you to control DNS records dynamically via Kubernetes
    resources in a DNS</st> <st c="64777">provider-agnostic way.</st>
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="64496">然而，ExternalDNS 不是一个 DNS。</st> <st c="64532">惊讶吧！</st> <st c="64542">它从
    Kubernetes API 读取资源，并配置外部 DNS 指向集群的公共端点。</st> <st c="64661">你也可以说，ExternalDNS
    让你能够通过 Kubernetes 资源以 DNS</st> <st c="64777">提供商无关的方式动态控制 DNS 记录。</st>
- en: <st c="64799">Let’s have a look at how ExternalDNS works together with the CoreDNS
    of Kubernetes and the Cloud DNS Service.</st> <st c="64910">In the next diagram,
    you can see the managed Kubernetes on the left.</st> <st c="64979">In this case,
    AWS EKS and the CoreDNS are running on Kubernetes to resolve internal DNS calls.</st>
    <st c="65074">When ExternalDNS is deployed, it observes the gateway, ingress,
    and service resources.</st> <st c="65161">When changes apply or new services</st>
    <st c="65196">come up, ExternalDNS updates the DNS records on the cloud provider
    or DNS</st> <st c="65270">service provider.</st>
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="64799">让我们来看看 ExternalDNS 如何与 Kubernetes 的 CoreDNS 和 Cloud DNS 服务一起工作。</st>
    <st c="64910">在下图中，你可以看到左侧是托管的 Kubernetes。</st> <st c="64979">在这种情况下，AWS EKS 和
    CoreDNS 正在 Kubernetes 上运行，以解析内部 DNS 请求。</st> <st c="65074">当部署了 ExternalDNS 后，它会观察网关、入口和服务资源。</st>
    <st c="65161">当发生变化或有新服务</st> <st c="65196">上线时，ExternalDNS 会更新云提供商或 DNS</st>
    <st c="65270">服务提供商上的 DNS 记录。</st>
- en: '![Figure 4.13: ExternalDNS](img/Figure_4.13_B31164.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.13：ExternalDNS](img/Figure_4.13_B31164.jpg)'
- en: '<st c="65554">Figure 4.13: ExternalDNS</st>'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="65554">图 4.13：ExternalDNS</st>
- en: '<st c="65578">However, when looking for an alternative approach, you usually
    have only two fallback options: create the DNS entries by yourself (manually)
    or have it automated in some way with a custom controller or function, or during
    the infrastructure</st> <st c="65820">creation process.</st>'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="65578">然而，在寻找替代方案时，你通常只有两种备选方式：自己创建 DNS 条目（手动）或通过自定义控制器、函数，或者在基础设施</st>
    <st c="65820">创建过程中实现自动化。</st>
- en: <st c="65837">Therefore, it is double painful to see the limitations</st> <st
    c="65892">of ExternalDNS:</st>
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '<st c="65837">因此，看到 ExternalDNS 的限制令人非常痛苦：</st> '
- en: <st c="65908">Missing fine-grained control; for example, ExternalDNS will create
    DNS records for all services and ingresses from</st> <st c="66024">any namespace</st>
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="65892">缺少细粒度的控制；例如，ExternalDNS 会为来自</st> <st c="66024">任何命名空间的所有服务和入口创建
    DNS 记录</st>
- en: <st c="66037">ExternalDNS gives you only A records; if you need a TXT or CNAME
    record, you have to do</st> <st c="66126">it manually</st>
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="66037">ExternalDNS 只提供 A 记录；如果你需要 TXT 或 CNAME 记录，你必须</st> <st c="66126">手动完成</st>
- en: <st c="66137">You will get default DNS configurations for the DNS name; otherwise,
    you have to manually</st> <st c="66227">define them</st>
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="66137">你将获得 DNS 名称的默认 DNS 配置；否则，你必须手动</st> <st c="66227">定义它们</st>
- en: <st c="66239">Besides that, you can also find increased costs (due to its outrageous
    DNS record creation behavior) and added complexity or latency.</st> <st c="66374">I
    can’t fully agree with those types of issues as it is a question of how you</st>
    <st c="66452">handle them.</st>
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="66239">除此之外，你还可能会遇到更高的费用（由于其不合理的 DNS 记录创建行为）以及增加的复杂性或延迟。</st> <st c="66374">我不能完全同意这些问题，因为这取决于你如何</st>
    <st c="66452">处理它们。</st>
- en: <st c="66464">Consider using ExternalDNS only when you have mastered other parts
    of Kubernetes networking and are able to do fine-grained management of network
    policies and the gateway API so that you have strict control over which services
    are reachable, and how.</st> <st c="66716">In addition, consider enabling the
    DNSSEC feature and also establishing</st> <st c="66788">DNS monitori</st><st c="66800">ng.</st>
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="66464">仅当你已经掌握 Kubernetes 网络的其他部分，并能够进行精细化的网络策略和网关 API 管理，从而严格控制哪些服务可达及如何访问时，才考虑使用
    ExternalDNS。</st> <st c="66716">此外，考虑启用 DNSSEC 功能，并建立</st> <st c="66788">DNS 监控。</st>
    <st c="66800">监控。</st>
- en: <st c="66804">Load balancing, EndpointSlices, and Topology-Aware Routing</st>
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="66804">负载均衡、EndpointSlices 和拓扑感知路由</st>
- en: <st c="66863">Lastly, to</st> <st c="66875">round off</st> <st c="66884">the
    network segment, we will briefly</st> <st c="66922">discuss</st> **<st c="66930">load
    balancing</st>**<st c="66944">,</st> **<st c="66946">EndpointSlices</st>**<st
    c="66960">, and</st> **<st c="66966">Topology-Aware Routing</st>**<st c="66988">.</st>
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="66863">最后，为了</st> <st c="66875">总结</st> <st c="66884">网络部分，我们将简要</st>
    <st c="66922">讨论</st> **<st c="66930">负载均衡</st>**<st c="66944">、</st> **<st c="66946">EndpointSlices</st>**<st
    c="66960">，以及</st> **<st c="66966">拓扑感知路由</st>**<st c="66988">。</st>
- en: <st c="66989">Load balancing is</st> <st c="67007">handled by the ingress controller
    or gateway API within the cluster.</st> <st c="67077">This can be outsourced to
    an external load balancer in combination with a cloud provider.</st> <st c="67167">All
    major cloud providers have their own approach, usually via their own controller,
    to manage the managed service load balancer.</st> <st c="67297">What is the difference
    between these options?</st> <st c="67343">Running your own load balancer within
    Kubernetes means that, first, all traffic gets routed to that entry point.</st>
    <st c="67456">With the correct setup, this can be multiple nodes with a very simple
    load distribution.</st> <st c="67545">The downside is that if one node is overloaded
    for some reason, it still gets the traffic to handle and distribute internally.</st>
    <st c="67672">A cloud load balancer will distribute the load across multiple nodes,
    and, depending on how the integration is done, it is aware of whether a node can
    handle more load or whether it should be redirected to another one.</st> <st c="67891">A</st>
    <st c="67893">downside of the public cloud load balancer is that you must also
    pay</st> <st c="67962">for it.</st>
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="66989">负载均衡由</st> <st c="67007">集群中的入口控制器或网关 API 处理。</st> <st c="67077">这可以通过与云服务提供商结合，外包给外部负载均衡器。</st>
    <st c="67167">所有主要的云服务提供商都有自己的方法，通常通过他们自己的控制器来管理托管服务的负载均衡器。</st> <st c="67297">这些选项之间有什么区别？</st>
    <st c="67343">在 Kubernetes 内部运行自己的负载均衡器意味着，首先，所有流量都会被路由到那个入口点。</st> <st c="67456">通过正确的设置，这可以是多个节点，并且负载分配非常简单。</st>
    <st c="67545">缺点是，如果某个节点由于某种原因超载，它仍然会处理并内部分配流量。</st> <st c="67672">云负载均衡器会将负载分配到多个节点，并且根据集成方式，它会知道某个节点是否能够处理更多的负载，或者是否应将流量重定向到另一个节点。</st>
    <st c="67891">一个</st> <st c="67893">公共云负载均衡器的缺点是，你必须为此付费。</st> <st c="67962">付费。</st>
- en: <st c="67969">EndpointSlices</st> <st c="67984">become relevant for large-scale
    clusters.</st> <st c="68027">Endpoints are API objects that define a list of IP
    addresses and ports.</st> <st c="68099">These addresses belong to the Pods that
    are dynamically assigned to a service.</st> <st c="68178">When a service is created,
    Kubernetes automatically creates an associated Endpoint object.</st> <st c="68269">The
    Endpoint object maintains the Pods’ IP addresses and port numbers that match the
    service’s selector criteria.</st> <st c="68383">EndpointSlices were introduced
    in Kubernetes 1.16\.</st> <st c="68434">They provide a way to distribute the network
    endpoints across multiple resources, reducing the load on the Kubernetes API server
    and improving the performance of large clusters.</st> <st c="68612">While in the
    past, a single large Endpoint object for a service became slow, multiple small
    EndpointSlice objects are now created, each representing a portion of</st> <st
    c="68774">the endpoints.</st>
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="67969">EndpointSlices</st> <st c="67984">在大规模集群中变得至关重要。</st> <st c="68027">Endpoint是定义IP地址和端口列表的API对象。</st>
    <st c="68099">这些地址属于动态分配给服务的Pod。</st> <st c="68178">当创建服务时，Kubernetes会自动创建一个关联的Endpoint对象。</st>
    <st c="68269">Endpoint对象维护与服务选择器标准匹配的Pod的IP地址和端口号。</st> <st c="68383">EndpointSlices在Kubernetes
    1.16中被引入。</st> <st c="68434">它们提供了一种将网络端点分布到多个资源上的方式，减少了Kubernetes API服务器的负载，并提高了大规模集群的性能。</st>
    <st c="68612">过去，单一的大型Endpoint对象会导致服务变慢，而现在创建了多个小型EndpointSlice对象，每个对象代表</st>
    <st c="68774">一部分端点。</st>
- en: <st c="68788">The control plane creates and manages EndpointSlices to have no
    more than 100 endpoints per slice.</st> <st c="68888">This can be changed, up
    to a maximum of 1,000\.</st> <st c="68935">For the kube-proxy, an EndpointSlice
    is the source of truth for routing</st> <st c="69007">internal traffic.</st>
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="68788">控制平面创建并管理EndpointSlices，每个slice的端点数不得超过100个。</st> <st c="68888">此限制可以调整，最大值为1,000。</st>
    <st c="68935">对于kube-proxy来说，EndpointSlice是路由</st> <st c="69007">内部流量的事实来源。</st>
- en: '<st c="69024">EndpointSlices are also required for</st> <st c="69062">Topology-Aware
    Routing.</st> <st c="69086">With Topology-Aware Routing, you can keep the traffic
    within the cloud provider’s</st> **<st c="69168">Availability Zone</st>** <st
    c="69185">(</st>**<st c="69187">AZ</st>**<st c="69189">).</st> <st c="69193">This
    has two main advantages: it reduces the</st> <st c="69238">network costs and improves
    the performance.</st> <st c="69282">Instead of a Pod in AZ 1 communicating with
    another Pod in AZ 2 and sending a lot of data, the Pod in AZ 1 will now talk to
    a replica (if available) that is also in AZ 1\.</st> <st c="69453">To make this
    work best, the incoming traffic should be distributed evenly via an external load
    balancer and you should have at least three endpoints per zone.</st> <st c="69612">Otherwise,
    the controller will fail to assign that endpoint with a chance of</st> <st c="69688">around
    50%.</st>'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="69024">EndpointSlices对于</st> <st c="69062">拓扑感知路由也是必需的。</st> <st c="69086">通过拓扑感知路由，你可以将流量保持在云提供商的</st>
    **<st c="69168">可用区</st>** <st c="69185">(</st>**<st c="69187">AZ</st>**<st c="69189">)内。</st>
    <st c="69193">这有两个主要优点：它减少了</st> <st c="69238">网络成本并提高了性能。</st> <st c="69282">而不是让AZ
    1中的Pod与AZ 2中的Pod通信并传输大量数据，AZ 1中的Pod现在将与AZ 1中的一个副本（如果有）进行通信。</st> <st c="69453">为了实现最佳效果，传入流量应通过外部负载均衡器均匀分配，并且每个区域应至少有三个端点。</st>
    <st c="69612">否则，控制器将无法分配该端点，失败的概率约为</st> <st c="69688">50%。</st>
- en: <st c="69700">Kubernetes as part of the platform control plane</st>
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="69700">Kubernetes作为平台控制平面的一部分</st>
- en: '<st c="69749">Looking back to the second chapter, we can see in the reference
    architecture that a platform can be highly distributed, based on many services
    that, from a higher viewpoint, might not even belong together.</st> <st c="69956">We
    discussed that Kubernetes might often become a central part of your platform.</st>
    <st c="70037">However, how central can it become?</st> <st c="70073">As said,
    Kubernetes is not just there to run workload; it is a platform (based on promise
    theory and a standardized model and API) for building platforms.</st> <st c="70228">This
    shifts Kubernetes with one foot into the platform control plane and does this
    in two ways: as a resource controller and as a</st> <st c="70358">platform or</st><st
    c="70369">chestrator.</st>'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="69749">回顾第二章，我们可以在参考架构中看到，平台可以是高度分布式的，基于许多服务，从更高的角度来看，这些服务可能甚至不属于同一组。</st>
    <st c="69956">我们讨论过，Kubernetes 很可能成为你平台的核心部分。</st> <st c="70037">然而，它能变得有多核心呢？</st>
    <st c="70073">如前所述，Kubernetes 不仅仅是用来运行工作负载的；它是一个平台（基于承诺理论和标准化模型及 API），用于构建平台。</st>
    <st c="70228">这使得 Kubernetes 将一只脚迈入平台控制平面，并通过两种方式实现这一点：作为资源控制器和作为</st> <st c="70358">平台或</st><st
    c="70369">协调器。</st>
- en: <st c="70381">Steering resources from within Kubernetes</st>
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="70381">从 Kubernetes 内部管理资源</st>
- en: <st c="70423">The open source</st> <st c="70440">Crossplane project is the only
    provider-independent solution for managing cloud resources from within Kubernetes.</st>
    <st c="70554">Initially created to manage other Kubernetes clusters from within
    Kubernetes, it quickly became the universal solution for handling cloud resources.</st>
    <st c="70703">Cloud resources are available as CRDs and can be defined as Kubernetes-native
    resources through a specification file.</st> <st c="70821">This gives users the
    option to define what they need and leave the resource creation on the promise
    theory of Kubernetes.</st> <st c="70943">For the different clouds, so-called providers
    are available, which define the available resources.</st> <st c="71042">A user
    can create single resources or whole compositions, which are multiple resources
    that</st> <st c="71134">bel</st><st c="71137">ong together.</st>
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="70423">开源的</st> <st c="70440">Crossplane 项目是唯一一个不依赖于任何提供商的解决方案，可以在 Kubernetes
    内部管理云资源。</st> <st c="70554">最初为了在 Kubernetes 内部管理其他 Kubernetes 集群而创建，它迅速成为处理云资源的通用解决方案。</st>
    <st c="70703">云资源作为 CRD 提供，可以通过规范文件将其定义为 Kubernetes 本地资源。</st> <st c="70821">这使得用户可以选择定义他们所需的内容，并将资源创建交给
    Kubernetes 的承诺理论来处理。</st> <st c="70943">对于不同的云，有所谓的提供商可用，用于定义可用的资源。</st> <st c="71042">用户可以创建单个资源或完整的组合，这些组合由多个资源组成，它们</st>
    <st c="71134">属于</st><st c="71137">同一组。</st>
- en: <st c="71151">The problem of external versus internally defined resources</st>
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="71151">外部资源与内部定义资源的问题</st>
- en: <st c="71211">What is the right approach to managing resources?</st> <st c="71262">Should
    they be defined by an infrastructure team or through input from demand forms?</st>
    <st c="71347">Can they be defined by the user via the platform?</st> <st c="71397">Anything
    is possible, but no simple</st> <st c="71433">answer exists.</st>
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="71211">管理资源的正确方法是什么？</st> <st c="71262">是应该由基础设施团队定义，还是通过需求表单的输入来定义？</st>
    <st c="71347">是否可以通过平台由用户来定义？</st> <st c="71397">一切皆有可能，但没有简单的</st> <st c="71433">答案。</st>
- en: '<st c="71447">Let’s have a look at the two approaches.</st> <st c="71489">First,
    let us take a look at a scenario at Financial One ACME, coming from a more traditional,
    conservative background: infrastructure teams have been fighting over the last
    few years for automation and a declarative approach.</st> <st c="71716">While
    they manage their on-premises environments through Ansible, they decided to use
    something simpler for the cloud providers: Terraform (or OpenTofu).</st> <st c="71870">We
    will not go through the whole stack, but until we hit the Kubernetes platform,
    everything is orchestrated through classic CI/CD push principles and IaC</st>
    <st c="72025">via Terraform.</st>'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="71447">让我们来看看这两种方法。</st> <st c="71489">首先，让我们看看来自传统保守背景下的 Financial One
    ACME 的一个场景：基础设施团队在过去几年里一直在争取自动化和声明性方法。</st> <st c="71716">虽然他们通过 Ansible 管理本地环境，但他们决定为云提供商使用更简单的工具：Terraform（或
    OpenTofu）。</st> <st c="71870">我们不会详细介绍整个技术栈，但直到我们进入 Kubernetes 平台之前，一切都通过经典的 CI/CD
    推送原则和 IaC</st> <st c="72025">通过 Terraform 来协调。</st>
- en: <st c="72039">Financial One ACME is starting a new project to develop custom
    software for their internal usage.</st> <st c="72138">The team will utilize the
    ACME platform and work on the system’s base architecture.</st> <st c="72222">They
    have defined that they will require certain file storage, a cache, a relational
    database, a notification service, and a message streaming service.</st> <st c="72374">As
    the platform provides some self-service, the team can copy the Terraform modules
    into their repository.</st> <st c="72481">From here, a predefined CI/CD pipeline
    will take over the configuration and deploy the resources in the defined environments.</st>
    <st c="72607">These self-defined but still externally managed resources are, in
    some ways, isolated from the rest of the system.</st> <st c="72722">They may live
    in the same repository or hierarchy and are managed by the team, but they are
    not</st> <st c="72818">strongly integrated.</st>
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="72039">Financial One ACME正在启动一个新项目，开发内部使用的定制软件。</st> <st c="72138">团队将利用ACME平台并着手系统的基础架构工作。</st>
    <st c="72222">他们已经确定需要某些文件存储、缓存、关系型数据库、通知服务和消息流服务。</st> <st c="72374">由于平台提供了一些自助服务，团队可以将Terraform模块复制到他们的仓库中。</st>
    <st c="72481">从这里开始，一个预定义的CI/CD管道将接管配置，并将资源部署到定义的环境中。</st> <st c="72607">这些自定义的但仍然由外部管理的资源在某些方面是与系统的其余部分隔离的。</st>
    <st c="72722">它们可能生活在同一个仓库或层级中，并由团队管理，但它们并没有</st> <st c="72818">紧密集成。</st>
- en: <st c="72838">On the other hand, they provide a certain level of stability.</st>
    <st c="72901">From within the platform user space, those resources are invisible,
    except that a discovery service exists.</st> <st c="73009">When an organization
    matures, the pipelines might be customized without notifying the owner or user.</st>
    <st c="73110">However, infrastructure and application are clearly separated, which
    is an advantage because of the totally different life cycles of</st> <st c="73243">the
    elements.</st>
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="72838">另一方面，它们提供了一定程度的稳定性。</st> <st c="72901">在平台用户空间内部，这些资源是不可见的，唯一存在的是一个发现服务。</st>
    <st c="73009">当一个组织逐渐成熟时，管道可能会被定制，而无需通知所有者或用户。</st> <st c="73110">然而，基础设施和应用程序是明确分离的，这是一个优势，因为它们的生命周期完全不同。</st>
    <st c="73243">这些元素。</st>
- en: <st c="73256">Fast forward and Financial One ACME has undergone a cloud-native
    transformation, leveraging platform engineering and IDPs to the maximum.</st>
    <st c="73395">Again, they plan to do a new internal project, which, of course,
    is completely different from the one before but somehow has exactly the same requirements.</st>
    <st c="73551">Some organizations’ behavior will never change.</st> <st c="73599">This
    time, the project team created a new project in their developer portal.</st> <st
    c="73676">Automatically, all base requirements will be pushed into a new Git repository.</st>
    <st c="73755">The chosen resources are selected and pushed to the platform, where
    a controller decides where it deploys those resources.</st> <st c="73878">Some
    end up as managed services on the cloud provider, others in a shared service account
    from a specialized team, and a few in the project’s namespace.</st> <st c="74031">After
    some time, the team understood that they had chosen the wrong configuration and,
    due to the adjustments, a migration to the new service took place.</st> <st c="74185">This
    scenario can be as true as the previous one but has</st> <st c="74242">different
    impacts.</st>
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="73256">快进到现在，Financial One ACME已经完成了云原生转型，最大限度地利用了平台工程和IDP。</st> <st
    c="73395">同样，他们计划开展一个新的内部项目，这个项目与之前的项目完全不同，但却有着完全相同的需求。</st> <st c="73551">一些组织的行为永远不会改变。</st>
    <st c="73599">这一次，项目团队在他们的开发者门户中创建了一个新项目。</st> <st c="73676">所有基本需求会自动推送到一个新的Git仓库中。</st>
    <st c="73755">所选的资源被选中并推送到平台，平台上的控制器决定在哪里部署这些资源。</st> <st c="73878">一些资源最终成为云服务商的托管服务，另一些则由专门的团队放在共享服务账户中，还有一些则在项目的命名空间内。</st>
    <st c="74031">过了一段时间，团队意识到他们选择了错误的配置，经过调整后，迁移到新服务的过程开始了。</st> <st c="74185">这个场景与之前的场景一样真实，但具有</st>
    <st c="74242">不同的影响。</st>
- en: <st c="74260">The team needs to know in more detail which requirements they
    have; on the other hand, they have to trust in the predefined deployments and
    configurations.</st> <st c="74417">The platform engineering team has, in collaboration
    with the operational team, defined best practices with guardrails, ensuring operability
    but also matching almost all the requirements of the users.</st> <st c="74617">Within
    the cluster user space, the team can find all deployed resources and address them
    as a service within the platform, even though they are not running in it.</st>
    <st c="74780">However, sudden changes on the user side might cause resources to
    suddenly spike or get deleted in other places.</st> <st c="74893">Managed service
    teams have to handle such changes without warning.</st> <st c="74960">In total,
    all depending resources are acting extremely volatile and dynamic, hard to predict,
    and difficult to manage.</st> <st c="75079">On the other hand, the project can
    focus fully on the flow and progress as external resources are handled from within
    the cluster, rather than learning how to manage those with outer-cluster resource
    management solutions such</st> <st c="75305">as IaC.</st>
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="74260">团队需要更详细地了解他们的需求；另一方面，他们必须信任预定义的部署和配置。</st> <st c="74417">平台工程团队与运营团队合作，定义了具有防护栏的最佳实践，确保可操作性，同时也几乎满足了用户的所有需求。</st>
    <st c="74617">在集群用户空间内，团队可以找到所有部署的资源，并将它们视为平台内的服务，即使它们并未在其中运行。</st> <st c="74780">然而，用户端的突然变化可能导致资源在其他地方突然增加或删除。</st>
    <st c="74893">管理服务团队必须在没有警告的情况下处理这些变化。</st> <st c="74960">总体而言，所有依赖资源都表现出极其不稳定和动态的特性，难以预测，也难以管理。</st>
    <st c="75079">另一方面，项目可以完全专注于流程和进展，因为外部资源是通过集群内部处理的，而不是通过外部集群资源管理解决方案（如 IaC）。</st>
- en: <st c="75312">Both approaches are fine.</st> <st c="75339">Both have pros and
    cons, and the one that is best for your organization often depends more on the
    human factor than any technological factor.</st> <st c="75481">However, what is
    clear is that internally defined cluster resources are more dynamic and shifted
    to the left, into the user’s responsibility, than in externally</st> <st c="75642">defined
    resources.</st>
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="75312">两种方法都可以。</st> <st c="75339">两者各有利弊，而对你的组织来说，最适合的方法往往更多地取决于人的因素，而不是技术因素。</st>
    <st c="75481">然而，显而易见的是，内部定义的集群资源比外部定义的更加动态，并且更多地转移到了用户的责任范围内。</st> <st c="75642">定义的资源。</st>
- en: <st c="75660">In the end, it becomes a philosophical discussion.</st> <st c="75712">Externally
    defined resources are more traditional, whereas the internally defined approach
    is progressive and future-oriented.</st> <st c="75839">However, we don’t have
    too many options to run cluster internal provisioning processes.</st> <st c="75927">Besides
    Crossplane, we have seen many meta-implementation controllers that read custom
    resources from the cluster and trigger CI/CD pipelines, for example.</st> <st
    c="76083">That’s a poor workaround, if</st> <st c="76112">someone asks.</st>
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="75660">最终，这变成了一场哲学讨论。</st> <st c="75712">外部定义的资源更为传统，而内部定义的方法则是进步和面向未来的。</st>
    <st c="75839">然而，我们没有太多选项来运行集群内的供应流程。</st> <st c="75927">除了 Crossplane，我们还看到了许多元实现控制器，它们从集群中读取自定义资源并触发
    CI/CD 流水线，例如。</st> <st c="76083">如果有人问。</st>
- en: <st c="76125">In this section, we looked at the fundamental capabilities that
    are required for Kubernetes, the challenges around them, and how we can solve
    them.</st> <st c="76274">Also, they don’t feel like any of those crazy implementations
    you see at conferences.</st> <st c="76360">Getting these basics right will make
    the difference and decide whether everything else on top will be a pleasure or
    a pain.</st> <st c="76484">Up next, we will close this chapter by looking into
    the approach of finding the right node sizes and the ramifications for flexibility</st>
    <st c="76619">and</st> <st c="76622">reliability.</st>
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="76125">在本节中，我们研究了 Kubernetes 所需的基本能力，围绕它们的挑战，以及如何解决它们。</st> <st c="76274">而且，它们不像你在会议上看到的那些疯狂的实现方式。</st>
    <st c="76360">正确掌握这些基础知识将决定其他一切是否会带来愉快或痛苦。</st> <st c="76484">接下来，我们将通过探讨找到合适的节点大小及其对灵活性</st>
    <st c="76619">和</st> <st c="76622">可靠性的影响来结束本章。
- en: <st c="76635">Designing for flexibility, reliability, and robustness</st>
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="76635">设计灵活性、可靠性和稳健性</st>
- en: '<st c="76690">In the previous part, we discussed cluster scalability and how
    the VPA, HPA, and CA play together.</st> <st c="76790">This helps to create a
    flexible, reliable, and robust system.</st> <st c="76852">A key part of this is
    also allowing customization as long as it doesn’t harm your system.</st> <st c="76942">Components
    of the cluster must play together seamlessly but must also be exchangeable where
    needed.</st> <st c="77042">This is sensitive: you have, on the one hand, a breathing
    cluster that grows and shrinks its demand over time; then, you have all the extensions
    on and around the cluster that allow you to serve the best possible feature set
    for your use case.</st> <st c="77286">You also have the continuously evolving
    open source community that frequently delivers updates and new developments, which
    should be integrated and made available for your users.</st> <st c="77465">As
    we told you earlier, this is why you must have a product mindset – to build the
    best possible platform for your users.</st> <st c="77587">Throw away things you
    don’t need or that are outdated, but keep the whole system as</st> <st c="77671">your
    core.</st>'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="76690">在上一部分中，我们讨论了集群的可扩展性，以及VPA、HPA和CA如何协同工作。</st> <st c="76790">这有助于创建一个灵活、可靠且强大的系统。</st>
    <st c="76852">其中一个关键部分是允许定制，前提是不会对系统造成损害。</st> <st c="76942">集群的各个组件必须无缝协作，但也必须在需要时能够互换。</st>
    <st c="77042">这非常敏感：一方面，你有一个随着时间推移而不断扩展和收缩需求的集群；另一方面，你还需要集群周围的所有扩展功能，以便为你的使用案例提供最佳的功能集。</st>
    <st c="77286">你还拥有一个不断发展的开源社区，该社区经常发布更新和新功能，这些更新和新功能应该集成并为你的用户提供。</st> <st c="77465">正如我们之前所说的，这就是为什么你必须拥有产品思维——为你的用户构建最佳的平台。</st>
    <st c="77587">扔掉不需要或过时的东西，但要保持整个系统作为</st> <st c="77671">你的核心。</st>
- en: '<st c="77681">Now, for some reason, we still see discussions about whether
    you should put all your workload on Kubernetes, and whether it is reliable.</st>
    <st c="77819">We have to look at this discussion from two perspectives: bottom-up
    from Kubernetes’s infrastructure and core responsibilities, and top-down from
    what the user can see</st> <st c="77987">an</st><st c="77989">d experience.</st>'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '<st c="77681">现在，出于某种原因，我们仍然看到关于是否应该将所有工作负载放在Kubernetes上，以及它是否可靠的讨论。</st> <st
    c="77819">我们必须从两个角度来看这个讨论：从Kubernetes的基础设施和核心职责的底层视角来看，以及从用户可以看到并体验到的顶层视角来看。</st> '
- en: <st c="78003">Optimize consumption versus leaving enough head space</st>
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="78003">优化资源消耗与留出足够的头部空间</st>
- en: <st c="78057">As we learned in</st> [*<st c="78075">Chapter 2</st>*](B31164_02.xhtml#_idTextAnchor055)<st
    c="78084">, the Kubernetes cluster and the workload it manages have an interesting
    relationship and influence on each other.</st> <st c="78199">Some applications
    require more CPU power, others scale up instead, and the rest just run on demand.</st>
    <st c="78299">Finding the right match isn’t easy, but an ideal target is high
    resource consumption as it optimizes the usage of the available resources and
    therefor</st><st c="78449">e</st> <st c="78452">the costs.</st>
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="78057">正如我们在</st> [*<st c="78075">第2章</st>*](B31164_02.xhtml#_idTextAnchor055)<st
    c="78084">中学到的那样，Kubernetes集群及其管理的工作负载之间有着一种有趣的关系，相互影响。</st> <st c="78199">有些应用程序需要更多的CPU能力，其他应用程序则会扩展，而其余的则根据需求运行。</st>
    <st c="78299">找到合适的匹配并不容易，但理想的目标是高资源消耗，因为它优化了可用资源的使用，从而</st><st c="78449">降低</st>
    <st c="78452">成本。</st>
- en: <st c="78462">How to make the clusters the right size</st>
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="78462">如何使集群达到合适的大小</st>
- en: <st c="78502">Evaluating the right size</st> <st c="78528">for the cluster is
    always a challenge.</st> <st c="78568">Finding the right solution is a clear case
    of</st> *<st c="78614">it depends</st>*<st c="78624">. Let’s take Financial One
    ACME, which has to provide a new cluster.</st> <st c="78693">We don’t know a lot
    about the expected workload, just that it requires some memory and has a few moving
    parts that are not that resource-demanding.</st> <st c="78841">So, we can take
    one of the</st> <st c="78868">following options:</st>
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="78502">评估集群的合适大小</st> <st c="78528">始终是一个挑战。</st> <st c="78568">找到合适的解决方案显然是一个</st>
    *<st c="78614">看情况而定</st>*<st c="78624">的问题。让我们以Financial One ACME为例，它必须提供一个新的集群。</st>
    <st c="78693">我们对预期的工作负载了解不多，只知道它需要一些内存，并且有几个不那么需要资源的活动组件。</st> <st c="78841">因此，我们可以选择以下</st>
    <st c="78868">选项之一：</st>
- en: <st c="78886">1x 16 vCPU, 64</st> <st c="78902">GB memory</st>
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="78886">1x 16 vCPU，64</st> <st c="78902">GB内存</st>
- en: <st c="78911">2x 8 vCPU, 32</st> <st c="78926">GB memory</st>
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="78911">2x 8 vCPU，32</st> <st c="78926">GB内存</st>
- en: <st c="78935">4x 4vCPU, 16</st> <st c="78949">GB memory</st>
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="78935">4x 4 vCPU，16</st> <st c="78949">GB内存</st>
- en: <st c="78958">8x 2vCPU, 8</st> <st c="78971">GB memory</st>
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="78958">8x 2vCPU，8</st> <st c="78971">GB 内存</st>
- en: <st c="78980">For availability reasons, the first option would be a bad idea.</st>
    <st c="79045">If you run an update in the cluster and the whole system goes down
    or you have to provision one new node, you need to shift all the apps over and
    shut down the old one.</st> <st c="79214">Option 4 comes with many nodes.</st>
    <st c="79246">Due to its small size, this can lead to a resource shortage as some
    base components that are required for the cluster will consume a part of the CPU
    and memory.</st> <st c="79407">In addition, depending on the cloud provider, it
    might be that you have other limitations, such as available IP addresses, bandwidth,
    and storage capacity.</st> <st c="79563">Also, if you have an app that needs 1
    vCPU, and might scale to 1.5 – 2.0 vCPU, it would kill an</st> <st c="79659">entire
    node.</st>
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="78980">出于可用性原因，第一个选项会是个坏主意。</st> <st c="79045">如果你在集群中进行更新，整个系统崩溃或你需要提供一个新的节点，你需要将所有应用程序迁移并关闭旧节点。</st>
    <st c="79214">选项 4 包含许多节点。</st> <st c="79246">由于其小巧的尺寸，这可能会导致资源短缺，因为一些基础组件将消耗部分
    CPU 和内存。</st> <st c="79407">此外，根据云提供商的不同，可能会有其他限制，比如可用的 IP 地址、带宽和存储容量。</st> <st
    c="79563">另外，如果你有一个需要 1 vCPU 的应用程序，并且可能扩展到 1.5 – 2.0 vCPU，它可能会导致一个</st> <st c="79659">完整节点崩溃。</st>
- en: <st c="79671">However, how many resources are used per node by Kubernetes?</st>
    <st c="79733">By default, for the CPU, we can use the</st> <st c="79773">following
    rules:</st>
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="79671">然而，Kubernetes 每个节点使用多少资源呢？</st> <st c="79733">默认情况下，对于 CPU，我们可以使用以下规则：</st>
- en: <st c="79789">6% of the</st> <st c="79800">first core</st>
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="79789">前 6%</st> <st c="79800">核心</st>
- en: <st c="79810">1% of the</st> <st c="79821">second core</st>
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="79810">第二核心的 1%</st> <st c="79821">核心</st>
- en: <st c="79832">0.5% of the next</st> <st c="79850">two cores</st>
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="79832">接下来 0.5%</st> <st c="79850">两个核心</st>
- en: <st c="79859">0.25% from the 5th</st> <st c="79879">core onward</st>
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="79859">从第 5 个核心起的 0.25%</st>
- en: <st c="79890">We also have some rough rules for</st> <st c="79925">the memory:</st>
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="79890">我们也有一些关于</st> <st c="79925">内存的粗略规则：</st>
- en: <st c="79936">25% of the first</st> <st c="79954">4 GB</st>
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="79936">前 25%</st> <st c="79954">4 GB</st>
- en: <st c="79958">20% of the following</st> <st c="79980">4 GB</st>
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="79958">以下内容的 20%</st> <st c="79980">4 GB</st>
- en: <st c="79984">10% of the next</st> <st c="80001">8 GB</st>
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="79984">接下来 10%</st> <st c="80001">8 GB</st>
- en: <st c="80005">6% of the next 112 GB (up to</st> <st c="80035">128 GB)</st>
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="80005">接下来的 112 GB（最多 128 GB）的 6%</st>
- en: <st c="80042">2% of anything above</st> <st c="80064">128 GB</st>
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="80042">任何超过 128 GB 的 2%</st>
- en: <st c="80070">In case the node has less than 1GB of memory, it is</st> <st c="80123">255
    MiB</st>
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="80070">如果节点内存小于 1GB，那么是</st> <st c="80123">255 MiB</st>
- en: <st c="80130">In addition, every</st> <st c="80150">node has an eviction threshold
    of 100 MB.</st> <st c="80192">If the resources are completely utilized and the
    threshold is crossed, Kubernetes starts cleaning up some Pods to prevent completely
    running out of memory.</st> <st c="80348">At learnk8s</st><st c="80359">, you
    can find a very detailed blog about</st> <st c="80401">it (</st>[<st c="80405">https://learnk8s.io/kubernetes-node-size</st>](https://learnk8s.io/kubernetes-node-size)<st
    c="80446">).</st>
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="80130">此外，每个</st> <st c="80150">节点有一个 100 MB 的驱逐阈值。</st> <st c="80192">如果资源完全利用且超出阈值，Kubernetes
    将开始清理一些 Pods，以防止内存完全耗尽。</st> <st c="80348">在 learnk8s</st><st c="80359">，你可以找到一篇非常详细的博客</st>
    <st c="80401">关于它（</st>[<st c="80405">https://learnk8s.io/kubernetes-node-size</st>](https://learnk8s.io/kubernetes-node-size)<st
    c="80446">）。</st>
- en: <st c="80449">Let’s visualize</st> <st c="80466">these numbers:</st>
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="80449">让我们可视化</st> <st c="80466">这些数字：</st>
- en: '|  | **<st c="80480">2 vCPU 8</st>** **<st c="80490">GB RAM</st>** | **<st
    c="80496">4 vCPU 16</st>** **<st c="80507">GB RAM</st>** | **<st c="80513">8 vCPU
    32</st>** **<st c="80524">GB RAM</st>** |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '|  | **<st c="80480">2 vCPU 8</st>** **<st c="80490">GB RAM</st>** | **<st
    c="80496">4 vCPU 16</st>** **<st c="80507">GB RAM</st>** | **<st c="80513">8 vCPU
    32</st>** **<st c="80524">GB RAM</st>** |'
- en: '| **<st c="80530">Kubelet +</st>** **<st c="80541">OS vCPU</st>** | <st c="80548">70
    m or</st> <st c="80557">0.07 vCPU</st> | <st c="80566">80 m or</st> <st c="80575">0.08
    vCPU</st> | <st c="80584">90 m or</st> <st c="80593">0.09 vCPU</st> |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| **<st c="80530">Kubelet +</st>** **<st c="80541">操作系统 vCPU</st>** | <st c="80548">70
    m 或</st> <st c="80557">0.07 vCPU</st> | <st c="80566">80 m 或</st> <st c="80575">0.08
    vCPU</st> | <st c="80584">90 m 或</st> <st c="80593">0.09 vCPU</st> |'
- en: '| **<st c="80602">Kubelet +</st>** **<st c="80613">OS memory</st>** | <st c="80622">1.8
    GB</st> | <st c="80629">2.6 GB</st> | <st c="80636">3.56 GB</st> |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| **<st c="80602">Kubelet +</st>** **<st c="80613">操作系统内存</st>** | <st c="80622">1.8
    GB</st> | <st c="80629">2.6 GB</st> | <st c="80636">3.56 GB</st> |'
- en: '| **<st c="80644">Eviction threshold</st>** | <st c="80663">100 MB</st> | <st
    c="80670">100 MB</st> | <st c="80677">100 MB</st> |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| **<st c="80644">驱逐阈值</st>** | <st c="80663">100 MB</st> | <st c="80670">100
    MB</st> | <st c="80677">100 MB</st> |'
- en: '| **<st c="80684">Available vCPU</st>** | <st c="80699">1930 m or</st> <st
    c="80710">1.9 vCPU</st> | <st c="80718">3920 m or</st> <st c="80729">3.9 vCPU</st>
    | <st c="80737">7910n or</st> <st c="80747">7.9 vCPU</st> |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| **<st c="80684">可用 vCPU</st>** | <st c="80699">1930 m 或</st> <st c="80710">1.9
    vCPU</st> | <st c="80718">3920 m 或</st> <st c="80729">3.9 vCPU</st> | <st c="80737">7910n
    或</st> <st c="80747">7.9 vCPU</st> |'
- en: '| **<st c="80755">Available memory</st>** | <st c="80772">6.1 GB</st> | <st
    c="80779">13.3 GB</st> | <st c="80787">28.34 GB</st> |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| **<st c="80755">可用内存</st>** | <st c="80772">6.1 GB</st> | <st c="80779">13.3
    GB</st> | <st c="80787">28.34 GB</st> |'
- en: '<st c="80796">Table 4.3: Resource consumption and available resources for different
    node sizes</st>'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="80796">表 4.3：不同节点大小的资源消耗和可用资源</st>
- en: <st c="80877">From those available</st> <st c="80899">resources, you also have
    to take away anything that is required to cluster, such as</st> `<st c="80983">DaemonSet</st>`
    <st c="80992">for logging and monitoring,</st> `<st c="81021">kube-proxy</st>`<st
    c="81031">, storage driver, and</st> <st c="81053">so on.</st>
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="80877">从这些可用的</st> <st c="80899">资源中，你还需要减去任何用于集群的资源，例如</st> `<st c="80983">DaemonSet</st>`
    <st c="80992">用于日志记录和监控，</st> `<st c="81021">kube-proxy</st>`<st c="81031">，存储驱动，等等。</st>
- en: <st c="81059">Doing the math, if you have many small nodes, the basic layer
    of used resources is larger than that of the one with big nodes.</st> <st c="81187">Let
    us look at an example.</st> <st c="81214">Two 2vCPU 8 GB RAM nodes require 140
    m of CPU and 3.6 GB of RAM in total, while one 4vCPU 16 GB RAM only requires 80
    m of CPU and 2.6 GB of RAM.</st> <st c="81359">Those are the costs for higher
    availability, but in a side-by-side comparison of just the available resources,
    a single node requires fewer resources to be assigned to Kubernetes.</st> <st
    c="81539">However, ideally, a node is utilized at its maximum capacity; we don’t
    run a virtual machine where 80% head space of non-utilized resources is standard!</st>
    <st c="81692">At least 80% utilization of a node would be the target to get the
    best performance/price ratio.</st> <st c="81788">This is also because of the energy
    proportionality of a server.</st> <st c="81852">The energy needed by a CPU doesn’t
    linearly scale with</st> <st c="81907">the workload.</st>
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="81059">进行计算，如果有许多小节点，所使用的基础资源层比大节点的要大。</st> <st c="81187">让我们来看一个例子。</st>
    <st c="81214">两个 2vCPU 8 GB RAM 节点总共需要 140 m 的 CPU 和 3.6 GB 的 RAM，而一个 4vCPU 16
    GB RAM 节点只需要 80 m 的 CPU 和 2.6 GB 的 RAM。</st> <st c="81359">这些是为了提高可用性所付出的成本，但在仅对比可用资源时，单个节点需要分配给
    Kubernetes 的资源较少。</st> <st c="81539">然而，理想情况下，节点应该达到其最大使用率；我们并不是运行一个虚拟机，标准情况下有
    80% 的未使用资源！</st> <st c="81692">至少 80% 的节点利用率应该是实现最佳性能/价格比的目标。</st> <st c="81788">这也是因为服务器的能源比例性。</st>
    <st c="81852">CPU 所需的能源并不会与</st> <st c="81907">工作负载呈线性比例增长。</st>
- en: <st c="81920">Imagine that a server can consume up to 200 watts.</st> <st c="81972">Then,
    most servers would need between 150 and 180 watts for around 50% of CPU utilization.</st>
    <st c="82063">This means the more a server is utilized, the better the energy/CPU
    utilization ratio, which is also more cost-efficient</st> <st c="82184">and sustainable.</st>
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="81920">假设一台服务器最多可以消耗 200 瓦特。</st> <st c="81972">那么，大多数服务器在 CPU 利用率达到
    50% 时需要消耗 150 到 180 瓦特之间的电量。</st> <st c="82063">这意味着，服务器的利用率越高，能源/CPU 的利用率越好，这也是更加具有成本效益</st>
    <st c="82184">和可持续的。</st>
- en: <st c="82200">We have to consider other factors while choosing the node sizes
    for</st> <st c="82269">the cluster:</st>
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="82200">在选择集群节点大小时，我们必须考虑其他因素：</st> <st c="82269">集群：</st>
- en: <st c="82281">If you provide just a few large nodes, but the user defines an
    anti-affinity so that on each node only one Pod of a replica is running and you
    don’t have enough nodes because the expected replicas are too high, then some
    Pods</st> <st c="82508">become unschedulable.</st>
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="82281">如果你提供的是少数几个大节点，但用户定义了反亲和性，使得每个节点上只能运行一个副本的 Pod，并且由于预期副本数过高导致节点不足，那么一些
    Pod</st> <st c="82508">将变得无法调度。</st>
- en: <st c="82529">Large nodes tend to be underutilized, so you spend more money
    on something you</st> <st c="82609">don’t use.</st>
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="82529">大节点往往资源利用率低，因此你会花更多的钱在一些你</st> <st c="82609">并不使用的东西上。</st>
- en: <st c="82619">If you have too many small nodes and you continuously run into
    pending Pods for which each cluster has to scale, that might make the users unhappy
    as it always takes time to scale up new nodes.</st> <st c="82814">Also, it might
    affect the servability of an app if it runs continuously under</st> <st c="82892">resource
    limitations.</st>
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="82619">如果你有太多小节点，并且不断遇到待处理的 Pods，每个集群都必须进行扩展，这可能会让用户不满，因为扩展新节点总是需要时间。</st>
    <st c="82814">此外，如果应用程序持续运行在</st> <st c="82892">资源限制下，也可能会影响其服务性。</st>
- en: <st c="82913">Many nodes cause a higher amount of network communication, container
    image pulls, and duplicate image storage on each node.</st> <st c="83038">Otherwise,
    in the worst case, you always pull an image from a registry again.</st> <st c="83116">With
    a small cluster, that isn’t a problem, but with a large number of nodes, this
    becomes</st> <st c="83207">quite chatty.</st>
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="82913">许多节点会导致更高的网络通信量、容器镜像拉取和每个节点上重复的镜像存储。</st> <st c="83038">否则，在最坏的情况下，你总是需要再次从注册表拉取镜像。</st>
    <st c="83116">对于小型集群来说，这不是问题，但对于大量节点来说，这会变得</st> <st c="83207">非常频繁。</st>
- en: <st c="83220">The more</st> <st c="83230">nodes there are, the more communication
    happens between them and the control plane.</st> <st c="83314">At some level of
    requests, this means increasing the node sizes of the</st> <st c="83385">control
    plane.</st>
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="83220">节点越多</st>，<st c="83230">它们之间以及控制平面之间的通信就越频繁。</st> <st c="83314">在某些请求量级下，这意味着需要增加</st>
    <st c="83385">控制平面的节点大小。</st>
- en: <st c="83399">Finding the right node and cluster size is a science in itself.</st>
    <st c="83464">Neither end of the extreme is good.</st> <st c="83500">Start looking
    at the kind of workload you expect.</st> <st c="83550">If you’re not sure, start
    with something medium-sized and adjust the node sizes if needed.</st> <st c="83641">Also,
    consider always having a little bit more memory available.</st> <st c="83706">The
    core components of Kubernetes per node don’t require a lot of CPU, but do require
    at least something between 2 and 3 GB of memory plus all the other</st> <st c="83859">default
    components</st><st c="83877">.</st>
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="83399">找到合适的节点和集群大小本身就是一门科学。</st> <st c="83464">极端的两端都不好。</st> <st c="83500">首先，考虑一下你期望的工作负载类型。</st>
    <st c="83550">如果不确定，可以从中等大小的节点开始，并根据需要调整节点大小。</st> <st c="83641">此外，始终考虑预留一些额外的内存。</st>
    <st c="83706">Kubernetes 每个节点的核心组件不需要大量的 CPU，但需要至少 2 到 3 GB 的内存，以及所有其他</st> <st
    c="83859">默认组件</st><st c="83877">。</st>
- en: <st c="83878">Solution space</st>
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="83878">解决方案空间</st>
- en: <st c="83893">Most public cloud providers come with the ability to run multiple
    instance types for Kubernetes nodes.</st> <st c="83997">This is helpful for different
    use cases, from isolating workloads to optimizing the utilization or migrating
    from one CPU architecture to another.</st> <st c="84144">We also talked about
    GPU utilization, which you can combine in such scenarios to run non-GPU workloads
    on CPU nodes while doing model training on the</st> <st c="84294">GPU instances.</st>
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="83893">大多数公共云提供商都支持运行多种实例类型的 Kubernetes 节点。</st> <st c="83997">这对于不同的使用场景非常有帮助，从隔离工作负载到优化利用率，或者从一种
    CPU 架构迁移到另一种架构。</st> <st c="84144">我们还讨论了 GPU 利用率，在这种场景下，你可以结合使用 CPU 节点运行非 GPU
    工作负载，同时在</st> <st c="84294">GPU 实例上进行模型训练。</st>
- en: <st c="84308">To do this, you have to manage and label the nodes correctly,
    and provide users with a transparent approach and support for defining their</st>
    <st c="84448">affinity settings.</st>
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="84308">为此，你必须正确管理和标记节点，并为用户提供透明的方法和支持，帮助定义他们的</st> <st c="84448">亲和性设置。</st>
- en: <st c="84466">To identify the right node sizes, learnk8s also provides you with
    an instance calculator (</st>[<st c="84557">https://learnk8s.io/kubernetes-instance-calculator</st>](https://learnk8s.io/kubernetes-instance-calculator)<st
    c="84608">), which you might consider before you start building your own Excel
    sheet for doing</st> <st c="84694">the math.</st>
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="84466">为了确定合适的节点大小，learnk8s 还为你提供了一个实例计算器（</st>[<st c="84557">https://learnk8s.io/kubernetes-instance-calculator</st>](https://learnk8s.io/kubernetes-instance-calculator)<st
    c="84608">），你可以在开始制作自己的 Excel 表格进行计算之前先考虑使用它。</st>
- en: <st c="84703">Also, when defining the cluster size and scale doesn’t look that
    relevant, with the right node sizes, you can have a direct impact on costs, utilization,
    application availability, user experience, and how many additional implementations
    you have to do to compensate for</st> <st c="84974">potential drawbacks</st><st
    c="84993">.</st>
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="84703">此外，在定义集群规模时，尽管规模看似不那么相关，但通过选择合适的节点大小，你可以直接影响成本、利用率、应用可用性、用户体验，以及为弥补</st>
    <st c="84974">潜在缺点</st><st c="84993">所需做的额外实现。</st>
- en: <st c="84994">Summary</st>
  id: totrans-373
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="84994">总结</st>
- en: '<st c="85002">In this chapter, we got closer to some of the relevant components
    of Kubernetes as the cornerstone for your platform.</st> <st c="85121">We first
    examined whether Kubernetes is the right choice, and also looked at why it often
    is the right way to go.</st> <st c="85235">With the promise theory at its heart
    and many robust features for running and extending a platform, Kubernetes is an
    almost perfect foundation for a platform.</st> <st c="85394">From here, we looked
    into some of the very basic elements of Kubernetes: storage, networking, CPU architectures,
    and GPU support.</st> <st c="85524">In this context, we learned about some design
    considerations and problems we might face while implementing Kubernetes.</st>
    <st c="85643">While Kubernetes as a foundation might feel different in every environment,
    it is possible to create a unified experience.</st> <st c="85766">This will come
    with major drawbacks, such as losing the features of certain cloud providers,
    flexibility,</st> <st c="85872">and customizability.</st>'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="85002">在这一章中，我们更深入地了解了Kubernetes的一些相关组件，它们构成了你平台的基石。</st> <st c="85121">我们首先考察了Kubernetes是否是正确的选择，并且探讨了它为何通常是一个明智的选择。</st>
    <st c="85235">Kubernetes以承诺理论为核心，具有许多强大的功能来运行和扩展平台，因此它几乎是平台的完美基础。</st> <st c="85394">从这里，我们探讨了Kubernetes的一些基本元素：存储、网络、CPU架构和GPU支持。</st>
    <st c="85524">在此背景下，我们了解了一些设计考量和在实现Kubernetes时可能遇到的问题。</st> <st c="85643">尽管Kubernetes作为基础在不同环境下可能有所不同，但完全可以创建一个统一的体验。</st>
    <st c="85766">然而，这将带来一些主要的缺点，比如失去某些云服务提供商的功能、灵活性，</st> <st c="85872">以及可定制性。</st>
- en: <st c="85892">Next, we discussed finding the balance between a very stiff and
    highly flexible system.</st> <st c="85981">Both can be seen as robust and reliable,
    but they come with very different challenges and problems.</st> <st c="86081">Therefore,
    we did a short thought experiment to find the right cluster sizes and node types
    before we closed this section by discussing approaches for implementing guardrails
    for the user space.</st> <st c="86276">This helps us provide flexibility within
    the user space but protects the platform from misbehavior and wrongly configured
    services by users.</st> <st c="86417">We learned about this in</st> <st c="86442">this
    chapter.</st>
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="85892">接下来，我们讨论了如何在一个非常严格与高度灵活的系统之间找到平衡。</st> <st c="85981">两者都可以被视为强大且可靠，但它们带来的是非常不同的挑战和问题。</st>
    <st c="86081">因此，我们进行了一个简短的思维实验，以确定正确的集群大小和节点类型，然后我们通过讨论如何在用户空间实施保护措施来结束这一部分内容。</st>
    <st c="86276">这帮助我们在用户空间内提供灵活性，但通过防止用户的不当行为和错误配置的服务，保护了平台。</st> <st c="86417">我们在</st>
    <st c="86442">这一章中学到了这些内容。</st>
- en: <st c="86455">In the next chapter, we will focus on the automation of platforms.</st>
    <st c="86523">Besides the infrastructure, automation is a critical component of
    a platform and, as you will see later on, can be a bottleneck and cost driver
    in the long run.</st> <st c="86684">You will learn how to design a proper release
    process, how to implement it in CI/CD and GitOps, and how to use this combination
    for the life cycle of the platform artifacts.</st> <st c="86858">We will also
    show you how to effectively observe</st> <st c="86907">this process</st><st c="86919">.</st>
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="86455">在下一章中，我们将重点关注平台的自动化。</st> <st c="86523">除了基础设施，自动化是平台的关键组件，正如你稍后将看到的，它在长期中可能成为瓶颈和成本驱动因素。</st>
    <st c="86684">你将学习如何设计一个适当的发布流程，如何在CI/CD和GitOps中实施该流程，并且如何使用这种组合来管理平台工件的生命周期。</st>
    <st c="86858">我们还将向你展示如何有效地观察</st> <st c="86907">这个过程</st><st c="86919">。</st>
- en: <st c="86920">Further Reading</st>
  id: totrans-377
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="86920">进一步阅读</st>
- en: <st c="86936">[1] Objects in</st> <st c="86952">Kubernetes:</st> [<st c="86964">https://kubernetes.io/docs/concepts/overview/working-with-objects/</st>](https://kubernetes.io/docs/concepts/overview/working-with-objects/
    )
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '<st c="86936">[1] Kubernetes中的对象</st>: [<st c="86964">https://kubernetes.io/docs/concepts/overview/working-with-objects/</st>](https://kubernetes.io/docs/concepts/overview/working-with-objects/)'
- en: <st c="87030">[2]</st> <st c="87035">Karpenter:</st> [<st c="87046">https://karpenter.sh/</st>](https://karpenter.sh/
    )
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="87030">[2]</st> <st c="87035">Karpenter:</st> [<st c="87046">https://karpenter.sh/</st>](https://karpenter.sh/
    )
- en: <st c="87067">[3] CSI drivers</st> <st c="87084">index:</st> [<st c="87091">https://kubernetes-csi.github.io/docs/drivers.html</st>](https://kubernetes-csi.github.io/docs/drivers.html
    )
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="87067">[3] CSI 驱动程序</st> <st c="87084">索引：</st> [<st c="87091">https://kubernetes-csi.github.io/docs/drivers.html</st>](https://kubernetes-csi.github.io/docs/drivers.html
    )
- en: <st c="87141">[4] RISC-V Kubernetes nodes on</st> <st c="87173">Scaleway:</st>
    [<st c="87183">https://www.scaleway.com/en/docs/bare-metal/elastic-metal/how-to/kubernetes-on-riscv/</st>](https://www.scaleway.com/en/docs/bare-metal/elastic-metal/how-to/kubernetes-on-riscv/
    )
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="87141">[4] RISC-V Kubernetes 节点在</st> <st c="87173">Scaleway 上：</st>
    [<st c="87183">https://www.scaleway.com/en/docs/bare-metal/elastic-metal/how-to/kubernetes-on-riscv/</st>](https://www.scaleway.com/en/docs/bare-metal/elastic-metal/how-to/kubernetes-on-riscv/
    )
- en: <st c="87268">[5]</st> <st c="87273">SpinKube:</st> [<st c="87283">https://www.spinkube.dev/</st>](https://www.spinkube.dev/
    )
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="87268">[5]</st> <st c="87273">SpinKube:</st> [<st c="87283">https://www.spinkube.dev/</st>](https://www.spinkube.dev/
    )
- en: <st c="87308">[6] SpinKube</st> <st c="87322">Overview:</st> [<st c="87332">https://www.spinkube.dev/docs/overview/</st>](https://www.spinkube.dev/docs/overview/
    )
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="87308">[6] SpinKube</st> <st c="87322">概述：</st> [<st c="87332">https://www.spinkube.dev/docs/overview/</st>](https://www.spinkube.dev/docs/overview/
    )
- en: <st c="87371">[7] Nvidia – Improving GPU Utilization in</st> <st c="87414">Kubernetes:</st>
    [<st c="87426">https://developer.nvidia.com/blog/improving-gpu-utilization-in-kubernetes/</st>](https://developer.nvidia.com/blog/improving-gpu-utilization-in-kubernetes/)
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="87371">[7] Nvidia – 改善 GPU 在</st> <st c="87414">Kubernetes 中的利用率：</st>
    [<st c="87426">https://developer.nvidia.com/blog/improving-gpu-utilization-in-kubernetes/</st>](https://developer.nvidia.com/blog/improving-gpu-utilization-in-kubernetes/)
- en: <st c="87500">[8] DRA</st> <st c="87509">with GPU:</st>
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="87500">[8] DRA</st> <st c="87509">与 GPU：</st>
- en: '[<st c="87518">https://docs.google.com/document/d/1BNWqgx_SmZDi-va_V31v3DnuVwYnF2EmN7D-O_fB6Oo/edit#heading=h.bxuci8gx6hna</st>](https://docs.google.com/document/d/1BNWqgx_SmZDi-va_V31v3DnuVwYnF2EmN7D-O_fB6Oo/edit#heading=h.bxuci8gx6hna)'
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[<st c="87518">https://docs.google.com/document/d/1BNWqgx_SmZDi-va_V31v3DnuVwYnF2EmN7D-O_fB6Oo/edit#heading=h.bxuci8gx6hna</st>](https://docs.google.com/document/d/1BNWqgx_SmZDi-va_V31v3DnuVwYnF2EmN7D-O_fB6Oo/edit#heading=h.bxuci8gx6hna)'
- en: '[<st c="87626">https://static.sched.com/hosted_files/colocatedeventseu2024/83/Best%20Practices%20for%20LLM%20Serving%20with%20DRA.pdf</st>](https://static.sched.com/hosted_files/colocatedeventseu2024/83/Best%20Practices%20for%20LLM%20Serving%20with%20DRA.pdf)'
  id: totrans-387
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[<st c="87626">https://static.sched.com/hosted_files/colocatedeventseu2024/83/Best%20Practices%20for%20LLM%20Serving%20with%20DRA.pdf</st>](https://static.sched.com/hosted_files/colocatedeventseu2024/83/Best%20Practices%20for%20LLM%20Serving%20with%20DRA.pdf)'
- en: '[<st c="87745">https://github.com/NVIDIA/k8s-dra-driver?tab=re</st>](https://github.com/NVIDIA/k8s-dra-driver?tab=re)'
  id: totrans-388
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[<st c="87745">https://github.com/NVIDIA/k8s-dra-driver?tab=re</st>](https://github.com/NVIDIA/k8s-dra-driver?tab=re)'
