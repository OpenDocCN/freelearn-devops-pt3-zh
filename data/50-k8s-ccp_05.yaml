- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: The On-Prem Kubernetes Reality Check
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地 Kubernetes 现实检查
- en: I know what you’re thinking – *0n-prem? Why is this guy teaching us about on-prem
    Kubernetes? It’s all in* *the cloud!*
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道你在想什么——*本地部署？为什么这个家伙在教我们本地 Kubernetes？不是都在* *云端吗！*
- en: Although it may seem like that from tech marketing and large cloud providers
    screaming Kubernetes at the top of their lungs, in the production world, there
    are a lot of on-prem Kubernetes clusters and a lot of engineers managing them.
    Mercedes-Benz, a popular German car manufacturer, hosts over 900 Kubernetes clusters
    on OpenStack, which is a private cloud solution. All those clusters are sitting
    in a data center, not in the public cloud. If you peel back the layers of the
    onion and wonder how cloud providers are running Kubernetes clusters, they’re
    doing something similar. They have several data centers that are running Kubernetes
    just like you would on-prem or on a server that you can buy on eBay. The only
    difference is the cloud providers are running Kubernetes at a major scale around
    the world. However, the *how* in how Kubernetes is running isn’t any different
    than how anyone else is running Kubernetes.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管从技术营销和大型云服务提供商大肆宣传 Kubernetes 似乎是这样，但在生产环境中，实际上有很多本地 Kubernetes 集群，也有很多工程师在管理它们。德国著名汽车制造商梅赛德斯·奔驰在
    OpenStack 上托管着超过 900 个 Kubernetes 集群，而 OpenStack 是一个私有云解决方案。所有这些集群都位于数据中心，而不是公共云中。如果你揭开云服务提供商运行
    Kubernetes 集群的层面，你会发现他们做的事情很相似。他们有几个数据中心，像你在本地部署或在 eBay 上购买的服务器一样运行 Kubernetes。唯一的区别是，云服务商是在全球范围内大规模地运行
    Kubernetes。然而，Kubernetes 运行的*方式*并没有什么不同。
- en: The truth is, this chapter could be an entire book – it could probably be a
    few books. Kubernetes, especially when it’s not abstracted away in the cloud,
    is an extremely large topic. There’s a reason why people say that Kubernetes is
    like a data center within itself. How and where you run on-prem Kubernetes alone
    is a deep topic. For example, what size infrastructure to use, how to scale your
    workloads, vertical and horizontal scaling, network bandwidth, high availability,
    and a lot more go into the conversation of what systems to use and where to run
    them.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，本章本身可以是一本完整的书——它可能是几本书。Kubernetes，尤其是在没有云抽象的情况下，是一个极为庞大的话题。人们说 Kubernetes
    就像是一个自带数据中心，这背后有其原因。仅仅是如何以及在哪里运行本地 Kubernetes 就是一个深奥的话题。例如，使用什么规模的基础设施，如何扩展工作负载，纵向和横向扩展，网络带宽，高可用性，以及更多的内容都涉及到使用何种系统以及在哪些地方运行它们的问题。
- en: By the end of this chapter, you’re going to understand just how complex running
    on-prem Kubernetes can be, but at the same time, how rewarding it can be to an
    organization that’s putting a lot of effort into Kubernetes. You’ll have the hands-on
    skills and theoretical knowledge to understand how to think about scaling an organization’s
    Kubernetes environment. One thing you’ll learn from this chapter is it’s a lot
    less about using the *cool tech* and more about thinking from an architecture
    perspective about how a platform team should look.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你将理解运行本地 Kubernetes 有多么复杂，但与此同时，对于那些在 Kubernetes 上投入大量努力的组织来说，它又是多么有回报。你将具备实践技能和理论知识，了解如何思考组织
    Kubernetes 环境的扩展问题。从本章你将学到的一件事是，使用*酷技术*远不如从架构角度思考平台团队应该是什么样的。
- en: 'In this chapter, we’re going to cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding operating systems and infrastructure
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解操作系统和基础设施
- en: Troubleshooting on-prem Kubernetes clusters
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排查本地 Kubernetes 集群的问题
- en: Introducing hybrid services
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍混合服务
- en: Exploring networking and system components
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索网络和系统组件
- en: Getting to know virtualized bare metal
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解虚拟化裸金属
- en: This chapter will be a combination of hands-on and theoretical knowledge. Because
    we only have one chapter to cover this topic, it’s safe to say that we can’t show
    everything you’ll need to know. However, this should be a good starting point
    for your production journey.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将结合实践操作和理论知识。由于我们只有一章来讨论这个话题，可以说我们无法展示你需要了解的所有内容。然而，这应该是你生产环境之旅的一个良好起点。
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To complete this chapter, you should first go over [*Chapter 2*](B19116_02.xhtml#_idTextAnchor038)
    and [*Chapter 3*](B19116_03.xhtml#_idTextAnchor060). Although that might sound
    obvious, we want to point it out as it’s crucial to understand the different deployment
    methods before diving into the on-prem needs of Kubernetes. Because cloud-based
    Kubernetes deployments abstract a lot of what you would do with on-prem, it still
    shows you the overall workflow of what components need to be deployed.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章，你应该首先阅读 [*第 2 章*](B19116_02.xhtml#_idTextAnchor038) 和 [*第 3 章*](B19116_03.xhtml#_idTextAnchor060)。虽然这听起来很显而易见，但我们还是要强调这一点，因为在深入了解
    Kubernetes 的本地需求之前，理解不同的部署方式是至关重要的。因为基于云的 Kubernetes 部署抽象了很多本地部署时需要做的事情，但它依然展示了组件需要部署的整体工作流。
- en: 'To work on this chapter, you should have some type of infrastructure and troubleshooting
    background. When it comes to on-prem Kubernetes clusters, they are extremely infrastructure-heavy,
    so getting through this chapter without that knowledge may be difficult. At the
    very least, you should have the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要学习本章内容，你应该具备一定的基础设施和故障排除背景。对于本地 Kubernetes 集群而言，它们对基础设施的依赖非常大，因此如果没有这些知识，完成本章可能会比较困难。至少，你应该具备以下内容：
- en: Linux knowledge
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linux 知识
- en: Server knowledge
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器知识
- en: The code for this chapter can be found in this book’s GitHub repository at [https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch4](https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch4).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在本书的 GitHub 仓库中找到，地址是：[https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch4](https://github.com/PacktPublishing/50-Kubernetes-Concepts-Every-DevOps-Engineer-Should-Know/tree/main/Ch4)。
- en: 'For the Kubeadm section of this chapter, you can follow along if you have two
    virtual machines available for your use. If you don’t, it’s perfectly fine: you
    can view this chapter from a theoretical perspective if that’s the case. However,
    if you have two extra VMs available, whether they’re on-prem or in the cloud,
    it would help you understand the overall explanations of this chapter a bit more..'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的 Kubeadm 部分，如果你有两台虚拟机可用，你可以跟着一起操作。如果没有也完全没问题：你可以从理论的角度来阅读本章内容。如果你有两台额外的虚拟机可用，无论是本地的还是云端的，都将帮助你更好地理解本章的整体讲解。
- en: Understanding operating systems and infrastructure
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作系统和基础设施的理解
- en: Everything starts at a server. It doesn’t matter if you’re running workloads
    on the cloud, on serverless platforms, or containers – everything starts at a
    server. The reason why engineers don’t always think about servers, or where workloads
    start in today’s world, is that the underlying infrastructure is abstracted away
    from us. In the cloud world, there aren’t a lot of times when you’ll have to ask,
    *what hardware are you using to run these VMs? Dell? HP?* Instead, you’re worried
    about what happens after the servers are deployed, which is sometimes called Day-Two
    Ops (insert more buzzwords here). What we mean by that is instead of ordering
    servers online, racking them, and configuring some virtualized hypervisor on them
    (ESXi, KVM, Hyper-V, and so on), engineers are more concerned now with automation,
    application deployments, platforms, and scalability.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都从服务器开始。不管你是在云端、无服务器平台还是容器上运行工作负载——一切都从服务器开始。工程师们不总是考虑服务器，或者说不总是考虑工作负载从哪里开始，这一点在今天的世界里是常见的，因为底层的基础设施已被抽象化了。在云计算的世界里，你很少需要问，*你用什么硬件来运行这些虚拟机？戴尔？惠普？*
    相反，你更关心的是服务器部署后的事情，这有时被称为“第二天操作”（这里插入更多流行词）。我们所指的意思是，现在工程师们更多关注的是自动化、应用部署、平台和可扩展性，而不是在线订购服务器、将它们上架并配置虚拟化管理程序（如
    ESXi、KVM、Hyper-V 等）。
- en: In many start-ups and small-to-medium-sized organizations, the typical reality
    is cloud computing. For larger organizations, another reality is on-prem workloads
    that are either virtualized or purely bare metal. If the combination of the cloud
    and on-prem gets brought up in discussion, this is where things such as hybrid
    solutions come into play, which you’ll learn about later in this chapter.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多初创公司和中小型组织中，典型的现实是使用云计算。对于大型组织，另一个现实是本地工作负载，这些工作负载要么是虚拟化的，要么是纯粹的裸金属。如果在讨论中提到云与本地结合的情况，这时像混合解决方案这样的内容就会出现，你将在本章稍后学习到这些内容。
- en: Let’s say you’re reading this right now and you’re working 100% in the cloud.
    You still need to understand VM sizes, scaling, the location of the VMs (the data
    center – regions, availability zones, geographies, and so on), network latency,
    and a large number of other pieces that fall into the systems and infrastructure
    category. For example, in the previous chapter, you learned about choosing worker
    node sizes for high CPU, high memory, and medium CPU/memory workloads within DigitalOcean
    and Linode.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你现在正在阅读这篇文章，并且100%在云端工作。你仍然需要了解虚拟机大小、扩展、虚拟机的位置（数据中心 – 区域、可用性区域、地理位置等）、网络延迟以及其他大量属于系统和基础设施类别的内容。例如，在上一章中，你学习了如何为DigitalOcean和Linode中的高CPU、高内存和中等CPU/内存工作负载选择工作节点大小。
- en: In this section, you’re going to learn about the core system and infrastructure
    needs that you must think about when architecting an on-prem Kubernetes platform.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将了解在架构本地Kubernetes平台时必须考虑的核心系统和基础设施需求。
- en: Kubeadm Deployment
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubeadm部署
- en: Before jumping into the theory, I wanted to showcase how you can bootstrap a
    Kubernetes cluster with Kubeadm. The primary reason is to show you what the process
    of actually deploying Kubernetes looks like while the pieces aren’t abstracted
    away from you. Abstraction is a great thing, but it’s only a great thing once
    you know the manual method of deployment. Otherwise, abstraction just ends up
    causing confusion.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入理论之前，我想展示一下如何通过Kubeadm启动Kubernetes集群。主要目的是展示实际部署Kubernetes的过程，同时让你看到部署过程中不被抽象化的部分。抽象化是一个很棒的东西，但只有在你了解手动部署的方法后，它才显得有意义。否则，抽象化只会引起混淆。
- en: For the Virtual Machines, the installation is based on Ubuntu. However, if you’re
    using another Linux distribution, it will work, but you’ll need to change the
    commands a bit to reflect the specific distro. For example, Ubuntu uses the Aptitude
    package manager and CentOS uses the Yum package manager.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于虚拟机，安装基于Ubuntu。然而，如果你使用的是其他Linux发行版，它也能工作，但你需要稍微修改命令以适应特定的发行版。例如，Ubuntu使用Aptitude包管理器，而CentOS使用Yum包管理器。
- en: Installing Control Plane and Worker Node
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装控制平面和工作节点
- en: Let’s get started.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: 'First, ensure that you update Ubuntu:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，确保更新Ubuntu：
- en: '[PRE0]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Install transport layer:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装传输层：
- en: '[PRE1]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Install Kubernetes package on Ubuntu:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Ubuntu上安装Kubernetes包：
- en: '[PRE2]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Update Ubuntu again now that the Kubernetes package exists:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新Ubuntu，确保Kubernetes包已存在：
- en: '[PRE4]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, change to the root user:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，切换到root用户：
- en: '[PRE5]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Install and configure the CRI-O container runtime:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装并配置CRI-O容器运行时：
- en: '[PRE6]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Exit out of root:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 退出root用户：
- en: '[PRE12]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Update Ubuntu again now that `CRI-O` is available:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，更新Ubuntu，确保`CRI-O`可用：
- en: '[PRE13]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Install `CRI-O`:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装`CRI-O`：
- en: '[PRE14]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Reload the Daemon and enable CRI-O:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新加载守护进程并启用CRI-O：
- en: '[PRE15]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Check to see CRI-O is installed properly:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查CRI-O是否安装正确：
- en: '[PRE17]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Turn off swap:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭交换分区：
- en: '[PRE18]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Configure `sysctl` settings and `ip` tables:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置`sysctl`设置和`ip`表：
- en: '[PRE19]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Install kubeadm:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装kubeadm：
- en: '[PRE27]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The next step is configuration.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是配置。
- en: Configuring the Control Plane
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置控制平面
- en: We need to define variables for the `kubeadm init` command. This will consist
    of IP addresses and the Pod CIDR range. Depending on where you are deploying it,
    you could either have just a public subnet, or a public and private subnet.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为`kubeadm init`命令定义变量。这将包括IP地址和Pod CIDR范围。根据你的部署位置，可能只有一个公共子网，或者同时有公共和私有子网。
- en: If you have just a public subnet, use the same value for the `ip_address` and
    `publicIP`, along with the `CIDR` range. If you have a private and public subnet,
    use the public IP for the `publicIP`, the private IP for the `ip_address`, and
    the private IP range for the `CIDR`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只有一个公共子网，使用相同的`ip_address`和`publicIP`值，以及`CIDR`范围。如果你有公共子网和私有子网，使用公共IP作为`publicIP`，私有IP作为`ip_address`，并使用私有IP范围作为`CIDR`。
- en: '[PRE28]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, initialize `kubeadm` on the Control Plane:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在控制平面上初始化`kubeadm`：
- en: '[PRE29]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'If you are deploying in the cloud, you may find yourself in a situation where
    the `init` fails because the Kubelet connect communicate with the API server.
    This typically happens in public clouds due to network restrictions. If it happens
    to you, open up the following ports:  .'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在云端部署，可能会遇到`init`失败的情况，因为Kubelet无法与API服务器进行通信。这种情况通常发生在公有云中，由于网络限制。如果发生这种情况，请打开以下端口：
- en: After the Kubeadm `init` is successful, you’ll see a few command outputs that
    show how to join more Control Planes and how to join Worker Nodes. Copy the Worker
    Node join command and run it on the Ubuntu server that you configured as the Worker
    node.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubeadm `init`成功之后，你会看到一些命令输出，显示如何加入更多控制平面以及如何加入工作节点。复制工作节点加入命令，并在你配置为工作节点的Ubuntu服务器上运行它。
- en: Next, install the CNI.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，安装CNI。
- en: 'If you don’t want to use Weave, you can see the network frameworks listed here:  .'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想使用Weave，可以查看这里列出的网络框架：。
- en: '[PRE30]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Next, we will look at system size.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一下系统规模。
- en: System size
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 系统规模
- en: Considerations about the system type, size, and how many nodes will be incredibly
    crucial for how you decide to think about on-prem deployments. What it all comes
    down to is what you’re planning on running on a Kubernetes cluster. If you’re
    just starting with your first Kubernetes cluster and you want to try containerizing
    an application to see how it works, how the dependency works, and ultimately starting
    on your journey, it’s going to be different than if you’re running 50+ Kubernetes
    clusters that are running stock trading/quants applications. At the end of the
    day, the system size that you use will be solely based on what workload you’re
    running.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 关于系统类型、大小和节点数量的考虑对于如何思考本地部署至关重要。最终，关键是你打算在Kubernetes集群上运行什么。如果你刚开始你的第一个Kubernetes集群，想尝试容器化一个应用程序，看看它是如何工作的，依赖是如何运作的，最终开始你的旅程，那将与运行50个以上Kubernetes集群、处理股票交易/量化应用的情况不同。归根结底，你使用的系统规模将完全取决于你正在运行的工作负载。
- en: 'Before you even think about creating a Kubernetes cluster on-prem, you must
    think about two important aspects:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在你考虑创建一个本地Kubernetes集群之前，你必须考虑两个重要方面：
- en: Do I have the hardware available and if not, what hardware do I have to buy?
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我有可用的硬件吗？如果没有，我需要购买什么硬件？
- en: What type of applications am I planning on running for the next 3 to 6 months?
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我计划在接下来的3到6个月里运行什么类型的应用程序？
- en: For example, let’s say that you buy 10 servers that you’re planning on running
    your application on. What size do the servers need to be? How will scaling work?
    Do you have a combination of memory-intensive apps and standard everyday apps?
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你购买了10台服务器，并计划在上面运行你的应用程序。那么这些服务器需要多大？如何进行扩展？你是有内存密集型应用和标准日常应用的组合吗？
- en: Another big consideration here is *scaling*. If you’re scaling horizontally,
    that means more Pods will be created, so more virtualized hardware will be consumed.
    If you’re scaling vertically, that means your Pods’ memory and CPU are increasing
    without you creating more Pods (which is known as vertical autoscaling). Not only
    do you have to plan for what applications you’re going to be running right off
    the bat, but you also have to plan for how those applications will be used. If
    you have 500 users today and you’re planning on having 2,000 users in 3 months
    based on company projections, that means the Pods will have an increased velocity
    in usage and that you may need more Pods. More usage means autoscaling, and autoscaling
    means more resources are needed.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的考虑因素是*扩展*。如果你是水平扩展，意味着会创建更多的Pod，因此会消耗更多的虚拟化硬件。如果你是垂直扩展，意味着你的Pod的内存和CPU会增加，而不需要创建更多的Pod（这就是垂直自动扩展）。你不仅要规划一开始运行什么应用程序，还要规划这些应用程序如何被使用。如果今天你有500个用户，并且根据公司预测在3个月内你会有2,000个用户，那就意味着Pod的使用速度会加快，可能需要更多的Pod。更多的使用意味着自动扩展，自动扩展意味着需要更多的资源。
- en: Sizing considerations
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尺寸考虑
- en: 'The following is a list of standard sizing considerations for when you’re building
    out a Kubernetes cluster:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是构建Kubernetes集群时标准尺寸的考虑事项：
- en: '**Standard workers**: These are your everyday web server Pods or middleware
    that still require virtualized hardware resources, but not at the same level as
    more intense applications. They are your more *generic apps* if you will. The
    worker nodes running here are mid-to-large in terms of size. If you’re just starting
    to get a Kubernetes cluster up and running and you’re maybe moving one or two
    containerized apps to Kubernetes as you get going, standard workers will be just
    fine to get the ball rolling.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准工作节点**：这些是你日常使用的Web服务器Pod或中间件，仍然需要虚拟化硬件资源，但不像更强大的应用那样要求高。它们可以算是你的*通用应用*。这里运行的工作节点中等到较大的尺寸。如果你刚开始搭建Kubernetes集群，并且可能在开始时将一两个容器化应用迁移到Kubernetes，标准工作节点完全可以帮助你起步。'
- en: '**Memory-intensive workers**: Applications that you know will require more
    memory/RAM than others should be accounted for with worker nodes that contain
    more RAM than the standard servers that are running as worker nodes. You want
    to ensure that if a Pod has to scale in replicas, or more Pods are added, you
    have enough memory. Otherwise, Pods won’t start and will stay pending until memory
    is allocated to them, and the scheduler can re-try scheduling the Pod for a node.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存密集型工作节点**：你知道将需要比其他应用程序更多内存/内存的应用程序，应该为这些应用程序安排更多 RAM 的工作节点，而不是标准的工作节点。你需要确保，如果一个
    Pod 需要进行副本扩展，或者添加了更多 Pods，你有足够的内存。否则，Pods 将无法启动，并且会一直处于待定状态，直到内存分配给它们，调度器才能重新尝试将
    Pod 调度到节点。'
- en: '**CPU-intensive workers**: Some applications will require more CPU and available
    threads to run the app. The same rules as those for memory-intensive apps apply
    here – if the scheduler can’t schedule the Pods because there aren’t enough resources
    available, the scheduler will wait until resources free up.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CPU 密集型工作节点**：某些应用程序会要求更多的 CPU 和可用线程来运行应用程序。与内存密集型应用程序相同的规则适用于此——如果调度器由于资源不足而无法调度
    Pods，它将等待直到资源释放。'
- en: '**Special-case workers**: A special-case Pod would usually be something such
    as an application that’s running a graphically intensive workload that needs a
    specific type of **Graphics Processing Unit** (**GPU**), which means the worker
    node needs a dedicated GPU or an app that requires faster bandwidth, so it requires
    a certain type of **Network Interface** **Card** (**NIC**).'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特殊情况工作节点**：特殊情况的 Pod 通常是运行图形密集型工作负载的应用程序，这些工作负载需要特定类型的 **图形处理单元**（**GPU**），这意味着工作节点需要一个专用的
    GPU，或者是一个需要更快带宽的应用程序，因此它需要某种类型的 **网络接口卡**（**NIC**）。'
- en: System location
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 系统位置
- en: 'When you run Kubernetes, there are primarily two types of nodes:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 Kubernetes 时，主要有两种类型的节点：
- en: '**Control Plane**: The Control Plane is where the API server lives. Without
    the API server, you can’t do much inside Kubernetes. It also contains the scheduler
    (how Pods know what node to go on), controller manager (controllers for Pods,
    Deployments, and so on to have the desired state), and the cluster store/database
    (etcd).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制平面**：控制平面是 API 服务器所在的地方。没有 API 服务器，你在 Kubernetes 中几乎无法做什么。它还包含调度器（Pods
    如何知道该去哪个节点）、控制器管理器（用于 Pods、Deployments 等的控制器，以确保达到所需的状态）以及集群存储/数据库（etcd）。'
- en: '**Worker node**: Worker nodes are where the Pods are installed after the Control
    Plane and on the worker node(s) run. They run the kubelet (the agent), container
    runtime (how containers run), kube-proxy (Kubernetes networking), and any other
    Pod that’s running.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作节点**：工作节点是 Pods 安装的地方，在控制平面之后，并且在工作节点上运行。它们运行 kubelet（代理）、容器运行时（容器的运行方式）、kube-proxy（Kubernetes
    网络）以及正在运行的任何其他 Pod。'
- en: Keeping these two node types in mind, you’ll have to figure out what automation
    techniques you want to use to run them and ultimately where/how you want to run
    them. It’s a consideration that you shouldn’t take lightly. The last thing that
    you want is to deploy Pods, then realize that the nodes they’re running on can’t
    handle the type of containerized app that’s running. As mentioned previously,
    the Control Plane is where the Kubernetes API sits. The Kubernetes API is how
    you do everything in Kubernetes. Without it, Kubernetes wouldn’t exist. With that
    being said, choosing where and how to run the Control Plane is the make or break
    between properly using Kubernetes and spending your days constantly troubleshooting.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记这两种节点类型后，你需要弄清楚想要使用哪些自动化技术来运行它们，最终决定在哪里/如何运行它们。这是一个不容小觑的考虑因素。你最不希望发生的事情就是部署了
    Pods，然后才意识到它们运行的节点无法处理正在运行的容器化应用程序。正如前面提到的，控制平面是 Kubernetes API 所在的位置。Kubernetes
    API 是你在 Kubernetes 中做一切事情的方式。如果没有它，Kubernetes 就无法存在。话虽如此，选择在哪里以及如何运行控制平面是正确使用
    Kubernetes 与每天都在不断排查故障之间的分水岭。
- en: Important note
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Explaining the Control Plane and worker nodes could take a chapter in itself.
    Because this book already expects you to know Kubernetes, we’re not diving into
    the types of Kubernetes nodes all that much. However, if you don’t know about
    the control plane and worker nodes, we highly recommend you take the time to learn
    about them before continuing. A great place to start is the Kubernetes docs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 解释控制平面和工作节点可能需要一章内容。由于本书已假设你了解 Kubernetes，我们不会深入探讨 Kubernetes 节点的类型。然而，如果你对控制平面和工作节点不了解，我们强烈建议你在继续之前花时间学习它们。一个很好的起点是
    Kubernetes 文档。
- en: Data centers and regional locations
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据中心和区域位置
- en: Data centers go down. Regions go down. **Internet Service Providers** (**ISPs**)
    go down. When you’re architecting where you want Kubernetes to run, there are
    several things that you must take into consideration.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中心会宕机，区域会宕机，**互联网服务提供商**（**ISP**）会宕机。当你架构 Kubernetes 运行位置时，有几个因素必须考虑。
- en: The first is where you’re running. For example, if your customers are in the
    UK, and you decide to run your data center in New Jersey, there’s going to be
    a ton of bandwidth issues and latency. Instead, it would make more sense to have
    a data center and a few co-locations throughout the UK.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 第一件事是你在哪里运行。例如，如果你的客户在英国，而你决定在新泽西州运行数据中心，那么就会出现大量带宽问题和延迟问题。相反，最好是在英国拥有一个数据中心和几个联合位置。
- en: Speaking of co-locations, you must make sure that you don’t have a single point
    of failure. The reality is that data centers do go down. They have internet issues,
    flooding, electric issues, and outages. If that occurs, the last thing that you
    want is to only have a single location. Instead, you should think about, at the
    very least, two data center locations. If one of the data centers fails, high
    availability needs to be put in place to *turn on* the other data center. For
    example, you could have a *hot/hot* or *hot/cold* high availability scenario.
    *Hot/hot* is recommended as all the data is being replicated by default to the
    second data center. If the first data center goes down, the second data center
    picks up where the first left off. Another consideration is where the data centers
    are. If the two data centers are only 5 miles away from each other and a storm
    comes, both could be impacted. Because of that, you want to put some distance
    between data centers.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 说到联合位置，你必须确保没有单点故障。现实情况是，数据中心确实会出现故障。它们可能会面临互联网问题、洪水、电力问题和停电等。如果发生这些问题，最后你最不希望的就是只有一个位置。相反，你应该至少考虑两个数据中心位置。如果其中一个数据中心发生故障，则需要启用高可用性来*启动*另一个数据中心。例如，你可以采用*热/热*或*热/冷*的高可用性方案。推荐使用*热/热*，因为所有数据默认会复制到第二个数据中心。如果第一个数据中心出现故障，第二个数据中心会接管第一个数据中心的工作。另一个需要考虑的因素是数据中心的位置。如果两个数据中心之间仅有
    5 英里远，而一场暴风雨来临时，两个数据中心都可能受到影响。因此，你需要在数据中心之间保持一定的距离。
- en: Where and how to run Kubernetes
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kubernetes 的运行位置和方式
- en: As you saw in the previous sections, the first step to figuring out your on-prem
    Kubernetes infrastructure is deciding what hardware and resources you need to
    run the applications you’re planning on running. Next, it’s all about figuring
    out what type of worker nodes you need. It’ll most likely be a combination of
    standard worker nodes and more intensive worker nodes with extra CPU and RAM.
    The final step (at least for this section) is figuring out how and where to run
    it.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在前面的部分看到的，弄清楚本地 Kubernetes 基础设施的第一步是决定你需要什么硬件和资源来运行你计划运行的应用程序。接下来，重点是弄清楚你需要什么类型的工作节点。它们很可能是标准工作节点和需要更多
    CPU 和内存的高强度工作节点的组合。最后一步（至少对于本节来说）是弄清楚如何以及在哪里运行它。
- en: 'There are several other options, but the following are a few of the popular
    ones:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他一些选项，但以下是一些流行的选项：
- en: '**OpenStack**: Although a lot of engineers in today’s world think OpenStack
    is dead, a lot of very large organizations are still using it. For example, almost
    every Telco provider uses OpenStack. Mercedes-Benz (at the time of writing) is
    hosting over 900 (yes, 900) Kubernetes clusters running in OpenStack. OpenStack
    gives you the feeling of being in the cloud, but it’s all on-prem and you’re hosting
    the private cloud yourself. The Open Infrastructure Foundation has put a lot of
    emphasis behind running Kubernetes on OpenStack with tools such as Magnum, which
    is the standard for running orchestration platforms (Kubernetes, Mesos, Docker
    Swarm, and so on), and Loki, which is the Linux/OpenStack/Kubernetes/infrastructure
    stack.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenStack**：尽管今天的许多工程师认为 OpenStack 已经死掉，但许多大型组织仍在使用它。例如，几乎每个电信服务提供商都在使用 OpenStack。奔驰（截至写作时）正在托管超过
    900 个（没错，900 个）运行在 OpenStack 上的 Kubernetes 集群。OpenStack 给你一种云端的感觉，但其实一切都在本地，你自己在托管私人云。开放基础设施基金会在使用工具（如
    Magnum）在 OpenStack 上运行 Kubernetes 上投入了大量精力，Magnum 是运行编排平台（如 Kubernetes、Mesos、Docker
    Swarm 等）的标准，而 Loki 则是 Linux/OpenStack/Kubernetes/基础设施栈。'
- en: '**Kubeadm**: If you don’t want to go the OpenStack route and if you’re using
    something such as a hypervisor, kubeadm is arguably the best option. There are
    a few different automated ways to get a Kubernetes cluster up and running, but
    kubeadm is more or less the most sophisticated. Using kubeadm, you can create
    a Kubernetes cluster that conforms to best practices. Other than installing the
    prerequisites on the Linux server for Kubernetes to run, kubeadm is pretty much
    automated. kubeadm has a set of commands that you can run that goes through several
    checks on the Linux server to confirm that it has all of the prerequisites and
    then installs the Control Plane. After that, there’s an output on the terminal
    that gives you a command to run more Control Planes and/or run worker nodes. You
    copy the command from the output, paste it into another server that you’re SSH’d
    into via the terminal, and run it. kubeadm is cool as well because it introduces
    you to the fact that running Kubernetes on-prem is straightforward. You can even
    run it on your laptop or a Rasberry Pi. There isn’t a high threshold to meet to
    run it, especially in a Dev environment.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubeadm**：如果你不想走 OpenStack 路线，并且使用的是像虚拟化管理程序这样的工具，那么 kubeadm 无疑是最佳选择。虽然有一些不同的自动化方式可以让
    Kubernetes 集群快速搭建，但 kubeadm 基本上是最为精细的。使用 kubeadm，你可以创建一个符合最佳实践的 Kubernetes 集群。除了在
    Linux 服务器上安装 Kubernetes 运行的前提条件外，kubeadm 基本上是自动化的。kubeadm 提供了一组命令，你可以运行这些命令，它会对
    Linux 服务器进行多个检查，以确认是否具备所有前提条件，然后安装控制平面。之后，终端会输出一个命令，告诉你如何运行更多的控制平面和/或运行工作节点。你只需将输出的命令复制，并粘贴到另一个通过终端
    SSH 连接的服务器上运行即可。kubeadm 的另一个优点是，它让你意识到在本地运行 Kubernetes 是非常简单的。你甚至可以在你的笔记本电脑或 Raspberry
    Pi 上运行它。要运行它的门槛并不高，尤其是在开发环境中。'
- en: '**Rancher**: Rancher acts as both a Kubernetes cluster creation tool and a
    Kubernetes cluster manager. Within Rancher, you can create a Kubernetes cluster
    and host it on Rancher, create a Kubernetes cluster in the cloud, or create a
    raw Kubernetes cluster by provisioning Linux virtual machines. You can also manage
    your Kubernetes clusters from Rancher. For example, if you have a bare-metal Kubernetes
    cluster that’s running with kubeadm or in OpenShift, you can manage it via Rancher.
    You can also manage cloud Kubernetes clusters.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Rancher**：Rancher 既是 Kubernetes 集群创建工具，又是 Kubernetes 集群管理工具。在 Rancher 中，你可以创建
    Kubernetes 集群并将其托管在 Rancher 上，或在云中创建 Kubernetes 集群，或者通过提供 Linux 虚拟机来创建原生 Kubernetes
    集群。你还可以通过 Rancher 管理你的 Kubernetes 集群。例如，如果你有一个使用 kubeadm 或 OpenShift 运行的裸金属 Kubernetes
    集群，你可以通过 Rancher 管理它。你还可以管理云中的 Kubernetes 集群。'
- en: '**Kubespray**: Kubespray, although (in our opinion) isn’t the best production-level
    option to go for, is still an option. Kubespray uses either Ansible or Vagrant
    to deploy a production-ready Kubernetes cluster on virtual machines or in the
    cloud. Because all you need is kubeadm instead of other *middleware*, such as
    Vagrant or Ansible, going straight for kubeadm saves you those extra hops needed
    to get a cluster created. Funnily enough, Kubespray uses kubeadm underneath the
    hood for cluster creation ([https://github.com/kubernetes-sigs/kubespray/blob/master/docs/comparisons.md](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/comparisons.md)),
    so that solidifies even more that there’s something to say about not going the
    extra hops to use Kubespray and instead, just go straight to kubeadm.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubespray**：虽然（在我们看来）Kubespray 并不是最佳的生产级选择，但它仍然是一个选项。Kubespray 使用 Ansible
    或 Vagrant 部署一个生产就绪的 Kubernetes 集群，可以在虚拟机或云环境中运行。因为你只需要 kubeadm，而不是像 Vagrant 或
    Ansible 这样的 *中间件*，所以直接使用 kubeadm 可以省去创建集群时的额外步骤。有趣的是，Kubespray 在幕后使用了 kubeadm
    来创建集群（[https://github.com/kubernetes-sigs/kubespray/blob/master/docs/comparisons.md](https://github.com/kubernetes-sigs/kubespray/blob/master/docs/comparisons.md)），这更进一步证明了不需要使用
    Kubespray 的额外步骤，直接使用 kubeadm 更加高效。'
- en: Operating system
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作系统
- en: 'To run the Kubernetes platform, you need an operating system to run it on.
    The two options that you have are as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行 Kubernetes 平台，你需要一个操作系统来承载它。你有以下两种选择：
- en: Run a bare-metal server and have the operating system run directly on the server
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行裸金属服务器，并让操作系统直接在服务器上运行
- en: Have a virtualized hypervisor, such as ESXi, that virtualizes the hardware and
    allows you to install the operating system on top of it
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有虚拟化管理程序，如 ESXi，可以虚拟化硬件并允许你在其上安装操作系统
- en: In today’s world, chances are you’re going to use a hypervisor. Unless there’s
    a specific need to run bare-metal servers and run the operating system directly
    on the server, engineers typically opt for a hypervisor. It’s much easier to manage,
    very scalable, and allows you to get a lot more out of the hardware.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天的世界里，您很可能会使用虚拟化管理程序。除非有特殊需求需要运行裸机服务器并直接在服务器上运行操作系统，否则工程师通常会选择虚拟化管理程序。这种方法更易于管理，具有很好的可扩展性，并且能让您充分利用硬件资源。
- en: When it comes to the operating system options, more or less, there are typically
    two available options. One is certainly used more than the other, but the other
    is gaining increased popularity.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在操作系统选项方面，大致上通常有两个可选项。一个肯定比另一个使用得更多，但另一个正在获得越来越高的关注度。
- en: Linux
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Linux
- en: More likely than not, you’ll be running worker nodes as Linux distributions.
    The most popular battle-tested distributions are Red Hat, CentOS, and Ubuntu.
    Linux is usually the *out-of-the-box* solution when it comes to Kubernetes worker
    nodes, and at the time of writing this book, you can only run Kubernetes Control
    Planes on Linux servers.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能，您将以 Linux 发行版的形式运行工作节点。经过实践验证的最流行的发行版是 Red Hat、CentOS 和 Ubuntu。Linux 通常是
    Kubernetes 工作节点的 *开箱即用* 解决方案，并且在写本书时，您只能在 Linux 服务器上运行 Kubernetes 控制平面。
- en: Windows
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Windows
- en: Although not seen all that much, especially with open-sourced and cross-platform
    versions of .NET, you can run Windows Server as a Kubernetes worker node. If you
    want to run Windows Server as a worker node, there are a few considerations. First,
    you must be running Windows Server LTSC 2019 or above. At the time of writing
    this book, the two options available are Windows Server 2019 and Windows Server
    2022.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管不太常见，尤其是在 .NET 开源和跨平台版本的影响下，您仍然可以将 Windows Server 作为 Kubernetes 工作节点来运行。如果您想将
    Windows Server 作为工作节点运行，需要考虑一些问题。首先，您必须运行 Windows Server LTSC 2019 或更高版本。在写本书时，当前可用的两个选项是
    Windows Server 2019 和 Windows Server 2022。
- en: With the Windows Server option, you will have to buy licenses and keep **Client
    Access License** (**CAL**) considerations in mind.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Windows Server 选项时，您需要购买许可证并考虑 **客户端访问许可证**（**CAL**）的问题。
- en: Now that you have an understanding of the overall operating system and infrastructure
    components of an on-prem Kubernetes cluster, in the next section, you’ll learn
    how to troubleshoot the environment you’re building.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经了解了本地 Kubernetes 集群的整体操作系统和基础设施组件，在下一部分中，您将学习如何排查您正在构建的环境。
- en: Troubleshooting on-prem Kubernetes clusters
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地 Kubernetes 集群的故障排除
- en: If you come from a systems administration/infrastructure background, troubleshooting
    Kubernetes clusters is going to come to you pretty naturally. At the end of the
    day, a Kubernetes cluster consists of servers, networking, infrastructure, and
    APIs, which are essentially what infrastructure engineers are working on day to
    day.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您来自系统管理员/基础设施背景，故障排除 Kubernetes 集群会对您来说非常自然。归根结底，Kubernetes 集群由服务器、网络、基础设施和
    API 组成，这些正是基础设施工程师日常工作的内容。
- en: If you’re a developer, some of these concepts may be new to you, such as troubleshooting
    networks. However, you’ll be very familiar with a few troubleshooting techniques
    as well, such as looking at and analyzing logs.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是开发人员，其中一些概念可能对您来说是新的，比如故障排除网络。然而，您也会对一些故障排除技术非常熟悉，例如查看和分析日志。
- en: 'The whole idea of troubleshooting a Kubernetes cluster is to look at two pieces:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 故障排除 Kubernetes 集群的整体思路是从两个方面进行观察：
- en: The cluster itself
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群本身
- en: The Pods running inside the cluster
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群内部运行的 Pods
- en: The cluster itself, including networking, servers, operating systems, and scalability,
    is going to be thought of from more of an infrastructure perspective, where something
    such as the **Certified Kubernetes Administrator** (**CKA**) comes into play nicely.
    The Pods, Deployments, container errors, and Pods not starting properly are going
    to be thought of more from a developer perspective, so learning the concepts of
    the **Certified Kubernetes Application Developer** (**CKAD**) would be a great
    stepping stone.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 集群本身，包括网络、服务器、操作系统和可扩展性，将更多地从基础设施的角度来考虑，此时类似 **认证 Kubernetes 管理员**（**CKA**）的概念非常适用。Pods、Deployments、容器错误以及
    Pods 无法正常启动等问题，将更多地从开发人员的角度来考虑，因此学习 **认证 Kubernetes 应用开发者**（**CKAD**）的概念将是一个很好的起点。
- en: In this section, you’re going to learn about the key ways to think about troubleshooting
    Kubernetes clusters and how to figure out problems in a digestible way.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，你将学习如何以易于理解的方式思考 Kubernetes 集群的故障排除，并找出问题的关键方法。
- en: Server logs and infrastructure troubleshooting
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务器日志和基础设施故障排除
- en: Although there’s an entire chapter in this book that goes over logging and observability
    ([*Chapter 7*](B19116_07.xhtml#_idTextAnchor161)) let’s talk about logging in
    a cluster sense. Typically, when you’re working with any type of observability
    metrics, such as logging, in the Kubernetes world, engineers are primarily thinking
    about the logs for an application. Those logs will help them troubleshoot a failing
    app and figure out what happened in the first place. However, from a Kubernetes
    on-prem perspective, server logging is a crucial piece.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书中有一整章内容讲解日志记录和可观察性（[*第 7 章*](B19116_07.xhtml#_idTextAnchor161)），但我们还是先来谈谈集群中的日志记录。通常，在
    Kubernetes 环境中，工程师主要关注应用程序的日志。这些日志帮助他们排查应用程序故障并弄清楚问题的根源。然而，从 Kubernetes 本地部署的角度来看，服务器日志是一个至关重要的部分。
- en: 'For the most part, unless otherwise specified, all the logs from the Control
    Plane and worker nodes typically go into `/var/log` on the Linux server. For Control
    Planes, the logs are at the following paths:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，除非另有说明，来自控制平面和工作节点的所有日志通常都会保存在 Linux 服务器的 `/var/log` 中。控制平面的日志位于以下路径：
- en: '`/``var/log/kube-apiserver.log`'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/``var/log/kube-apiserver.log`'
- en: '`/``var/log/kube-scheduler.log`'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/``var/log/kube-scheduler.log`'
- en: '`/``var/log/kube-controller-manager.log`'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/``var/log/kube-controller-manager.log`'
- en: 'For worker nodes, the logs are at the following paths:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于工作节点，日志位于以下路径：
- en: '`/``var/log/kubelet.log`'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/``var/log/kubelet.log`'
- en: '`/``var/log/kube-proxy.log`'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/``var/log/kube-proxy.log`'
- en: 'Out of the box, there isn’t a specific logging mechanism that Kubernetes uses.
    That’s essentially up to you to decide. The two primary ways that engineers capture
    logs for clusters are as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Kubernetes 并没有使用特定的日志机制。这基本上是由你来决定的。工程师收集集群日志的两种主要方式如下：
- en: Use a node logging agent that runs on every node across the cluster
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用在集群每个节点上运行的节点日志代理
- en: Have a log aggregator capture the logs from `/var/log` and send them to a logging
    platform
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用日志聚合器捕获 `/var/log` 中的日志并将其发送到日志平台
- en: You can find more documentation on troubleshooting clusters at [https://kubernetes.io/docs/tasks/debug/debug-cluster/](https://kubernetes.io/docs/tasks/debug/debug-cluster/).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [https://kubernetes.io/docs/tasks/debug/debug-cluster/](https://kubernetes.io/docs/tasks/debug/debug-cluster/)
    上找到更多关于故障排除集群的文档。
- en: Network observability
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络可观察性
- en: The networking piece of a Kubernetes cluster, which you’ll learn about shortly,
    is an extremely complex piece of Kubernetes within itself. Networking inside of
    Kubernetes is just as important as the Kubernetes API and all the other pieces
    that make up Kubernetes.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 集群的网络部分，稍后你将了解，它本身是 Kubernetes 中一个极为复杂的组件。Kubernetes 内部的网络和 Kubernetes
    API 以及其他所有组成 Kubernetes 的部分一样重要。
- en: The two things that you want to look out for are **cluster latency** and **Pod
    latency**. With cluster latency, it’s most likely going to come down to the standard
    systems administration troubleshooting around checking bandwidth, QoS on routers,
    how many packets are getting pushed through, NICs, and more. From a Pod latency
    perspective, it’ll most likely start at the cluster level in terms of the issues
    that the cluster may be having, but to troubleshoot, you’ll most likely look into
    something such as a service mesh.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要关注的两个方面是**集群延迟**和**Pod 延迟**。对于集群延迟，问题很可能归结为标准的系统管理故障排除，主要是检查带宽、路由器的 QoS、数据包传输量、网卡等。从
    Pod 延迟的角度来看，问题可能会从集群层面开始，涉及集群可能存在的问题，但要进行故障排除，你很可能需要查看类似服务网格的内容。
- en: Service mesh is a huge topic in itself, which could probably cover an entire
    course/book, but you’ll learn how to get started with it in the *Exploring networking
    and system* *components* section.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格本身是一个庞大的话题，可能需要涵盖一整本书/课程，但你将在*探索网络和系统* *组件*部分学习如何开始使用它。
- en: Kubernetes metrics
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 指标
- en: Most resources created in Kubernetes (Deployments, Pods, Services, and so on)
    have a metrics endpoint that can be found via the `/metrics` path when making
    an API call on Kubernetes resources.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中创建的大多数资源（如部署、Pod、服务等）都包含一个指标端点，可以通过 API 调用 Kubernetes 资源时的 `/metrics`
    路径找到。
- en: The metrics server collects logs from kubelets, which are agents that run on
    each Kubernetes node, and exposes them to the Kubernetes API server through the
    metrics API. However, this is primarily used for autoscaling needs. For example,
    the metrics will tell Kubernetes, *Hey, Pods are running low on memory utilization;
    we need a new Pod to handle application utilization*. Then, the vertical or horizontal
    autoscaler will kick off and do its job to create a new Pod, or vertically scale
    the current Pod(s).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 指标服务器收集来自kubelets的日志，kubelet是运行在每个Kubernetes节点上的代理，并通过指标API将它们暴露给Kubernetes
    API服务器。然而，这主要用于自动扩缩的需求。例如，指标会告诉Kubernetes，*嘿，Pods的内存利用率很低；我们需要一个新的Pod来处理应用程序的负载*。然后，垂直或水平自动扩展器会启动并完成任务，创建一个新的Pod或垂直扩展当前的Pod。
- en: If you want to collect metrics for, say, the monitoring platform that you use
    for observability, you’d want to collect metrics from the `/metrics/resource/resource_name`
    kubelet directly. Many observability platforms such as Prometheus will ingest
    these metrics and use them for troubleshooting and performance troubleshooting.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想收集例如用于可观察性的监控平台的度量指标，你需要直接从`/metrics/resource/resource_name` kubelet收集这些度量数据。像Prometheus这样的许多可观察性平台会接收这些度量数据，并用它们进行故障排除和性能调试。
- en: crictl
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: crictl
- en: Inside of every Kubernetes cluster, specifically running on the worker nodes,
    is a container runtime. Container runtimes such as containerd and CRI-O are mostly
    used in Kubernetes environments. Those container runtimes help make containers
    and Pods run. Because of that, it’s important to ensure that the container runtime
    is working as expected.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个Kubernetes集群内部，特别是在工作节点上，都有一个容器运行时。像containerd和CRI-O这样的容器运行时在Kubernetes环境中被广泛使用。容器运行时帮助容器和Pod运行。因此，确保容器运行时按预期工作非常重要。
- en: '`crictl` helps you troubleshoot the container runtime. You can run a few commands
    directed at a Pod that’ll help you understand what’s happening inside of a container.
    Keep in mind that `crictl` is in beta, but it’s still a great troubleshooting
    tool.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`crictl`帮助你排查容器运行时的问题。你可以运行一些针对Pod的命令，帮助你理解容器内部发生了什么。请记住，`crictl`仍处于beta阶段，但它依然是一个非常好的故障排除工具。'
- en: 'In the following example, `crictl` is listing a set of Pods:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，`crictl`列出了一个Pod集合：
- en: '[PRE31]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, it can look inside each Pod to see the containers that are running:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它可以查看每个Pod内部运行的容器：
- en: '[PRE32]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'You can also list out containers that are running and bypass Pods:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以列出正在运行的容器，并绕过Pod：
- en: '[PRE33]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: You can find more information about `crictl` at [https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md](https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md](https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md)找到更多关于`crictl`的信息。
- en: kubectl
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: kubectl
- en: 'Wrapping up this section, let’s talk about the `kubectl` command, which is
    the typical way that engineers interact with Kubernetes via the CLI. You’ll see
    three primary commands for troubleshooting:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节结束时，让我们讨论一下`kubectl`命令，这是工程师通过CLI与Kubernetes互动的典型方式。你会看到用于故障排除的三条主要命令：
- en: '`kubectl describe`: This command tells you the exact makeup of the Kubernetes
    Deployment, such as how it’s running, what containers are running inside of it,
    ports that are being used, and more. This is a great first step to understanding
    what could be going wrong inside of a Deployment.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubectl describe`：该命令告诉你Kubernetes部署的具体构成，例如它是如何运行的，里面运行了哪些容器，使用了哪些端口等。这是理解部署内部可能出问题的好第一步。'
- en: 'For example, if you had a Deployment called `nginx-deployment`, you’d run the
    following command:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你有一个名为`nginx-deployment`的Deployment，你需要运行以下命令：
- en: '[PRE34]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The following output showcases how `describe` looks for Deployments.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出展示了`describe`在Deployments中的表现。
- en: '![Figure 4.1 – Kubernetes Deployment output'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.1 – Kubernetes Deployment输出]'
- en: '](img/B19116_04_01.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B19116_04_01.jpg)'
- en: Figure 4.1 – Kubernetes Deployment output
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 – Kubernetes Deployment输出
- en: '`kubectl cluster-info dump`: This command is a literal dump of every single
    thing that’s happened on the cluster that was recorded. By default, all of the
    output is sent STDOUT, so you should ideally send the output to a file and look
    through it as it’s extremely verbose with a lot of data.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubectl cluster-info dump`：该命令是集群中每一项已记录事件的文字转储。默认情况下，所有输出都会发送到STDOUT，因此你最好将输出发送到文件并查看它，因为它非常冗长，包含大量数据。'
- en: 'The following screenshot has been cut off for simplicity, but it’s an example
    of the information shown with the `kubectl cluster-info` `dump` command:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图为简化起见有所裁剪，但它展示了通过`kubectl cluster-info` `dump`命令显示的信息：
- en: '![Figure 4.2 – Cluster dump output'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.2 – 集群转储输出'
- en: '](img/B19116_04_02.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B19116_04_02.jpg)'
- en: Figure 4.2 – Cluster dump output
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 集群转储输出
- en: '`kubectl logs`: This command is the bread and butter to understanding what’s
    happening inside of a Pod. For example, let’s say that you have a Pod called `nginx-deployment-588c8d7b4b-wmg9z`.
    You can run the following command to see the log output for the Pod:'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubectl logs`：此命令是理解Pod内部发生情况的基本命令。例如，假设您有一个名为`nginx-deployment-588c8d7b4b-wmg9z`的Pod。您可以运行以下命令来查看Pod的日志输出：'
- en: '[PRE35]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The following screenshot shows a sample of what logs look like for Pods.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了Pod日志的示例。
- en: '![Figure 4.3 – Nginx Pod log output'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.3 – Nginx Pod日志输出'
- en: '](img/B19116_04_03.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B19116_04_03.jpg)'
- en: Figure 4.3 – Nginx Pod log output
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – Nginx Pod日志输出
- en: Regardless of where a Kubernetes cluster is running, you’re always going to
    have to troubleshoot certain aspects of it. The tips in this section should help
    in an on-prem, and even sometimes a cloud, scenario.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 无论Kubernetes集群在哪里运行，您总是需要排查它的某些方面。本节中的提示应该有助于解决本地和有时云端的相关场景。
- en: Introducing hybrid services
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入混合服务
- en: From 2014 to 2015, what most organizations and engineers alike were reading
    sounded something similar to *data centers will go away*, *the cloud is the future*,
    and *everyone that isn’t in the cloud will be left behind*. Organizations started
    to feel pressured to move to the cloud and engineers started to get nervous because
    the skills they had honed for years were becoming obsolete. Coming back to the
    present, which is 2022 at the time of writing this book, mainframes still exist…
    so, yes, many organizations are still running on-prem workloads. Engineers that
    have an infrastructure and systems background are doing quite well for themselves
    in the new cloud-native era. The reason why is that 100% of the skills they have
    learned, other than racking and stacking servers, are still very relevant for
    the cloud and Kubernetes.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 从2014年到2015年，大多数组织和工程师看到的内容大致是*数据中心将消失*、*云是未来*和*所有不使用云的公司将被淘汰*。组织开始感受到向云迁移的压力，而工程师们则开始感到紧张，因为他们多年来培养的技能变得过时了。回到现在，也就是本书写作时的2022年，主机仍然存在……所以，是的，许多组织仍在运行本地工作负载。拥有基础设施和系统背景的工程师在新的云原生时代中过得相当不错。原因是，除了机架和堆叠服务器之外，他们所学到的100%的技能仍然非常适用于云和Kubernetes。
- en: In the *Understanding operating systems and infrastructure* section, you may
    remember reading about on-prem workloads and how they’re still relevant in today’s
    world. Although tech marketing may be making you feel otherwise, the truth is
    that on-prem workloads are still very much used today. They’re used so much that
    organizations such as AWS, Microsoft, and Google are realizing it, and they’re
    building services and platforms to support the need for a true hybrid environment,
    which means using on-prem and cloud workloads together, often managed in the same
    location.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在*理解操作系统和基础设施*部分，您可能还记得关于本地工作负载的内容，以及它们在今天的世界中仍然具有的重要性。尽管技术营销可能让你产生不同的感觉，事实是，本地工作负载在今天仍然被广泛使用。它们的使用量如此之大，以至于像AWS、微软和谷歌这样的组织也意识到了这一点，并且正在构建服务和平台来支持真正的混合环境需求，这意味着同时使用本地和云端工作负载，并且通常在同一个位置进行管理。
- en: In this section, you’re going to learn about the major cloud provider hybrid
    services, along with a few other companies that are helping in this area.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习主要云服务提供商的混合服务，以及一些其他公司在这一领域的努力。
- en: Azure Stack HCI
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure Stack HCI
- en: '**Azure Stack HCI** is the hybrid cloud offering from Microsoft. It gives you
    the ability to connect your on-prem environment to Azure. Azure Stack HCI typically
    comes running inside of a server from a vendor, though you can install it yourself
    on a compatible server with compatible hardware. It installs similar to any other
    operating system, but there’s a lot of complexity around the requirements. A few
    of the requirements include the following:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**Azure Stack HCI** 是微软的混合云产品。它使您能够将本地环境连接到Azure。Azure Stack HCI通常运行在供应商提供的服务器上，尽管您也可以在兼容的服务器和硬件上自行安装。它的安装过程与其他操作系统相似，但其要求有很大的复杂性。以下是一些要求：'
- en: At least one server with a maximum of 16 servers
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少一台服务器，最多16台服务器
- en: Required to deploy to two different sites
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须部署到两个不同的站点
- en: All servers must have the same manufacturer and use the same model
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有服务器必须来自相同制造商，并使用相同型号
- en: At least 32 GB of RAM
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少32GB RAM
- en: Virtualization support on the hardware (you have to turn this on in the BIOS)
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件需要支持虚拟化（你必须在BIOS中启用此功能）
- en: You can dive into the requirements a bit more; you’ll find that it goes pretty
    in-depth. From a time perspective, you’re probably better off buying an Azure
    Stack HCI-ready server from a vendor.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以深入了解一些需求，你会发现它涉及非常深入。从时间的角度来看，你可能更适合从供应商那里购买一台适配Azure Stack HCI的服务器。
- en: 'An interesting part of Azure Stack HCI is underneath the hood – it’s pretty
    much just Windows Server 2022 running Windows Admin Center. Because of that, you
    could completely bypass Azure Stack HCI and do the following instead:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Stack HCI的一个有趣之处在于其背后的原理——它实际上就是运行Windows Admin Center的Windows Server
    2022。因此，你可以完全绕过Azure Stack HCI，而选择做以下操作：
- en: Run a bunch of Windows Server 2022 Datacenter servers.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行一堆Windows Server 2022 Datacenter服务器。
- en: Cluster them up.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将它们集群化。
- en: Install Windows Admin Center.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装Windows Admin Center。
- en: Connect the servers to Azure.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将服务器连接到Azure。
- en: Run AKS on the servers.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在服务器上运行AKS。
- en: Google Anthos
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google Anthos
- en: '**Anthos** is arguably the most mature hybrid cloud solution that’s available
    right now. There are a ton of ways to automate the installation of Anthos with,
    for example, Ansible, and the hardware requirements to get it up and running are
    far more lite (at the time of writing this book) compared to Azure Stack HCI.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '**Anthos**可以说是目前最成熟的混合云解决方案。比如，可以使用Ansible等工具自动化安装Anthos，其硬件需求相比Azure Stack
    HCI（在写书时）轻量得多。'
- en: 'The hardware requirements are as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件需求如下：
- en: Four cores for CPU minimum, Eight cores recommended
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最低四核CPU，推荐八核
- en: 16 GB of RAM minimum, 32 GB recommended
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最低16GB RAM，推荐32GB
- en: 128 GB storage minimum, 256 GB recommended
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最低128GB存储，推荐256GB
- en: Like Azure Stack HCI, Anthos runs on-prem in your data center and connects to
    GCP to be managed inside of the GCP UI or with commands/APIs for GCP. The goal
    here is to run GCP to manage Kubernetes clusters on-prem and in the cloud.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 与Azure Stack HCI类似，Anthos在本地数据中心运行，并连接到GCP，通过GCP的UI或命令/API进行管理。这里的目标是通过GCP管理本地和云端的Kubernetes集群。
- en: A quick note about other infrastructure managers
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于其他基础设施管理工具的简要说明
- en: 'Although perhaps not considered hybrid cloud in itself as a platform, there
    are a few platforms that help you manage workloads anywhere. Two of the primary
    ones at the time of writing are as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它本身作为一个平台可能不被视为混合云，但有几个平台可以帮助你在任何地方管理工作负载。在写作时，以下是两个主要的：
- en: '**Azure Arc**: Azure Arc, as the name suggests, requires an Azure subscription.
    However, the cool thing about it is that you can manage Kubernetes clusters anywhere.
    If you have Kubernetes clusters in, for example, AWS, you can manage them with
    Azure Arc. If you have Kubernetes clusters on-prem, you can manage them with Azure
    Arc.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Arc**：顾名思义，Azure Arc需要Azure订阅。不过，它的一个亮点是，你可以在任何地方管理Kubernetes集群。如果你在AWS中有Kubernetes集群，可以通过Azure
    Arc管理它们。如果你有本地的Kubernetes集群，也可以用Azure Arc来管理。'
- en: '**Rancher**: Rancher is a vendor-agnostic solution that does all the management
    goodness that Azure Arc does, with a few other key features such as logging, deployments
    of Kubernetes servers, and security features to help you fully manage your Kubernetes
    clusters that are running anywhere.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Rancher**：Rancher是一个供应商无关的解决方案，提供类似Azure Arc的所有管理功能，还包括一些其他关键特性，如日志记录、Kubernetes服务器部署和安全功能，帮助你全面管理运行在任何地方的Kubernetes集群。'
- en: In the next section, you’ll learn about the overall network administration that’s
    needed inside of a Kubernetes cluster.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，你将了解Kubernetes集群中所需的整体网络管理。
- en: Exploring networking and system components
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索网络和系统组件
- en: 'Networking in a Kubernetes cluster, aside from the Kubernetes API itself, is
    what makes Kubernetes truly *tick*. Networking comes into play in various ways,
    including the following:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群中的网络，除了Kubernetes API本身，才是真正让Kubernetes *运转* 的关键。网络在多个方面发挥作用，包括以下几点：
- en: Pod-to-Pod communication
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod间通信
- en: Service-to-Service communication
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务间通信
- en: How nodes talk to each other inside of the cluster
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点如何在集群内部相互通信
- en: How users interact with your containerized applications
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户如何与容器化应用程序交互
- en: Without networking, Kubernetes wouldn’t be able to perform any actions. Even
    from a control plane/worker node perspective, worker nodes can’t successfully
    communicate with control planes unless proper networking is set up.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 没有网络，Kubernetes 将无法执行任何操作。即使从控制平面/工作节点的角度来看，工作节点也无法成功与控制平面通信，除非设置了适当的网络。
- en: This section could be, at the very least, two chapters in itself. Because we
    only have one section to hammer this knowledge down, let’s talk about the key
    components.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分至少可以分成两章来讲解。由于我们只有一个章节来讲解这些知识点，让我们谈谈关键组件。
- en: kube-proxy
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: kube-proxy
- en: When you first start to learn about how networking works inside of Kubernetes
    and how all resources communicate with each other, it all starts with kube-proxy.
    kube-proxy is almost like your switch/router in a data center. It runs on every
    node and is responsible for local cluster networking.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 当你第一次开始学习 Kubernetes 内部网络是如何工作的，以及所有资源如何相互通信时，一切都从 kube-proxy 开始。kube-proxy
    几乎就像数据中心中的交换机/路由器。它运行在每个节点上，负责本地集群的网络。
- en: 'It ensures the following:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 它确保以下内容：
- en: That each node gets a unique IP address
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个节点都会获得一个唯一的 IP 地址
- en: It implements local iptables or IPVS rules
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它实现了本地的 iptables 或 IPVS 规则
- en: It handles the routing and load balancing of traffic
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它处理流量的路由和负载均衡
- en: It enables communication for Pods
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使 Pods 之间的通信成为可能
- en: In short, it’s how every resource in a Kubernetes cluster communicates.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，它是 Kubernetes 集群中每个资源如何通信的方式。
- en: CNI
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNI
- en: The first step is kube-proxy, but to get it deployed, it needs to have a backend.
    That *backend* is the **Container Network Interface** (**CNI**). Attempting to
    run kube-proxy without a CNI is like trying to run a network on Cisco without
    having Cisco equipment – it doesn’t work.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是 kube-proxy，但为了部署它，需要有一个后端。这个*后端*就是**容器网络接口**（**CNI**）。试图在没有 CNI 的情况下运行
    kube-proxy，就像在没有 Cisco 设备的情况下尝试在 Cisco 上运行网络——根本行不通。
- en: The CNI is a network plugin, sometimes called a network framework, that has
    the responsibility of inserting a network framework into a Kubernetes cluster
    to enable communication, as in, to enable kube-proxy.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: CNI 是一个网络插件，有时被称为网络框架，负责将网络框架插入 Kubernetes 集群，以启用通信，也就是说，启用 kube-proxy。
- en: 'There are a ton of popular CNIs, including the following:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多流行的 CNIs，包括以下几种：
- en: Weave
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weave
- en: Flannel
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flannel
- en: Calico
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Calico
- en: Kubernetes resource communication
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 资源通信
- en: When you deploy Pods, especially microservices, which are *X* number of Pods
    running to make up one application, you need to ensure that Pod-to-Pod communication
    and Service-to-Service communication work.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 当你部署 Pods，尤其是微服务时，微服务是由*X*数量的 Pods 组成一个应用程序，你需要确保 Pod 到 Pod 以及服务到服务的通信能够正常工作。
- en: Pods communicate with each other via an IP address, which is given by kube-proxy.
    The way that services communicate with each other is by hostname and IP address,
    which is given by CoreDNS. Services provide a group of Pods associated with that
    service with a consistent DNS name and IP address. CoreDNS ensures the translation
    from hostnames to IP addresses.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Pods 通过 IP 地址相互通信，这个地址是由 kube-proxy 提供的。服务之间的通信方式是通过主机名和 IP 地址，而这些都是由 CoreDNS
    提供的。服务为与该服务相关联的一组 Pods 提供一致的 DNS 名称和 IP 地址。CoreDNS 确保主机名到 IP 地址的转换。
- en: DNS
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DNS
- en: Under the hood, Kubernetes runs **CoreDNS**, a popular open source DNS platform
    for converting IP addresses into names. When a Pod or a Service has a DNS name,
    it’s because the CoreDNS service (which is a running Pod) is running on Kubernetes
    properly.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在背后，Kubernetes 运行 **CoreDNS**，这是一个流行的开源 DNS 平台，用于将 IP 地址转换为名称。当一个 Pod 或服务有一个
    DNS 名称时，是因为 CoreDNS 服务（这是一个运行中的 Pod）在 Kubernetes 上正确运行。
- en: Service mesh and Ingress
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务网格与 Ingress
- en: Much like a lot of the other topics in this chapter, service meshes could be
    an entire chapter – the two topics mentioned here could be an entire book. However,
    let’s try to break it down into one section.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 就像本章中的许多其他主题一样，服务网格本身可以成为一整章——这里提到的两个主题甚至可以写成一本书。然而，让我们尝试将其简化为一节内容。
- en: An Ingress controller lets you have multiple Kubernetes Services being accessed
    via one controller or load balancer. For example, you could have three Kubernetes
    services named App1, App2, and App3, all connected to the same Ingress controller
    and accessible over the `/app1`, `/app2`, and `/app3` paths. This is possible
    via routing rules, which are created for the Ingress controller.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 控制器允许你通过一个控制器或负载均衡器访问多个 Kubernetes 服务。例如，你可以有三个名为 App1、App2 和 App3 的
    Kubernetes 服务，它们都连接到同一个 Ingress 控制器，并可以通过 `/app1`、`/app2` 和 `/app3` 路径进行访问。这是通过为
    Ingress 控制器创建路由规则来实现的。
- en: A service mesh, in short, helps you encrypt east-west traffic or service-to-service
    traffic, and troubleshoot network latency issues.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格简而言之，帮助你加密东西-东西流量或服务与服务之间的流量，并解决网络延迟问题。
- en: Sometimes, depending on the service mesh that you use, you may not need an Ingress
    controller as the service mesh may come built with one.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，取决于你使用的服务网格，你可能不需要 Ingress 控制器，因为服务网格可能自带一个。
- en: For Ingress controllers, check out Nginx Ingress, Traefik, and Istio. For service
    meshes, check out Istio.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Ingress 控制器，可以查看 Nginx Ingress、Traefik 和 Istio。对于服务网格，可以查看 Istio。
- en: In the next section, you’re going to learn about the ins and outs of how to
    think about virtualized bare metal and a few vendors that help on this journey.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，你将了解如何思考虚拟化裸金属的各个方面，并了解一些帮助你在这条路上前进的供应商。
- en: Getting to know virtualized bare metal
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解虚拟化裸金属
- en: 'If/when you’re planning to run Kubernetes on-prem, two questions may pop up:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 如果/当你计划在本地运行 Kubernetes 时，可能会出现两个问题：
- en: Where are we going to run Kubernetes?
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在哪里运行 Kubernetes？
- en: How are we going to run Kubernetes?
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何运行 Kubernetes？
- en: In today’s world, chances are you’re not going to run Kubernetes directly on
    bare metal (although you could, and some companies do). You’ll probably run Kubernetes
    on a hypervisor such as ESXi or in a private cloud such as OpenStack. You may
    also run Kubernetes on virtualized bare metal, which is different than running
    it on a hypervisor.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天的世界里，你很可能不会直接在裸金属上运行 Kubernetes（尽管你可以这样做，并且一些公司确实这么做）。你可能会在像 ESXi 这样的虚拟化管理程序上运行
    Kubernetes，或者在像 OpenStack 这样的私有云上运行。你也可能会在虚拟化裸金属上运行 Kubernetes，这与在虚拟化管理程序上运行是不同的。
- en: In this section, let’s learn what virtualized bare metal is and a few ways that
    you can run it.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，让我们了解什么是虚拟化裸金属以及如何运行它的几种方式。
- en: Virtualizing your environment
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虚拟化你的环境
- en: When thinking about virtualized bare metal, a lot of engineers will most likely
    think about a hypervisor such as VMware’s ESXi or Microsoft’s Hyper-V. Both are
    great solutions and allow you to take a bare-metal server that used to only be
    able to run one operating system and run multiple operating systems. There are
    many other pieces to a virtualized environment, such as virtualized hardware,
    networking, and more, all of which are extensive topics and could be an entire
    book in itself.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到虚拟化裸金属时，很多工程师很可能会想到像 VMware 的 ESXi 或 Microsoft 的 Hyper-V 这样的虚拟化管理程序。这两者都是非常好的解决方案，可以让你将曾经只能运行一个操作系统的裸金属服务器，变成可以运行多个操作系统的服务器。虚拟化环境还有许多其他组成部分，如虚拟化硬件、网络等，所有这些都是广泛的主题，足以成为一本完整的书。
- en: This not only helps you use as many resources out of the server as you can,
    but it also allows you to save on cost because servers are expensive.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅帮助你尽可能多地利用服务器的资源，还能帮助你节省成本，因为服务器很昂贵。
- en: The other solution is to run as close to bare metal as possible, but you don’t
    host it. Instead, you *rent* bare-metal servers from a hosting provider. When
    you rent the servers, they give you SSH or RDP access and you can access them
    the same way that you would if the servers were running in your data center. There’s
    a UI that you can use to create the servers, maybe some automated ways to do so
    if the platform allows it, and you can create Windows and/or Linux servers like
    you would on any other platform, such as if you were creating a web service or
    a server to host applications.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解决方案是尽可能接近裸金属运行，但你不进行托管。相反，你*租用*来自托管提供商的裸金属服务器。当你租用这些服务器时，提供商会给你 SSH 或 RDP
    访问权限，你可以像服务器位于你的数据中心一样访问它们。你可以使用一个用户界面来创建这些服务器，如果平台允许的话，可能还会有一些自动化方式，并且你可以像在任何其他平台上创建服务器一样，创建
    Windows 和/或 Linux 服务器，就像你在创建 Web 服务或托管应用程序的服务器时那样。
- en: Where to run Kubernetes
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在哪里运行 Kubernetes
- en: When thinking about where you’d want to run ESXi or Hyper-V, that’ll most likely
    come down to what servers you currently have, the partnerships with vendors your
    business has, and what resources (CPU, RAM, and so on) you need on the servers.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑在哪儿运行 ESXi 或 Hyper-V 时，这通常取决于您当前拥有的服务器、您公司与供应商的合作关系，以及您对服务器所需资源（如 CPU、RAM
    等）的需求。
- en: 'When it comes to the “*as close to bare metal as possible*” option, although
    there are many vendors, two stick out in the Kubernetes space:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 关于“*尽可能接近裸金属*”的选项，虽然有很多供应商，但在 Kubernetes 领域，有两个供应商尤为突出：
- en: '**Equinix**: Equinix allows you to – not only from a UI perspective but also
    from an automation perspective – use tools such as Terraform to create virtualized
    bare-metal servers for both Linux and Windows distributions. You can also manage
    networking pieces such as BGP and other routing mechanisms, as well as use on-demand,
    reserved, and spot servers:'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Equinix**：Equinix 允许您不仅从 UI 角度，而且从自动化角度，使用像 Terraform 这样的工具，为 Linux 和 Windows
    发行版创建虚拟化的裸金属服务器。您还可以管理网络组件，如 BGP 和其他路由机制，并使用按需、预留和竞价服务器：'
- en: '![Figure 4.4 – Equinix metal server creation'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.4 – Equinix Metal 服务器创建'
- en: '](img/B19116_04_04.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B19116_04_04.jpg)'
- en: Figure 4.4 – Equinix metal server creation
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – Equinix Metal 服务器创建
- en: 'In the following screenshot, you can see the general starting point in Equinix
    to start deploying servers:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下截图中，您可以看到在 Equinix 中启动服务器部署的一般起点：
- en: '![Figure 4.5 – Equinix Metal deployment page'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.5 – Equinix Metal 部署页面'
- en: '](img/B19116_04_05.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B19116_04_05.jpg)'
- en: Figure 4.5 – Equinix Metal deployment page
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 – Equinix Metal 部署页面
- en: '**OpenMetal**: OpenMetal is a full-blown virtualized bare-metal solution for
    running OpenStack. One of the cool parts about OpenMetal is that you get true
    SSH access to the literal servers that are running OpenStack, so you have a ton
    of flexibility and customization options, just like you would in any OpenStack
    environment. The only thing you don’t have access to is the actual hardware itself
    as that’s managed by OpenMetal, but you most likely don’t want access to it anyway
    if you’re looking for a virtualized bare-metal solution:'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenMetal**：OpenMetal 是一个完整的虚拟化裸金属解决方案，用于运行 OpenStack。OpenMetal 的一个很酷的特点是，您可以通过
    SSH 访问运行 OpenStack 的实际服务器，因此您可以像在任何其他 OpenStack 环境中一样，获得极大的灵活性和定制选项。唯一不能访问的就是实际的硬件，因为它由
    OpenMetal 管理，但如果您需要一个虚拟化的裸金属解决方案，您可能也不想直接访问硬件：'
- en: '![Figure 4.6 – OpenMetal dashboard'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.6 – OpenMetal 仪表板'
- en: '](img/B19116_04_06.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B19116_04_06.jpg)'
- en: Figure 4.6 – OpenMetal dashboard
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – OpenMetal 仪表板
- en: 'The following screenshot shows the standard UI in OpenStack, which is running
    on OpenMetal. This shows that nothing is different from using OpenStack on any
    other environment, which is great for engineers that are already used to it:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了在 OpenStack 中的标准 UI，该 UI 运行在 OpenMetal 上。这表明，使用 OpenStack 在任何其他环境中没有什么不同，对于那些已经习惯它的工程师来说，这是一个好消息：
- en: '![Figure 4.7 – OpenStack’s Overview dashboard'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.7 – OpenStack 概览仪表板'
- en: '](img/B19116_04_07.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B19116_04_07.jpg)'
- en: Figure 4.7 – OpenStack’s Overview dashboard
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – OpenStack 概览仪表板
- en: If you’re interested in running Kubernetes on-prem, but still want the feel
    of a *cloud*-based environment, OpenStack running on OpenMetal is a great place
    to start.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣在本地运行 Kubernetes，但仍然希望拥有类似 *云* 环境的感觉，运行在 OpenMetal 上的 OpenStack 是一个很好的起点。
- en: Summary
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: There’s a lot that wasn’t talked about in this chapter – storage, different
    interface types, hardware types, the ins and outs of Kubernetes clusters, and
    a lot more. The reason why is that this chapter could only be so long and a lot
    of the topics could take up an entire chapter.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中有很多内容没有涉及——存储、不同的接口类型、硬件类型、Kubernetes 集群的来龙去脉等等。原因是本章的篇幅有限，许多主题可能需要一个完整的章节来讲解。
- en: However, the goal of this chapter was to give you a place to start.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，本章的目标是为您提供一个起点。
- en: As you learned throughout this chapter and may have come to realize, managing
    Kubernetes on-prem can almost feel like an entire data center within itself. You
    have networking concerns, scalability concerns, storage concerns, network concerns…
    the list goes on and on. However, if you want the flexibility of managing Kubernetes
    yourself without relying on a cloud provider, then this chapter went over what
    you should think about from the beginning.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在这一章中学到的内容，并且可能已经意识到的那样，在本地管理 Kubernetes 几乎可以感觉像是管理一个完整的数据中心。你需要考虑网络问题、可扩展性问题、存储问题、网络问题……问题层出不穷。然而，如果你希望自己管理
    Kubernetes 而不依赖云服务提供商，那么本章已经介绍了你从一开始就需要考虑的事项。
- en: Running Kubernetes on-prem is no easy task. You will most likely have to have
    a team of engineers – or at the very least two to three engineers with a very
    strong systems administration and network administration background. If you don’t
    already have those skills, or if your team doesn’t, this chapter should have given
    you a good starting point on where to look.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地运行 Kubernetes 绝非易事。你很可能需要一支工程师团队——至少需要两到三名具有非常强的系统管理和网络管理背景的工程师。如果你还没有这些技能，或者你的团队没有，这一章应该为你提供了一个很好的起点，帮助你找到需要学习的方向。
- en: In the next chapter, you’ll start looking at the *how* and *why* behind deploying
    applications on Kubernetes.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将开始了解在 Kubernetes 上部署应用程序的*方式*和*原因*。
- en: Further reading
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resource:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于本章涉及的主题，可以查看以下资源：
- en: '*OpenStack Cookbook*, by Kevin Jackson, Cody Bunch, and Egle Sigler: [https://www.packtpub.com/product/openstack-cloud-computing-cookbook-fourth-edition/9781788398763](https://www.packtpub.com/product/openstack-cloud-computing-cookbook-fourth-edition/9781788398763)'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*OpenStack Cookbook*，作者 Kevin Jackson、Cody Bunch 和 Egle Sigler：[https://www.packtpub.com/product/openstack-cloud-computing-cookbook-fourth-edition/9781788398763](https://www.packtpub.com/product/openstack-cloud-computing-cookbook-fourth-edition/9781788398763)'
