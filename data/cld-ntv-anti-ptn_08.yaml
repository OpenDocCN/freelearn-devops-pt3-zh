- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Don’t Get Lost in the Data Jungle
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 别迷失在数据丛林中
- en: Data is at the crux of everything we do. Most operations in cloud native applications
    relate to generating, consuming, and modifying data in myriad forms. Choosing
    the right places to store our data in the cloud, knowing how to ingest data, and
    maintaining data integrity are paramount. While much of the value of the applications
    we produce lives in the business logic, fundamentally, that business logic operates
    on data. Therefore, the way we store data is instrumental in the operation of
    our application. Unlike traditional on-premise services, cloud native services
    present new and exciting opportunities that can reduce our operational and maintenance
    overhead significantly. However, when used incorrectly, these services can just
    as quickly hamper our efforts through some insidious anti-patterns.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是我们所做一切的核心。云原生应用程序的大多数操作都涉及以各种形式生成、消费和修改数据。选择合适的地方存储数据，了解如何摄取数据，并保持数据的完整性至关重要。虽然我们生产的应用程序的许多价值存在于业务逻辑中，但从根本上来说，这些业务逻辑是在数据上运行的。因此，我们存储数据的方式对应用程序的操作至关重要。与传统的本地服务不同，云原生服务提供了新的、令人兴奋的机会，可以显著降低我们的运营和维护成本。然而，如果使用不当，这些服务也能通过一些阴险的反模式迅速妨碍我们的努力。
- en: 'In this chapter, we are going to cover the following main anti-patterns that
    are present when persisting data in the cloud:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下在云中持久化数据时常见的反模式：
- en: Picking the wrong database or storage
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择错误的数据库或存储
- en: Data replication from production to development
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从生产到开发的数据复制
- en: Backup and recovery should theoretically work
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 备份和恢复应该在理论上是可行的
- en: Manual data ingestion
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动数据摄取
- en: No observability for data transfer errors
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据传输错误没有可观察性
- en: By the end of this chapter, you will have a solid understanding of cloud native
    data storage options for operational purposes and the trade-offs between them.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将对云原生数据存储选项有一个扎实的理解，特别是它们在操作目的中的应用以及它们之间的权衡。
- en: Picking the wrong database or storage
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择错误的数据库或存储
- en: “*When all you have is a hammer, everything looks like a nail*” is a refrain
    commonly used to describe overreliance on the same tool for every job. Having
    preferences is acceptable, but when teams pick a database or storage solution,
    we often see the same developers repeatedly reaching for the same tools. While
    familiarity with a particular toolset might be advantageous for rapid onboarding
    and development, it can lead to suboptimal solutions and anti-patterns. Cloud
    native applications have a wide range of databases and storage methods, so a well-rounded
    cloud application should consider all the available options. Before we dive into
    these options, let’s explore some required background knowledge to frame our conversations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: “*当你只有一把锤子时，一切看起来都像钉子*”是一个常用的说法，用来描述过度依赖同一工具处理所有问题的现象。有偏好是可以接受的，但当团队选择数据库或存储解决方案时，我们经常看到相同的开发者一次又一次地选择相同的工具。虽然熟悉特定工具集可能有助于快速上手和开发，但这可能导致次优的解决方案和反模式。云原生应用程序有着丰富的数据库和存储方式，因此一个全面的云应用程序应该考虑所有可用选项。在我们深入了解这些选项之前，先来看看一些必要的背景知识，以便更好地框定我们的讨论。
- en: Framing the conversation
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 框定讨论
- en: When discussing databases, it is essential to start by exploring the **consistency,
    availability, and partition tolerance** (**CAP**) theorem, normal forms, and time
    complexity. These three concepts explain the trade-offs and approaches to designing
    data models for myriad solutions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论数据库时，首先需要探索**一致性、可用性和分区容忍性**（**CAP**）定理、规范化形式和时间复杂度。这三个概念解释了为多种解决方案设计数据模型时的权衡和方法。
- en: CAP theorem
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CAP 定理
- en: 'As previously mentioned, the CAP theorem stands for consistency, availability,
    and partition tolerance, specifically concerning distributed datastores. The consensus
    is that a distributed database solution can only genuinely address two of these
    capabilities simultaneously:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，CAP 定理代表一致性、可用性和分区容忍性，专门针对分布式数据存储。共识是，分布式数据库解决方案只能真正同时解决这三者中的两个能力：
- en: '**Consistency** ensures that when a database read occurs, it will return the
    database state that results from all actions committed before we request the read.
    Strongly consistent databases maintain this paradigm, whereas eventually consistent
    databases will return a state that may or may not have all applied writes propagated;
    it represents the state of the database from some point in the past.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致性**确保当数据库读取发生时，它将返回由所有在我们请求读取之前提交的操作所导致的数据库状态。强一致性的数据库维持这一范式，而最终一致性的数据库将返回一个可能没有传播所有已应用写入的状态；它表示数据库在某个过去时刻的状态。'
- en: '**Availability** means that every request received by a valid database node
    must return a non-error response. In the context of a distributed datastore, this
    might conflict with the guarantee of consistency. How can we ensure that our system
    has received all transactions from all other nodes, especially in scenarios where
    we might have network partitions or delays? This brings us to partition tolerance.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用性**意味着每个有效数据库节点接收到的请求必须返回一个非错误响应。在分布式数据存储的上下文中，这可能与一致性保障发生冲突。我们如何确保系统已经接收到来自其他所有节点的所有事务，特别是在可能发生网络分区或延迟的情况下？这就引出了分区容忍性。'
- en: '**Partition tolerance** guarantees that the system will continue operating
    despite unreliable or late message delivery between nodes. If one of our nodes
    suffers catastrophic network failure, our datastore should keep operating. This
    is primarily an issue in distributed databases with multi-master configurations,
    such as some of the high-availability options discussed later in the chapter.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分区容忍性**保证系统会继续运行，即使节点之间的消息传递不可靠或延迟。如果其中一个节点发生了灾难性的网络故障，我们的数据存储应继续运行。这在具有多主配置的分布式数据库中尤为突出，例如本章稍后讨论的一些高可用选项。'
- en: In an ideal world, our chosen datastore would have all three of these properties,
    and some recent developments in this space push the limits of this exclusivity.
    However, this pattern is generally closely reflected in reality.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想的世界里，我们选择的数据存储会具备这三种属性，而且一些最近的技术进展推动了这种排他性的极限。然而，这种模式通常在现实中得到密切体现。
- en: '![](img/B22364_08_1.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22364_08_1.jpg)'
- en: Figure 8.1 – Euler diagram for exclusivity of the CAP theorem elements
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – CAP 定理元素的欧拉图
- en: Normal forms
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 范式
- en: '**Normal forms** refer to how we construct data in our database systems. Fundamentally,
    normal forms are a measure of normalization in our database. We will quickly review
    normal forms and use a common theme to provide examples for each. One point to
    keep in mind as we go through this section is that even though it may appear that
    the higher our normal form is, the better our database design is, in most cases,
    we also need to consider the performance and querying of our data and access patterns.
    We will only discuss the first three normal forms here as, typically, this is
    where most of the differences between cloud native databases lie:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**范式**是指我们在数据库系统中如何构建数据。从根本上说，范式是衡量我们数据库规范化程度的标准。我们将快速回顾范式，并通过一个共同的主题为每个范式提供示例。在这个过程中需要记住的一点是，尽管表面上看，范式越高，数据库设计越好，但在大多数情况下，我们还需要考虑数据的性能、查询以及访问模式。我们这里只讨论前三个范式，因为通常情况下，这也是云原生数据库之间差异的主要所在：'
- en: 'The first normal form of data (**1NF**) defines each cell as a unit that only
    contains a single value, and the names of columns in our data storage should be
    unique. Many storage solutions that support nested or unstructured data already
    fail this criterion. The following table shows a first normal form dataset for
    order information:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的第一范式（**1NF**）定义每个单元格为只包含一个单一值的单位，且数据存储中的列名应该是唯一的。许多支持嵌套或非结构化数据的存储解决方案已经不符合这一标准。下表展示了一个订单信息的第一范式数据集：
- en: '| **InvoiceItems** |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| **发票项** |'
- en: '| **InvoiceId (key)** | **ItemId (key)** | **Qty** | **SalespersonID** | **Salesperson**
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| **发票 ID（键）** | **项目 ID（键）** | **数量** | **销售人员 ID** | **销售人员** |'
- en: '| 123 | 312 | 10 | 10 | Aiden |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 123 | 312 | 10 | 10 | Aiden |'
- en: '| 123 | 432 | 5 | 10 | Aiden |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 123 | 432 | 5 | 10 | Aiden |'
- en: '| 456 | 321 | 20 | 8 | Gerald |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 456 | 321 | 20 | 8 | Gerald |'
- en: '| 789 | 432 | 10 | 8 | Gerald |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 789 | 432 | 10 | 8 | Gerald |'
- en: Table 8.1 – Invoices, items, and salespeople stored in a single table
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.1 – 发票、项目和销售人员存储在单个表中
- en: The second normal form (`Salesperson` column in the first table. In this scenario,
    our result only depends on the part of the key, the invoice ID.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二范式（`Salesperson`列在第一个表中）。在这种情况下，我们的结果仅依赖于键的一部分，即发票 ID。
- en: '| **InvoiceItems** |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| **发票项** |'
- en: '| **InvoiceId (key)** | **ItemId (key)** | **Qty** |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| **InvoiceId (主键)** | **ItemId (主键)** | **Qty** |'
- en: '| 123 | 312 | 10 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 123 | 312 | 10 |'
- en: '| 123 | 432 | 5 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 123 | 432 | 5 |'
- en: '| 456 | 321 | 20 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 456 | 321 | 20 |'
- en: '| 789 | 432 | 10 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 789 | 432 | 10 |'
- en: Table 8.2 – Invoices and items; note we have removed two columns in this table
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.2 – 发票和项目；请注意我们在此表中移除了两列
- en: 'Let’s add a new table to satisfy the second normal form by storing salespeople
    against invoice IDs:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来添加一个新表，通过存储销售人员与发票 ID 的对应关系来满足第二范式：
- en: '| **InvoiceSalesperson** |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| **InvoiceSalesperson** |'
- en: '| **InvoiceId (key)** | **SalespersonID** | **Salesperson** |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| **InvoiceId (主键)** | **SalespersonID** | **销售人员** |'
- en: '| 123 | 10 | Aiden |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 123 | 10 | Aiden |'
- en: '| 456 | 8 | Gerald |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 456 | 8 | Gerald |'
- en: '| 789 | 8 | Gerald |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 789 | 8 | Gerald |'
- en: Table 8.3 – Invoices and their relation to salespeople; note that we are storing
    less data now but can reconstruct the same level of detail
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.3 – 发票及其与销售人员的关系；请注意，我们现在存储的数据较少，但仍能重建相同的细节
- en: 'The third normal form (`InvoiceSalesperson` table is based on the `InvoiceId`
    key. However, the salesperson’s name depends on `SalespersonID`, which is a transitive
    dependency. To rectify this, let’s add a `Salesperson` table (*Table 8.5*):'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三范式（`InvoiceSalesperson` 表基于 `InvoiceId` 主键。然而，销售人员的姓名依赖于 `SalespersonID`，这是一种传递依赖。为了解决这个问题，让我们添加一个
    `销售人员` 表（*表 8.5*）：
- en: '| **InvoiceItems** |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| **InvoiceItems** |'
- en: '| **InvoiceId (key)** | **ItemId (key)** | **Qty** |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| **InvoiceId (主键)** | **ItemId (主键)** | **Qty** |'
- en: '| 123 | 312 | 10 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 123 | 312 | 10 |'
- en: '| 123 | 432 | 5 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 123 | 432 | 5 |'
- en: '| 456 | 321 | 20 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 456 | 321 | 20 |'
- en: '| 789 | 432 | 10 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 789 | 432 | 10 |'
- en: Table 8.4 – Invoices and items; this scenario is unchanged from our previous
    example
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.4 – 发票和项目；此场景与我们之前的示例没有变化
- en: We then have the same invoice salesperson mapping; however, we use an identifier
    rather than the salesperson’s name.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有相同的发票销售人员映射；不过，我们使用标识符而不是销售人员的姓名。
- en: '| **InvoiceSalesperson** |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| **InvoiceSalesperson** |'
- en: '| **InvoiceId (key)** | **SalespersonID** |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| **InvoiceId (主键)** | **SalespersonID** |'
- en: '| 123 | 10 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 123 | 10 |'
- en: '| 456 | 8 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 456 | 8 |'
- en: '| 789 | 8 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 789 | 8 |'
- en: Table 8.5 – Invoices and their relation to salespeople; however, we have removed
    the transitive dependency
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.5 – 发票及其与销售人员的关系；然而，我们已经移除了传递依赖
- en: 'Finally, we add a table with each of the salespeople in it:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们添加一个包含每个销售人员的表：
- en: '| **Salesperson** |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| **销售人员** |'
- en: '| **SalespersonID (key)** | **Salesperson** |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| **SalespersonID (主键)** | **销售人员** |'
- en: '| 10 | Aiden |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 10 | Aiden |'
- en: '| 8 | Gerald |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 8 | Gerald |'
- en: Table 8.6 – Maps salespeople IDs to their names; this once again reduces the
    data we store but can still be reconstructed with the right access patterns
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.6 – 将销售人员 ID 映射到其姓名；这再次减少了我们存储的数据，但仍可以通过正确的访问模式重建
- en: Our solution has now evolved to comply with the third normal form. As you can
    see, high levels of normalization require increasing dependence on relationships
    but provide greater consistency in our data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解决方案现在已经发展到符合第三范式。如你所见，高级别的规范化要求越来越依赖于关系，但能提供更高的一致性。
- en: Time complexity
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 时间复杂度
- en: Finally, we need to discuss time complexity and Big O notation. Big O notation
    describes the upper bound of a system’s execution time in relation to the size
    of the dataset being processed. A system with a constant lookup time for a record,
    regardless of its dataset, is *O(1)*. A system that linearly scales its lookup
    time with the number of items in our dataset is *O(n)*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要讨论时间复杂度和大 O 表示法。大 O 表示法描述了系统执行时间相对于处理的数据集大小的上界。一个具有恒定查找时间的系统，不管数据集大小，都是
    *O(1)*。一个随着数据集项目数量线性扩展查找时间的系统是 *O(n)*。
- en: A good example is a naive database implementation that checks every row in a
    database to see whether it matches our selection criteria. In this case, the implementation
    would be *O(n)* complexity; as the number of records grows, so does the number
    of checks we need to make on each lookup linearly. In reality, most database solutions
    will lie somewhere between these values. Complexity can scale at rates greater
    than *O(n)*, but you should find another one if a database ever offers that complexity.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的例子是一个天真的数据库实现，它检查数据库中的每一行，看看它是否符合我们的选择标准。在这种情况下，实施将是 *O(n)* 复杂度；随着记录数量的增加，我们需要对每一行查找进行线性检查。实际上，大多数数据库解决方案将位于这些值之间。复杂度可以以大于
    *O(n)* 的速率扩展，但如果一个数据库提供这样的复杂度，你应该寻找另一种方案。
- en: The right database for the right purpose
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 适合的数据库用于适合的目的
- en: 'We see four key types of databases utilized in cloud native systems for bulk
    data storage: relational, NoSQL, key-value, and graph (there are many other solutions,
    such as ledger/blockchain databases, hierarchical databases, and vector databases,
    but they are outside the scope of this section). Each has advantages and is useful
    for different data types but requires different approaches. A common anti-pattern
    is developers choosing the wrong cloud databases for their applications.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在云原生系统中看到四种关键类型的数据库用于大规模数据存储：关系型、NoSQL、键值型和图形型（还有许多其他解决方案，如分类账/区块链数据库、层次型数据库和向量数据库，但它们超出了本节的讨论范围）。每种数据库都有其优点，并且适用于不同的数据类型，但它们需要不同的处理方法。一个常见的反模式是开发人员为他们的应用程序选择了不合适的云数据库。
- en: Relational databases
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关系型数据库
- en: '**Relational databases** are the tried-and-true traditional database solution.
    They allow you to establish records and model the relationships between them.
    In this solution, the database usually conforms to a strict, predefined set of
    relationships and structures defined as a part of its schema. However, more and
    more relational database engines are providing the ability to store semi-structured
    and unstructured data. Due to their highly structured data models, relational
    databases make it very easy to maintain consistency and integrity of the data.
    Their inbuilt support of relationships makes it easy to query normalized data.
    In the cloud world, these databases are often offered as a *service* and may even
    have “*serverless*” offerings (more on why that’s quoted in a few paragraphs);
    however, we run into issues when we try to scale these systems. Typically, the
    scaling model involves adding additional capacity to these services through vertical
    scaling.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**关系型数据库**是经过验证的传统数据库解决方案。它们允许您建立记录并建模它们之间的关系。在这种解决方案中，数据库通常符合严格的、预定义的一组关系和结构，这些关系和结构作为其模式的一部分进行定义。然而，越来越多的关系型数据库引擎提供了存储半结构化和非结构化数据的能力。由于其高度结构化的数据模型，关系型数据库使得维护数据的一致性和完整性变得非常容易。其内建的关系支持使得查询规范化数据变得简单。在云计算领域，这些数据库通常作为*服务*提供，甚至可能有“*无服务器*”的服务（稍后会详细解释为何要加上引号）；然而，当我们尝试扩展这些系统时，就会遇到问题。通常，扩展模型是通过垂直扩展向这些服务添加额外的容量。'
- en: Some newer solutions provide automated, transparent sharding capability priced
    at a premium. At vast scales, with massive datasets, this can cause issues that
    can result in higher cloud bills. It’s also essential to note that in these systems,
    we’re typically limited to certain index types, such as binary trees, which have
    a time complexity of *O(log(n))*. When we query data in a relational database,
    a typical pattern is to join records and perform aggregations to return the result
    in the format we want. This pattern can be instrumental in scenarios where you
    know the structure of the data you want to store but not the access patterns of
    how you will query that data. The flexible access patterns allow you to expand
    your offerings without significant changes to the underlying database. You can
    provide new insights with new queries.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一些较新的解决方案提供了自动化、透明的分片能力，并且价格较高。在大规模数据集的情况下，这可能会引发问题，导致更高的云账单。还必须注意，在这些系统中，我们通常只能使用某些类型的索引，如二叉树，二叉树的时间复杂度为
    *O(log(n))*。当我们在关系型数据库中查询数据时，一个典型的模式是连接记录并进行聚合，以返回我们想要的结果。这种模式在您知道要存储的数据结构，但不确定如何查询这些数据的访问模式时非常有用。灵活的访问模式使您能够在不对底层数据库进行重大更改的情况下扩展您的服务。您可以通过新的查询提供新的洞察。
- en: The services that provide relational databases in the hyperscalers cover all
    familiar SQL flavors, such as MySQL, PostgreSQL, and SQL Server. Typically, these
    solutions focus on being consistent and partition-tolerant. However, many new
    services by hyperscalers also provide high availability.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 提供关系型数据库的超大规模服务商涵盖了所有常见的 SQL 类型，例如 MySQL、PostgreSQL 和 SQL Server。通常，这些解决方案专注于一致性和分区容忍性。然而，许多超大规模服务商的新服务也提供高可用性。
- en: NoSQL databases
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NoSQL 数据库
- en: '**NoSQL databases** provide an alternative to traditional relational databases.
    They are denormalized to some degree, and rather than allowing for flexible access
    patterns, they rely on access patterns designed into the data model itself.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**NoSQL 数据库**为传统的关系型数据库提供了一种替代方案。它们在一定程度上是非规范化的，并且它们依赖于内置于数据模型本身的访问模式，而不是允许灵活的访问模式。'
- en: 'All the hyperscalers have offerings in this space: Azure has Cosmos DB, GCP
    has Firestore, and AWS has DynamoDB. Unlike our strictly formatted SQL tables,
    NoSQL databases have no enforced schema. Columns can mix data types, and data
    can be deeply nested. There are compelling arguments for why you should do away
    with separate tables and instead put all your data into one big table. These services
    offer extreme scalability and performance at a low price point. However, they
    require fundamental shifts in thinking from the traditional relational database
    model.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的超大规模云服务商在这一领域都有产品：Azure 提供 Cosmos DB，GCP 提供 Firestore，AWS 提供 DynamoDB。与我们严格格式化的
    SQL 表不同，NoSQL 数据库没有强制的模式。列可以混合数据类型，数据可以深度嵌套。有一些有力的论据支持你放弃单独的表，而将所有数据放入一个大表中。这些服务在低价格下提供极高的可扩展性和性能。然而，它们要求从传统关系数据库模型转变思维方式。
- en: We must design our access patterns upfront to get the best value from our NoSQL
    database solution. This requirement can make development slightly more complicated
    because adding a new access pattern is more than just a case of writing a new
    query. We may require significant changes to our database design. Some database
    solutions in the NoSQL space (such as DynamoDB, Firestore, and Cosmos DB) can
    achieve close to *O(1)* complexity for properly structured access patterns but
    incur a penalty of *O(n)* complexity for improperly structured access patterns.
    Many of these solutions allow you to prioritize availability and partition tolerance
    or consistency and partition tolerance.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须在前期设计好访问模式，以便从我们的 NoSQL 数据库解决方案中获得最佳价值。这个要求可能使开发稍微复杂一些，因为添加新的访问模式不仅仅是编写一个新查询的问题。我们可能需要对数据库设计进行重大修改。一些
    NoSQL 领域的数据库解决方案（如 DynamoDB、Firestore 和 Cosmos DB）可以实现接近 *O(1)* 复杂度的正确结构化访问模式，但对于结构不当的访问模式，则会面临
    *O(n)* 复杂度的惩罚。许多这些解决方案允许你优先考虑可用性和分区容忍性，或者一致性和分区容忍性。
- en: Key-value stores
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 键值存储
- en: '**Key-value stores** are a straightforward type of database. Essentially, we
    provide a way to address (key) our stored data (value). NoSQL databases still
    allow for complex access patterns. Our key-value store has one access pattern:
    use the key to get the value stored at an address. These are typically high-performance
    in-memory datastores that may or may not offer some form of persistence. The typical
    use case for these datastores is a cache for complex queries or computational
    outputs from other systems. They can be helpful in our cloud arsenal when we have
    complex requests with low cardinality.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**键值存储**是一种简单的数据库类型。本质上，我们提供了一种方法来通过键（key）访问存储的数据（value）。NoSQL 数据库仍然允许复杂的访问模式。我们的键值存储只有一种访问模式：使用键获取存储在某个地址上的值。这些通常是高性能的内存数据库，可能会提供或不提供某种形式的持久化。此类数据存储的典型使用场景是复杂查询或来自其他系统的计算输出的缓存。当我们有低基数的复杂请求时，它们可以在云端武器库中提供帮助。'
- en: Graph databases
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图形数据库
- en: The final database type we will discuss is `OrderID` field is referenced on
    the order record, the shipping manifest, and the payment record. The shipping
    manifest and payment record contain foreign keys to the order record; however,
    the actual relationship is stored on the records themselves. In a graph database,
    the relationships are first-class objects. We have our objects (vertices) and
    our relationships (edges), and the data model is optimized for extremely fast
    traversal of relationships, allowing us to follow paths through our dataset in
    a performant way. This property can be advantageous when objects interact with
    each other in arbitrary ways, for example, with users on a social media site,
    interacting with other users, posts, communities, and so on.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的最后一种数据库类型是 `OrderID` 字段，它在订单记录、运输清单和支付记录中都有引用。运输清单和支付记录包含指向订单记录的外键；然而，实际的关系存储在这些记录本身中。在图形数据库中，关系是一级对象。我们有对象（顶点）和关系（边），数据模型优化了关系的极快遍历，使我们能够高效地在数据集之间跟踪路径。当对象之间以任意方式进行交互时，这种特性尤其有优势，例如社交媒体网站上的用户与其他用户、帖子、社区等的互动。
- en: Other database types
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他数据库类型
- en: Exploring other supporting services or nonstandard database types can also be
    advantageous. A key type of database that is often ignored is time-series databases.
    These might be implemented as standalone products or extensions to the previous
    database types. These databases are optimized for chronological access patterns
    and storage rather than the structures mentioned previously. Another common type
    of database or database extension is spatial databasing, specifically looking
    at geometric and geographic properties in queries. The key here is not to limit
    yourself to the preceding database structures but to also explore the options
    available for your edge cases.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 探索其他支持服务或非标准数据库类型也可能是有利的。一个常被忽视的关键数据库类型是时间序列数据库。这些可以作为独立产品或之前数据库类型的扩展实现。这些数据库优化了按时间顺序访问模式和存储，而不是前面提到的结构。另一种常见的数据库或数据库扩展是空间数据库，专门研究查询中的几何和地理属性。关键在于不仅限制于前述数据库结构，还要探索适用于你边缘案例的选项。
- en: In one example I worked on, the client used a Postgres database to store a list
    of customer addresses and identifiers. However, this system’s access patterns
    are unsuitable for a relational database. First, the data was not relational;
    each record was wholly independent, and second, the Postgres keyword `LIKE` was
    significantly used within the database’s query patterns. The client’s quick solution
    was to put a **generalized inverted index** (**GIN**) on every column. This enabled
    searching on arbitrary strings but made modifying the database unwieldy. Using
    a search service such as OpenSearch to store the queriable documents would have
    been straightforward, likely resulting in a lower cloud bill and better performance.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾经处理过的一个例子中，客户使用Postgres数据库存储客户地址和标识列表。然而，该系统的访问模式不适合关系数据库。首先，数据不是关系型的；每条记录完全独立，其次，在数据库查询模式中显著使用Postgres关键字`LIKE`。客户的快速解决方案是在每一列上放置一个**广义倒排索引**（**GIN**）。这使得可以在任意字符串上进行搜索，但使得修改数据库变得笨拙。使用诸如OpenSearch之类的搜索服务来存储可查询文档可能更为简单，很可能会导致更低的云账单和更好的性能。
- en: Manual, managed, serverless, and truly serverless databases
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手动、托管、无服务器和真正无服务器数据库
- en: When choosing databases, we must establish the need for the database types discussed
    earlier and the method by which we are going to consume the database in the cloud.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择数据库时，我们必须确定之前讨论的数据库类型的需求以及我们将如何在云中使用数据库。
- en: The naive approach to this from the on-premises mindset might be that we simply
    need to provision a cloud VM, install a database, and be good to go. While this
    manual approach will work, it must present a compelling value proposition. In
    this scenario, you are solely responsible for backups, patching the DB version
    and OS, and provisioning new machines. How you install, run, and maintain databases
    is unlikely to be a value differentiator for your business. Therefore, this manual
    option is generally considered an anti-pattern unless you need specific functionality
    or configurations that aren’t available in managed services. Instead, the baseline
    deployment of a database is typically as a managed service.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从传统思维方式出发的幼稚方法可能是我们只需提供一个云虚拟机，安装一个数据库，然后一切就搞定了。虽然这种手动方法可以行得通，但必须提供一个引人注目的价值主张。在这种情况下，你需要完全负责备份、更新数据库版本和操作系统，并且提供新的机器。如何安装、运行和维护数据库不太可能成为你业务的价值差异化因素。因此，通常认为这种手动选择是一种反模式，除非你需要特定功能或配置，而这些功能或配置在托管服务中不可用。相反，数据库的基线部署通常作为托管服务。
- en: This deployment method is where we see most companies start their cloud database
    adoption, as these managed services provide a way for them to use familiar tools
    (Postgres, MySQL, and SQL Server) while allowing the cloud provider to take care
    of backups, patching, and maintenance using battle tested and resilient methodologies.
    Many companies never find a compelling reason to leave this level, which is perfectly
    acceptable. We can also start to set up resilient architectures in this development
    mode with read replicas, automated failover, and multi-master configurations.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数公司在开始采用云数据库时采用了这种部署方法，因为这些托管服务为他们提供了一种使用熟悉工具（如Postgres、MySQL和SQL Server）的方式，同时允许云提供商负责备份、更新和维护，采用经过实战检验和弹性的方法。许多公司从未找到离开这个层面的充分理由，这是完全可以接受的。我们还可以在这种开发模式下开始设置具有读取副本、自动故障转移和多主配置的弹性架构。
- en: In the managed system, we typically see applications that have consistent, predictable
    patterns. However, some businesses have unpredictable traffic and usage, so you
    should move to a more scalable solution. This situation is where “serverless”
    solutions come into play. I use quotes in this scenario because they are serverless
    (i.e., they will automatically scale). Still, they do not scale down to zero,
    which many people consider true serverless. An anti-pattern we commonly see in
    this space is people migrating to these “serverless” solutions without considering
    non-relational data models.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在管理型系统中，我们通常看到具有一致性和可预测模式的应用程序。然而，一些企业的流量和使用情况是不可预测的，因此你应该转向一个更具扩展性的解决方案。这种情况就是“无服务器”解决方案发挥作用的地方。我在这个场景中使用引号，因为它们是无服务器的（即它们会自动扩展）。不过，它们并不会扩展到零，许多人认为这才是真正的无服务器。我们在这一领域常见的反模式是，人们在迁移到这些“无服务器”解决方案时，未考虑非关系型数据模型。
- en: Finally, we have truly serverless databases. These are typically NoSQL or document
    databases (such as DynamoDB, Firestore, and Cosmos DB in the major cloud providers
    in the **online transaction processing** (**OLTP**) space) that make trade-offs
    in ease of use for extreme scalability, cost-effectiveness, and performance. The
    anti-pattern we often see in this space is teams seeing this option and holding
    it as the pinnacle of achievement to build a system that utilizes this cloud native
    unique option without considering the downsides. Namely, your data is less portable,
    will be harder to hire for, and requires upfront knowledge of your access patterns.
    This combination can lead to bad initial experiences that cause teams to return
    to the familiar land of relational databases and not consider these databases
    for use cases where they would be a good fit.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有了真正的无服务器数据库。这些通常是 NoSQL 或文档型数据库（例如在主要云提供商中的 DynamoDB、Firestore 和 Cosmos
    DB，属于**在线事务处理**（**OLTP**）领域），它们在极端可扩展性、成本效益和性能方面做出了使用便利性上的妥协。我们在这一领域常看到的反模式是，团队看到这一选项，并将其视为成就的巅峰，建立一个利用这种云原生独特选项的系统，而没有考虑其缺点。也就是说，你的数据更难以迁移，招聘也更困难，而且需要事先了解你的访问模式。这种组合可能导致初期糟糕的体验，促使团队回到熟悉的关系型数据库领域，而没有考虑这些数据库在一些适合的场景中的应用。
- en: Ignoring storage requirements
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 忽视存储需求
- en: A common anti-pattern is using traditional storage mechanisms in the cloud without
    considering other options. Conventional filesystems evolved out of the need for
    on-device storage and provide considerable functionality. Network filesystems,
    such as FTP and NFS, became the de facto projection of these services into a multi-machine
    environment. The core principle in these systems is that a central server is responsible
    for coordinating access to the underlying storage. A common theme in this book
    is that centralization is usually an anti-pattern.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的反模式是，在云中使用传统的存储机制，而没有考虑其他选择。传统的文件系统是出于对设备内存储需求的演变而产生的，并提供了相当大的功能。网络文件系统，如
    FTP 和 NFS，成为这些服务在多机器环境中的事实标准。这些系统的核心原则是，中央服务器负责协调对底层存储的访问。本书的一个常见主题是，集中化通常是一个反模式。
- en: When we start to design a system that utilizes storage in the cloud, the first
    question we should ask is, “*Can we use blob storage?*” **Blob storage** is decentralized
    and scales horizontally, with much higher resiliency and durability than conventional
    network filesystems. In Azure, this service is Azure Blob Storage, GCP has Cloud
    Storage, and AWS has S3.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始设计一个利用云存储的系统时，我们应该首先问的第一个问题是，“*我们可以使用 Blob 存储吗？*” **Blob 存储**是去中心化的，能够水平扩展，且比传统的网络文件系统具有更高的弹性和持久性。在
    Azure 中，这项服务是 Azure Blob 存储，GCP 提供 Cloud Storage，而 AWS 提供 S3。
- en: You can think of blob storage as a key-value store that can store enormous values.
    For most cloud native use cases, this provides more than enough capability. Do
    you still need metadata? Put it in your database. Do you need locks? Use your
    database. Need backups? Use version history. Blob storage is likely the answer
    to your storage needs. There are cases where specialized or traditional filesystems
    still provide benefits, such as in high-performance computing, low-latency applications,
    and conventional filesystem migrations. So, remember that no one tool is the right
    solution to every problem.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将 Blob 存储视为一个键值存储，它可以存储巨大的数据值。对于大多数云原生使用案例来说，这已经足够提供所需的能力。你还需要元数据吗？放到数据库里。需要锁吗？使用数据库。需要备份吗？使用版本历史记录。Blob
    存储很可能是你存储需求的答案。有些情况下，专门的或传统的文件系统仍然能提供好处，例如在高性能计算、低延迟应用程序和传统文件系统迁移中。因此，记住没有一种工具可以解决所有问题。
- en: Ignoring the life cycle and archive policy
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 忽略生命周期和归档策略
- en: 'Storing data is easy. We send a request to our storage provider of choice and
    then forget about it until we need to use it. Therein lies the anti-pattern: failing
    to maintain the life cycle of your data appropriately can lead to severe consequences.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 存储数据很容易。我们向选择的存储提供商发送请求，然后忘记它，直到需要使用它为止。问题就在这里：未能适当维护数据的生命周期可能会导致严重后果。
- en: However, we might want to save some money here because we don’t necessarily
    want to access this data; we just want to keep it on file. This requirement is
    where the concept of storage tiers comes into play.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可能希望节省一些费用，因为我们不一定需要访问这些数据；我们只需要将其保存档案。这时，存储层的概念就派上用场了。
- en: 'Let’s take an example: we work at a large firm that has an internal tax function.
    Throughout the year, people upload receipts. We must access these receipts repeatedly
    during tax time as various functions perform their duties. Then, after the tax
    period, we just need to keep a copy in case of discrepancies. In all cloud providers,
    we can group their storage tiers into one of three broad categories:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个例子：我们在一家大型公司工作，负责内部税务功能。在一年中，人们会上传收据。在税务期间，我们需要反复访问这些收据，因为各种职能需要执行各自的职责。然后，在税务期过后，我们只需要保留一份副本，以防出现差异。在所有云服务提供商中，我们可以将他们的存储层分为三大类：
- en: '**Hot** is for data that needs to be accessed regularly and available at a
    moment’s notice. Typically, this tier strikes a good balance between cost to store
    and cost to retrieve. Consider this where we want our receipts to be at tax time.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**热存储**是指需要定期访问并且能够随时获取的数据。通常，这个层级在存储成本和检索成本之间取得了很好的平衡。可以考虑将收据存储在此层级，以便在税务期间使用。'
- en: '**Cold** is for data that needs to be accessed at a moment’s notice but is
    unlikely to be accessed often. We pay a little more when we want to access items
    in this tier but reap the benefits of our infrequent access with lower storage
    costs. This tier might be where we store all of our receipts during the year as
    they’re submitted.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**冷存储**是指需要随时访问但不太可能经常访问的数据。我们在访问这个层级中的数据时需要付出更多费用，但通过较低的存储成本，享受不频繁访问的好处。这个层级可能是我们在一年中提交所有收据时存储的地方。'
- en: '**Archive** is for data that we want to keep but do not have specific access
    speed requirements. This tier offers the most cost-effective storage solution
    with the highest access cost and slowest retrieval time (this might be on the
    order of hours rather than milliseconds). When we are done with all of our receipts
    for the year and just need to keep a record for posterity, we will move them to
    this tier.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归档**是指我们想要保留但没有特定访问速度要求的数据。这个层级提供最具成本效益的存储解决方案，但其访问成本最高，检索时间最慢（这可能是几个小时而不是毫秒级）。当我们完成一年的所有收据，并且只需要保留记录以备后用时，我们会将其移到这个层级。'
- en: Some data may need to be retained to comply with regulatory requirements, while
    other data may only need to be stored short-term as its validity rapidly decreases.
    We accomplish these use cases through data life cycles. Life cycle policy and
    management tools allow us to automate this process.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据可能需要保留，以符合监管要求，而其他数据可能只需要短期存储，因为其有效性会迅速降低。我们通过数据生命周期来实现这些使用案例。生命周期策略和管理工具使我们能够自动化这一过程。
- en: 'Typically, we take two actions in life cycle policies: we either change the
    storage tier of our data or delete our data. A life cycle policy might mix these
    two actions. For example, imagine we work for a company that creates detailed
    financial reports. Every month, we release a new report that is accessed frequently,
    then infrequently, and then it needs to be archived for six years. Our life cycle
    policy might look like this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们在生命周期策略中采取两种行动：要么改变数据的存储层，要么删除数据。一个生命周期策略可能会将这两种操作结合起来。例如，假设我们为一家公司工作，这家公司创建详细的财务报告。每个月，我们发布一份新报告，开始时频繁访问，然后逐渐不常访问，最后需要存档六年。我们的生命周期策略可能如下所示：
- en: Create a file in the hot tier.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在热存储层创建一个文件。
- en: Wait 31 days (report cycle).
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待31天（报告周期）。
- en: Move the file to the cold tier.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文件移动到冷存储层。
- en: Wait 334 days.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待334天。
- en: Move the file to the archive tier.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文件移动到归档存储层。
- en: Wait six years.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待六年。
- en: Delete the file.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除文件。
- en: If we kept our file in the hot tier, we would be paying for the convenience
    of frequent access without actually accessing the file. Therefore, our life cycle
    policy has allowed us to optimize our cloud storage spending.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将文件保持在热存储层，我们将支付频繁访问的便利费用，但实际上并未真正访问文件。因此，我们的生命周期策略使我们能够优化云存储开支。
- en: Data replication from production to development
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从生产环境到开发环境的数据复制
- en: We all need data to ensure that the systems we build in our development environment
    match all the weird and wonderful types of data that our users generate in production.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都需要数据来确保我们在开发环境中构建的系统能够匹配用户在生产环境中生成的各种奇怪和有趣的数据类型。
- en: This section is one of the few sections with an anti-pattern that is serious
    enough to name the entire section after. Under no circumstance should you copy
    user-generated data from production to development environments. While it may
    seem easy to get real-world use cases for your lower environment, lower environments
    typically have more lax security controls and broader access to developers. A
    few recent data breaches have directly involved this anti-pattern; real-world
    user data was available on test systems, and these test systems were breached.
    Instead, in this section, we will go through some alternatives to testing on production
    data and some common anti-patterns in creating data for test environments.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分是为数不多的几个反模式之一，反模式的严重性足以让整个部分以此命名。在任何情况下，都不应将用户生成的数据从生产环境复制到开发环境。虽然从生产环境获取真实的用例数据可能看起来很容易，但低环境通常具有更宽松的安全控制，并且开发人员的访问权限更广。最近的几起数据泄露直接涉及这一反模式；真实的用户数据出现在测试系统上，而这些测试系统被攻破。相反，在本节中，我们将讨论一些替代方案，以避免在生产数据上进行测试，以及在为测试环境创建数据时常见的反模式。
- en: But we mask our production data
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 但我们会对生产数据进行掩码处理
- en: The first anti-pattern we will discuss is using masked data from production
    systems in test environments. This procedure is only marginally better than using
    production data directly. The fallacy in this scenario is that we are coming from
    an insecure position (unmasked production data), applying a transform (our masking
    procedure), and assuming the output is secure (masked data). To illustrate why
    this is a problem, let us look at a parallel example, one based on FaaS. I was
    working with a client who had produced an authentication and logging wrapper for
    lambda functions. The wrapper applied some functionality that could be enabled
    with flags in the lambda function code. One of the flags enabled authentication.
    This pattern meant that, fundamentally, any created lambda functions started as
    insecure functions and then had to opt in to become secure. Instead, we inverted
    that dependency. We made all of the functions secure by default, and you could
    use a flag to turn authentication off for unauthenticated functions. This change
    made being insecure a conscious choice rather than an unconscious mistake. When
    we mask data, we risk making unconscious mistakes because we start from an insecure
    position. The solution is to start from a secure position and explicitly make
    any insecure additions to our data choices. So, we have to start from a secure
    position, which means we need to know our schema and generate data that tests
    its limits.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的第一个反模式是使用来自生产系统的掩码数据进行测试环境中的测试。这种做法仅比直接使用生产数据稍微好一些。这个场景中的谬误在于，我们从一个不安全的位置（未掩码的生产数据）开始，应用一个转换（我们的掩码过程），并假设输出是安全的（掩码数据）。为了说明这是一个问题，让我们看一个类比的例子，它基于FaaS。我曾与一个客户合作，该客户为Lambda函数开发了一个身份验证和日志包装器。这个包装器应用了一些功能，这些功能可以通过Lambda函数代码中的标志启用。一个标志启用了身份验证。这个模式意味着，从根本上说，任何创建的Lambda函数都是不安全的函数，然后必须选择启用安全性。相反，我们颠倒了这种依赖关系。我们默认让所有函数都是安全的，然后可以使用标志来关闭身份验证，以便使某些函数不进行身份验证。这一变化使得不安全成为一种有意识的选择，而不是无意识的错误。当我们掩码数据时，我们有可能犯无意识的错误，因为我们从一个不安全的位置开始。解决方案是从一个安全的位置开始，并明确地对我们的数据选择做出任何不安全的添加。因此，我们必须从一个安全的位置开始，这意味着我们需要了解我们的模式，并生成测试其极限的数据。
- en: Getting started with synthetic data
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用合成数据
- en: As we discussed earlier, the easiest way to ensure that the data you use is
    safe for lower environments is to ensure it doesn’t originate from production
    systems. Therefore, we need a reliable way to generate fake data for our system.
    Luckily, we are not the first people to have this issue! A multitude of open source
    libraries exist with the sole purpose of generating completely fake data. Usually,
    for cloud projects, JavaScript is used at some point in the development cycle,
    be it for frontend applications or backend servers with a runtime such as Node.js,
    Bun, or Deno, so it usually forms a good baseline language. In this case, the
    Faker.js ([fakerjs.dev](http://fakerjs.dev)) library provides a comprehensive
    set of generators to create fake data for testing. The other common language we
    see in testing frameworks is Python, which also has its own Faker library ([https://faker.readthedocs.io/en/master/](https://faker.readthedocs.io/en/master/)).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，确保你使用的数据在低环境中是安全的最简单方法是确保它不来自生产系统。因此，我们需要一种可靠的方法来为我们的系统生成虚假数据。幸运的是，我们不是第一个遇到这个问题的人！已经有许多开源库专门用于生成完全虚假的数据。通常，对于云项目，JavaScript在开发周期的某个阶段会被使用，无论是用于前端应用程序，还是后端服务器使用像Node.js、Bun或Deno这样的运行时，因此它通常是一个很好的基础语言。在这种情况下，Faker.js（[fakerjs.dev](http://fakerjs.dev)）库提供了一整套生成器，用于为测试生成虚假数据。另一个常见的语言是Python，它也有自己的Faker库（[https://faker.readthedocs.io/en/master/](https://faker.readthedocs.io/en/master/)）。
- en: These libraries form an excellent foundation upon which to build. These allow
    us to create bulk data to see how our system handles when under heavy load. We
    can use our production system’s utilization metrics to develop synthetic data.
    Synthetic data retains the schema and structure of our production data, but the
    contents of the records are pure fiction, making it great for functional testing.
    This approach allows us to load a similar amount of data present in production
    into lower environments, ensuring that the conditions we are testing under in
    our lower environments are similar to those under our higher environment. A common
    anti-pattern we see here is attempting to use only a small data set in lower environments.
    This anti-pattern is an issue because you first test the system at the production
    scale when you deploy it to production. Under this paradigm, scenarios and edge
    behaviors that only become present at the scale of the production system remain
    hidden during testing. These problems might be a poorly optimized SQL query or
    a missing index on a column. In these scenarios, small datasets are unlikely to
    be exposed to the issue.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这些库构成了一个极好的基础，我们可以在此基础上进行建设。这些允许我们创建大量数据，以查看我们的系统在负载较重时的处理情况。我们可以使用生产系统的利用率指标来开发合成数据。合成数据保留了生产数据的模式和结构，但记录内容纯属虚构，非常适合功能测试。这种方法允许我们在较低的环境中加载与生产环境中相似数量的数据，确保我们在较低环境下测试的条件与在较高环境下的条件相似。我们在这里经常看到的一个反模式是试图在低环境中仅使用小数据集。这种反模式是一个问题，因为你首次在生产规模下部署系统时才测试系统。在这种范式下，仅在生产系统规模下才会出现的场景和边缘行为在测试期间仍然隐藏。这些问题可能是SQL查询优化不良或列上缺少索引。在这些场景中，小数据集不太可能暴露出这些问题。
- en: Perfect synthetic data
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完美的合成数据
- en: When creating synthetic data, it can be easy to fall into the anti-pattern of
    developing *perfect synthetic data*. This anti-pattern only injects the data,
    formats, and usage patterns we expect to see in our production systems. While
    this might test our systems’ happy path, unfortunately, users are fantastic at
    stressing our systems in ways we never intended. What happens if the user signs
    up with an address and then that address is deleted or gets subdivided into an
    A/B block or any other myriad of problems? We can take a leaf from the domain
    of chaos engineering here. Instead of creating perfect data, we make data with
    a certain amount of corruption, usually expressed as a percentage of the total
    synthetic data. Perfect data only addresses usage by perfectly homogenous users,
    and we all know that our user base consists of a wildly different collection of
    individuals.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建合成数据时，很容易陷入开发*完美合成数据*的反模式中。这种反模式只注入我们在生产系统中预期看到的数据、格式和使用模式。虽然这可能测试我们系统的顺畅路径，但不幸的是，用户往往会以我们从未预料到的方式来测试我们的系统。如果用户注册时使用一个地址，然后该地址被删除或分割成
    A/B 区块或其他各种问题会发生什么呢？在这里，我们可以借鉴混沌工程的领域。与其创建完美数据，不如制造一定程度上有损坏的数据，通常表现为总合成数据的百分比。完美数据只适用于完全同质化用户的使用，而我们都知道我们的用户群体由大量不同的个体组成。
- en: 'There are some simple guidelines for creating synthetic data that I like to
    follow. I generally split these into two layers: one for structured data (SQL
    and Parquet) and one for unstructured/semi-structured data (NoSQL, CSV, JSON,
    and TXT). The unstructured data corruptions should be treated as an extension
    of the structured corruptions.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些简单的指导原则用于创建合成数据，我喜欢遵循这些原则。通常我将其分为两层：一层用于结构化数据（SQL 和 Parquet），另一层用于非结构化/半结构化数据（NoSQL、CSV、JSON
    和 TXT）。非结构化数据损坏应视为结构化损坏的延伸。
- en: 'Structured data can be corrupted in the following ways:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化数据可以通过以下方式进行损坏：
- en: '**Missing records**: What happens if we receive a partial object in a request
    to our service? What if a dependency is missing? What if the dependency existed
    when we created the record but manually deleted it afterward?'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺失记录**：如果我们在请求服务时收到了部分对象会发生什么？如果一个依赖项缺失会怎样？如果在创建记录时依赖项存在，但之后被手动删除了呢？'
- en: '**Unreachable state**: We may have transitive dependencies that are unreachable
    from a code perspective but permissible from a database perspective. What happens
    if we reach this state?'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可达状态**：从代码的角度来看，我们可能有一些不可达的传递依赖关系，但从数据库的角度来看是允许的。如果我们达到了这种状态会发生什么呢？'
- en: '**Corrupted records**: This is data that fundamentally does not make sense
    but is permissible by the system. What if the user accidentally entered their
    credit card number in the cardholder name field? What if one row of our CSV has
    an unescaped comma in a string?'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损坏的记录**：这是指在系统允许的情况下，数据本质上不合理的情况。如果用户不小心将信用卡号码输入到持卡人姓名字段中会怎样？如果我们的 CSV 文件中某一行的字符串中包含未转义的逗号，会发生什么？'
- en: '**Large records**: What happens when a user connects an infinite number of
    monkeys with an endless number of typewriters to your profanity filter?'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大记录**：当一个用户将无数只猴子和无尽数量的打字机连接到你的脏话过滤器时，会发生什么？'
- en: 'Unstructured data can be corrupted in the following additional ways:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化数据还可能以以下额外的方式受到损坏：
- en: '**Duplicate records**: What happens when we try to insert duplicate records,
    or multiple records that represent the same object?'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重复记录**：当我们尝试插入重复记录或多个表示相同对象的记录时，会发生什么情况？'
- en: '**Extra fields**: What happens when our system receives extra data from the
    client that it wasn’t expecting?'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多余字段**：当我们的系统收到来自客户端的额外数据时，会发生什么情况？'
- en: '**Missing fields**: What happens when our system doesn’t receive data from
    the client that it was expecting?'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺失字段**：当我们的系统未收到来自客户端的预期数据时，会发生什么情况？'
- en: '**Syntactically incorrect data**: This is data that doesn’t agree with the
    rules of the data medium in use (for example, not valid CSV or JSON). Missing
    a column? Forgot a curly brace?'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语法错误的数据**：这是指与正在使用的数据媒介规则不一致的数据（例如，CSV 或 JSON 格式无效）。缺少列？忘记了大括号？'
- en: From this, we see that perfect testing data should be imperfect by design. This
    allows us to discover our system’s edge behavior. Our test data should identify
    issues we might encounter before encountering them in production. However, it
    is impossible to be perfectly prophetic about the data issues we might see in
    production. The best type of corrupted data is when we find something in production.
    In that case, copy the methodology of the corrupted data (not the data itself!)
    into your synthetic data generation tool. This process allows us to find other
    ways in which this might impact production. For example, we have an issue where
    an invalid card number is entered. Then, the customer could rectify the card number,
    and all is good. If we add the pattern to our synthetic data, we can see how that
    data would have affected our system if it had been allowed to flow through to
    our billing run system or other application areas.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从这点来看，我们可以得出结论，完美的测试数据应当是设计上不完美的。这样可以帮助我们发现系统的边界行为。我们的测试数据应该在生产环境中遇到问题之前就能识别出可能遇到的问题。然而，对于我们在生产环境中可能遇到的数据问题，完全准确的预见是不可能的。最好的损坏数据类型是当我们在生产环境中发现某个问题时。在这种情况下，将损坏数据的方法（而非数据本身！）复制到你的合成数据生成工具中。这一过程可以帮助我们发现这种数据可能对生产产生的其他影响。例如，如果我们遇到一个无效的卡号输入问题，客户可以纠正卡号，一切恢复正常。如果我们将这一模式添加到合成数据中，我们就能看到如果该数据流入我们的账单系统或其他应用程序区域，如何影响我们的系统。
- en: Backup and recovery should theoretically work
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 备份和恢复理论上应该能够正常工作
- en: “*The best-laid plans of mice and men oft go awry*,” goes the famous line from
    Robert Burns’ “To a Mouse.” The nugget of wisdom here is that no matter how carefully
    we plan for every eventuality, we cannot be confident of its success until we
    execute it. We touched on this topic in [*Chapter 7*](B22364_07.xhtml#_idTextAnchor198)
    when we discussed ignoring reliability. We will go into this topic in more detail
    and explore how to address this anti-pattern with a specific focus on data. As
    discussed before, not testing your data resiliency will lead to unwanted downtime
    when you least expect it. Let’s dive into some ways to mitigate this.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: “*鼠和人的最佳计划常常会失败*，”这句名言出自罗伯特·彭斯的《致一只鼠》。其中的智慧是，无论我们如何小心地为每个可能的情况做好计划，直到我们执行它之前，我们都无法确信它会成功。我们在[*第七章*](B22364_07.xhtml#_idTextAnchor198)中提到过忽视可靠性这个话题。接下来我们将更详细地讨论这个话题，并探索如何专门针对数据来解决这一反模式。如前所述，不测试数据的韧性将导致在你最不期待的时刻出现意外停机。让我们深入探讨一些缓解方法。
- en: Have a plan
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 制定计划
- en: Having a plan is the first step toward resilient data architectures, and the
    key to that plan is understanding the shared responsibility model. If you are
    running your data solution self-hosted in the cloud against the recommendations
    of the first section of this chapter, then you are responsible for everything
    yourself. We often come across a disconnect when people shift to managed services.
    Inevitably, someone will find a checkbox on their managed cloud database instance
    that says **Enable backups** and see that it is ticked. Then, they will rest easy
    at night, thinking their data is safe because it is nebulously “handled by the
    cloud.” If this sounds all too familiar (even if it doesn’t), you probably need
    to consider putting together a recovery action plan.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个计划是构建弹性数据架构的第一步，而这个计划的关键在于理解共享责任模型。如果你按照本章第一部分的建议，在云中自托管自己的数据解决方案，那么你需要对一切负责。我们经常会遇到人们在转向托管服务时的断层。不可避免地，有人会在他们的托管云数据库实例中找到一个选项框，标记为**启用备份**，然后看到它已被勾选。接着，他们就会安然入睡，认为他们的数据是安全的，因为它被模糊地“由云处理”。如果这听起来很熟悉（即使不熟悉），你可能需要考虑制定一个恢复行动计划。
- en: 'Some key factors need to be considered when creating this plan, as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 制定该计划时需要考虑的一些关键因素如下：
- en: 'Define your **recovery time objective** (**RTO**) and **recovery point objective**
    (**RPO**). Respectively, these two measures are the answers to the following questions:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义你的**恢复时间目标**（**RTO**）和**恢复点目标**（**RPO**）。这两个指标分别是以下问题的答案：
- en: How much time can I afford for my service to be down?
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我能接受服务停机多长时间？
- en: How much data can I afford to lose when the service goes down?
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务停机时，我能承受丢失多少数据？
- en: A common anti-pattern here is to answer “None” and “Nothing.” Realistically,
    the costs of maintaining such a strategy are incongruent with reality. Typically,
    the question is answered in an order of magnitude, such as seconds, minutes, hours,
    or days.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个常见的反模式是回答“没有”和“没有”。实际上，维持这样的策略的成本与现实不符。通常，这个问题的回答是按量级的，如秒、分钟、小时或天。
- en: 'Once you’ve outlined the parameters for your recovery plan, you must design
    an architectural solution to achieve this. Suppose the scales we are looking at
    involve second or minute granularity. In that case, you likely need to look into
    live read replicas that can take over as the main DB in the case of failure or
    even multi-master DB configurations for ultra-low downtime applications. A solid
    incremental backup system is enough if we look at the order of hours or days (in
    the first scenario, we’d still have this requirement for resilience in depth).
    We can test our resilience architecture by detailing stressors our system may
    experience, such as a failure in a database instance or an entire cloud region
    going down. We then draft the system response when that stressor occurs, making
    changes to our architecture as required. There is an interesting parallel here
    between the stressors we choose to simulate and the actual resilience of our system.
    In the research and subsequent book by Barry O’Reilly, *Residues: Time, Change,
    and Uncertainty in Software Architecture*, he states that often our stressors
    do not exist in mutual exclusivity; for example, a network failure and a tsunami
    wiping out a data center are likely to have commonalities in the way our architecture
    will respond. Therefore, our stressor list does not need to be exhaustive. We
    just need to list and simulate stressors until our resultant architecture no longer
    requires any changes to support recovery from the stressor.'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '一旦你概述了恢复计划的参数，你必须设计一个架构解决方案来实现这些目标。假设我们关注的尺度涉及秒级或分钟级粒度，那么你可能需要考虑使用实时只读副本，这样在发生故障时可以接管作为主数据库，或者甚至考虑为超低停机时间的应用部署多主数据库配置。如果我们考虑的是小时或天级别的粒度（在第一个场景下，我们仍然需要这种深度恢复能力），那么一个稳健的增量备份系统就足够了。我们可以通过详细列出系统可能遭遇的压力点来测试我们的弹性架构，比如数据库实例的故障或整个云区域的宕机。接着，我们制定在这些压力点发生时系统的响应，并根据需要对架构进行调整。这里有一个有趣的平行之处，在于我们选择模拟的压力点和我们系统实际的弹性之间。例如，在巴里·奥赖利（Barry
    O''Reilly）的研究及其随后的著作《Residues: Time, Change, and Uncertainty in Software Architecture》中，他指出，往往我们的压力点并非相互独立；比如，网络故障和海啸摧毁数据中心在架构响应上可能会有相似之处。因此，我们的压力点列表并不需要详尽无遗。我们只需要列出并模拟这些压力点，直到最终的架构不再需要任何变更来支持从这些压力点恢复。'
- en: 'Once we design the resilience architecture, we can start reviewing the action
    plan. The action plan is a detailed step-by-step manual for restoring service;
    think of it as the user guide to your resilient architecture. Identifying all
    the functional and non-functional requirements needed to complete the operation
    is essential. Some good questions to ask yourself are the following:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们设计了弹性架构，我们可以开始审查行动计划。行动计划是恢复服务的详细逐步手册；可以将其视为你的弹性架构的用户指南。识别完成操作所需的所有功能性和非功能性要求是至关重要的。以下是一些你可以问自己的好问题：
- en: Who is going to act?
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁将执行操作？
- en: Where will they get the credentials?
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们将从哪里获取凭证？
- en: What are the resource identifiers that they will be acting on?
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们将操作的资源标识符是什么？
- en: What are the impacts on customers going to be?
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这将对客户产生什么影响？
- en: What steps will they need to perform?
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们需要执行哪些步骤？
- en: The final step is to run through your action plan. This dry run can be in a
    lower environment or a copy of production. Still, it’s essential to carry out
    the operations in your action plan to identify any gaps in the documentation.
    Ideally, you would do this with team members who are not involved in designing
    the action plan. This process prevents the team from performing the actions based
    on intent rather than the documentation itself. You can do this as often as required
    to refine the action plan until everyone is comfortable.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后的步骤是执行你的行动计划。这个模拟操作可以在较低环境中进行，或者是在生产环境的副本上进行。不过，执行行动计划中的操作以识别文档中任何遗漏是至关重要的。理想情况下，你应该与没有参与设计行动计划的团队成员一起进行此操作。这个过程可以防止团队根据意图而非文档本身来执行操作。你可以根据需要多次进行此操作，以便完善行动计划，直到每个人都感到舒适为止。
- en: Game day
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演练日
- en: When soldiers train for combat, they don’t do it solely in a classroom or through
    reading copious, possibly outdated documentation. A core part of their readiness
    comes from training activities that simulate real-world scenarios as closely as
    possible. This regime means that when they respond to situations, they don’t just
    know what to do in theory; they have real-world knowledge. Your team should practice
    responding to incidents similarly, using conditions close to real-world scenarios.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当士兵训练备战时，他们不会仅仅在课堂上或者通过阅读大量可能已经过时的文档来进行。准备工作的核心部分来自模拟真实世界场景的训练活动。这个制度意味着，当他们响应情况时，他们不仅知道该做什么，理论上能知道如何应对；他们还具备了实际的知识。你的团队也应该类似地练习应对事件，使用接近真实世界场景的条件。
- en: 'The first stage with any game day is planning. At its inception, the game day
    should have a clearly defined scope and boundaries to ensure the safety of the
    scenario. The last thing you want is a hypothetical incident response becoming
    an actual incident! The planning should include a scenario that tests a specific
    action plan. These scenarios can be as real or as fake as you want, and your list
    of stressors from designing your architecture might be an excellent place to start.
    Some of my favorites are the following:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 任何演练日的第一阶段都是规划。开始时，演练日应该有明确定义的范围和边界，以确保场景的安全性。你最不希望看到的就是假设的事件响应变成实际事件！规划应包括测试特定行动计划的场景。这些场景可以尽可能真实或虚拟，而你在设计架构时的压力测试列表可能是一个很好的起点。以下是我最喜欢的一些：
- en: A senior engineer with too many permissions (does this sound familiar from [*Chapter
    6*](B22364_06.xhtml#_idTextAnchor165)?) had production database access. They accidentally
    dropped one of our tables. How can we get the data back?
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一位权限过多的高级工程师（这是不是让你想起了[*第六章*](B22364_06.xhtml#_idTextAnchor165)？）有生产数据库的访问权限。他们不小心删除了我们的一个表。我们如何恢复数据？
- en: Your database needs critical security updates that require a database restart.
    How will you ensure continuity?
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的数据库需要进行关键的安全更新，这需要重新启动数据库。你如何确保持续性？
- en: Someone scheduled the deletion of the blob storage encryption key. Did we detect
    it? How will we prevent it?
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有人计划删除Blob存储加密密钥。我们发现了吗？我们如何防止它？
- en: Even though the scenarios may be fake, the tools and processes used should be
    the same as those we use in a real incident. The response should be as close as
    possible to the required real-world response.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这些场景可能是虚拟的，使用的工具和流程也应该与我们在实际事件中使用的相同。响应应该尽可能接近真实世界所需的响应。
- en: Remember those RTO and RPO goals we defined when formulating the plan? The game
    day is a perfect litmus test for those goals. Going into the event, everyone should
    be aware of these objectives, the deadline should be enforced, and, ideally, meeting
    the objective should be incentivized.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们在制定计划时定义的那些RTO和RPO目标吗？比赛日是检验这些目标的完美试金石。在活动开始之前，每个人都应该清楚这些目标，截止日期应该得到执行，理想情况下，达成目标应该有激励措施。
- en: A game day is a great way to build inter-team communication and break down silos
    within the business. Involve all affected teams, even non-technical teams. How
    will sales operate with missing data? Does the marketing team need to create a
    statement? The implications of an actual event likely spread beyond the confines
    of the technical team, so why not utilize your simulated event to manage the complete
    response? Your technical team will need additional technical-only game days, but
    a full-scale game day can be highly productive to test your entire business’s
    resilience.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 比赛日是促进团队间沟通并打破业务内部壁垒的好方法。要让所有受影响的团队参与，甚至是非技术团队。销售团队在缺少数据的情况下如何操作？市场营销团队需要发布声明吗？实际事件的影响往往超出了技术团队的范围，为什么不利用模拟事件来管理整个响应过程呢？你的技术团队可能需要更多仅限技术的比赛日，但全方位的比赛日对于测试整个业务的韧性非常有帮助。
- en: 'Executing the game day is fun: set up your simulated scenario, inform your
    operational team of the situation, and then watch them perform the recovery strategy.
    Make sure that the team is aware of the scope and boundaries of the game day before
    they begin executing to avoid the consequences we mentioned earlier. While testing
    your incident response, you should document your team’s actions. This process
    enables you to identify gaps in your existing action plan and refine it for future
    game days or an actual incident.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 执行比赛日是有趣的：设置你的模拟场景，告知运营团队当前的情况，然后观察他们执行恢复策略。在他们开始执行之前，确保团队了解比赛日的范围和边界，以避免我们之前提到的后果。在测试事件响应时，应该记录团队的行动。这个过程可以帮助你发现现有行动计划中的空白，并为未来的比赛日或实际事件进行优化。
- en: This process should be followed by a healthy and blameless postmortem for both
    the simulated event (e.g., how did this theoretical event occur in the first place?
    How can we stop it from happening in the real world?) and the actual response
    itself (e.g., did we meet our RTO and RPO? Was our procedure efficient?).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程之后应该进行一次健康且无责备的事后总结，既包括模拟事件（例如，这个理论事件最初是如何发生的？我们如何防止它在现实中发生？），也包括实际响应本身（例如，我们是否达到了RTO和RPO目标？我们的流程是否高效？）。
- en: 'We will use the documentation generated during the execution phase after the
    event for a post-game day retrospective. This retrospective can follow the standard
    Agile retrospective format:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在活动后使用执行阶段生成的文档进行赛后回顾。这次回顾可以按照标准的敏捷回顾格式进行：
- en: What went well?
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么地方做得好？
- en: What went wrong?
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么地方出了问题？
- en: What did we learn?
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们学到了什么？
- en: What changes can we make to do better next time?
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一次我们可以做出哪些改进？
- en: 'We can usually separate the points raised through this retrospective into two
    distinct categories:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常可以将通过回顾提到的要点分为两类：
- en: Points about the execution of the recovery plan
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于恢复计划执行的要点
- en: Points about the execution of the game day itself
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于比赛日执行本身的要点
- en: Both are important to collect, but use the first set to feed into improving
    your recovery plan and the second set to host an even better game day next time!
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 两者都很重要，但使用第一组数据来改进你的恢复计划，第二组数据则帮助你在下次举办更好的比赛日！
- en: The real thing
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 真实的情况
- en: 'If you follow the preceding advice when an actual incident occurs, the response
    should be that of a well-oiled machine rolling into action. That does not absolve
    you of your surrounding responsibilities. You should still do the following:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在实际事件发生时遵循前述建议，响应应该是一个运转良好的机器顺利启动。但这并不意味着你可以免除周围的责任。你仍然需要做以下事项：
- en: Execute according to procedure
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照流程执行
- en: Document all actions taken
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录所有采取的行动
- en: Try to achieve RTO and RPO targets
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试达到RTO和RPO目标
- en: Conduct a postmortem and retrospective
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行事后总结和回顾
- en: You will (hopefully!) get very few of these opportunities to execute the recovery
    plan for real, so this is where you will get your most valuable data.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你将（希望！）很少有机会真正执行恢复计划，因此，这将是你获得最有价值数据的时刻。
- en: Manual data ingestion
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动数据摄取
- en: 'When talking to other engineers about problems they experience when writing
    code, they will often say that the computer is not doing what they are telling
    it to do. My answer is usually the same: “*Computers will do exactly what you
    tell them to do*.” There is an old joke that illustrates this point very well.
    A programmer’s partner asks them to go to the shops and pick up a loaf of bread,
    and if they have eggs, get a dozen. The programmer returns with a dozen loaves
    of bread. When questioned why, they reply, “*Well, they had eggs*.” Computers
    are literal, but when you finally have the computer exhibiting the behavior that
    you want, the good news is that it will execute the actions precisely the same
    ad infinitum, barring some external influence. The downside is that computers
    are bad at performing actions that we haven’t predicted. On the other hand, humans
    have evolved to excel at performing in situations we haven’t anticipated. However,
    you lose the perfect execution criteria of computers.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当与其他工程师讨论他们在编写代码时遇到的问题时，他们经常会说计算机没有按照他们的意愿执行。我的答案通常是相同的：“*计算机将会按照你告诉它的方式执行*。”有一个古老的笑话很好地说明了这一点。一个程序员的伴侣要求他们去商店买一条面包，如果他们有鸡蛋，就拿一打。程序员带着一打面包回来。当被问及原因时，他们回答说：“*噢，他们有鸡蛋*。”计算机是字面的，但是当你最终让计算机表现出你想要的行为时，好消息是它将无限精确地执行这些动作，除非有外部影响。不利之处在于计算机在我们未预测到的行动方面表现不佳。另一方面，人类已经进化到擅长在我们未预料到的情况下表现出色。然而，你失去了计算机的完美执行标准。
- en: What does this have to do with data? What would you choose if we want our data
    to be ingested the same way every time? A fallible human who might be able to
    sort out the edge cases on the fly or a significantly less fallible automated
    system that is deterministic in the way that the same input will always produce
    the same output?
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这与数据有什么关系？如果我们希望我们的数据每次都以相同的方式被摄入，你会选择什么？一个可能能够即时处理边缘情况的不可靠人类，还是一个在相同输入情况下始终产生相同输出的远不那么容易出错的自动化系统？
- en: The first data ingestion pipeline
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一个数据摄入管道
- en: The first stage of shifting to an automated data ingestion system is to define
    the happy path. We discussed this concept when talking about synthetic data. How
    would you want the system to operate if all your data was perfect? This allows
    you to feed perfect data into the system and receive perfect results. In an ideal
    world, we wouldn’t need to ever progress beyond this state. In my experience,
    I have never encountered a data source that met the perfect criteria. So, let
    us start pushing data through our pipeline, and if our data doesn’t hit our perfect
    criteria, we can deal with the issues as they arise. This might involve removing
    invalid records from the source dataset or manipulating the data to meet our perfect
    data criteria.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 转向自动化数据摄入系统的第一阶段是定义顺利路径。我们在讨论合成数据时讨论过这个概念。如果你的所有数据都是完美的，你希望系统如何运行？这允许您将完美的数据输入系统并获得完美的结果。在理想的世界中，我们永远不需要超越这个状态。根据我的经验，我从未遇到过符合完美标准的数据源。因此，让我们开始推送数据通过我们的管道，如果我们的数据不符合我们的完美标准，我们可以在问题出现时处理这些问题。这可能涉及从源数据集中删除无效记录或操作数据以满足我们的完美数据标准。
- en: 'This has enabled us to combine the best of both worlds. Our automated system
    processes all of our well-formatted data to produce deterministic results, and
    our human operators can intervene when the computerized system cannot process
    the records. This allows the human element to exercise their judgment when required
    to allow all records to be correctly ingested. However, this setup still has one
    key issue: cloud services can quickly ingest our data, processing millions of
    records per second. On the other hand, while being more versatile, humans move
    at a glacial pace.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够兼顾两者的优势。我们的自动化系统处理所有格式良好的数据以生成确定性结果，而当计算机系统无法处理记录时，我们的人类操作员可以介入。这允许人类成员在需要时行使判断力，以确保所有记录都被正确摄入。然而，这种设置仍然存在一个关键问题：云服务可以快速摄入我们的数据，每秒处理数百万条记录。另一方面，虽然更为灵活，但人类的操作速度则极为缓慢。
- en: Failure granularity
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 失败的粒度
- en: When ingesting data, we want to ensure we choose the correct failure granularity
    for our data ingestion pipeline. A naive approach would be to fail the pipeline
    whenever an error is encountered. As our datasets grow and our ingestion pipelines
    become more complex, the chances of the pipeline not experiencing a failure rapidly
    approaches zero. It is an infrequent case, in my experience, that a data pipeline
    provides value through an all-or-nothing approach.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据摄入时，我们希望确保选择适合我们数据摄入管道的正确失败粒度。一种简单的方法是在遇到错误时立即失败管道。随着数据集的增长和摄入管道变得更加复杂，管道不经历故障的可能性迅速接近零。根据我的经验，数据管道通过全有或全无的方法提供价值的情况很少见。
- en: Typically, an incomplete dataset still offers more value than no data at all,
    and that is where this naive approach falls over. This is where it is crucial
    to consider your failure granularity. This means we need to discover the smallest
    unit of data that becomes non-functional when there is an error. This might mean
    we fail a single file, row/column, or cell in our dataset. By constraining the
    failure to the smallest unit of non-functional data, we can still leverage our
    dataset for other purposes, collect the failing units of data, and then process
    those failures asynchronously, enhancing the dataset as time goes on by using
    human judgment to deal with these edge cases.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，即使是不完整的数据集也比没有数据具有更大的价值，这就是这种简单方法的局限所在。在这里至关重要的是考虑你的失败粒度。这意味着我们需要找出数据的最小单元，在发生错误时变得不可用。这可能意味着我们在数据集中失败了单个文件、行/列或单元格。通过将失败限制在最小的非功能数据单元上，我们仍然可以利用数据集进行其他目的，收集失败的数据单元，然后异步处理这些故障，通过人类判断处理这些边缘情况，随着时间的推移增强数据集。
- en: This might consist of an automatic prefiltering stage that determines whether
    the data matches our specifications. Records that do match are passed onto our
    data ingestion pipeline, and records that do not match our specification are passed
    to a dead letter queue for later triaging.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能包括一个自动预过滤阶段，用于确定数据是否符合我们的规范。符合规范的记录将被传递到我们的数据摄入管道，而不符合规范的记录将被传递到死信队列进行后续分析。
- en: Scaling the pipeline
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展管道
- en: Human labor for mundane tasks will always be the most expensive to scale. The
    human requirement to scale experiences hysteresis with the time required to hire,
    onboard, and train new resources. With the adoption of cloud native services,
    we barely even have to lift a finger to increase our throughput. In fact, with
    auto-scaling, even those few mouse clicks and keyboard strokes may be redundant!
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 人力进行平凡任务的成本始终是最昂贵的。与时间、成本和培训新资源的需求相关联。随着云原生服务的采用，我们甚至几乎不需要动一根手指来增加我们的吞吐量。事实上，通过自动扩展，甚至那些少数的鼠标点击和键盘敲击也可能是多余的！
- en: Once the initial pipeline is built, the dead letter queue becomes a valuable
    resource. As we fix issues with data in the dead letter queue, we understand the
    types of problems we expect to see with our data. By analyzing how our human experts,
    with domain knowledge, rectify this problem, we can begin to provide edge case
    automation for these cases, codifying their knowledge into instructions that our
    pipeline can execute. As our pipeline scales, this automation allows it to improve
    its resilience, allowing our adaptable human element to deal with new problems
    requiring their expertise.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦初始管道建立完成，死信队列就成为一个宝贵的资源。随着我们修复死信队列中的数据问题，我们了解到我们预期在数据中看到的问题类型。通过分析我们的具有领域知识的人类专家如何纠正这个问题，我们可以开始为这些情况提供边缘案例的自动化，将他们的知识编码成指令，供我们的管道执行。随着管道的扩展，这种自动化使其提高了韧性，允许我们适应新问题，并需要他们的专业知识。
- en: Automating these cases also allows us to increase the recency of our data. Rather
    than waiting for a human to come and rectify these errors after they have been
    detected as errors, we have extended our specification to include these types
    of data.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化这些情况还允许我们增加数据的实时性。我们不再需要等待人类在检测到错误后来修正这些错误，我们已经扩展了我们的规范，包括这些类型的数据。
- en: Making the jump to streaming
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转向流式处理
- en: As our pipeline becomes increasingly automatic, and if our upstream data sources
    support it, we can increase the frequency of our data ingestion to be closer to
    real time. Instead of a manual ingestion process performed once a week due to
    human limitations, we can shift to running our pipeline much more frequently.
    We have seen clients achieve a shift from monthly data ingestions to hourly data
    ingestions with this process.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们的管道变得越来越自动化，如果我们的上游数据源支持，我们可以增加数据摄取的频率，使其更接近实时。与每周一次的人工摄取过程（由于人力限制）不同，我们可以将管道的运行频率提高得多。我们已经看到客户通过这个过程实现了从每月数据摄取到每小时数据摄取的转变。
- en: The final stage is rather than a schedule-driven process that pulls all data
    that has occurred in a period, we shift to a streaming model where the presence
    of new data kicks off the ingestion pipeline. The advantage of using cloud native
    services in this space is that, often, the scheduled pipelines you have already
    created can be run as streaming pipelines with minimal changes.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最终阶段不再是一个按计划驱动的过程，按计划提取一段时间内所有发生的数据，而是转向流式模型，在新数据出现时启动数据摄取管道。在这个领域使用云原生服务的优势在于，通常你已经创建的定时管道可以以最小的修改作为流式管道运行。
- en: No observability for data transfer errors
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据传输错误无可观察性
- en: I will repeat the mantra used countless times throughout this book, “*You can’t
    fix what you can’t measure*.” The same is valid for data transfer. You need to
    be able to view the state of your data transfers so you can make informed decisions
    based on the data you have. The observability method is up to the user, but it
    is important to note that simply getting the observability data is half the battle.
    The other half is getting it in front of the eyes that will most impact the quality
    of your data pipeline.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我会重复这本书中无数次提到的箴言：“*你无法修复你无法衡量的东西*。” 同样适用于数据传输。你需要能够查看数据传输的状态，以便根据已有的数据做出明智的决策。可观察性方法由用户决定，但重要的是要注意，单纯获取可观察性数据只是成功的一半。另一半是将数据展示给那些最能影响你数据管道质量的人。
- en: The data integrity dependency
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据完整性依赖性
- en: 'Let me pose a hypothetical scenario we have seen play out at clients all too
    often. You have a successful app with a great dev team. To better understand your
    customers, you create a new data team to track how users interact with your application.
    To accomplish this, your developers quickly cobble together some cloud native
    data pipeline tools to feed data to the data team. The data team struggles to
    make progress because the data coming through is of poor quality, so the data
    team spends excessive time simply getting the data to a usable state. This causes
    the data team to be less effective due to both lack of time and lack of good quality
    data. The development team is just throwing data over the metaphorical fence and
    letting the data team deal with the fallout. The development team is the beneficiary
    of the data, as they will be the ones who can consume the data artifacts that
    the data team produces to understand better what they are building. Here, we see
    the dichotomy: the data team is rarely the team that will benefit from the data,
    but they are the ones who need to ensure that the data is correct to show that
    they are doing their jobs.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我举一个我们在客户中常常看到的假设场景。你有一个成功的应用程序，并且有一个出色的开发团队。为了更好地了解你的客户，你成立了一个新的数据团队来追踪用户如何与应用互动。为此，你的开发人员迅速将一些云原生数据管道工具拼凑在一起，供数据团队使用。然而，数据团队由于数据质量差，进展缓慢，花费过多时间将数据整理到可用状态。这导致数据团队因缺乏时间和优质数据而效率低下。开发团队只是把数据“扔”给数据团队，让他们应对后果。开发团队是数据的受益者，因为他们将是使用数据团队生产的数据信息来更好地理解他们所构建内容的人。在这里，我们看到了一种二分法：数据团队很少是数据的受益者，但他们却需要确保数据是正确的，以证明他们在做他们的工作。
- en: Inverting the dependency
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反转依赖关系
- en: I worked for a client previously in a company with a very large (non-software)
    engineering function. These engineers are tasked with ensuring that specific safety
    parameters are met. Part of that included ingesting sensor data from the field.
    One data engineer was responsible for maintaining the data pipeline. This configuration
    is all good in a static environment, but as we all know thanks to Werner Vogels,
    “*Everything fails all the time*.” What happened was that some sensors, data loggers,
    or even networking equipment would fail and be replaced, changing the topology
    of the data. The data would then show up as unrecognized, and the data engineer
    would go and chase down the responsible engineer for the correct parameters to
    ingest the data correctly. In this scenario, the data engineer did not benefit
    from the data but was responsible for reactively fixing the data. Alongside this
    client, we designed a solution that monitored pipeline health, found inconsistencies,
    and told the engineer responsible that the data was not being appropriately ingested.
    They could then log in to a UI to fix the data topology so it would be ingested
    correctly on the next run. As the responsibility for this data sat with the engineer,
    we noticed that not only did they reactively fix the data they were responsible
    for but they proactively went and updated the topology to prevent future pipeline
    failures. We had inverted the dependency!
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前为一位客户工作，这家公司有一个非常庞大的（非软件）工程职能。这些工程师的任务是确保特定的安全参数得以满足。其中一部分工作包括从现场获取传感器数据。一名数据工程师负责维护数据管道。这种配置在静态环境下没有问题，但正如我们所有人通过沃尔纳·福格尔所知，“*一切都会不断失败*。”
    发生的情况是，一些传感器、数据记录器甚至网络设备会出现故障并被替换，改变数据的拓扑结构。然后，数据会显示为未被识别，数据工程师会去找负责的工程师，以获取正确的参数来正确地获取数据。在这种情况下，数据工程师并未从数据中受益，而是负责反应性地修复数据。与该客户合作时，我们设计了一个解决方案来监控管道的健康状况，发现不一致之处，并通知负责的工程师数据未能被正确获取。然后，他们可以登录到一个用户界面修复数据拓扑，以便在下次运行时正确获取数据。由于这部分数据的责任由工程师承担，我们注意到，他们不仅反应性地修复了自己负责的数据，还主动去更新拓扑，防止未来的管道故障。我们反转了依赖关系！
- en: This is the power of having the right eyes on the observability data and empowering
    the beneficiaries to maintain it themselves. This lets our data engineers focus
    instead on the bigger picture and deal with problems in the data domain rather
    than playing catchup with other domains.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是在观察性数据上拥有正确视角并授权受益者自行维护的力量。这让我们的数据工程师能够专注于更大的图景，处理数据领域的问题，而不是忙于追赶其他领域的进度。
- en: Maintaining the dependency
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维护依赖关系
- en: Now that we have inverted the data dependency between our producers and consumers,
    we can start to examine how to preserve the integrity of the link. As developers
    move forward, they rarely stop to think about their changes’ impact on the broader
    data ecosystem, of which their data is only a tiny part. The key to negotiating
    this minefield is typically through data contracts. A data contract is a specification
    that defines the format of the data that the application will produce. These specifications
    represent a mutual understanding of the underlying schema between data producers
    and consumers. If we use a common specification framework, such as JSON Schema,
    we can add tests for conformity of our data as part of the definition of done.
    This definition allows us to identify when we will cause breaking changes and
    preemptively notify downstream users that the schema is changing.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经反转了生产者和消费者之间的数据依赖关系，我们可以开始研究如何保持链路的完整性。随着开发人员的推进，他们很少停下来思考自己所做的更改对更广泛数据生态系统的影响，而他们的数据只是其中的一小部分。应对这一难题的关键通常是通过数据契约。数据契约是一种规范，定义了应用程序将要生成的数据格式。这些规范代表了数据生产者和消费者之间对基础架构的共同理解。如果我们使用一个通用的规范框架，如JSON
    Schema，我们可以为数据的一致性添加测试，作为完成定义的一部分。这个定义可以帮助我们识别何时会造成破坏性更改，并提前通知下游用户该架构正在发生变化。
- en: Mature operations in this space also allow for the adoption of more modern tools,
    such as data catalogs. These catalogs will enable us to register the data and
    its schema so that it can be utilized by anyone who needs it within the organization.
    It is also vital to centrally track these new dependencies as they grow so that
    we know who to notify when a data contract requires a breaking change.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 该领域的成熟操作还允许采用更现代的工具，如数据目录。这些目录将使我们能够注册数据及其架构，以便组织中需要的人可以使用它。同样，随着这些新依赖关系的增长，集中跟踪它们是至关重要的，这样我们就能知道在数据合同需要重大变更时，应该通知谁。
- en: So, now we have a solid understanding of how data observability is important
    for reacting to pipeline failures, preemptively acting, and treating our data
    services as first-class citizens in our application stack.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们已经清楚地理解了数据可观察性在应对管道故障、预防性行动和将数据服务视为我们应用堆栈中的一等公民方面的重要性。
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The cloud offers all new ways for us to manage one of our most important assets:
    our data! However, falling into the anti-patterns in this chapter can not only
    have implications for your bottom line but also for the durability, availability,
    and security of your data. By understanding the concepts in this chapter, you
    are well equipped to navigate the cloud native data jungle and build effective
    architectures. Next, we will look at how we can connect all the parts of our architecture
    together.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算为我们管理最重要的资产之一——我们的数据——提供了全新的方式！然而，陷入本章中的反模式不仅可能对你的盈利造成影响，还可能影响数据的持久性、可用性和安全性。通过理解本章中的概念，你将能够顺利导航云原生数据的丛林并构建有效的架构。接下来，我们将看看如何将我们架构的各个部分连接起来。
