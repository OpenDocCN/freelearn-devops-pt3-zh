- en: 'Chapter 14: Deploying and Building Applications in Kubernetes'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章：在Kubernetes中部署和构建应用程序
- en: It's difficult to overstate the impact Kubernetes has had on the world of DevOps.
    Over the years since it was open sourced by Google in 2014, Kubernetes has experienced
    a meteoric rise in popularity. In that period, Kubernetes has become the preeminent
    solution for orchestrating cloud-native container workloads, differentiating itself
    from a field of orchestrators such as Apache Mesos and Docker Swarm. By providing
    a common API over heterogeneous environments, Kubernetes has become the common
    tool for deploying applications across cloud and hybrid environments.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 很难夸大Kubernetes对DevOps世界的影响。自从2014年由Google开源以来，Kubernetes在这几年中经历了迅猛的流行。在此期间，Kubernetes已经成为编排云原生容器工作负载的主要解决方案，将其与Apache
    Mesos和Docker Swarm等编排工具区分开来。通过在异构环境上提供统一的API，Kubernetes已经成为跨云和混合环境部署应用程序的通用工具。
- en: So, what is Kubernetes? According to its documentation, *"Kubernetes is a portable,
    extensible, open source platform for managing containerized workloads and services,
    that facilitates both declarative configuration and automation"* ([https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/](https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/)).
    That is a lot to unpack. I'll sum up that statement a little differently. Kubernetes
    is a set of APIs and abstractions that makes running containerize applications
    easier. It provides services such as service discovery, load balancing, storage
    abstraction and orchestration, automated rollouts and rollbacks, self-healing,
    and secret, certificate, and configuration management. Furthermore, if Kubernetes
    doesn't offer a specific bit of functionality you need directly, there is likely
    a solution available in the vibrant open source ecosystem built around the core
    of Kubernetes. The Kubernetes ecosystem is a vast set of tools for you to achieve
    your operational objectives without needing to reinvent the wheel.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，Kubernetes究竟是什么？根据它的文档，*"Kubernetes是一个可移植、可扩展的开源平台，用于管理容器化的工作负载和服务，既支持声明式配置也支持自动化"*（[https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/](https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/)）。这有很多内容需要解读。我会用不同的方式总结这一声明。Kubernetes是一组API和抽象层，使得运行容器化应用程序变得更加容易。它提供了诸如服务发现、负载均衡、存储抽象与编排、自动化发布与回滚、自愈功能，以及密钥、证书和配置管理等服务。此外，如果Kubernetes没有直接提供你所需要的某些特定功能，可能在围绕Kubernetes核心构建的充满活力的开源生态系统中就有解决方案可用。Kubernetes生态系统是一个庞大的工具集，可以帮助你实现运营目标，而无需重新发明轮子。
- en: All of the aforementioned functionality is exposed through the Kubernetes API
    and is infinitely programmable.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 上述所有功能都通过Kubernetes API暴露出来，且具有无限的可编程性。
- en: 'This chapter will not be a deep dive into all aspects of Kubernetes. To properly
    explore Kubernetes in depth would require multiple books. The good news is there
    are many great books on the topic: [https://www.packtpub.com/catalogsearch/result?q=kubernetes](https://www.packtpub.com/catalogsearch/result?q=kubernetes).
    Also, the fantastic community-driven documentation ([https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/))
    for Kubernetes is an invaluable resource for getting a deeper understanding of
    it.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章不会深入探讨Kubernetes的各个方面。要深入全面地探索Kubernetes需要多本书的内容。好消息是，关于Kubernetes有许多很棒的书籍：[https://www.packtpub.com/catalogsearch/result?q=kubernetes](https://www.packtpub.com/catalogsearch/result?q=kubernetes)。此外，Kubernetes的社区驱动文档（[https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)）是一个宝贵的资源，可以帮助你更深入地了解Kubernetes。
- en: The goal of this chapter is to provide a starting point for your journey in
    programming Kubernetes using Go. We will start by creating a simple Go program
    to deploy a Kubernetes resource to a local Kubernetes cluster to run a load-balanced
    HTTP service. We will then learn how to extend the Kubernetes API with custom
    resources to show how Kubernetes can be used to orchestrate and manage any external
    resource. We will build custom pet resources that will be stored in our pet store
    service running within the cluster to illustrate the concept of managing external
    resources. By the end of this chapter, you will be equipped with the knowledge
    to work effectively with the Kubernetes API and understand some of the core design
    principles of Kubernetes.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是为你使用 Go 编程 Kubernetes 提供一个起点。我们将从创建一个简单的 Go 程序开始，将 Kubernetes 资源部署到本地
    Kubernetes 集群中，运行一个负载均衡的 HTTP 服务。然后，我们将学习如何通过自定义资源扩展 Kubernetes API，展示如何利用 Kubernetes
    协同管理任何外部资源。我们将构建自定义的宠物资源，这些资源将存储在集群中运行的宠物商店服务中，以此说明管理外部资源的概念。通过本章学习，你将掌握有效使用 Kubernetes
    API 的知识，并理解 Kubernetes 的一些核心设计原则。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Interacting with the Kubernetes API
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 Kubernetes API 交互
- en: Deploying a load-balanced HTTP application using Go
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Go 部署一个负载均衡的 HTTP 应用
- en: Extending Kubernetes with custom resources and operators
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自定义资源和操作员扩展 Kubernetes
- en: Building a pet store operator
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个宠物商店操作员
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter will require the following tools:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将需要以下工具：
- en: 'Docker: [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Docker: [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)'
- en: 'KinD: [https://kind.sigs.k8s.io/#installation-and-usage](https://kind.sigs.k8s.io/#installation-and-usage)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'KinD: [https://kind.sigs.k8s.io/#installation-and-usage](https://kind.sigs.k8s.io/#installation-and-usage)'
- en: 'operator-sdk: [https://sdk.operatorframework.io/docs/installation/](https://sdk.operatorframework.io/docs/installation/)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'operator-sdk: [https://sdk.operatorframework.io/docs/installation/](https://sdk.operatorframework.io/docs/installation/)'
- en: 'Tilt.dev: [https://docs.tilt.dev/install.html](https://docs.tilt.dev/install.html)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tilt.dev: [https://docs.tilt.dev/install.html](https://docs.tilt.dev/install.html)'
- en: 'ctlptl: [https://github.com/tilt-dev/ctlptl#how-do-i-install-it](https://github.com/tilt-dev/ctlptl#how-do-i-install-it)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ctlptl: [https://github.com/tilt-dev/ctlptl#how-do-i-install-it](https://github.com/tilt-dev/ctlptl#how-do-i-install-it)'
- en: The code files for this chapter can be downloaded from [https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/14](https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/14)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以从 [https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/14](https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/14)
    下载
- en: Interacting with the Kubernetes API
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与 Kubernetes API 交互
- en: In the introduction, we talked about the Kubernetes API as if it is just one
    thing, although in a sense it can be thought of in that way. However, the Kubernetes
    API we have been talking about is an aggregation of multiple APIs served by the
    core of Kubernetes, the control plane API server. The API server exposes an HTTP
    API that exposes the aggregated API and allows for the query and manipulation
    of API objects such as Pods, Deployments, Services, and Namespaces.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍中，我们将 Kubernetes API 当作一个整体进行讨论，尽管从某种意义上讲，它确实可以被这样理解。然而，我们一直在讨论的 Kubernetes
    API 实际上是由 Kubernetes 核心部分——控制平面 API 服务器提供的多个 API 的聚合。API 服务器暴露了一个 HTTP API，公开了聚合后的
    API，并允许查询和操作如 Pods、Deployments、Services 和 Namespaces 等 API 对象。
- en: In this section, we will learn how to use KinD to create a local cluster. We
    will use the local cluster to manipulate a namespace resource using `kubectl`.
    We will examine the basic structure of a Kubernetes resource and see how we can
    address individual resources by their Group, Version, Kind, Name, and usually,
    Namespace. Lastly, we'll discuss authentication and the `kubeconfig` file. This
    section will prepare us for interacting with the Kubernetes API at a lower level
    using Go.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用 KinD 创建一个本地集群。我们将使用本地集群通过 `kubectl` 操作一个命名空间资源。我们将研究 Kubernetes
    资源的基本结构，并查看如何通过它们的 Group、Version、Kind、Name，通常还有 Namespace，来定位单个资源。最后，我们将讨论身份验证和
    `kubeconfig` 文件。本节将为我们通过 Go 在更低层次上与 Kubernetes API 进行交互做准备。
- en: Creating a KinD cluster
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个 KinD 集群
- en: 'Prior to getting started interacting with the Kubernetes API, let''s build
    a local Kubernetes cluster using **KinD**. This is a tool that enables us to create
    a Kubernetes cluster locally using Docker rather than running as services on the
    host. To create the cluster, run the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始与Kubernetes API交互之前，让我们使用**KinD**构建一个本地Kubernetes集群。这是一个工具，允许我们通过Docker在本地创建Kubernetes集群，而不是作为主机上的服务运行。要创建集群，请运行以下命令：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The preceding command will create a cluster named `kind`. It will build a Kubernetes
    control plane and set the current context of `kubectl` to point to the newly created
    cluster.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将创建一个名为`kind`的集群。它将构建一个Kubernetes控制平面，并将`kubectl`的当前上下文设置为指向新创建的集群。
- en: 'You can list the clusters created by `kind` by running the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过运行以下命令列出`kind`创建的集群：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can see from the output of `get clusters` that there is a new cluster named
    `kind` created.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从`get clusters`的输出中可以看到，创建了一个名为`kind`的新集群。
- en: Using kubectl to interact with the API
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用kubectl与API交互
- en: 'Kubernetes offers a command-line tool for interacting with the API, `kubectl`.
    There are some nice developer experience features in `kubectl`, but its main use
    is to perform `kubectl`:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes提供了一个命令行工具用于与API交互，即`kubectl`。`kubectl`提供了一些很好的开发者体验功能，但其主要用途是执行`kubectl`操作：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding command creates a namespace named `petstore`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令创建了一个名为`petstore`的命名空间：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding command creates the same namespace with an inline YAML document.
    Next, let''s use `kubectl` to fetch the namespace as YAML:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令通过内联YAML文档创建了相同的命名空间。接下来，让我们使用`kubectl`获取该命名空间的YAML文件：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The preceding command fetched the `petstore` namespace and output the entire
    resource in the `.yaml` format. Pay special attention to the top-level keys, `apiVersion`,
    `kind`, `metadata`, `spec`, and `status`. The values and structures in these keys
    will be common to all resources in Kubernetes.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令获取了`petstore`命名空间并以`.yaml`格式输出了整个资源。请特别注意顶级键`apiVersion`、`kind`、`metadata`、`spec`和`status`。这些键中的值和结构在Kubernetes中的所有资源中都是通用的。
- en: The Group Version Kind (GVK) namespace name
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分组版本种类（GVK）命名空间名称
- en: 'In the Kubernetes API, you can identify any resource by the combination of
    its group, kind, version, name, and usually, namespace. I say usually namespace
    since not all resources belong to a namespace. A namespace is an example of a
    resource that exists outside a namespace (as well as other low-level resources
    such as Nodes and PersistentVolumes). However, most other resources such as Pods,
    Services, and Deployments exist within a namespace. For the namespace example
    from the previous section, the group is omitted, since it is in the Kubernetes
    core API and is assumed by the API server. Effectively, the identifier for the
    `petstore` namespace is `apiVersion: v1`, `kind: Namespace`, and `metadata.name:
    petstore`.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '在Kubernetes API中，你可以通过其分组、种类、版本、名称以及通常的命名空间的组合来标识任何资源。我说“通常的命名空间”是因为并非所有资源都属于命名空间。命名空间是存在于命名空间之外的资源的一个例子（还有其他低级资源，如节点和持久卷）。然而，大多数其他资源，如Pods、Services和Deployments，存在于命名空间中。在前一部分中提到的命名空间示例中，分组被省略了，因为它位于Kubernetes核心API中，并由API服务器假定。实际上，`petstore`命名空间的标识符是`apiVersion:
    v1`、`kind: Namespace`和`metadata.name: petstore`。'
- en: Internalize the idea of a group, version, kind, namespace, and name. It will
    be critical to understand how to interact with the Kubernetes API.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 内化组、版本、种类、命名空间和名称的概念。这对于理解如何与Kubernetes API交互至关重要。
- en: The spec and status sections
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`spec`和`status`部分'
- en: Each resource in Kubernetes has a `spec` and a `status` section. The `spec`
    section of the resource is a structure that describes the desired state of the
    resource. It is Kubernetes' job to reconcile the state of the system to achieve
    that desired state. In some cases, `spec` will describe the desired state of an
    external system. For example, `spec` can be a description of a load balancer,
    including the desired external IP. The reconciler for that resource would be responsible
    for creating a network interface and setting up routing to ensure that the IP
    routes to that specific network interface.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的每个资源都有`spec`和`status`部分。资源的`spec`部分是描述资源期望状态的结构。Kubernetes的任务是将系统的状态调整为该期望状态。在某些情况下，`spec`会描述外部系统的期望状态。例如，`spec`可以描述一个负载均衡器，包括期望的外部IP。该资源的调和器将负责创建网络接口并设置路由，以确保IP路由到该特定的网络接口。
- en: The `status` section of the resource is a structure that describes the current
    state of the resource. It is intended to be mutated by Kubernetes, not the user.
    For example, `status` for a Deployment includes the number of ready replicas of
    a given Deployment. `spec` for the Deployment will contain the desired number
    of replicas. It is Kubernetes' job to drive toward that desired state and update
    the `status` with the current state of the resource.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`status`部分是资源的一个结构，描述了资源的当前状态。它应该由Kubernetes进行修改，而不是由用户修改。例如，Deployment的`status`包含给定Deployment的就绪副本数量。Deployment的`spec`将包含所需的副本数。Kubernetes的任务是朝着所需状态推进，并用资源的当前状态更新`status`。'
- en: We will learn more about `spec` and `status` as we progress in this chapter.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 随着本章的深入，我们将更多地了解`spec`和`status`。
- en: Authentication
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 身份验证
- en: 'So far, we have just assumed access to the Kubernetes cluster, but that was
    actually handled for us by `kind` and its ability to set the default context for
    `kubectl`. The default context for `kubectl` is stored in your home directory.
    You can see what was set by running the following command:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只是假设能够访问Kubernetes集群，但实际上这一点是由`kind`处理的，它能为`kubectl`设置默认上下文。`kubectl`的默认上下文存储在你的主目录中。你可以通过运行以下命令查看已设置的上下文：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the preceding output, I've omitted the certificate data to provide a more
    concise view of the config. It contains all the information we need to create
    a secure connection to the local cluster instance. Note the address of the service
    and the names of the cluster and the user.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的输出中，我省略了证书数据，以便提供更简洁的配置视图。它包含了我们建立与本地集群实例的安全连接所需的所有信息。请注意服务的地址、集群的名称以及用户的名称。
- en: 'By running the following command, we can get the `kubeconfig` for the `kind`
    cluster:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下命令，我们可以获得`kind`集群的`kubeconfig`：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If you `cat` the contents of the file, you will see a very similar structure
    in `~/.kube/config`. The `kubeconfig` file is a convenient way to encapsulate
    the information needed to authenticate to the API server and is used with many
    of the tools in the Kubernetes ecosystem. For example, you can override the context
    of `kubectl` to use a different `kubeconfig` with the following command:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你`cat`该文件的内容，你会看到`~/.kube/config`中有一个非常相似的结构。`kubeconfig`文件是一个便捷的方式，用来封装与API服务器进行身份验证所需的信息，并与Kubernetes生态系统中的许多工具一起使用。例如，你可以通过以下命令覆盖`kubectl`的上下文，使用不同的`kubeconfig`：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The preceding command will list all the namespaces in the `kind` cluster, but
    it will use the local `kubeconfig` file we just created.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将列出`kind`集群中的所有命名空间，但它将使用我们刚刚创建的本地`kubeconfig`文件。
- en: There are a variety of tools for managing whichever cluster you are using. One
    great example is `kubectx` ([https://ahmet.im/blog/kubectx/](https://ahmet.im/blog/kubectx/))
    from Ahmet Alp Balkan, which can be used to work fluently with multiple clusters.
    As I mentioned previously, the vibrant open source ecosystem provides a wide variety
    of tools to make your experience using Kubernetes delightful.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种工具可以用来管理你所使用的集群。其中一个很棒的例子是Ahmet Alp Balkan的`kubectx`（[https://ahmet.im/blog/kubectx/](https://ahmet.im/blog/kubectx/)），它可以帮助你流畅地管理多个集群。正如我之前提到的，充满活力的开源生态系统提供了各种各样的工具，让你使用Kubernetes的体验更加愉快。
- en: 'Finally, let''s clean up the `petstore` namespace and delete our `kind` cluster:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们清理`petstore`命名空间并删除我们的`kind`集群：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this section, we learned the basics of interacting with the Kubernetes API
    and the basic structure of Kubernetes resources. We are able to create a local
    Kubernetes experience, and we are ready to approach building an application to
    interact with Kubernetes using Go.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们学习了与Kubernetes API交互的基础知识以及Kubernetes资源的基本结构。我们能够创建本地的Kubernetes体验，并且已经准备好使用Go来构建与Kubernetes交互的应用程序。
- en: In the next section, we are going to leverage what we have learned about the
    Kubernetes API and use that to build a Go application to deploy a load-balanced
    HTTP application.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将利用我们所学的Kubernetes API知识，构建一个Go应用程序，用于部署一个负载均衡的HTTP应用。
- en: Deploying a load-balanced HTTP application using Go
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Go部署一个负载均衡的HTTP应用程序
- en: Now that we understand a bit more about the Kubernetes API and the resources
    exposed by the API, we can move away from `kubectl` toward using Go.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对Kubernetes API及其暴露的资源有了更深入的了解，可以开始从`kubectl`转向使用Go。
- en: In this section, we will use Go to do many of the same things we did in the
    previous section using `kubectl`. We will authenticate using our default context
    and create a namespace. However, we will not stop there. We will deploy a load-balanced
    HTTP application to our cluster and watch the logs stream to STDOUT as we make
    requests to the service.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Go完成许多在上一节中使用`kubectl`做的相同操作。我们将使用默认上下文进行身份验证，并创建一个命名空间。然而，我们不会停在那里。我们将向集群部署一个负载均衡的HTTP应用程序，并在向服务发送请求时，查看日志如何流式输出到STDOUT。
- en: 'The code for this section can be found at [https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/14/workloads](https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/14/workloads).
    The demo we are about to walk through can be executed with the following commands:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的代码可以在[https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/14/workloads](https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/14/workloads)找到。我们接下来要讲解的示例可以通过以下命令执行：
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding command will create a KinD cluster named `workloads` and use
    a config file that will enable host network ingress for the cluster. We will use
    ingress to expose the service running in the cluster on `localhost:port`. The
    command then deploys the NGINX ingress controller and waits for it to be ready.
    Finally, we run our Go program to deploy our application. After the service has
    been deployed and is running, open a browser at `http://localhost:8080/hello`.
    You should see the following when you browse there:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令将创建一个名为`workloads`的KinD集群，并使用一个配置文件来启用集群的主机网络入口。我们将使用入口来公开运行在集群中的服务，地址是`localhost:port`。然后，命令将部署NGINX入口控制器，并等待它准备就绪。最后，我们运行Go程序来部署我们的应用程序。在服务部署并运行后，打开浏览器并访问`http://localhost:8080/hello`。你应该会看到如下内容：
- en: '![Figure 14.1 – The deployed NGINX hello world'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.1 – 部署的NGINX hello world'
- en: '](img/B17626_14_001.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17626_14_001.jpg)'
- en: Figure 14.1 – The deployed NGINX hello world
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1 – 部署的NGINX hello world
- en: 'You should see the request logs stream to STDOUT. They should look like the
    following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能够看到请求日志流输出到STDOUT。它们应该如下所示：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If you refresh the page, you should see the server name change, indicating that
    the requests are load balancing across the two pod replicas in the deployment.
    Press *Ctrl* + *C* to terminate the Go program.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你刷新页面，你应该看到服务器名称变化，表明请求正在跨部署中的两个Pod副本进行负载均衡。按*Ctrl* + *C*来终止Go程序。
- en: 'To tear down the cluster, run the following command:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要销毁集群，请运行以下命令：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The preceding command will delete the `kind` cluster named `workloads`. Next,
    let's explore this Go application to understand what just happened.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令将删除名为`workloads`的`kind`集群。接下来，让我们探索这个Go应用程序，了解刚刚发生了什么。
- en: It all starts with main
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一切从`main`开始
- en: 'Let''s dive right into the code and see what is happening in this Go program:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接进入代码，看看这个Go程序到底在做什么：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the preceding code, we establish a context derived from the background context.
    This is largely ineffectual in this scenario but would be a powerful tool in the
    future if you needed to cancel a request that is taking too long. Next, we create
    `clientSet`, which is a strongly typed client for interacting with the Kubernetes
    API. We then use `clientSet` in `createNamespace`, `deployNginx`, and `listenToPodLogs`.
    Finally, we wait for a signal to terminate the program. That's it!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们建立了一个从背景上下文派生的上下文。在这个场景中，这基本上没有什么效果，但如果你需要取消一个正在运行时间过长的请求，它将是一个非常强大的工具。接下来，我们创建了`clientSet`，它是一个强类型的客户端，用来与Kubernetes
    API进行交互。然后我们在`createNamespace`、`deployNginx`和`listenToPodLogs`中使用了`clientSet`。最后，我们等待一个信号来终止程序。就这样！
- en: Next, let's delve into each function, starting with `getClientSet`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入探讨每个函数，从`getClientSet`开始。
- en: Creating a ClientSet
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建ClientSet
- en: 'Let''s take a look at `getClientSet`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`getClientSet`：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the preceding code, you can see that we build flag bindings to either use
    the existing `~/.kube/config` context or accept a `kubeconfig` file via an absolute
    file path. We then build a config using this flag or default. The config is then
    used to create `*kubernetes.ClientSet`. As we learned in the `kubectl` section,
    `kubeconfig` contains all the information we need to connect and authenticate
    to the server. We now have a client ready to interact with the Kubernetes cluster.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，你可以看到我们构建了标志绑定，用来使用现有的`~/.kube/config`上下文，或者通过绝对文件路径接受`kubeconfig`文件。然后，我们使用这个标志或默认值构建配置。接着，这个配置被用来创建`*kubernetes.ClientSet`。正如我们在`kubectl`部分所学到的，`kubeconfig`包含了我们连接和认证服务器所需的所有信息。现在我们有了一个客户端，准备与Kubernetes集群进行交互。
- en: Next, let's see the `ClientSet` in action.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看 `ClientSet` 的实际操作。
- en: Creating a namespace
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个命名空间
- en: 'Now that we have a `ClientSet`, we can use it to create the resource we need
    to deploy our load-balanced HTTP application. Let''s take a look at `createNamespace`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个 `ClientSet`，可以用它来创建我们需要部署的资源，以运行负载均衡的 HTTP 应用程序。我们来看看 `createNamespace`：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the preceding code, we build a `corev1.Namespace` structure, supplying the
    name in the `ObjectMeta` field. If you recall from our YAML example that created
    a namespace using `kubectl`, this field maps to `metadata.name`. The Go structures
    of the Kubernetes resource map closely to their YAML representations. Finally,
    we use `clientSet` to create the namespace via the Kubernetes API server and return
    the namespace. The `metav1.CreateOptions` contains some options for changing the
    behavior of the `create` operation, but we will not explore this structure in
    this book.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们构建了一个 `corev1.Namespace` 结构体，在 `ObjectMeta` 字段中提供名称。如果你还记得我们之前使用 `kubectl`
    创建命名空间的 YAML 示例，这个字段对应的是 `metadata.name`。Kubernetes 资源的 Go 结构与它们的 YAML 表现非常接近。最后，我们使用
    `clientSet` 通过 Kubernetes API 服务器创建命名空间并返回命名空间。`metav1.CreateOptions` 包含一些选项，用于更改
    `create` 操作的行为，但我们在本书中不会探讨这个结构。
- en: We have now created the namespace where we will deploy our application. Let's
    see how we will deploy the application.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经创建了用于部署应用程序的命名空间。接下来，让我们看看如何部署应用程序。
- en: Deploying the application into the namespace
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将应用程序部署到命名空间中
- en: 'Now that we have `clientSet` and namespace created, we are ready to deploy
    the resources that will represent our application. Let''s have a look at the `deployNginx`
    func:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了 `clientSet` 和命名空间，准备好部署将代表我们应用程序的资源。我们来看看 `deployNginx` 函数：
- en: '[PRE15]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the preceding code, we create the NGINX deployment resource and wait for
    the replicas of the deployment to be ready. After the deployment is ready, the
    code creates the service resource to load-balance across the pods in the deployment.
    Finally, we create the ingress resource to expose the service on a local host
    port.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们创建了 NGINX 部署资源，并等待部署的副本准备就绪。部署就绪后，代码创建了服务资源，以便在部署中的 pods 之间进行负载均衡。最后，我们创建了
    ingress 资源，以便在本地主机端口上公开该服务。
- en: Next, let's review each of these functions to understand what they are doing.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们查看这些函数，了解它们在做什么。
- en: Creating the NGINX deployment
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 NGINX 部署
- en: 'The first function in deploying our application is `createNginxDeployment`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 部署应用程序的第一个函数是 `createNginxDeployment`：
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The preceding code initializes `matchLabel` with a key/value pair that will
    be used to connect the Deployment with the Service. We also initialize `ObjectMeta`
    for the Deployment resource using the namespace and `matchLabel`. Next, we build
    a Deployment structure containing a spec with two desired replicas, a `LabelSelector`
    using the `matchLabel` we built earlier, and a pod template that will run a single
    container with the `nginxdemos/hello:latest` image exposing port `80` on the container.
    Finally, we create the deployment specifying the namespace and the Deployment
    structure we've built.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码初始化了 `matchLabel`，它是一个键值对，将用于将 Deployment 与 Service 连接。我们还为 Deployment 资源初始化了
    `ObjectMeta`，使用命名空间和 `matchLabel`。接下来，我们构建了一个包含规范的 Deployment 结构，期望有两个副本，使用我们之前构建的
    `matchLabel` 的 `LabelSelector`，并且有一个 Pod 模板，运行一个容器，使用 `nginxdemos/hello:latest`
    镜像，并在容器上暴露端口 `80`。最后，我们创建了部署，指定了命名空间和我们构建的 Deployment 结构。
- en: Now that we have created our Deployment, let's see how we wait for the pods
    in the Deployment to become ready.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了 Deployment，让我们看看如何等待 Deployment 中的 pods 变为就绪状态。
- en: Waiting for ready replicas to match desired replicas
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 等待准备就绪的副本与期望副本匹配
- en: 'When a Deployment is created, pods for each replica need to be created and
    start running before they will be able to service requests. There is nothing about
    Kubernetes or the API requests we are authoring that requires us to wait for these
    pods. This is here just to provide some user feedback and illustrate a use for
    the status portion of the resource. Let''s take a look at how we wait for the
    Deployment state to match the desired state:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建一个 Deployment 时，需要为每个副本创建 pod，并使其开始运行，才能处理请求。我们编写的 Kubernetes 或 API 请求并没有要求我们等待这些
    pods。这只是为了提供一些用户反馈，并展示资源状态部分的用法。我们来看看如何等待 Deployment 的状态与期望的状态匹配：
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the preceding code, we loop to check for the desired number of replicas to
    match the number of ready replicas and return if they do. If they do not match,
    then we sleep for a second and try again. This code is not very resilient, but
    it illustrates the goal-seeking nature of Kubernetes operations.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们通过循环检查期望的副本数是否与就绪副本数匹配，若匹配则返回。如果不匹配，则等待一秒钟再试。这个代码并不非常健壮，但它展示了Kubernetes操作的目标导向性质。
- en: Now that we have a running deployment, we can build the Service to load-balance
    across the pods in the deployment.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个正在运行的部署，我们可以构建一个Service，以在部署中的Pods之间进行负载均衡。
- en: Creating a Service to load-balance
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建用于负载均衡的Service
- en: 'The two pod replicas in the deployment are now running the NGINX demo on port
    `80`, but each has its own interface. We can address traffic to each one individually,
    but it would be more convenient to address a single address and load-balance the
    requests. Let''s create a Service resource to do that:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 部署中的两个Pod副本现在在`80`端口运行NGINX演示，但每个副本都有自己的接口。我们可以将流量定向到每个副本，但更方便的方法是定向到一个地址并进行负载均衡请求。让我们创建一个Service资源来实现这一点：
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code, we initialize the same `matchLabel` and `ObjectMeta`
    as we did in the deployment. However, instead of creating a Deployment resource,
    we create a Service resource structure, specifying the Selector to match on and
    the port to expose over **Transmission Control Protocol** (**TCP**). The Selector
    label is the key to ensuring that the correct pods are in the backend pool for
    the load balancer. Finally, we create the Service as we have with the other Kubernetes
    resources.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们初始化了与部署中相同的`matchLabel`和`ObjectMeta`。然而，我们并没有创建一个Deployment资源，而是创建了一个Service资源结构，指定了要匹配的Selector和要暴露的**传输控制协议**（**TCP**）端口。Selector标签是确保负载均衡器的后端池中包含正确Pods的关键。最后，我们像其他Kubernetes资源一样创建了Service。
- en: We only have one step left. We need to expose our service via an ingress so
    that we can send traffic into the cluster via a port on the local machine.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只剩下一步了。我们需要通过入口来暴露我们的服务，这样我们就可以通过本地机器上的端口将流量发送到集群中。
- en: Creating an ingress to expose our application on a local host port
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建入口以在本地主机端口暴露我们的应用程序
- en: 'At this point, we are unable to reach our service via `localhost:port`. We
    can forward traffic into the cluster via `kubectl`, but I''ll leave that for you
    to explore. We are going to create an ingress and open a port on our local host
    network. Let''s see how we create the ingress resource:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们无法通过`localhost:port`访问我们的服务。我们可以通过`kubectl`将流量转发到集群中，但这个部分留给你自己探索。接下来我们将创建一个入口并在本地主机网络上打开一个端口。让我们来看一下如何创建入口资源：
- en: '[PRE19]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the preceding code, we initialize a prefix, the same `objMeta` as previously,
    and `ingressPath`, which will map the path prefix of `/hello` to the service name
    and port name we created. Yes, Kubernetes does the magic of tying the networking
    together for us! Next, we build the Ingress structure as we saw with the previous
    structures and create the ingress using `clientSet`. With this last bit, we deploy
    our entire application stack using Go and the Kubernetes API.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们初始化了一个前缀，与之前相同的`objMeta`，以及`ingressPath`，它将路径前缀`/hello`映射到我们创建的服务名和端口名。是的，Kubernetes为我们做了将网络连接起来的“魔法”！接下来，我们按照之前结构的方式构建Ingress结构，并使用`clientSet`创建入口。通过这最后一步，我们使用Go和Kubernetes
    API部署了整个应用程序堆栈。
- en: Next, let's return to `main.go` and look at how we can use Kubernetes to stream
    the logs of the pods to show the incoming HTTP requests while the program is running.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们回到`main.go`，看看如何使用Kubernetes流式传输Pods的日志，展示程序运行时的HTTP请求。
- en: Streaming pod logs for the NGINX application
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为NGINX应用程序流式传输Pod日志
- en: 'The Kubernetes API exposes a bunch of great features for running workloads.
    One of the most basic and useful is the ability to access logs for running pods.
    Let''s see how we can stream logs from multiple running pods to STDOUT:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes API提供了许多出色的功能来运行工作负载。其中最基础和最有用的功能之一就是能够访问正在运行的Pods的日志。让我们来看一下如何将多个运行中的Pods的日志流式传输到STDOUT：
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the preceding code, `listenToPodLogs` lists the pods in the given namespace
    and then starts `go func` for each one. In `go func`, we use the Kubernetes API
    to request a stream of `podLogs`, which returns `io.ReadCloser` to deliver logs
    from the pod as they are created. We then tell STDOUT to read from that pipe,
    and the logs land in our STDOUT.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`listenToPodLogs` 列出了给定命名空间中的 pods，然后为每个 pod 启动了 `go func`。在 `go func`
    中，我们使用 Kubernetes API 请求一个 `podLogs` 的流，它返回一个 `io.ReadCloser`，可以从 pod 中实时读取日志。然后我们告诉
    STDOUT 从这个管道中读取，日志就会被输出到我们的 STDOUT。
- en: If you thought that getting logs from your running workloads was going to be
    a lot tougher than this, I don't think you would be alone. Kubernetes is quite
    complex, but the concept that everything is exposed as an API makes the platform
    incredibly flexible and programmable.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为从正在运行的工作负载中获取日志会比这更困难，我想你不会是唯一一个这样想的人。Kubernetes 确实非常复杂，但由于一切都以 API 的形式暴露出来，这使得该平台极其灵活和可编程。
- en: We have explored every function except `waitForExitSignal`, which is relatively
    trivial and doesn't add anything to the Kubernetes story told here. If you'd like
    to take a look at it, refer to the source repository.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探索了除了 `waitForExitSignal` 的所有功能，这个功能相对简单，并没有为此处讲述的 Kubernetes 相关内容增添什么。如果你想了解它，可以参考源代码仓库。
- en: Having explored this example of using the Kubernetes API to programmatically
    deploy an application using Go, I hope you will take away from the experience
    a feeling of empowerment to go and learn, build, and feel relatively comfortable
    interacting with the Kubernetes API. There is so much more to the Kubernetes API,
    and it's ever-growing. In fact, in the next section, we are going to start talking
    about how we can extend the Kubernetes API with our own custom resources.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个使用 Kubernetes API 来以 Go 编程方式部署应用的示例，我希望你能从中获得一种力量感，去学习、构建，并且在与 Kubernetes
    API 交互时感到相对舒适。Kubernetes API 的内容远不止这些，而且它还在不断发展。事实上，在接下来的部分，我们将开始讨论如何通过自定义资源扩展
    Kubernetes API。
- en: Extending Kubernetes with custom resources and operators
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展 Kubernetes 与自定义资源和操作符
- en: In the previous sections, we've learned that the Kubernetes API is not just
    a single API but also an aggregation of APIs backed by cooperative services called
    **operators** and **controllers**. Operators are extensions to Kubernetes that
    make use of custom resources to manage systems and applications via controllers.
    Controllers are components of operators that execute control loops for a kind
    of resource. A control loop for a custom resource is an iterative process that
    observes a desired state of the resource and works, possibly over several loops,
    to drive the state of a system to that desired state.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分，我们已经了解到 Kubernetes API 不仅仅是一个单一的 API，而是由一系列聚合的 API 组成，这些 API 由名为 **操作符**
    和 **控制器** 的协作服务支持。操作符是对 Kubernetes 的扩展，它们利用自定义资源通过控制器来管理系统和应用。控制器是操作符的组件，执行某种资源的控制循环。自定义资源的控制循环是一个迭代过程，它观察资源的期望状态，并可能通过多个循环来推动系统状态朝着期望的状态发展。
- en: Those previous sentences are rather abstract. I like to sum it up differently.
    Kubernetes is a platform for automation. An automation is a series of steps and
    decision trees that drives to reach an end goal. I like to think of operators
    in a similar way. I think of writing operators as taking a runbook, the human
    steps for completing an operational activity, and making the computer execute
    the automation. Operators and controllers are like crystallizing operational knowledge
    into code to be run in Kubernetes.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的句子比较抽象。我喜欢换个方式总结。Kubernetes 是一个自动化平台。自动化是一系列步骤和决策树，驱动实现最终目标。我喜欢以类似的方式看待操作符。我认为编写操作符就像是将一份操作手册——人类完成操作活动的步骤——转化为让计算机执行的自动化。操作符和控制器就像是将操作知识结晶成代码，在
    Kubernetes 中运行。
- en: 'Custom resources can represent anything. They can be things related to Kubernetes
    resources, or they can be something completely external to Kubernetes. For an
    example of a custom resource related to cluster workloads, in [*Chapter 9*](B17626_09.xhtml#_idTextAnchor461),
    *Observability with OpenTelemetry*, we discussed the OTel collector and deployed
    it via its container image in `docker-compose`, but we could have used the Kubernetes
    operator for OTel to do the same thing in a Kubernetes cluster. The OTel operator
    exposes a custom resource, like the following:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义资源可以表示任何内容。它们可以是与 Kubernetes 资源相关的事物，也可以是完全与 Kubernetes 无关的外部事物。举个例子，关于集群工作负载的自定义资源，在[*第九章*](B17626_09.xhtml#_idTextAnchor461)，*使用
    OpenTelemetry 进行可观察性*中，我们讨论了 OTel 收集器并通过其容器镜像在 `docker-compose` 中部署它，但我们也可以使用
    Kubernetes 操作器来做同样的事情，在 Kubernetes 集群中运行它。OTel 操作器暴露了一个自定义资源，像下面这样：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the preceding code block, we see a custom resource describing the OTel collector
    from [https://github.com/open-telemetry/opentelemetry-operator](https://github.com/open-telemetry/opentelemetry-operator).
    This custom resource describes in a domain-specific language how the OpenTelemetry
    operator should configure and run an OpenTelemetry collector. However, a custom
    resource can as easily be a custom `Pet` resource that represents a pet in a pet
    store, as we will see in the next section.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们看到一个自定义资源，描述了来自 [https://github.com/open-telemetry/opentelemetry-operator](https://github.com/open-telemetry/opentelemetry-operator)
    的 OTel 收集器。这个自定义资源以特定领域的语言描述了 OpenTelemetry 操作器应该如何配置和运行 OpenTelemetry 收集器。然而，一个自定义资源也可以像我们在下一部分看到的那样，轻松表示一个自定义的
    `Pet` 资源，用于表示宠物店中的宠物。
- en: 'Do you remember how to identify the group, version, kind, namespace, and name
    for the preceding resource? The answer is `group: opentelemetry.io`, `version:
    v1alpha1`, `kind: OpenTelemetryCollector`, `namespace: default`, and `name: simplest`.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '你还记得如何识别前述资源的组、版本、类型、命名空间和名称吗？答案是 `group: opentelemetry.io`、`version: v1alpha1`、`kind:
    OpenTelemetryCollector`、`namespace: default` 和 `name: simplest`。'
- en: In this section, I want to impress upon you that if someone were to strip away
    pods, nodes, storage, networks, and much of the rest of the Kubernetes container
    workload scheduling and all that was left was the Kubernetes API server, it would
    still be an incredibly useful piece of software. In this section, we are going
    to cover a bit of background about operators, **custom resource definitions**
    (**CRDs**), controllers, and powerful features of the Kubernetes API server. We
    will not be able to cover all of it in depth, but this survey will help to implement
    our first operator and hopefully encourage you to learn more about extending the
    Kubernetes API.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我想强调的是，如果有人去除掉 Pods、节点、存储、网络以及 Kubernetes 容器工作负载调度的其他部分，只剩下 Kubernetes
    API 服务器，它仍然会是一个极其有用的软件。在这一部分，我们将介绍一些关于操作器、**自定义资源定义**（**CRDs**）、控制器以及 Kubernetes
    API 服务器强大功能的背景知识。我们无法深入覆盖所有内容，但这次概览将有助于我们实现第一个操作器，并希望能激励你深入学习如何扩展 Kubernetes API。
- en: Custom Resource Definitions
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义资源定义
- en: 'CRDs are resources that can be applied to a Kubernetes cluster to create a
    new RESTful resource path for a custom resource. Let''s take a look at the example
    of a CronJob from the Kubernetes docs: [https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#create-a-customresourcedefinition](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#create-a-customresourcedefinition).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: CRDs 是可以应用于 Kubernetes 集群的资源，用于为自定义资源创建新的 RESTful 资源路径。让我们来看一下 Kubernetes 文档中关于
    CronJob 的示例：[https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#create-a-customresourcedefinition](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#create-a-customresourcedefinition)。
- en: '[PRE22]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As you can see from the preceding YAML, a CRD is specified as any other resource
    in Kubernetes. The CRD resource has `group`, `version`, `kind`, and `name`, but
    within the `spec`, you can see metadata describing a new resource type with a
    strongly typed schema, using OpenAPI V3 to describe the schema. Also, note that
    the spec contains the group, version, and kind of the custom resource. As implied
    by the YAML structure, there can be multiple versions of the custom resource served
    at any given time, but only one version can be marked as the storage version.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从之前的YAML中看到的，CRD被指定为Kubernetes中的任何资源。CRD资源有`group`、`version`、`kind`和`name`，但在`spec`中，你可以看到描述新资源类型的元数据，并使用OpenAPI
    V3来描述模式。同样，注意到spec包含了自定义资源的group、version和kind。如YAML结构所示，自定义资源可以在任何给定时间提供多个版本，但只能有一个版本标记为存储版本。
- en: In the next section, we'll discuss how Kubernetes is able to store only one
    version but serve multiple versions.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论Kubernetes如何能够仅存储一个版本但提供多个版本。
- en: Custom resource versioning and conversion
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义资源版本管理和转换
- en: 'As mentioned in the previous section, Kubernetes will store only one version
    of a resource. A new version of a resource is usually introduced when there is
    a change to the schema of that resource – for example, a new field was added or
    some other mutation of the schema. In this case, Kubernetes would need some way
    to translate between resource versions. The Kubernetes approach to this is to
    use conversion Webhooks. That means that you can register a Webhook to convert
    from the storage version of a resource to the requested version. This forms a
    hub and spoke model for versioning where the hub is the storage version and the
    spokes are the other supported versions. You can see an example of this in the
    Kubernetes docs here: [https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#configure-customresourcedefinition-to-use-conversion-webhooks](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#configure-customresourcedefinition-to-use-conversion-webhooks).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如前节所述，Kubernetes仅存储资源的一个版本。资源的新版本通常在资源的模式发生变化时引入——例如，添加了一个新字段或对模式进行了其他变更。在这种情况下，Kubernetes需要某种方法来在资源版本之间进行转换。Kubernetes的做法是使用转换Webhook。这意味着你可以注册一个Webhook，将资源的存储版本转换为请求的版本。这形成了一个中心和分支模型，其中中心是存储版本，分支是其他受支持的版本。你可以在Kubernetes文档中看到一个示例：[https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#configure-customresourcedefinition-to-use-conversion-webhooks](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#configure-customresourcedefinition-to-use-conversion-webhooks)。
- en: Take that in for a moment. This is a powerful feature for any API platform to
    offer. Having a standardized way of translating one API version to another allows
    for a more graceful adoption of components in a microservice environment.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 稍微消化一下这个概念。这是任何API平台都应该提供的强大功能。拥有一种标准化的方法来将一个API版本转换为另一个版本，使得在微服务环境中更容易平滑地采用组件。
- en: Structured schema, validation, and defaulting
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结构化模式、验证和默认值
- en: As we saw in the previous example of the CronJob CRD spec, we are able to use
    OpenAPI to describe a strongly typed schema for resources. This is highly beneficial
    for generating API clients for programming languages that may need to interact
    with the API. Furthermore, we have the ability to describe a variety of validations
    to ensure different aspects of structure and values for resources. For example,
    we are able to describe what fields are required, valid ranges of values, valid
    patterns of strings, and many other aspects of the structures and values. Additionally,
    we can provide default values for fields and specify them in the schema.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的CronJob CRD规格示例中看到的，我们可以使用OpenAPI来描述资源的强类型模式。这对于为需要与API交互的编程语言生成API客户端非常有益。此外，我们还可以描述各种验证，以确保资源的结构和值的各个方面。例如，我们可以描述哪些字段是必需的，值的有效范围，字符串的有效模式，以及结构和值的许多其他方面。此外，我们还可以为字段提供默认值并在模式中指定它们。
- en: Beyond just the schema, the API server exposes validating and mutating Webhooks
    that can fill the void where the schema fails – for example, if you want to validate
    or mutate a resource based on some logic that is beyond the scope of schema. These
    Webhooks can be employed to make the developer experience when using your customer
    resources much better than accepting a possibly invalid resource, or defaulting
    some difficult-to-calculate value so that the user doesn't need to provide it.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 除了架构之外，API 服务器还暴露了验证和变更的 Webhooks，这些 Webhooks 可以填补架构失败的空白——例如，如果你想基于某些超出架构范围的逻辑来验证或更改资源。这些
    Webhooks 可以被用来改善开发者在使用自定义资源时的体验，比起接受可能无效的资源，或者默认某些难以计算的值，从而避免用户需要提供这些值。
- en: Controllers
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 控制器
- en: The heart of reconciliation is a controller, which executes a control loop for
    a specific resource kind. The controller watches a resource kind in the Kubernetes
    API and observes that there has been a change. The controller receives the new
    version of the resource, observes the desired state, observes the state of the
    system it controls, and attempts to make progress toward changing the state of
    the system into the desired state expressed in the resource. A controller does
    not act on the difference between the version of a resource but rather on the
    current desired state. I've noticed there is an initial drive for people who are
    new to controller development to try to think about acting only on things that
    have changed between two resource versions, but that is not recommended.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 协调的核心是控制器，它为特定资源类型执行控制循环。控制器监视 Kubernetes API 中的资源类型，并注意到资源发生了变化。控制器接收到资源的新版本，观察期望的状态，观察它控制的系统的当前状态，并试图推进系统状态朝着资源中表达的期望状态发生变化。控制器并不是根据资源版本之间的差异来操作，而是根据当前的期望状态进行操作。我注意到，对于新接触控制器开发的人来说，通常会倾向于只考虑基于两个资源版本之间的差异来行动，但这并不推荐。
- en: Usually, a controller has the ability to reconcile many resources concurrently
    but will never reconcile the same resource concurrently. This simplifies the model
    for reconciliation quite a bit.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，控制器能够并发地协调多个资源，但永远不会并发地协调相同的资源。这简化了协调模型的实现。
- en: Furthermore, most controllers will run with only one leader at a time. For example,
    if there are two instances of your operator running, only one will be a leader
    at a time. The other will be idle, waiting to become the leader if the other process
    crashes.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，大多数控制器一次只会有一个领导者。例如，如果有两个操作符实例在运行，只有一个会成为领导者，另一个则处于空闲状态，等待当另一个进程崩溃时成为领导者。
- en: Standing on the shoulders of giants
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 站在巨人的肩膀上
- en: I'm sure this sounds quite complex, and it truly is. However, we can thankfully
    rely on some projects that have paved the way to make building operators, controllers,
    and CRDs so much easier. There is a vibrant, growing ecosystem for Kubernetes
    operators.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信这听起来非常复杂，实际上也确实如此。然而，我们可以幸运地依赖一些开创性的项目，这些项目使得构建操作符、控制器和 CRD 变得更加容易。Kubernetes
    操作符有着一个充满活力且日益壮大的生态系统。
- en: The projects that most come to mind and which we will depend on in the next
    section are `controller-runtime` ([https://github.com/kubernetes-sigs/controller-runtime](https://github.com/kubernetes-sigs/controller-runtime)),
    `kubebuilder` ([https://github.com/kubernetes-sigs/kubebuilder](https://github.com/kubernetes-sigs/kubebuilder)),
    and `operator-sdk` ([https://github.com/operator-framework/operator-sdk](https://github.com/operator-framework/operator-sdk)).
    `controller-runtime` provides a set of Go libraries that makes it easier to build
    controllers and is used in both `kubebuilder` and `operator-sdk`. `kubebuilder`
    is a framework for building Kubernetes APIs and offers a set of tools that makes
    it easy to generate API structure, controllers, and related manifests for Kubernetes
    APIs. `operator-sdk` is a component in the Operator Framework ([https://github.com/operator-framework](https://github.com/operator-framework)),
    which extends from `kubebuilder` and attempts to solve life cycle, publication,
    and other higher-level problems faced by operator developers.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将依赖的项目包括 `controller-runtime` ([https://github.com/kubernetes-sigs/controller-runtime](https://github.com/kubernetes-sigs/controller-runtime))，`kubebuilder`
    ([https://github.com/kubernetes-sigs/kubebuilder](https://github.com/kubernetes-sigs/kubebuilder))
    和 `operator-sdk` ([https://github.com/operator-framework/operator-sdk](https://github.com/operator-framework/operator-sdk))。`controller-runtime`
    提供了一组 Go 库，使得构建控制器更加简单，且在 `kubebuilder` 和 `operator-sdk` 中都有使用。`kubebuilder` 是一个用于构建
    Kubernetes API 的框架，提供了一套工具，可以轻松生成 API 结构、控制器和 Kubernetes API 相关的清单。`operator-sdk`
    是操作员框架 ([https://github.com/operator-framework](https://github.com/operator-framework))
    的一个组件，它扩展自 `kubebuilder`，并试图解决操作员开发者面临的生命周期、发布和其他更高层次的问题。
- en: If you are interested in a highly ambitious project that extends the Kubernetes
    API to create declarative cluster infrastructure and enables Kubernetes to build
    new Kubernetes clusters, I encourage you to check out the Cluster API ([https://github.com/kubernetes-sigs/cluster-api](https://github.com/kubernetes-sigs/cluster-api)).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对一个雄心勃勃的项目感兴趣，想要扩展 Kubernetes API 来创建声明式集群基础设施，并使 Kubernetes 能够构建新的 Kubernetes
    集群，我鼓励你查看 Cluster API ([https://github.com/kubernetes-sigs/cluster-api](https://github.com/kubernetes-sigs/cluster-api))。
- en: I hope this section has left you in awe of how powerful the Kubernetes API is
    and spurred you on to want to learn more. I believe we have covered enough of
    the basics of extending the Kubernetes API that we can approach building our own
    reconciler without too much trouble. In the upcoming section, we will use `operator-sdk`
    to build a `Pet` resource and operator to reconcile pets in a pet store service.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望本节内容让你对 Kubernetes API 的强大功能感到惊叹，并激发你想要进一步学习的兴趣。我相信我们已经涵盖了扩展 Kubernetes API
    的基础知识，因此我们可以毫不费力地着手构建自己的协调器。在接下来的章节中，我们将使用 `operator-sdk` 来构建一个 `Pet` 资源和操作员，以协调宠物商店服务中的宠物。
- en: Building a pet store operator
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个宠物商店操作员
- en: In this section, we will build on the background information we learned in the
    previous section about CRDs, operators, and controllers to implement our own operator.
    This operator will have only one CRD, `Pet`, and only one controller to reconcile
    those `Pet` resources. The desired state of `Pet` will be reconciled to our pet
    store service, which we used in previous chapters.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们将基于上一节中关于 CRD、操作员和控制器的背景信息，来实现我们自己的操作员。该操作员将只有一个 CRD，`Pet`，并且只有一个控制器来协调这些
    `Pet` 资源。`Pet` 的期望状态将与我们在前几章中使用的宠物商店服务进行协调。
- en: As we discussed in the previous section, this will be an example of using Kubernetes
    control loops to reconcile the state of a resource that has no dependency on other
    resources within Kubernetes. Remember, you can model anything in CRDs and use
    Kubernetes as a tool for building robust APIs for any type of resource.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中讨论的，这将是一个使用 Kubernetes 控制循环来协调一个与 Kubernetes 中其他资源无依赖关系的资源状态的示例。记住，你可以在
    CRD 中建模任何内容，并使用 Kubernetes 作为构建任何类型资源的强大 API 的工具。
- en: In this section, you will learn to build an operator from scratch. You will
    define a new CRD and controller. You will examine the build tools and the different
    code generation tools used to eliminate the majority of boilerplate code. You
    will deploy your controller and the pet store service to a local `kind` cluster
    and learn how to use `Tilt.dev` for faster inner-loop development cycles. The
    code for this repository is located at [https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/14/petstore-operator](https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/14/petstore-operator).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习如何从头开始构建一个操作符。你将定义一个新的CRD和控制器。你将研究构建工具和不同的代码生成工具，用于消除大部分的模板代码。你将把控制器和宠物商店服务部署到本地的`kind`集群，并学习如何使用`Tilt.dev`实现更快的内循环开发周期。此仓库的代码位于[https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/14/petstore-operator](https://github.com/PacktPublishing/Go-for-DevOps/tree/rev0/chapter/14/petstore-operator)。
- en: Initializing the new operator
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化新操作符
- en: 'In this section, we will initialize the new operator using the `operator-sdk`
    command-line tool. This will be used to scaffold out a project structure for our
    operator:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用`operator-sdk`命令行工具初始化新的操作符。这将用于为我们的操作符搭建项目结构：
- en: '[PRE23]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'By executing the preceding command, `operator-sdk` will scaffold a new operator
    project using an example domain, which will form the suffix of the group name
    for our future CRDs. The `–repo` flag is based on the repo for the book''s code,
    but you would want that to reflect the repo path for your project or omit it and
    allow it to default. Let''s see what is in the repo after scaffolding:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行前面的命令，`operator-sdk`将使用一个示例域来搭建一个新的操作符项目，这将形成我们未来CRD的组名后缀。`–repo`标志基于书本代码的仓库，但你应该希望它反映你项目的仓库路径，或者省略它并让它使用默认值。让我们在搭建之后看看仓库中有什么：
- en: '[PRE24]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The preceding listing shows the top-level structure of the project. The Dockerfile
    contains commands to build the controller image. The Makefile contains a variety
    of helpful tasks; however, we will not use it much in this walk-through. The `PROJECT`
    file contains metadata about the operator. The `config` directory contains the
    manifests needed to describe and deploy the operator and CRDs to Kubernetes. The
    `hack` directory contains a boilerplate license header that will be added to generated
    files and is a good place to put helpful development or build scripts. The rest
    of the files are just regular Go application code.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的列表展示了项目的顶层结构。Dockerfile包含构建控制器镜像的命令。Makefile包含各种有用的任务；然而，在本教程中我们不会多加使用它。`PROJECT`文件包含关于操作符的元数据。`config`目录包含描述和部署操作符及CRD到Kubernetes所需的清单。`hack`目录包含一个模板许可头，将被添加到生成的文件中，它是放置有用的开发或构建脚本的好地方。其余的文件只是常规的Go应用程序代码。
- en: 'Now that we have a general idea of what was scaffolded for us, we can move
    on to generating our `Pet` resources and controller:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对为我们搭建的结构有了大致了解，可以继续生成我们的`Pet`资源和控制器：
- en: '[PRE25]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: By executing the preceding commands, I've instructed `operator-sdk` to create
    a new API in the `petstore` group with the `v1alpha1` version of the `Pet` kind
    and generate both the CRD and the controller for the type. Note that the command
    created `api/v1alpha1/pet_types.go` and `controllers/pet_controller.go`, and then
    ran `make generate` and `make manifests`. Shortly, we will see that `code` comments
    in both of the Go files cause `make generate` and `make manifests` to generate
    CRD manifests as well as Kubernetes' **Role-Based Authorization Controls** (**RBAC**)
    for the controller. The RBAC entries for the operator will give rights to the
    controller to perform CRUD operations on the newly generated resource. The CRD
    manifest will contain the schema for our newly created resource.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行前面的命令，我已经指示`operator-sdk`在`petstore`组中创建一个新的API，使用`Pet`种类的`v1alpha1`版本，并生成该类型的CRD和控制器。请注意，命令创建了`api/v1alpha1/pet_types.go`和`controllers/pet_controller.go`，然后运行了`make
    generate`和`make manifests`。很快，我们将看到`code`注释在这两个Go文件中导致`make generate`和`make manifests`生成CRD清单以及Kubernetes的**基于角色的授权控制**（**RBAC**）用于控制器。操作符的RBAC条目将赋予控制器对新生成的资源执行CRUD操作的权限。CRD清单将包含我们新创建资源的架构。
- en: 'Next, let''s take a quick look at the files that have changed:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们快速查看已更改的文件：
- en: '[PRE26]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As we can see, there are quite a few changes to files. I will not go into depth
    on each of the changes. The most notable is the generation of `config/crd/bases/petstore.example.com_pets.yaml`,
    which contains the CRD for our `Pet` resource. In operator projects, it is common
    to describe the resources in the API in the `api/` directory, the Kubernetes manifests
    under `config/`, and the controllers under `controllers/`.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，文件进行了相当多的更改。我不会深入讨论每一项更改。最值得注意的是生成了`config/crd/bases/petstore.example.com_pets.yaml`，它包含了我们的`Pet`资源的CRD。在运维项目中，通常将API中的资源描述放在`api/`目录下，Kubernetes清单文件放在`config/`目录下，控制器放在`controllers/`目录下。
- en: 'Next, let''s see what has been generated in `api/v1alpha1/pet_types.go`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看在`api/v1alpha1/pet_types.go`中生成了什么内容：
- en: '[PRE27]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The preceding code shows a snippet from the `pet_types.go` file. The `create
    api` command has generated a `Pet` resource with `spec` and `status`. The `PetSpec`
    contains one field named `Foo`, which will serialize with the key `foo` and is
    optional to provide when creating or updating the resource. `status` contains
    nothing.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码展示了`pet_types.go`文件中的一个代码片段。`create api`命令生成了一个包含`spec`和`status`的`Pet`资源。`PetSpec`包含一个名为`Foo`的字段，该字段会序列化为键`foo`，并且在创建或更新资源时是可选的。`status`目前为空。
- en: Note the comments in the file. They instruct us that this is the place to add
    new fields to the type and to run `make` after we do to ensure that the CRD manifests
    are updated in the `config/` directory.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意文件中的注释。它们指示我们在这里添加新字段到类型中，并在完成后运行`make`，以确保`config/`目录下的CRD清单文件得到更新。
- en: 'Now, let''s look at the rest of the file:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下文件的其余部分：
- en: '[PRE28]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Here, we can see the definition of `Pet` and `PetList`, which both get registered
    in the following schema builder. Note the `//+kubebuilder build` comments. These
    build comments instruct `kubebuilder` on how to generate the CRD manifests.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到`Pet`和`PetList`的定义，它们都会在接下来的架构构建器中注册。请注意`//+kubebuilder build`注释。这些构建注释指示`kubebuilder`如何生成CRD清单文件。
- en: Note that `Pet` has the spec and status defined with the `json` tags that we
    have seen in the other Kubernetes resources we have worked with. `Pet` also includes
    both `TypeMeta`, which informs Kubernetes of the group version kind information,
    and `ObjectMeta`, which contains the name, namespace, and other metadata about
    the resource.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`Pet`已经定义了带有`json`标签的`spec`和`status`，这些标签我们在之前处理过的其他Kubernetes资源中也见过。`Pet`还包括`TypeMeta`，它提供了Kubernetes的组版本种类信息，以及`ObjectMeta`，它包含了资源的名称、命名空间和其他元数据。
- en: With these structures, we already have a fully functional custom resource. However,
    the resource doesn't represent the fields we want to represent our pet resource
    and will need to be updated to better represent our pet structure.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些结构，我们已经拥有一个功能完全的自定义资源。然而，当前的资源并没有表示我们希望用来表示宠物资源的字段，因此需要更新以更好地表示我们的宠物结构。
- en: 'Next, let''s look at what was generated for `PetReconciler` in `controllers/pet_controller.go`,
    the controller that will run the control loop for reconciling pets:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看在`controllers/pet_controller.go`中为`PetReconciler`生成了什么内容，`PetReconciler`是运行对宠物资源进行对账的控制循环的控制器：
- en: '[PRE29]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the preceding code, we can see a `PetReconciler` type that embeds a `client.Client`,
    which is a generic Kubernetes API client, and `*runtime.Scheme`, which contains
    the known types and the schemas registered. If we continue downward, we can see
    a collection of `//+kubebuilder:rbac build` comments that instruct the code generator
    to create RBAC rights for the controller to be able to manipulate the `Pet` resource.
    Next, we can see the `Reconcile func`, which will be called each time a resource
    has been changed and needs to be reconciled with the pet store. Finally, we can
    see the `SetupWithManager` function, which is called from `main.go` to start the
    controller and inform it and the manager what kind of resource the controller
    will reconcile.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们可以看到一个`PetReconciler`类型，它嵌入了一个`client.Client`，这是一个通用的Kubernetes API客户端，还有一个`*runtime.Scheme`，它包含已注册的已知类型和架构。如果我们继续往下看，可以看到一系列`//+kubebuilder:rbac
    build`注释，它们指示代码生成器为控制器创建RBAC权限，以便它能够操作`Pet`资源。接下来，我们可以看到`Reconcile func`，它会在每次资源发生更改且需要与宠物商店对账时被调用。最后，我们可以看到`SetupWithManager`函数，它从`main.go`中被调用，以启动控制器并告知它和管理器控制器将会对哪个资源进行对账。
- en: 'We have covered the impactful changes from the scaffolding process. We can
    proceed to implement our `Pet` resource to reflect the domain model we have in
    the pet store. The `pet` entity in our pet store has three mutable, required properties,
    `Name`, `Type`, and `Birthday`, and one read-only property, `ID`. We need to add
    these to our `Pet` resource to expose them to the API:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经覆盖了脚手架过程中的重要变化。接下来我们可以继续实现我们的`Pet`资源，以反映宠物商店中的领域模型。我们在宠物商店中的`pet`实体有三个可变的必填属性，分别是`Name`、`Type`和`Birthday`，以及一个只读属性`ID`。我们需要将这些属性添加到我们的`Pet`资源中，以便暴露给API：
- en: '[PRE30]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The preceding are the code changes I've made to `Pet` to reflect the domain
    model of the pet store service. Note `// +kubebuilder:validation:Enum` preceding
    the `PetType` type. That indicates to the CRD manifest generator that the schema
    should add validation to ensure only those strings can be supplied for the `Type`
    field of `PetSpec`. Also, note that each of the fields in `spec` does not have
    the `omitempty` JSON tag. That will inform the CRD manifest generator that those
    fields are required.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以上是我对`Pet`所做的代码修改，目的是反映宠物商店服务的领域模型。请注意`PetType`类型前面的`// +kubebuilder:validation:Enum`注解。这是告诉CRD清单生成器，模式应当添加验证，确保`PetSpec`中的`Type`字段只能提供这些字符串。此外，注意`spec`中的每个字段都没有`omitempty`
    JSON标签。这将告诉CRD清单生成器，这些字段是必填的。
- en: The status of `Pet` has only an `ID` field, which is allowed to be empty. This
    will store the unique identifier returned from the pet store service.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pet`的状态只有一个`ID`字段，这个字段可以为空。它将存储从宠物商店服务返回的唯一标识符。'
- en: 'Now that we have defined our `Pet`, let''s reconcile `pet` with the pet store
    in the controller loop:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的`Pet`，让我们在控制器循环中协调`pet`和宠物商店：
- en: '[PRE31]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The preceding code has been added to reconcile the `pet` resource. When we receive
    a change from the API server, we are not given much information. We are only provided
    with `NamespacedName` of the pet. `NamespacedName` contains both the namespace
    and the name of the pet that has changed. Remember that `PetReconciler` has a
    `client.Client` embedded on it. It provides us with access to the Kubernetes API
    server. We use the `Get` method to request the pet we need to reconcile. If the
    pet is not found, we return an empty reconcile result and a nil error. This informs
    the controller to wait for another change to occur. If there is an error making
    the request, we return an empty reconcile result and an error. If the error is
    not nil, the reconciler will try again and back off exponentially.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码已被添加到对`pet`资源的协调中。当我们从API服务器接收到变化时，我们并没有获得很多信息。我们只会得到变化的宠物的`NamespacedName`。`NamespacedName`包含了发生变化的宠物的命名空间和名称。记住，`PetReconciler`嵌入了一个`client.Client`。它为我们提供了访问Kubernetes
    API服务器的权限。我们使用`Get`方法请求需要协调的宠物。如果没有找到该宠物，我们会返回一个空的协调结果和`nil`错误。这通知控制器等待另一次变化发生。如果请求过程中发生了错误，我们会返回空的协调结果和错误。如果错误不为`nil`，协调器会再次尝试并进行指数退避。
- en: If we are able to fetch the pet, we then create a patch helper, which will allow
    us to track changes to the `Pet` resource during the reconciliation loop and patch
    the resource change back to the Kubernetes API server at the end of the reconcile
    loop. The defer ensures that we patch at the end of the `Reconcile` func.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够成功获取宠物信息，我们将创建一个补丁助手，这将允许我们在协调循环过程中追踪`Pet`资源的变化，并在协调循环结束时将资源变化补丁回传给Kubernetes
    API服务器。`defer`确保我们会在`Reconcile`函数的最后进行补丁操作。
- en: If the pet has no deletion timestamp set, then we know that Kubernetes has not
    marked the resource for deletion, so we call `ReconcileNormal`, where we will
    attempt to persist the desired state to the pet store. Otherwise, we call `ReconcileDelete`
    to delete the pet from the pet store.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果宠物没有设置删除时间戳，那么我们知道Kubernetes并未标记该资源为删除状态，因此我们调用`ReconcileNormal`，在这个过程中我们会尝试将期望的状态持久化到宠物商店中。否则，我们会调用`ReconcileDelete`将宠物从宠物商店中删除。
- en: 'Let''s next look at `ReconcileNormal` and understand what we do when we have
    a state change to a non-deleted pet resource:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们来看一下`ReconcileNormal`，并理解当我们遇到非删除的宠物资源状态变化时应该做什么：
- en: '[PRE32]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In `ReconcileNormal`, we always make sure that `PetFinalizer` has been added
    to the resource. Finalizers are the way that Kubernetes knows when it can garbage-collect
    a resource. If a resource still has a finalizer on it, then Kubernetes will not
    delete the resource. Finalizers are useful in controllers when a resource has
    some external resource that needs to be cleaned up prior to deletion. In this
    case, we need to remove `Pet` from the pet store prior to the Kubernetes `Pet`
    resource being deleted. If we didn't, we may have pets in the pet store that don't
    ever get deleted.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在`ReconcileNormal`中，我们始终确保`PetFinalizer`已经被添加到资源中。Finalizer是Kubernetes知道何时可以垃圾回收资源的一种方式。如果资源上仍然有finalizer，Kubernetes将不会删除该资源。在控制器中，当资源具有需要在删除之前清理的外部资源时，finalizer非常有用。在这种情况下，我们需要在Kubernetes
    `Pet`资源被删除之前从宠物商店中移除`Pet`。如果我们不这么做，可能会在宠物商店中留下从未被删除的宠物。
- en: After we set the finalizer, we build a pet store client. We won't go into more
    detail here, but suffice it to say that it builds a gRPC client for the pet store
    service. With the pet store client, we query for the pet in the store. If we can't
    find the pet, then we create one in the store; otherwise, we update the pet in
    the store to reflect the desired state specified in the Kubernetes `Pet` resource.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置finalizer之后，我们构建一个宠物商店客户端。我们在这里不做更多细节说明，但可以简单说，它构建了一个用于宠物商店服务的gRPC客户端。通过宠物商店客户端，我们查询商店中的宠物。如果找不到该宠物，我们会在商店中创建一个；否则，我们会更新商店中的宠物，以反映Kubernetes
    `Pet`资源中指定的期望状态。
- en: 'Let''s take a quick look at the `createPetInStore` func:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下`createPetInStore`函数：
- en: '[PRE33]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: When we create the pet in the pet store, we call `AddPets` on the gRPC client
    with the Kubernetes `Pet` resource desired state and record `ID` in the Kubernetes
    `Pet` resource status.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在宠物商店创建宠物时，我们会在gRPC客户端上调用`AddPets`，传递Kubernetes `Pet`资源的期望状态，并在Kubernetes
    `Pet`资源的状态中记录`ID`。
- en: 'Let''s move on to the `updatePetInStore` func:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续看`updatePetInStore`函数：
- en: '[PRE34]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: When we update the pet in store, we use the fetched store pet and update the
    fields with the desired state from the Kubernetes `Pet` resource.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们更新商店中的宠物时，我们会使用获取的商店宠物，并使用Kubernetes `Pet`资源中的期望状态更新字段。
- en: If at any point in the flow we run into an error, we bubble up the error to
    `Reconcile`, where it will trigger a re-queue of the reconciliation loop, backing
    off exponentially. The actions in `ReconcileNormal` are idempotent. They can run
    repeatedly to achieve the same state and in the face of errors will retry. Reconciliation
    loops can be pretty resilient to failures.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在流程的任何一点遇到错误，我们会将错误传递到`Reconcile`，在那里它会触发重新排队的对账循环，并且会进行指数回退。`ReconcileNormal`中的操作是幂等的。它们可以重复运行，以达到相同的状态，并且在面对错误时会重试。对账循环对于失败通常是非常具有弹性的。
- en: 'That''s about it for `ReconcileNormal`. Let''s look at what happens in `ReconcileDelete`:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是`ReconcileNormal`的全部内容。让我们看看在`ReconcileDelete`中发生了什么：
- en: '[PRE35]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: In `ReconcileDelete` in the preceding code block, we get a pet store client
    to interact with the pet store. If `pet.Status.ID` is not empty, we attempt to
    delete the pet from the pet store. If that operation is successful, we will remove
    the finalizer, informing Kubernetes that it can then delete the resource.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码块中的`ReconcileDelete`中，我们获取一个宠物商店客户端来与宠物商店交互。如果`pet.Status.ID`不为空，我们尝试从宠物商店删除该宠物。如果该操作成功，我们将移除finalizer，通知Kubernetes它可以删除该资源。
- en: You have extended Kubernetes and created your first CRD and controller! Let's
    give it a run.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经扩展了Kubernetes并创建了你的第一个CRD和控制器！让我们试运行一下。
- en: 'To start the project and see your Kubernetes operator in action, run the following:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动项目并查看你的Kubernetes运算符在运行中，执行以下命令：
- en: '[PRE36]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The preceding commands will create a `kind` cluster and a local `Tilt.dev`.
    Once you do, you should see something like the following:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令将创建一个`kind`集群和本地的`Tilt.dev`。一旦你完成，你应该会看到类似如下的内容：
- en: '![Figure 14.2 – Tilt''s All Resources web view'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.2 – Tilt的所有资源Web视图'
- en: '](img/B17626_14_002.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17626_14_002.jpg)'
- en: Figure 14.2 – Tilt's All Resources web view
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2 – Tilt的所有资源Web视图
- en: Wait for each of the services on the left panel to turn green. Once they are,
    it means that the pet store operator and Service have deployed successfully. If
    you click on one of the Services listed on the left, it will show you the log
    output for that component. `petstore-operator-controller-manager` is your Kubernetes
    controller. Next, we are going to apply some pets to our Kubernetes cluster and
    see what happens.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 等待左侧面板上的每个服务变为绿色。一旦它们变绿，意味着宠物商店操作员和服务已经成功部署。如果你点击左侧列出的其中一个服务，它将显示该组件的日志输出。`petstore-operator-controller-manager`是你的Kubernetes控制器。接下来，我们将一些宠物应用到Kubernetes集群中，看看会发生什么。
- en: 'Let''s first look at the pet samples we are going to apply. The samples are
    in `config/samples/petstore_v1alpha1_pet.yaml`:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看看我们要应用的宠物样本。样本位于`config/samples/petstore_v1alpha1_pet.yaml`：
- en: '[PRE37]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We have two pets, `Thor` and `Tron`. We can apply them with the following command:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个宠物，`Thor`和`Tron`。我们可以通过以下命令应用它们：
- en: '[PRE38]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'That should have replied that they were created, and you should then be able
    to fetch them by running the following command:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该已经回复了它们被创建的消息，接下来你应该可以通过运行以下命令来获取它们：
- en: '[PRE39]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We can see that we have two pets defined. Let''s make sure they have IDs. Run
    the following command:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们定义了两个宠物。让我们确保它们有ID。运行以下命令：
- en: '[PRE40]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: I've omitted some noisy content from the preceding code, but this is roughly
    what you should see. Tron has an ID generated from the pet store service; it was
    applied to the Kubernetes `Pet` resource status.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我已省略了前面代码中的一些噪声内容，但这大致是你应该看到的。Tron有一个由宠物商店服务生成的ID；它被应用到Kubernetes的`Pet`资源状态中。
- en: 'Now, let''s test our reconciliation loop by changing the name of `Thor` to
    `Thorbert`:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过将`Thor`的名称更改为`Thorbert`来测试我们的同步循环：
- en: '[PRE41]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This will open your default editor. You can go and change the value of `Thor`
    to `Thorbert` to cause a new reconcile loop.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打开你的默认编辑器。你可以去更改`Thor`的值为`Thorbert`，以触发一个新的同步循环。
- en: 'You should see something similar to this output in your browser, with Tilt
    in the pet store operator logs:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能在浏览器中看到类似的输出，并且Tilt中会有宠物商店操作员的日志。
- en: '[PRE42]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: As you can see from the preceding code, `Thor` is now changed to `Thorbert`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从前面的代码中看到的，`Thor`现在已经更改为`Thorbert`。
- en: 'Finally, let''s delete these pets by running the following command:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们通过运行以下命令来删除这些宠物：
- en: '[PRE43]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: After deleting the resources, you should be able to check back in Tilt and see
    the log output reflecting that the `delete` operations succeeded.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 删除资源后，你应该能够在Tilt中查看日志输出，反映`delete`操作已成功。
- en: In this section, you learned to build an operator from scratch, extended the
    Kubernetes API with a custom resource that reconciled state to an external Service,
    and used some really useful tools along the way.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你学习了如何从零开始构建操作员，扩展Kubernetes API并创建一个自定义资源，该资源能够将状态同步到外部服务，并在此过程中使用了一些非常有用的工具。
- en: Summary
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how to use Go to deploy and manipulate resources
    in Kubernetes. We built upon that knowledge to extend Kubernetes with our custom
    `Pet` resources and learned how to continuously reconcile the desired state of
    our pets with the state of the pet store. We learned that we can extend Kubernetes
    to represent any external resources and that it provides a robust platform to
    describe nearly any domain.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用Go部署和操作Kubernetes中的资源。我们在此基础上扩展了Kubernetes，创建了自定义的`Pet`资源，并学会了如何持续同步宠物的期望状态与宠物商店的状态。我们学到了如何扩展Kubernetes以表示任何外部资源，并且它提供了一个强大的平台来描述几乎任何领域。
- en: You should be able to take what you learned in this chapter and apply it to
    automate interactions with Kubernetes resources and extend Kubernetes to natively
    expose your own resources through the Kubernetes API. I bet you can think of some
    services and resources at your company that you would like to be able to manage
    by simply applying some YAML to your Kubernetes cluster. You are now empowered
    with the knowledge to solve those problems.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能够将本章中学到的内容应用于自动化与Kubernetes资源的交互，并通过Kubernetes API将Kubernetes扩展到原生暴露你自己的资源。我敢打赌，你能想到你公司里的一些服务和资源，你希望能够通过简单地将一些YAML应用到Kubernetes集群中来管理它们。你现在已经具备了解决这些问题的知识。
- en: In the next chapter, we will learn about using Go to program the cloud. We'll
    learn how to mutate cloud resources using Go client libraries to interact with
    cloud service provider APIs, and how to use those cloud services and infrastructure
    after we've provisioned them.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习如何使用 Go 编程来管理云端资源。我们将学习如何通过 Go 客户端库与云服务提供商的 API 进行交互，以便修改云资源，并在资源配置完成后，如何使用这些云服务和基础设施。
