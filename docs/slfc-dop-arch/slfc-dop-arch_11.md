

# 第十一章：数据种植在您的开发环境中

在本章中，我们将深入探讨填充开发环境以实现真实数据的重要性和过程，这对准确的开发和测试至关重要。我们还将探讨数据掩码作为确保敏感数据在整个过程中保持保护的基本技术。

我们将涵盖以下主要主题：

+   **准确数据对开发和测试的好处**：我们将查看开发环境中的数据如何带来真实感、改善错误检测、提供性能调优的机会，并帮助满足合规性和验证需求。

+   **在您的环境中种植数据**：在这一部分，我们将探讨将测试数据导入 Salesforce 组织的实际步骤——从数据生成到导入——并讨论如何在注意复杂数据关系的同时自动化这些过程。

+   **通过数据掩码保护敏感数据**：最后，我们将确保理解数据掩码的重要性，并讨论如何实施它。我们还将讨论合规最佳实践以及一些帮助实现这些实践的工具。

到本章结束时，您将全面了解如何在开发环境中种植真实数据，同时通过数据掩码确保敏感信息的保护。

# 技术要求

有多种方式可以将 Salesforce 开发环境种植为真实数据用于测试，每种方式都有其要求。如果您没有使用包括此功能的专用 Salesforce DevOps 解决方案，则可以使用 Salesforce 提供的工具和 API。

虽然数据导入向导内置于 Salesforce 中，数据加载器是一个独立的下载工具，但它有自己的依赖项。您可以在[`developer.salesforce.com/tools/data-loader`](https://developer.salesforce.com/tools/data-loader)了解更多信息，包括下载链接。

要使用 Bulk API，您需要用您选择的编程语言编写脚本，从而使编程语言本身成为一种要求。本章后面会给出 Python 的示例，但也可以使用其他能够进行 REST 调用的语言。我们将在本章中统一使用 Bulk API V2。

# 准确数据对开发和测试的好处

创建一个现实的开发环境是有效系统开发的基石，特别是在像 Salesforce 这样复杂且数据驱动的平台上。建立这种现实感的最关键方面之一是确保用于开发和测试的数据的准确性和完整性。高质量、真实的数据为理解系统在部署到实际生产环境后如何运行奠定了必要的基础。这对于帮助开发人员做出明智的设计决策，并在开发生命周期早期预见潜在的陷阱或问题至关重要。

精确的高保真数据的主要好处在于它为开发环境带来了极高的现实感。当开发人员和**质量保证**（**QA**）测试人员能够访问与真实世界数据相似的数据时，他们能够从中获得关于系统在实际操作条件下如何运行的宝贵视角。这种现实感渗透到开发过程的各个方面，包括**用户界面**（**UI**）和用户体验设计、核心功能测试、集成测试、性能测试等。利用准确的数据可以确保系统在生产部署的复杂性和需求面前得到全面评估和准备。

精准的测试数据还在有效的错误检测和调试中扮演着不可或缺的角色。当系统在真实数据条件和使用模式下运行时，缺陷、缺点和行为不一致的问题往往会显现出来。与合成数据集或占位符数据不同，高保真测试数据包含了实际数据的全部复杂性、多样性和细微差别。这意味着，使用伪造数据或样本数据可能无法发现的问题，将在使用准确的数据集时被揭示出来。早期发现缺陷能够使问题得到及时处理，从而避免它们发展成更大的问题。

性能调优和优化是另一个精准测试数据能带来巨大价值的领域。在真实数据负载下，性能剖面和系统行为可能与使用合成数据集时观察到的有显著偏差。通过利用准确的负载测试数据，开发人员可以模拟现实世界中的使用模式和数据量，识别任何瓶颈、缓慢或容量限制。这使得精确的性能调优成为可能，从而确保系统上线时能够达到最佳的吞吐量和响应性。

此外，真实的测试数据在合规性测试中也起着至关重要的作用，确保符合监管要求。在那些要求标准合规的高度监管行业中，使用精准的真实数据进行全面测试至关重要。这能够确认系统在运行实际工作负载时，始终满足必要的合规性需求。若使用不准确的数据进行合规性测试，可能会错过生产环境中出现的违规问题。

高度准确的测试数据在开发和测试中的许多好处不容小觑。它为整个开发生命周期带来了现实感，能够主动发现缺陷，进行性能优化，并确保合规验证。在开发团队中，它还可以帮助减少新开发人员和/或新开发环境的适应时间，因为与其花时间逐步构建现实数据，不如相对快速地设置一个一致的、已知良好的数据集。花时间精心整理准确的测试数据有助于构建在生产环境中平稳运行的高度可靠的系统。这使其成为有效的 Salesforce 开发和**持续交付**（**CD**）工作流的基础支柱。

# 在你的环境中种植数据

向开发和测试环境中种植大量高保真、现实的数据，对于模拟 Salesforce 数据中心平台上实时生产系统的行为是不可或缺的。彻底地向沙盒环境种植具有代表性的数据，使得在现实条件下能够进行全面的验证。然而，在规模上高效完成这一任务需要精心实施强大的工具、自动化和数据建模最佳实践。

提供数据给你的环境有多种方法，我们将逐一介绍每一种方法。我们将从决定是镜像生产数据还是生成虚拟数据开始，然后查看加载这些数据的可选方案。通过本节的学习，你应该能够为你的环境准备好数据。

## 使用生产数据

向开发环境种植数据是一个至关重要的任务，确保在这些环境中进行的测试和开发能够反映真实的场景。在 Salesforce 中，这通常涉及将生产环境中的示例数据填充到沙盒等环境中——这是 Salesforce 默认不做的，因此需要开发人员、架构师和管理员来完成。这个过程不仅为环境提供了现实数据，还促进了对系统在类似生产环境中的行为的更好理解。

这个种植过程的初步步骤是从生产环境中提取数据。Salesforce 提供了多种工具和功能来帮助完成这一任务。提取的数据可以是生产数据的副本或其中的一个子集，具体取决于开发或测试场景的需要以及目标沙盒环境的容量。必须选择一个能够涵盖可能在生产环境中遇到的不同数据场景的代表性数据样本。

一旦数据被提取，下一步是将这些数据导入到开发环境中，如沙盒（Sandbox）。此时，Salesforce 数据加载器（Data Loader）、导入向导（Import Wizard）和批量 API（Bulk API）等工具发挥作用。它们支持不同规模的数据导入，确保开发环境中的数据能够高效且顺利地填充。我们将在本章后续部分介绍这些工具。

在这一数据种植过程中，一个关键考虑因素是敏感数据的潜在存在，特别是**个人身份信息**（**PII**）。当将数据从生产环境转移到开发环境时，确保敏感信息的安全处理至关重要。这通常需要对敏感数据进行掩码处理，以防止未经授权的访问或泄露。由于这一话题的重要性，我们将在后续章节深入探讨数据掩码技术，目前我们将集中讨论数据种植过程。

从生产环境到开发环境的数据种植是一个细致的过程，需要深入理解所涉及的工具和方法。通过在开发环境中准确复制生产数据场景，开发人员和测试人员能够更好地理解、测试和优化系统，从而在构建健壮且可靠的变更中起到重要作用。

## 加载生产数据时的挑战与约束

生产数据并非没有挑战和问题。Salesforce 组织通常会有各种验证规则和自动化流程（例如工作流、流程构建器或触发器），以维护数据完整性并自动化业务流程。在进行数据种植时，这些自动化流程可能会成为障碍。例如，一个组织可能会有一个验证规则，防止在“已关闭赢单”阶段插入商机，因为真实数据预计会经过整个销售过程。这在需要插入绕过这些通常阶段的测试数据时，构成了一个重要挑战。

克服这一挑战的一种方法是创建复杂的脚本，模拟记录的实际生命周期。这意味着在初始阶段插入商机，然后通过编程方式将其推进到所需阶段，最终到达“已关闭赢单”阶段。这种方法尊重现有的验证和自动化，但实施起来可能既耗时又复杂。

另一种策略是暂时禁用数据种植过程中某些验证规则和自动化。这种做法具有一定风险，因为它涉及修改生产环境的配置，必须谨慎进行。确保这些更改已被充分记录，并在数据种植后恢复至原始状态至关重要。

Salesforce 提供了一些工具和功能，允许在数据加载过程中绕过某些自动化。例如，使用数据加载器（Data Loader）并选择“插入空值（Insert Null Values）”选项，或使用特定的 API 头信息，可以帮助绕过一些自动化过程。

重要的是要理解，绕过标准的数据种子流程可能会对测试产生影响。如果种子数据没有经过通常的业务流程，它可能无法准确地代表现实世界的场景。因此，尽管绕过验证和自动化可以让数据种子过程变得更容易，但仍然需要在此过程中平衡真实测试环境的需求。

## 生成测试数据

如果你无法使用生产数据的副本，无论是由于政策原因还是访问限制，另一种选择是生成真实的模拟数据。起点是生成足够大且多样的合成数据集，以模仿实际业务数据的变异性。高质量的数据合成工具和库能够生成合理的对象、关系和数据量，反映出运营数据。这远远超过了有限的手动数据输入，能够更好地测试系统功能。

对于 Salesforce 环境，开发人员和测试人员可以使用多种工具和库来生成真实的数据。以下是一些值得注意的工具：

+   **Mockaroo**：Mockaroo 是一个在线工具，允许你生成自定义数据集。它提供了一个用户友好的界面，用于设计你的数据架构，并且可以生成成千上万行真实的测试数据，这些数据可以导入 Salesforce。

+   **GenerateData.com**：与 Mockaroo 类似， [GenerateData.com](http://GenerateData.com) 是一个在线工具，用于创建自定义测试数据的大型数据集。它还提供了定义数据结构的灵活性，这些结构可以在 Salesforce 中使用。

+   **Snowfakery**：Snowfakery 是 Salesforce.org 设计的一个工具，用于生成真实、复杂且相关的数据，以供测试使用。它允许用户创建“食谱”，即生成数据的指令。

+   **数据生成工具包**：此工具可以帮助在 Salesforce 中生成复杂数据，根据预定义的架构提供真实的测试数据。

## 导入测试数据

将合成数据安全地导入目标 Salesforce 组织是为开发和测试准备环境的关键步骤。Salesforce 提供了多种工具来简化这个过程，每种工具适用于不同规模和复杂度的数据加载。

### 导入向导

Salesforce 导入向导提供了一个直观的界面，用于将较小的数据集导入 Salesforce。例如，要导入一个包含 200 个新线索的 CSV 文件，你可以导航到 Salesforce 中的**线索**对象，点击**导入线索**，然后按照步骤将 CSV 列与 Salesforce 中的相应字段进行映射。

导入向导允许您轻松地映射字段并通过几次点击启动导入，使其非常适合较为简单的数据导入或较小的数据量。一旦将 CSV 列映射到 Salesforce 字段，您只需点击导入数据。导入向导通过几次点击提供了一种用户友好的方式将 CSV 数据导入到 Salesforce。然而，这种方法也有一些限制——您一次只能导入一个对象，并且任何查找字段需要通过其底层 ID 值进行映射，因为该过程没有按名称查找的逻辑。

### Bulk API

Salesforce Bulk API 旨在用于编程方式导入大量数据。例如，要导入包含 200 万个帐户记录的 CSV 文件，您可以编写一个 Python 或 Java 脚本，利用 Bulk API。该脚本将读取 CSV 文件，将数据映射到相应的 Salesforce 对象字段，然后使用 Bulk API 加载记录。

Bulk API V2 处理大规模数据加载的复杂性，如记录分批和错误恢复。因此，对于大型或复杂的数据集，推荐使用 Bulk API 而不是导入向导。通过脚本映射字段和管理导入过程，Bulk API 能够高效地将数百万条记录加载到 Salesforce 中。

Bulk API V2 还可以在一次操作中导入不同对象的记录。这个功能使 Bulk API V2 与 Salesforce 提供的更具声明性、直接的工具有所区别，但它要求架构师为数据集的成功导入做准备。

为了准备数据集进行 Bulk API V2 操作的最关键步骤是彻底了解您打算导入的不同对象之间的关系。Salesforce 数据模型通常涉及复杂的关系，例如查找关系和主从关系。确保这些关系在数据集中正确表示至关重要。这意味着需要仔细映射父子记录并理解它们之间的联系，这将显著影响导入顺序和数据的完整性。

一旦对象关系清晰，下一步是为 Bulk API V2 准备和序列化数据。此过程涉及将数据格式化为兼容的格式（通常为 CSV 或 JSON）。在这一步骤中，必须特别注意如何在数据中表示关系。例如，可以使用外部 ID 来链接相关记录，而不是 Salesforce ID，因为在导入之前，Salesforce ID 可能不可用。这一步通常涉及数据清洗和转换，以确保其符合 Salesforce 的数据标准和约束。

使用 Bulk API V2 进行多个对象导入时，或许最关键的方面是确定操作顺序。由于 Salesforce 强制执行引用完整性，父记录必须在子记录之前导入。因此，制定一个战略性的导入顺序至关重要。这可能涉及创建一个依赖关系树或层级结构，清晰地列出不同对象应导入的顺序。

虽然 Bulk API V2 能够高效处理大量数据，但在处理大数据集时，始终存在因数据质量问题或 Salesforce 限制（如治理限制）而导致错误或部分成功的风险。为这些情况做好准备，包括设置适当的错误处理和重试机制。这可能包括解析 Bulk API 的错误响应，并根据需要调整数据集或导入过程。

使用 Salesforce 的 Bulk API V2 成功导入不同对象的记录是一项复杂但可实现的任务。它需要对数据模型有深入理解，精心准备数据集，合理规划导入顺序，强大的错误处理能力以及全面的测试和验证。

下面是一个示例 Python 脚本，它将记录导入到账户对象中。该脚本使用`salesforce-bulk`库，你可以通过`pip install salesforce-bulk`将其添加到你的 Python 项目中：

```
from salesforce_bulk import SalesforceBulk
import csv
# Salesforce credentials
username = 'your_username'
password = 'your_password'
security_token = 'your_security_token'
# Create a new bulk API connection
bulk = SalesforceBulk(username=username, password=password, security_token=security_token)
# Define the CSV file and the object in Salesforce
csv_file = 'accounts.csv'
sf_object = 'Account'
# Create a new job for the data import
job = bulk.create_insert_job(sf_object, contentType='CSV')
# Open the CSV file and read its contents
with open(csv_file, 'r') as f:
    reader = csv.reader(f)
    csv_data = [row for row in reader]
# Split the CSV data into batches (Salesforce Bulk API has a batch size limit)
batch_size = 10000  # adjust this based on your needs
batches = [csv_data[i:i + batch_size] for i in range(0, len(csv_data), batch_size)]
# Add each batch of data to the job
for batch in batches:
    csv_batch = '\n'.join([','.join(row) for row in batch])
    bulk.post_batch(job, csv_batch)
# Close the job
bulk.close_job(job)
# Check the results of the job
job_results = bulk.get_batch_results(job)
for result in job_results:
    print(result)
```

### 数据加载器

Salesforce 数据加载器提供了一个平衡，介于易用的导入向导和完全程序化的 Bulk API 之间。数据加载器不仅支持**图形用户界面**（**GUI**），还有支持脚本集成的命令行选项，可以高效地导入从数千到数百万条记录的数据集。

例如，要从 CSV 文件中导入 75,000 条联系人记录及其相关账户数据，你需要启动数据加载器，验证你的 Salesforce 组织，选择联系人对象，将 CSV 列映射到 Salesforce 字段，并开始导入过程。

数据加载器处理批量操作并映射复杂的数据关系。因此，对于中等规模的数据集或需要一定自定义的导入，数据加载器在导入向导的简单性和 Bulk API 脚本化之间找到一个有用的平衡点。

## 数据加载自动化

通过脚本或软件自动化端到端的数据种植工作流程，对于提高开发环境的效率、一致性和可靠性至关重要，从而为现实测试和开发做好准备。人工过程通常较慢、重复且容易出错，因此不适合需要精度和速度的任务。相比之下，自动化通过脚本化数据生成、转换、验证和加载过程，消除了这些问题，确保每个沙盒或临时组织中的数据集完全相同，无需人工干预，从而节省时间并减少错误的可能性。

这种自动化的实际方法可能涉及编写数据生成过程的脚本，以创建一个相当大且多样化的数据集，尽可能接近实际业务数据。这个脚本可以安排在指定的时间间隔运行，或者根据特定事件触发，确保为开发和测试活动提供持续的最新数据。

数据生成后，可以实施自动化的数据转换过程，确保数据与它将要加载的 Salesforce 环境的架构和业务逻辑保持一致。这可能包括字段映射、数据类型转换和数据清理等任务，所有这些都为数据加载做好准备。同时，也要考虑数据截断——如果数据生成来自生产环境，并且数据量超出了目标沙箱的容量，当然需要特别处理。

验证是自动化工作流中的关键步骤，确保数据在加载到开发环境之前的准确性和相关性。可以开发自动化验证脚本来检查数据的一致性、完整性和是否遵循业务规则，从而确保数据具有高质量，并能代表实际场景。

数据加载过程是最后一步，在这一过程中，我们之前讨论过的一些工具，例如 Salesforce 数据加载器或 Bulk API，可以通过脚本自动化数据加载过程。例如，可以编写一个脚本，使用数据加载器导入数据，处理导入过程中出现的任何错误，并记录导入结果以供审核。这将确保数据高效且可靠地加载到 Salesforce 环境中。

这种自动化的总体好处是确保不同开发环境之间的一致性。通过脚本化整个数据播种工作流，可以确保每个环境中的数据集几乎相同，从而为测试和开发提供统一的环境。这在敏捷或**持续集成/持续部署**（**CI/CD**）设置中尤为重要，因为一致性和速度至关重要。我们说是“接近”相同，因为从生产环境提取源数据（并进行转换/屏蔽）时，根据请求的时间间隔不同，可能会导致不同的结果。

可以建立持续的数据播种设置，使开发环境始终保持更新的最新数据，这对于持续的测试和开发非常有利，尤其是在需求频繁变化的动态项目中。

## 处理关系

在将数据提取并导入开发环境时，您应捕捉并重建不同对象和记录之间的关系链接和依赖。Salesforce 数据具有复杂的、相互关联的关系，涉及诸如账户、联系人、机会等多个实体。如果在导入数据时不保持这些关系，可能会迅速破坏数据完整性，并导致种子数据集中的级联问题。

为了维护数据完整性，您应研究对象元数据和数据模型，以清晰地了解对象之间的关键关系和依赖。例如，联系人通过`AccountId`字段与账户相关联。机会与账户和联系人之间存在关系。

当您开始提取这些数据时，应该小心地一起提取相关的父记录和子记录。例如，账户应该与其子记录（如联系人和机会）一起提取。相关的`ContactId`和`OpportunityId`字段需要正确填充，以链接记录。

同样，在将数据集导入目标环境时，需要适当处理这些相同的依赖关系。这包括按正确顺序导入数据，以便提前建立依赖关系。必须先插入父记录（如账户），然后再插入子记录（如联系人和机会），并且任何用于关系的外部 ID 字段都应映射到目标环境中的字段值。

对于如连接对象和多对多关系等高级关系，映射关系字段并按正确顺序插入记录同样重要。建议测试插入的数据集以验证关系。

## 测试数据管理的注意事项

在这些场景中，确保数据完整性得到维护，并且在导入过程中安全地处理敏感信息至关重要。负责任地使用这些工具，并根据要导入的数据规模进行操作，将简化数据注入过程，使 Salesforce 组织为开发和测试活动做好准备。

同样重要的是对生成的数据实施严格的安全协议。以下是实现这一目标的一些步骤：

+   **数据清洗**：敏感信息应在用于沙盒环境之前进行匿名化或屏蔽。

+   **数据保护**：应通过加密、令牌化、屏蔽和类似技术来定义并实施数据安全政策。

+   **数据访问**：数据加载工具的访问控制应配置正确。

数据播种创建了代表性的 Salesforce 环境，以便进行真实的验证。现实的数据集、大量加载工具、自动化、建模关系和严格的安全措施是确保数据播种顺利高效的最佳实践。投资这些技术将带来更好的测试、更少的缺陷和更平滑的生产部署。

# 通过数据掩码保护敏感数据

在管理大量信息时，保护敏感数据是首要任务，尤其是在 Salesforce 环境中。数据掩码已经成为解决隐私、合规、安全和保密问题的关键技术，尤其是在处理测试和开发环境中的敏感数据时。

## 了解数据掩码

数据掩码，也称为**数据混淆**或**匿名化**，通过替换、加密或更改原始数据，使用修改过的虚构版本来保护敏感信息。这保留了数据在测试中的效用，同时消除了暴露敏感信息的风险。

常见的数据掩码方法包括以下几种：

+   **静态数据掩码（SDM）**：在将数据转移到测试环境之前对数据进行静态掩码

+   **动态数据掩码（DDM）**：在数据访问时进行实时掩码

+   **格式保留加密（FPE）**：加密数据同时保持原始数据格式

### 隐私问题

数据掩码通过确保在安全性较低的测试环境中仍然保护个人身份信息和机密数据，来维护隐私。个人和组织期望他们的数据能够得到安全处理，而掩码有助于保护这种保密性。在需要第三方承包商为客户的项目工作时，这一点尤其重要。全职员工可能有权限访问生产环境及其数据，但承包商不太可能拥有这些权限。

### 合规性和监管要求

数据掩码还通过遵守规定的数据保护法规来满足合规需求，例如**通用数据保护条例**（**GDPR**）和**健康保险流通与问责法案**（**HIPAA**）。这有助于与客户和利益相关者建立信任。合规不仅仅是为了避免处罚——它保持着至关重要的信任。

### 安全性与保密性

此外，数据掩码通过将敏感信息转换为真实但虚构的数据，减少了未经授权的数据访问和泄露威胁，从而显著降低了风险，增强了安全性。

## 实施数据掩码

在 Salesforce 中实施数据掩码对确保敏感信息的隐私和安全至关重要，特别是在将数据从生产环境转移到较不安全的开发或测试环境时。有几种方法可以在 Salesforce 中启用数据掩码，接下来我们将依次介绍其中的几种：

+   **编程掩码**：你可以编写自定义代码来实现数据掩码编程。这可以完全控制数据掩码的方式，以满足特定需求。然而，它需要对数据结构和 Salesforce 平台有深入了解，并且可能需要投入大量时间来开发和维护。

+   **Salesforce 数据掩码**：这是 Salesforce 自有的托管数据掩码解决方案。它通过掩码敏感字段和对象（包括标准和自定义字段）来帮助遵守数据法规。可以根据数据敏感性配置不同的掩码级别，掩码后的数据无法恢复或在其他环境中以可读形式复制。数据掩码安装并配置在生产组织中，然后在从该生产组织创建的沙盒中执行掩码。这有助于保护沙盒中与生产环境相同的受管数据，如 PII，从而加快测试速度。

+   **带有数据掩码的 DevOps 工具**：如 Gearset 和 DataMasker 等工具具有内置的数据掩码功能。例如，Gearset 可以启动数据部署，然后根据配置的设置在加载到目标组织之前掩码选定的数据。DataMasker 可以快速掩码大型数据集，防止电子邮件和自动化事故，并帮助遵守 GDPR 和 HIPAA 等法规。它与 DevOps 工具如 Copado 集成，并提供真实的掩码格式。

这些方法提供了适应不同技术技能、资源和需求的选项。它们确保敏感信息得到保护，同时仍能在 Salesforce 环境中进行有意义的开发和测试工作——这是一个需要平衡的关键。正确的数据掩码方法是保护隐私并推动进展的关键。

## 合规性和最佳实践

管理测试数据以满足数据保护法并遵循最佳实践，需要细致的方法。与敏感数据相关的一些主要法规包括 GDPR、**儿童在线隐私保护法**（**COPPA**）、**个人信息保护法**（**PIPL**）、HIPAA 和**加利福尼亚消费者隐私法案**（**CCPA**），但这并不是一个详尽无遗的列表。以下是上述法规的概述：

+   GDPR 对处理欧盟公民数据提出了严格要求，要求你负责任地处理数据并明确声明用途。

+   COPPA 保护儿童的在线隐私，对面向儿童的网站或服务提出了具体要求。

+   中国的 PIPL 类似于 GDPR，重点保护中国公民的个人信息。

+   HIPAA 与医疗保健数据相关，主要在该行业中引起注意，因为它要求安全处理患者的健康数据，概述了对机密性、完整性和可用性的保护措施。

+   CCPA 强调保护加利福尼亚居民的个人信息。

除了这些规定外，还有一些最佳实践是必须遵循的。建议使用数据屏蔽来隐匿敏感细节的同时保持数据的测试效用——有些人可能会选择实施自定义方法，以在特定范围内随机化数据，从而保持一定的相关性。例如，以`AnnualRevenue`字段为例——虽然它是一个数字，但他们不会期望看到 1 或某个非常低的值，而是希望保持一个类似的随机数据范围。

加密在存储和传输敏感数据过程中增加了安全性。应用强大的访问控制确保只有授权用户才能查看您的数据，而**基于角色的访问控制**（**RBAC**）则实现了分层访问。将敏感数据排除在源代码管理库之外，防止未经授权的访问和泄漏，即使是无意间的。

使用屏蔽或虚拟数据时，保持数据关系和格式的完整性对于现实且有意义的测试至关重要。这包括保持引用完整性并与应用逻辑对齐，以确保测试数据不会引起任何问题或错误，从而避免在测试过程中出现假阴性。

遵守规定并遵循数据管理的最佳实践，对于安全且高效的测试至关重要。使用虚拟数据或经过屏蔽的真实数据的目标是确保隐私和安全，同时支持高效的测试和开发。谨慎的数据管理对于可靠且顺利的 DevOps 流程有着重要贡献，促进了从开发到生产的过渡，同时保持数据完整性。一个很好的最佳实践是使用 Salesforce 提供的数据分类元数据字段，并确保它们准确无误，且对新字段进行正确填充。这里的准确信息将有助于审计、屏蔽和第三方工具的使用。

## 工具和资源

在进行 Salesforce 的数据种植和屏蔽时，拥有正确的工具和资源至关重要。组建一个强大的工具包并与资源网络连接，可以显著提高效率，并使得有效的数据管理实践得以实现，为安全且高效的测试环境提供支持。

Salesforce 提供了强大的本地工具，如 Data Mask，用于在沙箱环境中屏蔽敏感信息，这对于测试过程中遵循数据隐私合规性至关重要。像 Salesforce Data Loader 这样的数据加载工具以及第三方工具在自动化环境之间及外部系统间的数据导入导出中起着关键作用。

额外的数据屏蔽工具，如 DataMasker 和具有内置屏蔽功能的 DevOps 工具，提供了不同的技术来保护数据的同时保持其实用性。加密工具为静态和传输中的数据增加了额外的安全层，帮助确保数据在其生命周期中的安全性。

参与在线社区、论坛、文档、培训材料、会议和专家的互动，能够提供关于 Salesforce 数据管理、数据播种、数据掩码和合规性方面最新工具、趋势和最佳实践的宝贵见解。

拥有一套量身定制的工具包，并与资源网络连接，可以帮助你成功应对 Salesforce 数据管理的复杂性。这使得组织能够通过有效的数据实践，提高效率，确保合规性，并构建安全、富有成效的测试环境。

# 摘要

在本章中，我们讨论了为开发环境提供准确、真实的数据的重要性，以确保在 Salesforce 中进行强有力的测试和开发。

我们探讨了使用来自生产环境的示例数据，以及另一种通过各种工具生成和导入测试数据的替代方法。

我们讨论了数据掩码的重要性，以保护敏感信息并遵守全球数据保护法规（如 GDPR 和 HIPAA），并探讨了在非生产环境中管理数据的最佳实践。

这些技术和策略可以作为一种有效的数据管理方法，应用于你的开发和测试环境。这样，你将能够充分满足组织的安全性和合规性需求，同时拥有一套工作数据，加速开发生命周期。

在下一章中，我们将开始研究市场上专为 Salesforce DevOps 设计的产品，首先深入探索 Gearset。我们将考察其功能和特点，以及它如何融入 Salesforce DevOps 工具的广泛生态中。
